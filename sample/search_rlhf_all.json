{
  "databases": [
    "acm",
    "arxiv",
    "ieee",
    "scopus"
  ],
  "limit": null,
  "limit_per_database": 2000,
  "number_of_papers": 1868,
  "number_of_papers_by_database": {
    "ACM": 169,
    "IEEE": 193,
    "Scopus": 1317,
    "arXiv": 555
  },
  "papers": [
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) emerges as a promising paradigm for aligning large language models (LLMs). However, a notable challenge in RLHF is overoptimization, where beyond a certain threshold, the pursuit of higher rewards leads to a decline in human preferences. In this paper, we observe the weakness of KL regularization which is commonly employed in existing RLHF methods to address overoptimization. To mitigate this limitation, we scrutinize the RLHF objective in the offline dataset and propose uncertainty-penalized RLHF (UP-RLHF), which incorporates uncertainty regularization during RL-finetuning. To enhance the uncertainty quantification abilities for reward models, we first propose a diverse low-rank adaptation (LoRA) ensemble by maximizing the nuclear norm of LoRA matrix concatenations. Then we optimize policy models utilizing penalized rewards, determined by both rewards and uncertainties provided by the diverse reward LoRA ensembles. Our experimental results, based on two real human preference datasets, showcase the effectiveness of diverse reward LoRA ensembles in quantifying reward uncertainty. Additionally, uncertainty regularization in UP-RLHF proves to be pivotal in mitigating overoptimization, thereby contributing to the overall performance.",
      "authors": [
        "Zhai, Yuanzhao",
        "Zhang, Han",
        "Lei, Yu",
        "Yu, Yue",
        "Xu, Kele",
        "Feng, Dawei",
        "Ding, Bo",
        "Wang, Huaimin"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages, 5 figures,",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-30",
      "selected": null,
      "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles",
      "urls": [
        "http://arxiv.org/pdf/2401.00243.pdf",
        "http://arxiv.org/pdf/2401.00243v1",
        "http://arxiv.org/abs/2401.00243v1"
      ]
    },
    {
      "abstract": "In this work, we consider the offline preference-based reinforcement learning problem. We focus on the two-phase learning approach that is prevalent in previous reinforcement learning from human preference works. We find a challenge in applying two-phase learning in the offline PBRL setting that the learned utility model can be too hard for the learning agent to optimize during the second learning phase. To overcome the challenge, we propose a two-phasing learning approach under behavior regularization through action clipping. The insight is that the state-actions which are poorly covered by the dataset can only provide limited information and increase the complexity of the problem in the second learning phase. Our method ignores such state-actions during the second learning phase to achieve higher learning efficiency. We empirically verify that our method has high learning efficiency on a variety of datasets in robotic control environments.",
      "authors": [
        "Xu, Yinglun",
        "Singh, Gagandeep"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-30",
      "selected": null,
      "title": "Efficient Two-Phase Offline Deep Reinforcement Learning from Preference Feedback",
      "urls": [
        "http://arxiv.org/pdf/2401.00330.pdf",
        "http://arxiv.org/pdf/2401.00330v1",
        "http://arxiv.org/abs/2401.00330v1"
      ]
    },
    {
      "abstract": "Nowadays industrial robots have become the key equipment in the context of smart manufacturing and the assembly process is seen as one of the dominant fields of robotic applications. However, robotic assembly still greatly relies on manual programming and performs in a highly controlled and structured environment in a repetitive manner with weak generalization. Recent successes in robot learning show that endowing robot intelligence to obtain skills autonomously is a promising approach. The existing robot learning methods are difficult to apply due to the requirement of sufficient trial-and-error exploration which is hardware-cost and time-consuming. When encountering an unfamiliar task, it is natural for human to use their prior knowledge as guidance to derive the explorative action and then leaning the related skills from the accumulated experience. Inspired by that, this paper proposes a knowledge-guided robot learning method with predictive model to improve the safety and efficiency of assembly skills acquisition. Concretely, based on Cartesian compliance control, a knowledge-guided exploration strategy (KGES) using the fuzzy logic about position/force feedback is built to provide direction and limit the range of exploration in the early learning stages. Upon KGES, a predictive model-based reinforcement learning method is proposed to optimize the local searching trajectory, where the training data, generated from the trained ensemble predictive models with a knowledge-guided branched progressive rollout method, is used for policy optimization. Finally, the proposed method is tested in two peg-in-hole assembly tasks in MuJoCo environment, and the results show that the robot can learn the assembly skill faster and perform better in success rate than model-free and knowledge-free settings while maintaining the contact force within a safe range. \u00c2\u00a9 2023 Elsevier Ltd",
      "authors": [
        "Quan Liu",
        "Zhenrui Ji",
        "Wenjun Xu",
        "Zhihao Liu",
        "Bitao Yao",
        "Zude Zhou"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.eswa.2023.121037",
      "keywords": [
        "Model-based reinforcement learning",
        "Compliance control",
        "Knowledge-guided robot learning",
        "Smart manufacturing",
        "Robotic assembly"
      ],
      "number_of_pages": 20,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 12.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0957-4174",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.873,
        "snip": 2.582,
        "subject_areas": [
          "Engineering (all)",
          "Artificial Intelligence",
          "Computer Science Applications"
        ],
        "title": "Expert Systems with Applications"
      },
      "publication_date": "2023-12-30",
      "selected": null,
      "title": "Knowledge-guided robot learning on compliance control for robotic assembly task with predictive model",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.eswa.2023.121037",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166654759&origin=inward"
      ]
    },
    {
      "abstract": "Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference is deterministic. IPO uses a root-finding MSE loss to solve the ignoring KL-regularization problem. In this paper, we'll figure out, although IPO fix the problem when preference is deterministic, but both DPO and IPO fails the KL-regularization term because the support of preference distribution not equal to reference distribution. Then, we design a simple and intuitive off-policy preference optimization algorithm from an importance sampling view, which we call Maximum Preference Optimization (MPO), and add off-policy KL-regularization terms which makes KL-regularization truly effective. The objective of MPO bears resemblance to RLHF's objective, and likes IPO, MPO is off-policy. So, MPO attains the best of both worlds. To simplify the learning process and save memory usage, MPO eliminates the needs for both reward model and reference policy.",
      "authors": [
        "Jiang, Zaifan",
        "Huang, Xing",
        "Wei, Chao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-27",
      "selected": null,
      "title": "Preference as Reward, Maximum Preference Optimization with Importance Sampling",
      "urls": [
        "http://arxiv.org/pdf/2312.16430.pdf",
        "http://arxiv.org/pdf/2312.16430v4",
        "http://arxiv.org/abs/2312.16430v4"
      ]
    },
    {
      "abstract": "Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation.Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement.Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance.",
      "authors": [
        "Liu, Wenhao",
        "Wang, Xiaohua",
        "Wu, Muling",
        "Li, Tianlong",
        "Lv, Changze",
        "Ling, Zixuan",
        "Zhu, Jianhao",
        "Zhang, Cenyuan",
        "Zheng, Xiaoqing",
        "Huang, Xuanjing"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-26",
      "selected": null,
      "title": "Aligning Large Language Models with Human Preferences through Representation Engineering",
      "urls": [
        "http://arxiv.org/pdf/2312.15997.pdf",
        "http://arxiv.org/abs/2312.15997v1",
        "http://arxiv.org/pdf/2312.15997v1"
      ]
    },
    {
      "abstract": "In the realm of human mobility, the decision-making process for selecting the next-visit location is intricately influenced by a trade-off between spatial and temporal constraints, which are reflective of individual needs and preferences. This trade-off, however, varies across individuals, making the modeling of these spatial-temporal dynamics a formidable challenge. To address the problem, in this work, we introduce the \"Spatial-temporal Induced Hierarchical Reinforcement Learning\" (STI-HRL) framework, for capturing the interplay between spatial and temporal factors in human mobility decision-making. Specifically, STI-HRL employs a two-tiered decision-making process: the low-level focuses on disentangling spatial and temporal preferences using dedicated agents, while the high-level integrates these considerations to finalize the decision. To complement the hierarchical decision setting, we construct a hypergraph to organize historical data, encapsulating the multi-aspect semantics of human mobility. We propose a cross-channel hypergraph embedding module to learn the representations as the states to facilitate the decision-making cycle. Our extensive experiments on two real-world datasets validate the superiority of STI-HRL over state-of-the-art methods in predicting users' next visits across various performance metrics.",
      "authors": [
        "Zhang, Zhaofan",
        "Xiao, Yanan",
        "Jiang, Lu",
        "Yang, Dingqi",
        "Yin, Minghao",
        "Wang, Pengyang"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to AAAI 2024",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-25",
      "selected": null,
      "title": "Spatial-Temporal Interplay in Human Mobility: A Hierarchical Reinforcement Learning Approach with Hypergraph Representation",
      "urls": [
        "http://arxiv.org/pdf/2312.15717.pdf",
        "http://arxiv.org/pdf/2312.15717v1",
        "http://arxiv.org/abs/2312.15717v1"
      ]
    },
    {
      "abstract": "As the latest advancements in natural language processing, large language models (LLMs) have achieved human-level language understanding and generation abilities in many real-world tasks, and even have been regarded as a potential path to the artificial general intelligence. To better facilitate research on LLMs, many open-source LLMs, such as Llama 2 and Falcon, have recently been proposed and gained comparable performances to proprietary models. However, these models are primarily designed for English scenarios and exhibit poor performances in Chinese contexts. In this technical report, we propose YAYI 2, including both base and chat models, with 30 billion parameters. YAYI 2 is pre-trained from scratch on a multilingual corpus which contains 2.65 trillion tokens filtered by our pre-training data processing pipeline. The base model is aligned with human values through supervised fine-tuning with millions of instructions and reinforcement learning from human feedback. Extensive experiments on multiple benchmarks, such as MMLU and CMMLU, consistently demonstrate that the proposed YAYI 2 outperforms other similar sized open-source models.",
      "authors": [
        "Luo, Yin",
        "Kong, Qingchao",
        "Xu, Nan",
        "Cao, Jia",
        "Hao, Bao",
        "Qu, Baoyu",
        "Chen, Bo",
        "Zhu, Chao",
        "Zhao, Chenyang",
        "Zhang, Donglei",
        "Feng, Fan",
        "Zhao, Feifei",
        "Sun, Hailong",
        "Yang, Hanxuan",
        "Pan, Haojun",
        "Liu, Hongyu",
        "Guo, Jianbin",
        "Du, Jiangtao",
        "Wang, Jingyi",
        "Li, Junfeng",
        "Sun, Lei",
        "Liu, Liduo",
        "Dong, Lifeng",
        "Liu, Lili",
        "Wang, Lin",
        "Zhang, Liwen",
        "Wang, Minzheng",
        "Wang, Pin",
        "Yu, Ping",
        "Li, Qingxiao",
        "Yan, Rui",
        "Zou, Rui",
        "Li, Ruiqun",
        "Huang, Taiwen",
        "Wang, Xiaodong",
        "Wu, Xiaofei",
        "Peng, Xin",
        "Zhang, Xina",
        "Fang, Xing",
        "Xiao, Xinglin",
        "Hao, Yanni",
        "Dong, Yao",
        "Wang, Yigang",
        "Liu, Ying",
        "Jiang, Yongyu",
        "Wang, Yungan",
        "Wang, Yuqi",
        "Wang, Zhangsheng",
        "Yu, Zhaoxin",
        "Luo, Zhen",
        "Mao, Wenji",
        "Wang, Lei",
        "Zeng, Dajun"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-22",
      "selected": null,
      "title": "YAYI 2: Multilingual Open-Source Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2312.14862.pdf",
        "http://arxiv.org/abs/2312.14862v1",
        "http://arxiv.org/pdf/2312.14862v1"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of Large Language Models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in targeting the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between machine agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.",
      "authors": [
        "Kaufmann, Timo",
        "Weng, Paul",
        "Bengs, Viktor",
        "H\u00fcllermeier, Eyke"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-22",
      "selected": null,
      "title": "A Survey of Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2312.14925v1",
        "http://arxiv.org/pdf/2312.14925.pdf",
        "http://arxiv.org/abs/2312.14925v1"
      ]
    },
    {
      "abstract": "In this work, we propose REBEL, an algorithm for sample efficient reward regularization based robotic reinforcement learning from human feedback (RRLHF). Reinforcement learning (RL) performance for continuous control robotics tasks is sensitive to the underlying reward function. In practice, the reward function often ends up misaligned with human intent, values, social norms, etc., leading to catastrophic failures in the real world. We leverage human preferences to learn regularized reward functions and eventually align the agents with the true intended behavior. We introduce a novel notion of reward regularization to the existing RRLHF framework, which is termed as agent preferences. So, we not only consider human feedback in terms of preferences, we also propose to take into account the preference of the underlying RL agent while learning the reward function. We show that this helps to improve the over-optimization associated with the design of reward functions in RL. We experimentally show that REBEL exhibits up to 70% improvement in sample efficiency to achieve a similar level of episodic reward returns as compared to the state-of-the-art methods such as PEBBLE and PEBBLE+SURF.",
      "authors": [
        "Chakraborty, Souradip",
        "Bhaskar, Amisha",
        "Singh, Anukriti",
        "Tokekar, Pratap",
        "Manocha, Dinesh",
        "Bedi, Amrit Singh"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-22",
      "selected": null,
      "title": "REBEL: A Regularization-Based Solution for Reward Overoptimization in Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2312.14436.pdf",
        "http://arxiv.org/abs/2312.14436v1",
        "http://arxiv.org/pdf/2312.14436v1"
      ]
    },
    {
      "abstract": "Preference-based Reinforcement Learning (PbRL) is an active area of research, and has made significant strides in single-agent actor and in observer human-in-the-loop scenarios. However, its application within the co-operative multi-agent RL frameworks, where humans actively participate and express preferences for agent behavior, remains largely uncharted. We consider a two-agent (Human-AI) cooperative setup where both the agents are rewarded according to human's reward function for the team. However, the agent does not have access to it, and instead, utilizes preference-based queries to elicit its objectives and human's preferences for the robot in the human-robot team. We introduce the notion of Human-Flexibility, i.e. whether the human partner is amenable to multiple team strategies, with a special case being Specified Orchestration where the human has a single team policy in mind (most constrained case). We propose a suite of domains to study PbRL for Human-AI cooperative setup which explicitly require forced cooperation. Adapting state-of-the-art single-agent PbRL algorithms to our two-agent setting, we conduct a comprehensive benchmarking study across our domain suite. Our findings highlight the challenges associated with high degree of Human-Flexibility and the limited access to the human's envisioned policy in PbRL for Human-AI cooperation. Notably, we observe that PbRL algorithms exhibit effective performance exclusively in the case of Specified Orchestration which can be seen as an upper bound PbRL performance for future research.",
      "authors": [
        "Bhambri, Siddhant",
        "Verma, Mudit",
        "Murthy, Anil",
        "Kambhampati, Subbarao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-21",
      "selected": null,
      "title": "Benchmarking Multi-Agent Preference-based Reinforcement Learning for Human-AI Teaming",
      "urls": [
        "http://arxiv.org/pdf/2312.14292v1",
        "http://arxiv.org/pdf/2312.14292.pdf",
        "http://arxiv.org/abs/2312.14292v1"
      ]
    },
    {
      "abstract": "Efficient allocation is important in nature and human society where individuals often compete for finite resources. The Minority Game is perhaps the simplest model that provides deep insights into how human coordinate to maximize the resource utilization. However, this model assumes the static strategies that are provided a priori, failing to capture their adaptive nature. Here, we turn to the paradigm of reinforcement learning, where individuals' strategies are evolving by evaluating both the past experience and rewards in the future. Specifically, we adopt the Q-learning algorithm, each player is endowed with a Q-table that guides their decision-making. We reveal that the population is able to reach the optimal allocation when individuals appreciate both the past experience and rewards in the future, and they are able to balance the exploitation of their Q-tables and the exploration by randomly acting. The optimal allocation is ruined when individuals tend to use either exploitation-only or exploration-only, where only partial coordination and even anti-coordination are observed. Mechanism analysis reveals that a moderate level of exploration can escape local minimums of metastable periodic states, and reaches the optimal coordination as the global minimum. Interestingly, the optimal coordination is underlined by a symmetry-breaking of action preferences, where nearly half of the population choose one side while the other half prefer the other side. The emergence of optimal coordination is robust to the population size and other game parameters. Our work therefore provides a natural solution to the Minority Game and sheds insights into the resource allocation problem in general. Besides, our work demonstrates the potential of the proposed reinforcement learning paradigm in deciphering many puzzles in the socio-economic context.",
      "authors": [
        "Zheng, Guozhong",
        "Cai, Weiran",
        "Qi, Guanxiao",
        "Zhang, Jiqiang",
        "Chen, Li"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages, 7 figures, 1 table. A working paper, comments are welcome",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-20",
      "selected": null,
      "title": "Optimal coordination in Minority Game: A solution from reinforcement learning",
      "urls": [
        "http://arxiv.org/pdf/2312.14970.pdf",
        "http://arxiv.org/pdf/2312.14970v1",
        "http://arxiv.org/abs/2312.14970v1"
      ]
    },
    {
      "abstract": "Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). However, the mainstream distributed RLHF training methods typically adopt a fixed model placement strategy, referred to as the Flattening strategy. This strategy treats all four interdependent models involved in RLHF as a single entity, distributing them across all devices and applying parallelism techniques designed for a single model, regardless of the different workloads inherent to each model. As a result, this strategy exacerbates the generation bottlenecks in the RLHF training and degrades the overall training efficiency. To address these issues, we propose an adaptive model placement framework that offers two flexible model placement strategies. The Interleaving strategy helps reduce memory redundancy and communication costs of RLHF training by placing models without dependencies on exclusive devices with careful orchestration. On the other hand, the Separation strategy improves the throughput of model training by separating the training and inference runtime of the RLHF pipeline with additional shadow models. Furthermore, our framework provides a simple user interface and allows for the agile allocation of models across devices in a fine-grained manner for various training scenarios, involving models of varying sizes and devices of different scales. Extensive experiments have demonstrated that our Interleaving and Separation strategies can achieve notable improvements up to 11X, compared to the current SOTA approaches. The results highlight the effectiveness and adaptability of our approaches in accelerating the training of distributed RLHF.",
      "authors": [
        "Xiao, Youshao",
        "Wu, Weichang",
        "Zhou, Zhenglei",
        "Mao, Fagui",
        "Zhao, Shangchun",
        "Ju, Lin",
        "Liang, Lei",
        "Zhang, Xiaolu",
        "Zhou, Jun"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-19",
      "selected": null,
      "title": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training",
      "urls": [
        "http://arxiv.org/abs/2312.11819v2",
        "http://arxiv.org/pdf/2312.11819v2",
        "http://arxiv.org/pdf/2312.11819.pdf"
      ]
    },
    {
      "abstract": "This paper presents an Exploratory 3D Dance generation framework, E3D2, designed to address the exploration capability deficiency in existing music-conditioned 3D dance generation models. Current models often generate monotonous and simplistic dance sequences that misalign with human preferences because they lack exploration capabilities. The E3D2 framework involves a reward model trained from automatically-ranked dance demonstrations, which then guides the reinforcement learning process. This approach encourages the agent to explore and generate high quality and diverse dance movement sequences. The soundness of the reward model is both theoretically and experimentally validated. Empirical experiments demonstrate the effectiveness of E3D2 on the AIST++ dataset. Project Page: https://sites.google.com/view/e3d2.",
      "authors": [
        "Wang, Zilin",
        "Zhuang, Haolin",
        "Li, Lu",
        "Zhang, Yinmin",
        "Zhong, Junjie",
        "Chen, Jun",
        "Yang, Yu",
        "Tang, Boshi",
        "Wu, Zhiyong"
      ],
      "categories": null,
      "citations": null,
      "comments": "AAAI-24",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-18",
      "selected": null,
      "title": "Explore 3D Dance Generation via Reward Model from Automatically-Ranked Demonstrations",
      "urls": [
        "http://arxiv.org/pdf/2312.11442v1",
        "http://arxiv.org/pdf/2312.11442.pdf",
        "http://arxiv.org/abs/2312.11442v1"
      ]
    },
    {
      "abstract": "This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees. Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their powerful practical implementations.",
      "authors": [
        "Xiong, Wei",
        "Dong, Hanze",
        "Ye, Chenlu",
        "Wang, Ziqi",
        "Zhong, Han",
        "Ji, Heng",
        "Jiang, Nan",
        "Zhang, Tong"
      ],
      "categories": null,
      "citations": null,
      "comments": "37 pages; mathematical foundation and practical algorithms of RLHF",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-18",
      "selected": null,
      "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint",
      "urls": [
        "http://arxiv.org/abs/2312.11456v2",
        "http://arxiv.org/pdf/2312.11456.pdf",
        "http://arxiv.org/pdf/2312.11456v2"
      ]
    },
    {
      "abstract": "While generative AI excels in content generation, it does not always increase user engagement. This can be attributed to two main factors. First, generative AI generates content without incorporating explicit or implicit feedback about user interactions. Even if the generated content seems to be more informative or well-written, it does not necessarily lead to an increase in user activities, such as clicks. Second, there is a concern with the quality of the content generative AI produces, which often lacks the distinctiveness and authenticity that human-created content possesses. These two factors can lead to content that fails to meet specific needs and preferences of users, ultimately reducing its potential to be engaging. This paper presents a generic framework of how to improve user engagement with generative AI by leveraging user feedback. Our solutions employ rejection sampling, a technique used in reinforcement learning, to boost engagement metrics. We leveraged the framework in the context of email notification subject lines generation for an online social network, and achieved significant engagement metric lift including +1% Session and +0.4% Weekly Active Users. We believe our work offers a universal framework that enhances user engagement with generative AI, particularly when standard generative AI reaches its limits in terms of enhancing content to be more captivating. To the best of our knowledge, this represents an early milestone in the industry's successful use of generative AI to enhance user engagement.",
      "authors": [
        "Zeng, Jingying",
        "Yang, Jaewon",
        "Malik, Waleed",
        "Yan, Xiao",
        "Huang, Richard",
        "He, Qi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-16",
      "selected": null,
      "title": "Let AI Entertain You: Increasing User Engagement with Generative AI and Rejection Sampling",
      "urls": [
        "http://arxiv.org/pdf/2312.12457v1",
        "http://arxiv.org/abs/2312.12457v1",
        "http://arxiv.org/pdf/2312.12457.pdf"
      ]
    },
    {
      "abstract": "Recent Text-to-Image (T2I) generation models such as Stable Diffusion and Imagen have made significant progress in generating high-resolution images based on text descriptions. However, many generated images still suffer from issues such as artifacts/implausibility, misalignment with text descriptions, and low aesthetic quality. Inspired by the success of Reinforcement Learning with Human Feedback (RLHF) for large language models, prior works collected human-provided scores as feedback on generated images and trained a reward model to improve the T2I generation. In this paper, we enrich the feedback signal by (i) marking image regions that are implausible or misaligned with the text, and (ii) annotating which words in the text prompt are misrepresented or missing on the image. We collect such rich human feedback on 18K generated images and train a multimodal transformer to predict the rich feedback automatically. We show that the predicted rich human feedback can be leveraged to improve image generation, for example, by selecting high-quality training data to finetune and improve the generative models, or by creating masks with predicted heatmaps to inpaint the problematic regions. Notably, the improvements generalize to models (Muse) beyond those used to generate the images on which human feedback data were collected (Stable Diffusion variants).",
      "authors": [
        "Liang, Youwei",
        "He, Junfeng",
        "Li, Gang",
        "Li, Peizhao",
        "Klimovskiy, Arseniy",
        "Carolan, Nicholas",
        "Sun, Jiao",
        "Pont-Tuset, Jordi",
        "Young, Sarah",
        "Yang, Feng",
        "Ke, Junjie",
        "Dvijotham, Krishnamurthy Dj",
        "Collins, Katie",
        "Luo, Yiwen",
        "Li, Yang",
        "Kohlhoff, Kai J",
        "Ramachandran, Deepak",
        "Navalpakkam, Vidhya"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-15",
      "selected": null,
      "title": "Rich Human Feedback for Text-to-Image Generation",
      "urls": [
        "http://arxiv.org/pdf/2312.10240.pdf",
        "http://arxiv.org/pdf/2312.10240v1",
        "http://arxiv.org/abs/2312.10240v1"
      ]
    },
    {
      "abstract": "This paper presents an approach that combines Human-In-The-Loop Reinforcement Learning (HITL RL) with principles derived from music theory to facilitate real-time generation of musical compositions. HITL RL, previously employed in diverse applications such as modelling humanoid robot mechanics and enhancing language models, harnesses human feedback to refine the training process. In this study, we develop a HILT RL framework that can leverage the constraints and principles in music theory. In particular, we propose an episodic tabular Q-learning algorithm with an epsilon-greedy exploration policy. The system generates musical tracks (compositions), continuously enhancing its quality through iterative human-in-the-loop feedback. The reward function for this process is the subjective musical taste of the user.",
      "authors": [
        "Aju Ani Justus"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/BigData59044.2023.10386567",
      "keywords": [
        "Audio Machine Learning",
        "Music Generation",
        "Human-In-The-Loop",
        "Human-Agent Teaming",
        "RLHF",
        "Algorithmic Music",
        "HITL RL",
        "Reinforcement Learning",
        "Human Feedback"
      ],
      "number_of_pages": 6,
      "pages": "4479-4484",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-2446-4",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE International Conference on Big Data (BigData)"
      },
      "publication_date": "2023-12-15",
      "selected": null,
      "title": "Music Generation using Human-In-The-Loop Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10386567"
      ]
    },
    {
      "abstract": "Reward models play a key role in aligning language model applications towards human preferences. However, this setup creates an incentive for the language model to exploit errors in the reward model to achieve high estimated reward, a phenomenon often termed \\emph{reward hacking}. A natural mitigation is to train an ensemble of reward models, aggregating over model outputs to obtain a more robust reward estimate. We explore the application of reward ensembles to alignment at both training time (through reinforcement learning) and inference time (through reranking). First, we show that reward models are \\emph{underspecified}: reward models that perform similarly in-distribution can yield very different rewards when used in alignment, due to distribution shift. Second, underspecification results in overoptimization, where alignment to one reward model does not improve reward as measured by another reward model trained on the same data. Third, overoptimization is mitigated by the use of reward ensembles, and ensembles that vary by their \\emph{pretraining} seeds lead to better generalization than ensembles that differ only by their \\emph{fine-tuning} seeds, with both outperforming individual reward models. However, even pretrain reward ensembles do not eliminate reward hacking: we show several qualitative reward hacking phenomena that are not mitigated by ensembling because all reward models in the ensemble exhibit similar error patterns.",
      "authors": [
        "Eisenstein, Jacob",
        "Nagpal, Chirag",
        "Agarwal, Alekh",
        "Beirami, Ahmad",
        "D'Amour, Alex",
        "Dvijotham, DJ",
        "Fisch, Adam",
        "Heller, Katherine",
        "Pfohl, Stephen",
        "Ramachandran, Deepak",
        "Shaw, Peter",
        "Berant, Jonathan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-14",
      "selected": null,
      "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking",
      "urls": [
        "http://arxiv.org/abs/2312.09244v2",
        "http://arxiv.org/pdf/2312.09244v2",
        "http://arxiv.org/pdf/2312.09244.pdf"
      ]
    },
    {
      "abstract": "We propose a framework that leverages foundation models as teachers, guiding a reinforcement learning agent to acquire semantically meaningful behavior without human feedback. In our framework, the agent receives task instructions grounded in a training environment from large language models. Then, a vision-language model guides the agent in learning the multi-task language-conditioned policy by providing reward feedback. We demonstrate that our method can learn semantically meaningful skills in a challenging open-ended MineDojo environment while prior unsupervised skill discovery methods struggle. Additionally, we discuss observed challenges of using off-the-shelf foundation models as teachers and our efforts to address them.",
      "authors": [
        "Nam, Taewook",
        "Lee, Juyong",
        "Zhang, Jesse",
        "Hwang, Sung Ju",
        "Lim, Joseph J.",
        "Pertsch, Karl"
      ],
      "categories": null,
      "citations": null,
      "comments": "2nd Workshop on Agent Learning in Open-Endedness (ALOE) at NeurIPS\n  2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-14",
      "selected": null,
      "title": "LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers",
      "urls": [
        "http://arxiv.org/pdf/2312.08958v1",
        "http://arxiv.org/pdf/2312.08958.pdf",
        "http://arxiv.org/abs/2312.08958v1"
      ]
    },
    {
      "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior - for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.",
      "authors": [
        "Burns, Collin",
        "Izmailov, Pavel",
        "Kirchner, Jan Hendrik",
        "Baker, Bowen",
        "Gao, Leo",
        "Aschenbrenner, Leopold",
        "Chen, Yining",
        "Ecoffet, Adrien",
        "Joglekar, Manas",
        "Leike, Jan",
        "Sutskever, Ilya",
        "Wu, Jeff"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-14",
      "selected": null,
      "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
      "urls": [
        "http://arxiv.org/pdf/2312.09390.pdf",
        "http://arxiv.org/abs/2312.09390v1",
        "http://arxiv.org/pdf/2312.09390v1"
      ]
    },
    {
      "abstract": "Customizing robotic behaviors to be aligned with diverse human preferences is an underexplored challenge in the field of embodied AI. In this paper, we present Promptable Behaviors, a novel framework that facilitates efficient personalization of robotic agents to diverse human preferences in complex environments. We use multi-objective reinforcement learning to train a single policy adaptable to a broad spectrum of preferences. We introduce three distinct methods to infer human preferences by leveraging different types of interactions: (1) human demonstrations, (2) preference feedback on trajectory comparisons, and (3) language instructions. We evaluate the proposed method in personalized object-goal navigation and flee navigation tasks in ProcTHOR and RoboTHOR, demonstrating the ability to prompt agent behaviors to satisfy human preferences in various scenarios. Project page: https://promptable-behaviors.github.io",
      "authors": [
        "Hwang, Minyoung",
        "Weihs, Luca",
        "Park, Chanwoo",
        "Lee, Kimin",
        "Kembhavi, Aniruddha",
        "Ehsani, Kiana"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-14",
      "selected": null,
      "title": "Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences",
      "urls": [
        "http://arxiv.org/pdf/2312.09337.pdf",
        "http://arxiv.org/pdf/2312.09337v1",
        "http://arxiv.org/abs/2312.09337v1"
      ]
    },
    {
      "abstract": "Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning, if the target objective is differentiable. For many interesting problems, this is however not the case. Common objectives like intersection over union (IoU), bilingual evaluation understudy (BLEU) score or rewards cannot be optimized with supervised learning. A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years. Examples include aligning large language models via human feedback, code generation, object detection or control problems. This makes RL techniques relevant to the larger machine learning audience. The subject is, however, time intensive to approach due to the large range of methods, as well as the often very theoretical presentation. In this introduction, we take an alternative approach, different from classic reinforcement learning textbooks. Rather than focusing on tabular problems, we introduce reinforcement learning as a generalization of supervised learning, which we first apply to non-differentiable objectives and later to temporal problems. Assuming only basic knowledge of supervised learning, the reader will be able to understand state-of-the-art deep RL algorithms like proximal policy optimization (PPO) after reading this tutorial.",
      "authors": [
        "Jaeger, Bernhard",
        "Geiger, Andreas"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-13",
      "selected": null,
      "title": "An Invitation to Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2312.08365v1",
        "http://arxiv.org/pdf/2312.08365v1",
        "http://arxiv.org/pdf/2312.08365.pdf"
      ]
    },
    {
      "abstract": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at https://github.com/cassidylaidlaw/hidden-context",
      "authors": [
        "Siththaranjan, Anand",
        "Laidlaw, Cassidy",
        "Hadfield-Menell, Dylan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-13",
      "selected": null,
      "title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF",
      "urls": [
        "http://arxiv.org/pdf/2312.08358.pdf",
        "http://arxiv.org/pdf/2312.08358v1",
        "http://arxiv.org/abs/2312.08358v1"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted in many safety-critical applications. Their security is thus essential. Even with considerable efforts spent on reinforcement learning from human feedback (RLHF), recent studies have shown that LLMs are still subject to attacks such as adversarial perturbation and Trojan attacks. Further research is thus needed to evaluate their security and/or understand the lack of it. In this work, we propose a framework for conducting light-weight causality-analysis of LLMs at the token, layer, and neuron level. We applied our framework to open-source LLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based on a layer-level causality analysis, we show that RLHF has the effect of overfitting a model to harmful prompts. It implies that such security can be easily overcome by `unusual' harmful prompts. As evidence, we propose an adversarial perturbation method that achieves 100\\% attack success rate on the red-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we show the existence of one mysterious neuron in both Llama2 and Vicuna that has an unreasonably high causal effect on the output. While we are uncertain on why such a neuron exists, we show that it is possible to conduct a ``Trojan'' attack targeting that particular neuron to completely cripple the LLM, i.e., we can generate transferable suffixes to prompts that frequently make the LLM produce meaningless responses.",
      "authors": [
        "Zhao, Wei",
        "Li, Zhe",
        "Sun, Jun"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-13",
      "selected": null,
      "title": "Causality Analysis for Evaluating the Security of Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2312.07876.pdf",
        "http://arxiv.org/pdf/2312.07876v1",
        "http://arxiv.org/abs/2312.07876v1"
      ]
    },
    {
      "abstract": "Risk sensitivity is a fundamental aspect of biological motor control that accounts for both the expectation and variability of movement cost in the face of uncertainty. However, most computational models of biological motor control rely on model-based risk-sensitive optimal control, which requires an accurate internal representation in the central neural system to predict the outcomes of motor commands. In reality, the dynamics of human-environment interaction is too complex to be accurately modeled, and noise further complicates system identification. To address this issue, this paper proposes a novel risk-sensitive computational mechanism for biological motor control based on reinforcement learning (RL) and adaptive dynamic programming (ADP). The proposed ADP-based mechanism suggests that humans can directly learn an approximation of the risk-sensitive optimal feedback controller from noisy sensory data without the need for system identification. Numerical validation of the proposed mechanism is conducted on the arm-reaching task under divergent force field. The preliminary computational results align with the experimental observations from the past literature of computational neuroscience.",
      "authors": [
        "Leilei Cui",
        "Bo Pang",
        "Zhong-Ping Jiang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/CDC49753.2023.10384286",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "7944-7949",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 1.7,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-1399-9",
        "issn": "0743-1546",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 0.692,
        "snip": 0.637,
        "subject_areas": [
          "Modeling and Simulation",
          "Control and Optimization",
          "Control and Systems Engineering"
        ],
        "title": "2019 IEEE 58th Conference on Decision and Control (CDC)"
      },
      "publication_date": "2023-12-13",
      "selected": null,
      "title": "Reinforcement-Learning-Based Risk-Sensitive Optimal Feedback Mechanisms of Biological Motor Control",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10384286"
      ]
    },
    {
      "abstract": "BackgroundIndividuals with cocaine use disorder or gambling disorder demonstrate impairments in cognitive flexibility: the ability to adapt to changes in the environment. Flexibility is commonly assessed in a laboratory setting using probabilistic reversal learning, which involves reinforcement learning, the process by which feedback from the environment is used to adjust behavior.AimsIt is poorly understood whether impairments in flexibility differ between individuals with cocaine use and gambling disorders, and how this is instantiated by the brain. We applied computational modelling methods to gain a deeper mechanistic explanation of the latent processes underlying cognitive flexibility across two disorders of compulsivity.MethodWe present a re-analysis of probabilistic reversal data from individuals with either gambling disorder (n = 18) or cocaine use disorder (n = 20) and control participants (n = 18), using a hierarchical Bayesian approach. Furthermore, we relate behavioural findings to their underlying neural substrates through an analysis of task-based functional magnetic resonanceimaging (fMRI) data.ResultsWe observed lower \u2018stimulus stickiness\u2019 in gambling disorder, and report differences in tracking expected values in individuals with gambling disorder compared to controls, with greater activity during reward expected value tracking in the cingulate gyrus and amygdala. In cocaine use disorder, we observed lower responses to positive punishment prediction errors and greater activity following negative punishment prediction errors in the superior frontal gyrus compared to controls.ConclusionsUsing a computational approach, we show that individuals with gambling disorder and cocaine use disorder differed in their perseverative tendencies and in how they tracked value neurally, which has implications for psychiatric classification.",
      "authors": [
        "Katharina Z\u00fchlsdorff",
        "Juan Verdejo-Rom\u00e1n",
        "Luke Clark",
        "Natalia Albein-Urios",
        "Carles Soriano-Mas",
        "Rudolf N. Cardinal",
        "Trevor W. Robbins",
        "Jeffrey W. Dalley",
        "Antonio Verdejo-Garc\u00eda",
        "Jonathan W. Kanen"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1192/bjo.2023.611",
      "keywords": [
        "gambling disorder",
        "expected value",
        "reinforcement learning",
        "Cocaine use disorder",
        "prediction error"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2056-4724",
        "publisher": "Cambridge University Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "BJPsych Open"
      },
      "publication_date": "2023-12-11",
      "selected": null,
      "title": "Computational modelling of reinforcement learning and functional neuroimaging of probabilistic reversal for dissociating compulsive behaviours in gambling and cocaine use disorders",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179797715&origin=inward",
        "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/701B73DBCDBD948943E3E3DFCBB1C217/S2056472423006117a.pdf/div-class-title-computational-modelling-of-reinforcement-learning-and-functional-neuroimaging-of-probabilistic-reversal-for-dissociating-compulsive-behaviours-in-gambling-and-cocaine-use-disorders-div.pdf"
      ]
    },
    {
      "abstract": "Despite numerous successes, the field of reinforcement learning (RL) remains far from matching the impressive generalisation power of human behaviour learning. One possible way to help bridge this gap be to provide RL agents with richer, more human-like feedback expressed in natural language. To investigate this idea, we first extend BabyAI to automatically generate language feedback from the environment dynamics and goal condition success. Then, we modify the Decision Transformer architecture to take advantage of this additional signal. We find that training with language feedback either in place of or in addition to the return-to-go or goal descriptions improves agents' generalisation performance, and that agents can benefit from feedback even when this is only available during training, but not at inference.",
      "authors": [
        "McCallum, Sabrina",
        "Taylor-Davies, Max",
        "Albrecht, Stefano V.",
        "Suglia, Alessandro"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted at Workshop on Goal-conditioned Reinforcement Learning,\n  NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-07",
      "selected": null,
      "title": "Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2312.04736v1",
        "http://arxiv.org/pdf/2312.04736.pdf",
        "http://arxiv.org/pdf/2312.04736v1"
      ]
    },
    {
      "abstract": "Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available at github.com/mnoukhov/elastic-reset.",
      "authors": [
        "Noukhovitch, Michael",
        "Lavoie, Samuel",
        "Strub, Florian",
        "Courville, Aaron"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published at NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-06",
      "selected": null,
      "title": "Language Model Alignment with Elastic Reset",
      "urls": [
        "http://arxiv.org/pdf/2312.07551.pdf",
        "http://arxiv.org/pdf/2312.07551v1",
        "http://arxiv.org/abs/2312.07551v1"
      ]
    },
    {
      "abstract": "Executing contact-rich manipulation tasks necessitates the fusion of tactile and visual feedback. However, the distinct nature of these modalities poses significant challenges. In this paper, we introduce a system that leverages visual and tactile sensory inputs to enable dexterous in-hand manipulation. Specifically, we propose Robot Synesthesia, a novel point cloud-based tactile representation inspired by human tactile-visual synesthesia. This approach allows for the simultaneous and seamless integration of both sensory inputs, offering richer spatial information and facilitating better reasoning about robot actions. The method, trained in a simulated environment and then deployed to a real robot, is applicable to various in-hand object rotation tasks. Comprehensive ablations are performed on how the integration of vision and touch can improve reinforcement learning and Sim2Real performance. Our project page is available at https://yingyuan0414.github.io/visuotactile/ .",
      "authors": [
        "Yuan, Ying",
        "Che, Haichuan",
        "Qin, Yuzhe",
        "Huang, Binghao",
        "Yin, Zhao-Heng",
        "Lee, Kang-Won",
        "Wu, Yi",
        "Lim, Soo-Chul",
        "Wang, Xiaolong"
      ],
      "categories": null,
      "citations": null,
      "comments": "Project page: https://yingyuan0414.github.io/visuotactile/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-04",
      "selected": null,
      "title": "Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing",
      "urls": [
        "http://arxiv.org/abs/2312.01853v2",
        "http://arxiv.org/pdf/2312.01853.pdf",
        "http://arxiv.org/pdf/2312.01853v2"
      ]
    },
    {
      "abstract": "Increasing interest in ensuring safety of next-generation Artificial Intelligence (AI) systems calls for novel approaches to embedding morality into autonomous agents. Traditionally, this has been done by imposing explicit top-down rules or hard constraints on systems, for example by filtering system outputs through pre-defined ethical rules. Recently, instead, entirely bottom-up methods for learning implicit preferences from human behavior have become increasingly popular, such as those for training and fine-tuning Large Language Models. In this paper, we provide a systematization of existing approaches to the problem of introducing morality in machines - modeled as a continuum, and argue that the majority of popular techniques lie at the extremes - either being fully hard-coded, or entirely learned, where no explicit statement of any moral principle is required. Given the relative strengths and weaknesses of each type of methodology, we argue that more hybrid solutions are needed to create adaptable and robust, yet more controllable and interpretable agents. In particular, we present three case studies of recent works which use learning from experience (i.e., Reinforcement Learning) to explicitly provide moral principles to learning agents - either as intrinsic rewards, moral logical constraints or textual principles for language models. For example, using intrinsic rewards in Social Dilemma games, we demonstrate how it is possible to represent classical moral frameworks for agents. We also present an overview of the existing work in this area in order to provide empirical evidence for the potential of this hybrid approach. We then discuss strategies for evaluating the effectiveness of moral learning agents. Finally, we present open research questions and implications for the future of AI safety and ethics which are emerging from this framework.",
      "authors": [
        "Tennant, Elizaveta",
        "Hailes, Stephen",
        "Musolesi, Mirco"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-04",
      "selected": null,
      "title": "Learning Machine Morality through Experience and Interaction",
      "urls": [
        "http://arxiv.org/pdf/2312.01818.pdf",
        "http://arxiv.org/pdf/2312.01818v1",
        "http://arxiv.org/abs/2312.01818v1"
      ]
    },
    {
      "abstract": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be \"superficial.\" This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
      "authors": [
        "Lin, Bill Yuchen",
        "Ravichander, Abhilasha",
        "Lu, Ximing",
        "Dziri, Nouha",
        "Sclar, Melanie",
        "Chandu, Khyathi",
        "Bhagavatula, Chandra",
        "Choi, Yejin"
      ],
      "categories": null,
      "citations": null,
      "comments": "26 pages, 8 figures. Project website:\n  https://allenai.github.io/re-align/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-04",
      "selected": null,
      "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
      "urls": [
        "http://arxiv.org/pdf/2312.01552v1",
        "http://arxiv.org/pdf/2312.01552.pdf",
        "http://arxiv.org/abs/2312.01552v1"
      ]
    },
    {
      "abstract": "This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning. Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to LLMs on intermediate planning results. This makes the whole path planning process a `white box' and human feedback guides LLM A* to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning. In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques. Comparative analysis against A* and RL shows that LLM A* is more efficient in terms of search space and achieves an on-a-par path with A* and a better path than RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks.",
      "authors": [
        "Xiao, Hengjia",
        "Wang, Peng"
      ],
      "categories": null,
      "citations": null,
      "comments": "5 figures, 8 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-04",
      "selected": null,
      "title": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics",
      "urls": [
        "http://arxiv.org/pdf/2312.01797v1",
        "http://arxiv.org/abs/2312.01797v1",
        "http://arxiv.org/pdf/2312.01797.pdf"
      ]
    },
    {
      "abstract": "In recent years, reinforcement learning has achieved significant advances in practical domains such as robotics. However, conveying intricate objectives to agents in reinforcement learning (RL) remains challenging, often necessitating detailed reward function design. In this study, we introduce an innovative approach, MEETRE, which integrates max-entropy exploration strategies with random encoders. This offers a streamlined and efficient solution for human-involved preference-based RL without the need for meticulously designed reward functions. Furthermore, MEETRE sidesteps the need for additional models or representation learning, leveraging the power of randomly initialized encoders for effective exploration. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Isabel Y.N Guan",
        "Xin Liu",
        "Gary Zhang",
        "Estella Zhao",
        "Zhenzhong Jia"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ROBIO58561.2023.10355039",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-2571-3",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE International Conference on Robotics and Biomimetics, ROBIO 2023"
      },
      "publication_date": "2023-12-04",
      "selected": null,
      "title": "METREE: Max-Entropy Exploration with Random Encoding for Efficient RL with Human Preferences",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10355039",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182551602&origin=inward"
      ]
    },
    {
      "abstract": "Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.",
      "authors": [
        "Xu, Wanqiao",
        "Dong, Shi",
        "Lu, Xiuyuan",
        "Lam, Grace",
        "Wen, Zheng",
        "Van Roy, Benjamin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-02",
      "selected": null,
      "title": "RLHF and IIA: Perverse Incentives",
      "urls": [
        "http://arxiv.org/pdf/2312.01057v3",
        "http://arxiv.org/pdf/2312.01057.pdf",
        "http://arxiv.org/abs/2312.01057v3"
      ]
    },
    {
      "abstract": "The remarkable abilities of large language models (LLMs) like GPT-4 partially stem from post-training processes like Reinforcement Learning from Human Feedback (RLHF) involving human preferences encoded in a reward model. However, these reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We release our model on huggingface: https://huggingface.co/corbyrosset/axiomatic_preference_model",
      "authors": [
        "Rosset, Corby",
        "Zheng, Guoqing",
        "Dibia, Victor",
        "Awadallah, Ahmed",
        "Bennett, Paul"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to EMNLP 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-02",
      "selected": null,
      "title": "Axiomatic Preference Modeling for Longform Question Answering",
      "urls": [
        "http://arxiv.org/pdf/2312.02206.pdf",
        "http://arxiv.org/abs/2312.02206v1",
        "http://arxiv.org/pdf/2312.02206v1"
      ]
    },
    {
      "abstract": "For motor learning, the processing of behavioral outcomes is of high significance. The feedback-related negativity (FRN) is an event-related potential, which is often described as a correlate of the reward prediction error in reinforcement learning. The number of studies examining the FRN in motor tasks is increasing. This meta-analysis summarizes the component in the motor domain and compares it to the cognitive domain. Therefore, a data set of a previous meta-analysis in the cognitive domain that comprised 47 studies was reanalyzed and compared to additional 25 studies of the motor domain. Further, a moderator analysis for the studies in the motor domain was conducted. The FRN amplitude was higher in the motor domain than in the cognitive domain. This might be related to a higher task complexity and a higher feedback ambiguity of motor tasks. The FRN latency was shorter in the motor domain than in the cognitive domain. Given that sensory information can be used as an external feedback predictor prior to the presentation of the final feedback, reward processing in the motor domain may have been faster and reduced the FRN latency. The moderator variable analysis revealed that the feedback modality influenced the FRN latency, with shorter FRN latencies after bimodal than after visual feedback. Processing of outcome feedback seems to share basic principles in both domains; however, differences exist and should be considered in FRN studies. Future research is motivated to scrutinize the effects of bimodal feedback and other moderators within the motor domain. \u00c2\u00a9 2023 The Authors. Psychophysiology published by Wiley Periodicals LLC on behalf of Society for Psychophysiological Research.",
      "authors": [
        "Fa\u00c3\u009fbender, L.",
        "Krause, D.",
        "Weigelt, M."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.14439",
      "keywords": [
        "ACC",
        "reinforcement learning",
        "reward positivity",
        "motor learning",
        "error processing",
        "ERPs"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Feedback processing in cognitive and motor tasks: A meta-analysis on the feedback-related negativity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172143727&origin=inward"
      ]
    },
    {
      "abstract": "Humans are subject to a variety of cognitive biases, such as the framing-effect or the gambler's fallacy, that lead to decisions unfitting of a purely rational agent. Previous studies have shown that the ventromedial prefrontal cortex (vmPFC) plays a key role in making rational decisions and that stronger vmPFC activity is associated with attenuated cognitive biases. Accordingly, dysfunctions of the vmPFC are associated with impulsive decisions and pathological gambling. By applying a gambling paradigm in a between-subjects design with 33 healthy adults, we demonstrate that vmPFC excitation via transcranial direct current stimulation (tDCS) reduces the framing-effect and the gambler's fallacy compared to sham stimulation. Corresponding magnetoencephalographic data suggest improved inhibition of maladaptive options after excitatory vmPFC-tDCS. Our analyses suggest that the underlying mechanism might be improved reinforcement learning, as effects only emerge over time. These findings encourage further investigations of whether excitatory vmPFC-tDCS has clinical utility in treating pathological gambling or other behavioral addictions.",
      "authors": [
        "Kroker, Thomas",
        "Wyczesany, Miroslaw",
        "Rehbein, Maimu Alissa",
        "Roesmann, Kati",
        "Wessing, Ida",
        "Wiegand, Anja",
        "B\u00f6lte, Jens",
        "Jungh\u00f6fer, Markus"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-023-43264-x",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Excitatory stimulation of the ventromedial prefrontal cortex reduces cognitive gambling biases via improved feedback learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174619652&origin=inward",
        "https://www.nature.com/articles/s41598-023-43264-x.pdf"
      ]
    },
    {
      "abstract": "Williams syndrome (WS) is a rare genetic condition characterized by high social interest and approach motivation as well as intellectual disability and anxiety. Despite the fact that social stimuli are believed to have an increased intrinsic reward value in WS, it is not known whether this translates to learning and decision making. Genes homozygously deleted in WS are linked to sociability in the general population, making it a potential model condition for understanding the social brain. Probabilistic reinforcement learning was studied with either social or non-social rewards for correct choices. Social feedback improved learning in individuals with Williams syndrome but not in typically developing controls or individuals with other intellectual disabilities. Computational modeling indicated that these effects on social feedback were mediated by a shift towards higher weight given to rewards relative to punishments and increased choice consistency. We conclude that reward learning in WS is characterized by high volatility and a tendency to learn how to avoid punishment rather than how to gain rewards. Social feedback can partly normalize this pattern and promote adaptive reward learning.",
      "authors": [
        "Kleberg, Johan Lundin",
        "Willfors, Charlotte",
        "Bj\u00f6rlin Avdic, Hanna",
        "Riby, Deborah",
        "Galazka, Martyna A.",
        "Guath, Mona",
        "Nordgren, Ann",
        "Stranneg\u00e5rd, Claes"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-022-26055-8",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Social feedback enhances learning in Williams syndrome",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145512306&origin=inward",
        "https://www.nature.com/articles/s41598-022-26055-8.pdf"
      ]
    },
    {
      "abstract": "Learning to predict action outcomes in morally conflicting situations is essential for social decision-making but poorly understood. Here we tested which forms of Reinforcement Learning Theory capture how participants learn to choose between self-money and other-shocks, and how they adapt to changes in contingencies. We find choices were better described by a reinforcement learning model based on the current value of separately expected outcomes than by one based on the combined historical values of past outcomes. Participants track expected values of self-money and other-shocks separately, with the substantial individual difference in preference reflected in a valuation parameter balancing their relative weight. This valuation parameter also predicted choices in an independent costly helping task. The expectations of self-money and other-shocks were biased toward the favored outcome but fMRI revealed this bias to be reflected in the ventromedial prefrontal cortex while the pain-observation network represented pain prediction errors independently of individual preferences. How we juggle morally conflicting outcomes during learning remains unknown. Here, by comparing variants of reinforcement learning models, the authors show that participants differ substantially in their preference, with some choosing actions that benefit themselves while others choose actions that prevent harm.",
      "authors": [
        "Fornari, Laura",
        "Ioumpa, Kalliopi",
        "Nostro, Alessandra D.",
        "Evans, Nathan J.",
        "De Angelis, Lorenzo",
        "Speer, Sebastian P. H.",
        "Paracampo, Riccardo",
        "Gallo, Selene",
        "Spezio, Michael",
        "Keysers, Christian",
        "Gazzola, Valeria"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41467-023-36807-3",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 24.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2041-1723",
        "publisher": "Nature Publishing Group",
        "sjr": 5.116,
        "snip": 3.268,
        "subject_areas": [
          "Chemistry (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Physics and Astronomy (all)"
        ],
        "title": "Nature Communications"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Neuro-computational mechanisms and individual biases in action-outcome learning under moral conflict",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149936936&origin=inward",
        "https://www.nature.com/articles/s41467-023-36807-3.pdf"
      ]
    },
    {
      "abstract": "The classification problem is essential to machine learning, often used in fault detection, condition monitoring, and behavior recognition. In recent years, due to the rapid development of incremental learning, reinforcement learning, transfer learning, and continual learning algorithms, the contradiction between the classification model and new data has been alleviated. However, due to the lack of feedback, most classification algorithms take long to search and may deviate from the correct results. Because of this, we propose a continual learning classification method with human-in-the-loop (H[sbnd]CLCM) based on the artificial immune system. H[sbnd]CLCM draws lessons from the mechanism that humans can enhance immune response through various intervention technologies and brings humans into the test learning process in a supervisory role. The human experience is integrated into the test phase, and the parameters corresponding to the error identification data are adjusted online. It enables it to converge to an accurate prediction model at the lowest cost and to learn new data categories without retraining the classifier. \u00e2\u0080\u00a2 All necessary steps and formulas of H[sbnd]CLCM are provided. \u00e2\u0080\u00a2 H[sbnd]CLCM adds manual intervention to improve the classification ability of the model. \u00e2\u0080\u00a2 H[sbnd]CLCM can recognize new types of data. \u00c2\u00a9 2023 The Author(s)",
      "authors": [
        "Liu, J.",
        "Li, D.",
        "Shan, W.",
        "Liu, S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.mex.2023.102374",
      "keywords": [
        "Human-in-the-loop",
        "Artificial immune system",
        "Continual learning",
        "Classification"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "MethodsX"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Continual learning classification method with human-in-the-loop",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171198857&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial step of learning a reward model from human feedback, often expressed as preferences between pairs of text generations produced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by optimizing it to maximize the reward model through a reinforcement learning algorithm. However, an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a preference model, which is conditioned on two inputs given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. To demonstrate the effectiveness of our approach, we present experimental results involving the fine-tuning of a LLM for a text summarization task. We believe NLHF offers a compelling avenue for preference learning and policy optimization with the potential of advancing the field of aligning LLMs with human preferences.",
      "authors": [
        "Munos, R\u00e9mi",
        "Valko, Michal",
        "Calandriello, Daniele",
        "Azar, Mohammad Gheshlaghi",
        "Rowland, Mark",
        "Guo, Zhaohan Daniel",
        "Tang, Yunhao",
        "Geist, Matthieu",
        "Mesnard, Thomas",
        "Michi, Andrea",
        "Selvi, Marco",
        "Girgin, Sertan",
        "Momchev, Nikola",
        "Bachem, Olivier",
        "Mankowitz, Daniel J.",
        "Precup, Doina",
        "Piot, Bilal"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Nash Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2312.00886v3",
        "http://arxiv.org/abs/2312.00886v3",
        "http://arxiv.org/pdf/2312.00886.pdf"
      ]
    },
    {
      "abstract": "Preference-based feedback is important for many applications in reinforcement learning where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback (RLHF) on large language models. For many applications of RLHF, the cost of acquiring the human feedback can be substantial. In this work, we take advantage of the fact that one can often choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and formalize this as an offline contextual dueling bandit problem. We give an upper-confidence-bound style algorithm for this problem and prove a polynomial worst-case regret bound. We then provide empirical confirmation in a synthetic setting that our approach outperforms existing methods. After, we extend the setting and methodology for practical use in RLHF training of large language models. Here, our method is able to reach better performance with fewer samples of human preferences than multiple baselines on three real-world datasets.",
      "authors": [
        "Mehta, Viraj",
        "Das, Vikramjeet",
        "Neopane, Ojash",
        "Dai, Yijia",
        "Bogunovic, Ilija",
        "Schneider, Jeff",
        "Neiswanger, Willie"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration",
      "urls": [
        "http://arxiv.org/pdf/2312.00267v1",
        "http://arxiv.org/abs/2312.00267v1",
        "http://arxiv.org/pdf/2312.00267.pdf"
      ]
    },
    {
      "abstract": "Visual (perceptual) reasoning is a critical skill in many medical specialties, including pathology, diagnostic imaging, and dermatology. However, in an ever-compressed medical curriculum, learning and practicing this skill can be challenging. Previous studies (including work with pigeons) have suggested that using reward-feedback-based activities, novices can gain expert levels of visual diagnostic accuracy in shortened training times. But is this level of diagnostic accuracy a result of image recognition (categorization) or is it the acquisition of diagnostic expertise? To answer this, the authors measured electroencephalographic data (EEG) and two components of the human event-related brain potential (reward positivity and N170) to explore the nature of visual expertise in a novice-expert study in pathology visual diagnosis. It was found that the amplitude of the reward positivity decreased with learning in novices (suggesting a decrease in reliance on feedback, as in other studies). However, this signal remained significantly different from the experts whose reward positivity signal did not change over the course of the experiment. There were no changes in the amplitude of the N170 (a reported neural marker of visual expertise) in novices over time. Novice N170 signals remained statistically and significantly lower in amplitude compared to experts throughout task performance. These data suggest that, while novices gained the ability to recognize (categorize) pathologies through reinforcement learning as quantified by the change in reward positivity, increased accuracy, and decreased time for responses, there was little change in the neural marker associated with visual expertise (N170). This is consistent with the multi-dimensional and complex nature of visual expertise and provides insight into future training programs for novices to bridge the expertise gap.",
      "authors": [
        "Anderson, Sarah J.",
        "Warren, Amy L.",
        "Abdullayeva, Nia",
        "Krigolson, Olav",
        "Hecker, Kent G."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10459-023-10232-z",
      "keywords": [
        "Electroencephalography",
        "N170",
        "Reward positivity",
        "Visual reasoning",
        "Pathology",
        "Feedback/reinforcement based learning",
        "Neuroeducation"
      ],
      "number_of_pages": 14,
      "pages": "1579-1592",
      "publication": {
        "category": "Journal",
        "cite_score": 5.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13824996",
        "publisher": "Springer Netherlands",
        "sjr": 1.2,
        "snip": 1.901,
        "subject_areas": [
          "Education"
        ],
        "title": "Advances in Health Sciences Education"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Pathologists aren\u2019t pigeons: exploring the neural basis of visual recognition and perceptual expertise in pathology",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10459-023-10232-z.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159312960&origin=inward"
      ]
    },
    {
      "abstract": "This study investigates the effects of error-based and reinforcement training on the acquisition and long-term retention of free throw accuracy in basketball. Sixty participants were divided into four groups (n\u2009=\u200915 per group): (i) the error-based group (sensory feedback), (ii) the reinforcement group (binary feedback including success or failure), (iii) the mixed group (sensory feedback followed by binary feedback), and (iv) the control group (without training). Free throws success was recorded before training (PreT), immediately after (Postd0), one day later (Postd1), and seven days later (Postd7). The error-based group, but not the reinforcement group, showed a significant immediate improvement in free throw accuracy (PreT vs Postd0). Interestingly, over time (Postd0 vs Postd1 vs Postd7), the reinforcement group significantly improved its accuracy, while the error-based group decreased it, returning to the PreT level (PreT vs Post7). The mixed group showed the advantage of both training methods, i.e., fast acquisition and retention on a long-term scale. Error-based learning leads to better acquisition, while reinforcement learning leads to better retention. Therefore, the combination of both types of learning is more efficient for both acquisition and retention processes. These findings provide new insight into the acquisition and retention of a fundamental basketball skill in free throw shooting.",
      "authors": [
        "Truong, Charl\u00e8ne",
        "Ruffino, C\u00e9lia",
        "Crognier, Alexandre",
        "Paizis, Christos",
        "Crognier, Lionel",
        "Papaxanthis, Charalambos"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-022-26568-2",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Error-based and reinforcement learning in basketball free throw shooting",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146103358&origin=inward",
        "https://www.nature.com/articles/s41598-022-26568-2.pdf"
      ]
    },
    {
      "abstract": "Turner syndrome is a genetic condition caused by a complete or partial loss of one of the X chromosomes. Previous studies indicate that Turner syndrome is associated with challenges in social skills, but the underlying mechanisms remain largely unexplored. A possible mechanism is a reduced social influence on learning. The current study examined the impact of social and non-social feedback on learning in women with Turner syndrome (n\u2009=\u200935) and a sex- and age-matched control group (n\u2009=\u200937). Participants were instructed to earn points by repeatedly choosing between two stimuli with unequal probabilities of resulting in a reward. Mastering the task therefore required participants to learn through feedback which of the two stimuli was more likely to be rewarded. Data were analyzed using computational modeling and analyses of choice behavior. Social feedback led to a more explorative choice behavior in the control group, resulting in reduced learning compared to non-social feedback. No effects of social feedback on learning were found in Turner syndrome. The current study thus indicates that women with Turner syndrome may be less sensitive to social influences on reinforcement learning, than the general population.",
      "authors": [
        "Bj\u00f6rlin Avdic, Hanna",
        "Stranneg\u00e5rd, Claes",
        "Engberg, Hedvig",
        "Willfors, Charlotte",
        "Nordgren, Ida",
        "Fris\u00e9n, Louise",
        "Hirschberg, Angelica Lind\u00e9n",
        "Guath, Mona",
        "Nordgren, Ann",
        "Kleberg, Johan Lundin"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-023-42628-7",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Reduced effects of social feedback on learning in Turner syndrome",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172009283&origin=inward",
        "https://www.nature.com/articles/s41598-023-42628-7.pdf"
      ]
    },
    {
      "abstract": "The identification of the electrical stimulation parameters for neuromodulation is a subject-specific and time-consuming procedure that presently mostly relies on the expertise of the user (e.g., clinician, experimenter, bioengineer). Since the parameters of stimulation change over time (due to displacement of electrodes, skin status, etc.), patients undergo recurrent, long calibration sessions, along with visits to the clinics, which are inefficient and expensive. To address this issue, we developed an automatized calibration system based on reinforcement learning (RL) allowing for accurate and efficient identification of the peripheral nerve stimulation parameters for somatosensory neuroprostheses. We developed an RL algorithm to automatically select neurostimulation parameters for restoring sensory feedback with transcutaneous electrical nerve stimulation (TENS). First, the algorithm was trained offline on a dataset comprising 49 subjects. Then, the neurostimulation was then integrated with a graphical user interface (GUI) to create an intuitive AI-based mapping platform enabling the user to autonomously perform the sensation characterization procedure. We assessed the algorithm against the performance of both experienced and na\u00efve and of a brute force algorithm (BFA), on 15 nerves from five subjects. Then, we validated the AI-based platform on six neuropathic nerves affected by distal sensory loss. Our automatized approach demonstrated the ability to find the optimal values of neurostimulation achieving reliable and comfortable elicited sensations. When compared to alternatives, RL outperformed the na\u00efve and BFA, significantly decreasing the time for mapping and the number of delivered stimulation trains, while improving the overall quality. Furthermore, the RL algorithm showed performance comparable to trained experimenters. Finally, we exploited it successfully for eliciting sensory feedback in neuropathic patients. Our findings demonstrated that the AI-based platform based on a RL algorithm can automatically and efficiently calibrate parameters for somatosensory nerve stimulation. This holds promise to avoid experts\u2019 employment in similar scenarios, thanks to the merging between AI and neurotech. Our RL algorithm has the potential to be used in other neuromodulation fields requiring a mapping process of the stimulation parameters. Trial registration: ClinicalTrial.gov (Identifier: NCT04217005)",
      "authors": [
        "Borda, Luigi",
        "Gozzi, Noemi",
        "Preatoni, Greta",
        "Valle, Giacomo",
        "Raspopovic, Stanisa"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1186/s12984-023-01246-0",
      "keywords": [
        "Neuropathy",
        "TENS",
        "AI",
        "Neurostimulation",
        "Reinforcement learning",
        "Automatic calibration",
        "Electrical stimulation",
        "Sensory feedback"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1743-0003",
        "publisher": "BioMed Central Ltd.",
        "sjr": 1.134,
        "snip": 1.924,
        "subject_areas": [
          "Rehabilitation",
          "Health Informatics"
        ],
        "title": "Journal of NeuroEngineering and Rehabilitation"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Automated calibration of somatosensory stimulation using reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172175842&origin=inward",
        "https://jneuroengrehab.biomedcentral.com/counter/pdf/10.1186/s12984-023-01246-0"
      ]
    },
    {
      "abstract": "Two prominent types of uncertainty that have been studied extensively are expected and unexpected uncertainty. Studies suggest that humans are capable of learning from reward under both expected and unexpected uncertainty when the source of variability is the reward. How do people learn when the source of uncertainty is the environment\u2019s state and the rewards themselves are deterministic? How does their learning compare with the case of reward uncertainty? The present study addressed these questions using behavioural experimentation and computational modelling. Experiment 1 showed that human subjects were generally able to use reward feedback to successfully learn the task rules under state uncertainty, and were able to detect a non-signalled reversal of stimulus-response contingencies. Experiment 2, which combined all four types of uncertainties\u2014expected versus unexpected uncertainty, and state versus reward uncertainty\u2014highlighted key similarities and differences in learning between state and reward uncertainties. We found that subjects performed significantly better in the state uncertainty condition, primarily because they explored less and improved their state disambiguation. We also show that a simple reinforcement learning mechanism that ignores state uncertainty and updates the state-action value of only the identified state accounted for the behavioural data better than both a Bayesian reinforcement learning model that keeps track of belief states and a model that acts based on sampling from past experiences. Our findings suggest a common mechanism supports reward-based learning under state and reward uncertainty.",
      "authors": [
        "Ez-zizi, Adnane",
        "Farrell, Simon",
        "Leslie, David",
        "Malhotra, Gaurav",
        "Ludwig, Casimir J.H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s42113-022-00165-y",
      "keywords": [
        "Bayesian reinforcement learning",
        "Expected and unexpected uncertainty",
        "Sampling-based learning",
        "Reinforcement learning"
      ],
      "number_of_pages": 25,
      "pages": "626-650",
      "publication": {
        "category": "Journal",
        "cite_score": 4.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2522-087X",
        "publisher": "Springer Nature",
        "sjr": 0.968,
        "snip": 0.919,
        "subject_areas": [
          "Developmental and Educational Psychology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Computational Brain and Behavior"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Reinforcement Learning Under Uncertainty: Expected Versus Unexpected Uncertainty and State Versus Reward Uncertainty",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s42113-022-00165-y.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150413395&origin=inward"
      ]
    },
    {
      "abstract": "Dopamine fundamentally contributes to reinforcement learning, but recent accounts also suggest a contribution to specific action selection mechanisms and the regulation of response vigour. Here, we examine dopaminergic mechanisms underlying human reinforcement learning and action selection via a combined pharmacological neuroimaging approach in male human volunteers (n\u2009=\u200931, within-subjects; Placebo, 150\u2009mg of the dopamine precursor L-dopa, 2\u2009mg of the D2 receptor antagonist Haloperidol). We found little credible evidence for previously reported beneficial effects of L-dopa vs. Haloperidol on learning from gains and altered neural prediction error signals, which may be partly due to differences experimental design and/or drug dosages. Reinforcement learning drift diffusion models account for learning-related changes in accuracy and response times, and reveal consistent decision threshold reductions under both drugs, in line with the idea that lower dosages of D2 receptor antagonists increase striatal DA release via an autoreceptor-mediated feedback mechanism. These results are in line with the idea that dopamine regulates decision thresholds during reinforcement learning, and may help to bridge action selection and response vigor accounts of dopamine. The neurotransmitter dopamine is known to regulate learning and decision-making. Here, the authors show that pharmacologically enhancing dopamine levels influences the regulation of speed and accuracy during reward learning.",
      "authors": [
        "Chakroun, Karima",
        "Wiehler, Antonius",
        "Wagner, Ben",
        "Mathar, David",
        "Ganzer, Florian",
        "van Eimeren, Thilo",
        "Sommer, Tobias",
        "Peters, Jan"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41467-023-41130-y",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 24.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2041-1723",
        "publisher": "Nature Publishing Group",
        "sjr": 5.116,
        "snip": 3.268,
        "subject_areas": [
          "Chemistry (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Physics and Astronomy (all)"
        ],
        "title": "Nature Communications"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "Dopamine regulates decision thresholds in human reinforcement learning in males",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169668917&origin=inward",
        "https://www.nature.com/articles/s41467-023-41130-y.pdf"
      ]
    },
    {
      "abstract": "The renewed interest in psychedelic research provides growing evidence of potentially unique effects on various aspects of reward processing systems. Using the Research Domain Criteria (RDoC) framework, as proposed by the National Institute of Mental Health, we aim to synthesize the existing literature concerning the impact of lysergic acid diethylamide (LSD) on the RDoC\u2019s Positive Valence Systems (PVS) domain, and to identify potential avenues for further research. Two LSD-related terms (lysergic acid diethylamide and LSD) and 13 PVS-related terms (reward, happiness, bliss, motivation, reinforcement learning, operant, conditioning, satisfaction, decision making, habit, valence, affect, mood) were used to search electronic databases such as PubMed, Scopus, PsychINFO, and Web of Science for relevant articles. A manual search of the reference list resulted in nine additional articles. After screening, articles and data were evaluated and included based on their relevance to the objective of investigating the effects of LSD on the PVS. Articles and data were excluded if they did not provide information about the PVS, were observational in nature, lacked comparators or reference groups, or were duplicates. A risk of bias assessment was performed using the National Toxicology Program\u2019s Office of Health Assessment and Translation (NTP OHAT) risk of bias (RoB) tool. Data from the included articles were collected and structured based on the RDoC bio-behavioral matrix, specifically focusing on the PVS domain and its three constituent constructs: reward responsiveness, reward learning, and reward valuation. We reviewed 28 clinical studies with 477 participants. Lysergic acid diethylamide, assessed at self-report (23 studies), molecular (5 studies), circuit (4 studies), and paradigm (3 studies) levels, exhibited dose-dependent mood improvement (20 short-term and 3 long-term studies). The subjective and neural effects of LSD were linked to the 5-HT2A receptor (molecular). Animal studies (14 studies) suggested LSD could mildly reinforce conditioned place preference without aversion and reduce responsiveness to other rewards. Findings on reward learning were inconsistent but hinted at potential associative learning enhancements. Reward valuation measures indicated potential reductions in effort expenditure for other reinforcers. Our findings are consistent with our previous work, which indicated classical psychedelics, primarily serotonin 2A receptor agonists, enhanced reward responsiveness in healthy individuals and patient populations. Lysergic acid diethylamide exhibits a unique profile in the reward learning and valuation constructs. Using the RDoC-based framework, we identified areas for future research, enhancing our understanding of the impact of LSD on reward processing. However, applying RDoC to psychedelic research faces limitations due to diverse study designs that were not initially RDoC-oriented. Limitations include subjective outcome measure selection aligned with RDoC constructs and potential bias in synthesizing varied studies. Additionally, some human studies were open-label, introducing potential bias compared to randomized, blinded studies.",
      "authors": [
        "Pouyan, Niloufar",
        "Younesi Sisi, Farnaz",
        "Kargar, Alireza",
        "Scheidegger, Milan",
        "McIntyre, Roger S.",
        "Morrow, Jonathan D."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s40263-023-01044-1",
      "keywords": [],
      "number_of_pages": 37,
      "pages": "1027-1063",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "11727047",
        "publisher": "Springer International Publishing",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CNS Drugs"
      },
      "publication_date": "2023-12-01",
      "selected": null,
      "title": "The effects of Lysergic Acid Diethylamide (LSD) on the Positive Valence Systems: A Research Domain Criteria (RDoC)-Informed Systematic Review",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177692365&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s40263-023-01044-1.pdf"
      ]
    },
    {
      "abstract": "Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.",
      "authors": [
        "Chen, Hailin",
        "Jiao, Fangkai",
        "Li, Xingxuan",
        "Qin, Chengwei",
        "Ravaut, Mathieu",
        "Zhao, Ruochen",
        "Xiong, Caiming",
        "Joty, Shafiq"
      ],
      "categories": null,
      "citations": null,
      "comments": "version v4, included latest top-performing open-sourced LLMs",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-28",
      "selected": null,
      "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?",
      "urls": [
        "http://arxiv.org/abs/2311.16989v4",
        "http://arxiv.org/pdf/2311.16989v4",
        "http://arxiv.org/pdf/2311.16989.pdf"
      ]
    },
    {
      "abstract": "The adoption of Reinforcement Learning (RL) in several human-centred applications provides robots with autonomous decision-making capabilities and adaptability based on the observations of the operating environment. In such scenarios, however, the learning process can make robots' behaviours unclear and unpredictable to humans, thus preventing a smooth and effective Human-Robot Interaction (HRI). As a consequence, it becomes crucial to avoid robots performing actions that are unclear to the user. In this work, we investigate whether including human preferences in RL (concerning the actions the robot performs during learning) improves the transparency of a robot's behaviours. For this purpose, a shielding mechanism is included in the RL algorithm to include human preferences and to monitor the learning agent's decisions. We carried out a within-subjects study involving 26 participants to evaluate the robot's transparency in terms of Legibility, Predictability, and Expectability in different settings. Results indicate that considering human preferences during learning improves Legibility with respect to providing only Explanations, and combining human preferences with explanations elucidating the rationale behind the robot's decisions further amplifies transparency. Results also confirm that an increase in transparency leads to an increase in the safety, comfort, and reliability of the robot. These findings show the importance of transparency during learning and suggest a paradigm for robotic applications with human in the loop.",
      "authors": [
        "Angelopoulos, Georgios",
        "Mangiacapra, Luigi",
        "Rossi, Alessandra",
        "Di Napoli, Claudia",
        "Rossi, Silvia"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-28",
      "selected": null,
      "title": "Increasing Transparency of Reinforcement Learning using Shielding for Human Preferences and Explanations",
      "urls": [
        "http://arxiv.org/pdf/2311.16838v1",
        "http://arxiv.org/pdf/2311.16838.pdf",
        "http://arxiv.org/abs/2311.16838v1"
      ]
    },
    {
      "abstract": "This research investigates the role of prompt engineering in enhancing the performance and generalisation of large-scale language models (LLMs) across a wide range of Natural Language Processing (NLP) tasks. The study introduces a comprehensive framework for prompt...",
      "authors": [
        "Ratnayake, Himath",
        "Wang, Can"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1007/978-981-99-8391-9_6",
      "keywords": [
        "Prompt Engineering",
        "Natural Language Processing",
        "Large-Scale Language Models",
        "Prompting Framework"
      ],
      "number_of_pages": 16,
      "pages": "66-81",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-981-99-8390-2",
        "issn": "1611-3349",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "AI 2023: Advances in Artificial Intelligence: 36th Australasian Joint Conference on Artificial Intelligence, AI 2023, Brisbane, QLD, Australia, November 28\u2013December 1, 2023, Proceedings, Part II"
      },
      "publication_date": "2023-11-28",
      "selected": null,
      "title": "A Prompting Framework to\u00a0Enhance Language Model Output",
      "urls": [
        "https://dl.acm.org/doi/10.1007/978-981-99-8391-9_6",
        "https://link.springer.com/content/pdf/10.1007/978-981-99-8391-9_6.pdf"
      ]
    },
    {
      "abstract": "Learning from human feedback is a prominent technique to align the output of large language models (LLMs) with human expectations. Reinforcement learning from human feedback (RLHF) leverages human preference signals that are in the form of ranking of response pairs to perform this alignment. However, human preference on LLM outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response. In this work we investigate data efficiency of modeling human feedback that is in natural language. Specifically, we fine-tune an open-source LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or even less) of human feedback in natural language in the form of critiques and revisions of responses. We show that this model is able to improve the quality of responses from even some of the strongest LLMs such as ChatGPT, BARD, and Vicuna, through critique and revision of those responses. For instance, through one iteration of revision of ChatGPT responses, the revised responses have 56.6% win rate over the original ones, and this win rate can be further improved to 65.9% after applying the revision for five iterations.",
      "authors": [
        "Jin, Di",
        "Mehri, Shikib",
        "Hazarika, Devamanyu",
        "Padmakumar, Aishwarya",
        "Lee, Sungjin",
        "Liu, Yang",
        "Namazifar, Mahdi"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted by Workshop on Instruction Tuning and Instruction Following\n  at NeurIPS 2023, Submitted to AAAI 2024",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-24",
      "selected": null,
      "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language",
      "urls": [
        "http://arxiv.org/abs/2311.14543v1",
        "http://arxiv.org/pdf/2311.14543.pdf",
        "http://arxiv.org/pdf/2311.14543v1"
      ]
    },
    {
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a \"jailbreak backdoor\" into the model. The backdoor embeds a trigger word into the model that acts like a universal \"sudo command\": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.",
      "authors": [
        "Rando, Javier",
        "Tram\u00e8r, Florian"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted as conference paper in ICLR 2024",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-24",
      "selected": null,
      "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
      "urls": [
        "http://arxiv.org/abs/2311.14455v2",
        "http://arxiv.org/pdf/2311.14455.pdf",
        "http://arxiv.org/pdf/2311.14455v2"
      ]
    },
    {
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has played a crucial role in the success of large models such as ChatGPT. RLHF is a reinforcement learning framework which combines human feedback to improve learning effectiveness and performance. However, obtaining preferences feedback manually is quite expensive in commercial applications. Some statistical commercial indicators are usually more valuable and always ignored in RLHF. There exists a gap between commercial target and model training. In our research, we will attempt to fill this gap with statistical business feedback instead of human feedback, using AB testing which is a well-established statistical method. Reinforcement Learning from Statistical Feedback (RLSF) based on AB testing is proposed. Statistical inference methods are used to obtain preferences for training the reward network, which fine-tunes the pre-trained model in reinforcement learning framework, achieving greater business value. Furthermore, we extend AB testing with double selections at a single time-point to ANT testing with multiple selections at different feedback time points. Moreover, we design numerical experiences to validate the effectiveness of our algorithm framework.",
      "authors": [
        "Han, Feiyang",
        "Wei, Yimin",
        "Liu, Zhaofeng",
        "Qi, Yanxing"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-24",
      "selected": null,
      "title": "Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing",
      "urls": [
        "http://arxiv.org/pdf/2311.14766v1",
        "http://arxiv.org/abs/2311.14766v1",
        "http://arxiv.org/pdf/2311.14766.pdf"
      ]
    },
    {
      "abstract": "Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\" -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.",
      "authors": [
        "Dumoulin, Vincent",
        "Johnson, Daniel D.",
        "Castro, Pablo Samuel",
        "Larochelle, Hugo",
        "Dauphin, Yann"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-23",
      "selected": null,
      "title": "A density estimation perspective on learning from pairwise human preferences",
      "urls": [
        "http://arxiv.org/abs/2311.14115v3",
        "http://arxiv.org/pdf/2311.14115.pdf",
        "http://arxiv.org/pdf/2311.14115v3"
      ]
    },
    {
      "abstract": "Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available in https://github.com/yk7333/D3PO/tree/main.",
      "authors": [
        "Yang, Kai",
        "Tao, Jian",
        "Lyu, Jiafei",
        "Ge, Chunjiang",
        "Chen, Jiaxin",
        "Li, Qimai",
        "Shen, Weihan",
        "Zhu, Xiaolong",
        "Li, Xiu"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-22",
      "selected": null,
      "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model",
      "urls": [
        "http://arxiv.org/pdf/2311.13231.pdf",
        "http://arxiv.org/pdf/2311.13231v2",
        "http://arxiv.org/abs/2311.13231v2"
      ]
    },
    {
      "abstract": "Language serves as a vehicle for conveying thought, enabling communication among individuals. The ability to distinguish between diverse concepts, identify fairness and injustice, and comprehend a range of legal notions fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited. This paper seeks to address the philosophical question: How can we effectively teach logical reasoning to LLMs while maintaining a deep understanding of the intricate relationship between language and logic? By focusing on bolstering LLMs' capabilities in logical reasoning, we aim to expand their applicability in law and other logic-intensive disciplines. To this end, we propose a Reinforcement Learning from Logical Feedback (RLLF) approach, which serves as a potential framework for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation methodology, we explore new avenues for research in this domain and contribute to the development of LLMs capable of handling complex legal reasoning tasks while acknowledging the fundamental connection between language and logic.",
      "authors": [
        "Nguyen, Ha-Thanh",
        "Fungwacharakorn, Wachara",
        "Satoh, Ken"
      ],
      "categories": null,
      "citations": null,
      "comments": "ALP@JURIX2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-22",
      "selected": null,
      "title": "Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications",
      "urls": [
        "http://arxiv.org/pdf/2311.13095v1",
        "http://arxiv.org/pdf/2311.13095.pdf",
        "http://arxiv.org/abs/2311.13095v1"
      ]
    },
    {
      "abstract": "Foundation models, specifically Large Language Models (LLMs), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align LLM's. These reward models are additionally used at inference-time to estimate LLM responses' adherence to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance - measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting to detect these distribution shifts in prompts and responses.",
      "authors": [
        "LeVine, Will",
        "Pikus, Benjamin",
        "Chen, Anthony",
        "Hendryx, Sean"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-21",
      "selected": null,
      "title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift",
      "urls": [
        "http://arxiv.org/abs/2311.14743v7",
        "http://arxiv.org/pdf/2311.14743.pdf",
        "http://arxiv.org/pdf/2311.14743v7"
      ]
    },
    {
      "abstract": "Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.",
      "authors": [
        "Wallace, Bram",
        "Dang, Meihua",
        "Rafailov, Rafael",
        "Zhou, Linqi",
        "Lou, Aaron",
        "Purushwalkam, Senthil",
        "Ermon, Stefano",
        "Xiong, Caiming",
        "Joty, Shafiq",
        "Naik, Nikhil"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-21",
      "selected": null,
      "title": "Diffusion Model Alignment Using Direct Preference Optimization",
      "urls": [
        "http://arxiv.org/pdf/2311.12908.pdf",
        "http://arxiv.org/pdf/2311.12908v1",
        "http://arxiv.org/abs/2311.12908v1"
      ]
    },
    {
      "abstract": "<p>\n<b>Introduction</b>\u00e2\u0080\u0083Large language models (LLMs) are designed for recognizing, summarizing, translating, predicting, and generating text-based content from knowledge gained from extensive data sets. ChatGPT4 (Generative Pre-trained Transformer 4) (OpenAI, San Francisco, California, United States) is a transformer-based LLM model pretrained on public data as well as data obtained from third-party sources using deep learning techniques of fine tuning and reinforcement learning from human feedback to predict the next text. We wanted to explore the role of LLM as a teaching assistant (TA) in plastic surgery.</p>\n <p>\n<b>Material and Methods</b>\u00e2\u0080\u0083TA roles were first identified in available literature, and based on the roles, a list of suitable tasks was created where LLM could be used to perform the task. Prompts designed to be fed in to the LLM (specifically ChatGPT) to generate appropriate output, were then created and fed to the ChatGPT model. The outputs generated were scored by evaluators and compared for interobserver agreement.</p>\n <p>\n<b>Results</b>\u00e2\u0080\u0083A final set of eight TA roles were identified where a LLM could be utilized to generate content. These contents were scored for usefulness and accuracy. These were scored independently by the eight study authors in a scoring sheet created for the study. Interobserver agreements for content accuracy, usefulness, and clarity were 100% for content generated for the following: interactive case studies (generation), simulation of preoperative consultations, and generation of ethical considerations.</p>\n <p>\n<b>Discussion</b>\u00e2\u0080\u0083LLMs in general and ChatGPT (on which this study is based) in specific, can generate answers to questions and prompts based on huge amount of text fed into the model for training the underlying language model. The answers generated have been found to be accurate, readable, and even indistinguishable from human-generated text. This capability of automated content synthesis can be exploited to generate summaries to text, answer short and long answers, and generate case scenarios. We could identify a few such scenarios where the LLM could in general be utilized to play the role of a TA and aid plastic surgery residents in particular. In addition, these models could also be used by students to obtain feedback and gain reflection which itself stimulates critical thinking.</p>\n <p>\n<b>Conclusion</b>\u00e2\u0080\u0083Incorporating LLMs into the educational arsenal of plastic surgery residency programs can provide a dynamic, interactive, and individualized learning experience for residents and prove to be worthy TAs of future.</p>\n ",
      "authors": [
        "Devi Prasad Mohapatra",
        "Friji Meethale Thiruvoth",
        "Satyaswarup Tripathy",
        "Sheeja Rajan",
        "Madhubari Vathulya",
        "Palukuri Lakshmi",
        "Veena K. Singh",
        "Ansar Ul Haq"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1055/s-0043-1772704",
      "keywords": [
        "ChatGPT in education",
        "large language models (LLM)",
        "future of surgical training",
        "plastic surgical education",
        "educational technology"
      ],
      "number_of_pages": 8,
      "pages": "413-420",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09700358",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Indian Journal of Plastic Surgery"
      },
      "publication_date": "2023-11-21",
      "selected": null,
      "title": "Leveraging Large Language Models (LLM) for the Plastic Surgery Resident Training: Do They Have a Role?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178156684&origin=inward"
      ]
    },
    {
      "abstract": "Learning from human feedback is an effective way to improve robotic learning in exploration-heavy tasks. Compared to the wide application of binary human feedback, scalar human feedback has been used less because it is believed to be noisy and unstable. In this paper, we compare scalar and binary feedback, and demonstrate that scalar feedback benefits learning when properly handled. We collected binary or scalar feedback respectively from two groups of crowdworkers on a robot task. We found that when considering how consistently a participant labeled the same data, scalar feedback led to less consistency than binary feedback; however, the difference vanishes if small mismatches are allowed. Additionally, scalar and binary feedback show no significant differences in their correlations with key Reinforcement Learning targets. We then introduce Stabilizing TEacher Assessment DYnamics (STEADY) to improve learning from scalar feedback. Based on the idea that scalar feedback is muti-distributional, STEADY re-constructs underlying positive and negative feedback distributions and re-scales scalar feedback based on feedback statistics. We show that models trained with \\textit{scalar feedback + STEADY } outperform baselines, including binary feedback and raw scalar feedback, in a robot reaching task with non-expert human feedback. Our results show that both binary feedback and scalar feedback are dynamic, and scalar feedback is a promising signal for use in interactive Reinforcement Learning.",
      "authors": [
        "Yu, Hang",
        "Aronson, Reuben M.",
        "Allen, Katherine H.",
        "Short, Elaine Schaertl"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Human-Computer Interaction",
          "Robotics"
        ],
        "title": "IROS 2023"
      },
      "publication_date": "2023-11-17",
      "selected": null,
      "title": "From \"Thumbs Up\" to \"10 out of 10\": Reconsidering Scalar Feedback in Interactive Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2311.10284v1",
        "http://arxiv.org/pdf/2311.10284.pdf",
        "http://arxiv.org/pdf/2311.10284v1"
      ]
    },
    {
      "abstract": "We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.",
      "authors": [
        "Chen, Yangyi",
        "Sikka, Karan",
        "Cogswell, Michael",
        "Ji, Heng",
        "Divakaran, Ajay"
      ],
      "categories": null,
      "citations": null,
      "comments": "The feedback datasets will be released at:\n  https://huggingface.co/datasets/YangyiYY/LVLM_NLF",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-16",
      "selected": null,
      "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback",
      "urls": [
        "http://arxiv.org/pdf/2311.10081v1",
        "http://arxiv.org/pdf/2311.10081.pdf",
        "http://arxiv.org/abs/2311.10081v1"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) often struggle when faced with situations where they lack the prerequisite knowledge to generate a sensical response. In these cases, models tend to fabricate and hallucinate, rather than appropriately signaling uncertainty as humans would. This behavior misaligns with human conversational norms and presents challenges surrounding responsible and ethical AI development. This work aims to systematically investigate LLMs' behaviors in such situations. We curate an adversarial question-answering benchmark containing unanswerable questions targeting information absent from the LLM's training data. Concretely, these unanswerable questions contain non-existent concepts or false premises. When presented with such unanswerable questions, an LLM should appropriately convey uncertainty, and be able to challenge the premise and refuse to generate a response. While facing answerable valid questions, a model should demonstrate a positive correlation between accuracy and confidence. Using a model-agnostic unified confidence elicitation approach, we observe that LLMs that have gone through instruction finetuning and reinforcement learning from human feedback (RLHF) perform significantly better than their counterparts that do not. Moreover, uncertainty expression 1 through our elicitation method does not always stay consistent with the perceived confidence of the direct response of an LLM. Our findings call for further research into teaching LLMs to proactively and reliably express uncertainty.",
      "authors": [
        "Liu, Genglin",
        "Wang, Xingyao",
        "Yuan, Lifan",
        "Chen, Yangyi",
        "Peng, Hao"
      ],
      "categories": null,
      "citations": null,
      "comments": "21 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-16",
      "selected": null,
      "title": "Prudent Silence or Foolish Babble? Examining Large Language Models' Responses to the Unknown",
      "urls": [
        "http://arxiv.org/pdf/2311.09731v1",
        "http://arxiv.org/abs/2311.09731v1",
        "http://arxiv.org/pdf/2311.09731.pdf"
      ]
    },
    {
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.",
      "authors": [
        "Wang, Jiongxiao",
        "Wu, Junlin",
        "Chen, Muhao",
        "Vorobeychik, Yevgeniy",
        "Xiao, Chaowei"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-16",
      "selected": null,
      "title": "On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models",
      "urls": [
        "http://arxiv.org/abs/2311.09641v1",
        "http://arxiv.org/pdf/2311.09641v1",
        "http://arxiv.org/pdf/2311.09641.pdf"
      ]
    },
    {
      "abstract": "Grey-box fuzzing is the lightweight approach of choice for finding bugs in sequential programs. It provides a balance between efficiency and effectiveness by conducting a biased random search over the domain of program inputs using a feedback function from observed test executions. For distributed system testing, however, the state-of-practice is represented today by only black-box tools that do not attempt to infer and exploit any knowledge of the system's past behaviours to guide the search for bugs. In this work, we present Mallory: the first framework for greybox fuzz-testing of distributed systems. Unlike popular black-box distributed system fuzzers, such as Jepsen, that search for bugs by randomly injecting network partitions and node faults or by following human-defined schedules, Mallory is adaptive. It exercises a novel metric to learn how to maximise the number of observed system behaviors by choosing different sequences of faults, thus increasing the likelihood of finding new bugs. Our approach relies on timeline-driven testing. Mallory dynamically constructs Lamport timelines of the system behaviour and further abstracts these timelines into happens-before summaries, which serve as a feedback function guiding the fuzz campaign. Subsequently, Mallory reactively learns a policy using Q-learning, enabling it to introduce faults guided by its real-time observation of the summaries. We have evaluated Mallory on a diverse set of widely-used industrial distributed systems. Compared to the start-of-the-art black-box fuzzer Jepsen, Mallory explores 54.27% more distinct states within 24 hours while achieving a speed-up of 2.24\u00c3\u0097. At the same time, Mallory finds bugs 1.87\u00c3\u0097 faster, thereby finding more bugs within the given time budget. Mallory discovered 22 zero-day bugs (of which 18 were confirmed by developers), including 10 new vulnerabilities, in rigorously-tested distributed systems such as Braft, Dqlite and Redis. 6 new CVEs have been assigned. \u00c2\u00a9 2023 Copyright held by the owner/author(s).",
      "authors": [
        "Meng, R.",
        "Roychoudhury, A.",
        "P\u00c3\u00aerlea, G.",
        "Sergey, I."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3576915.3623097",
      "keywords": [
        "greybox fuzzing",
        "reactive system testing",
        "distributed systems"
      ],
      "number_of_pages": 15,
      "pages": "1615-1629",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400700507",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security"
      },
      "publication_date": "2023-11-15",
      "selected": null,
      "title": "Greybox Fuzzing of Distributed Systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179846837&origin=inward"
      ]
    },
    {
      "abstract": "Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). In domains like teaching and emotional support, carefully constructing grounding prevents misunderstanding. However, it is unclear whether large language models (LLMs) leverage these dialogue acts in constructing common ground. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLMs use these grounding acts, simulating them taking turns from several dialogue datasets, and comparing the results to humans. We find that current LLMs are presumptive grounders, biased towards assuming common ground without using grounding acts. To understand the roots of this behavior, we examine the role of instruction tuning and reinforcement learning with human feedback (RLHF), finding that RLHF leads to less grounding. Altogether, our work highlights the need for more research investigating grounding in human-AI interaction.",
      "authors": [
        "Shaikh, Omar",
        "Gligori\u0107, Kristina",
        "Khetan, Ashna",
        "Gerstgrasser, Matthias",
        "Yang, Diyi",
        "Jurafsky, Dan"
      ],
      "categories": null,
      "citations": null,
      "comments": "16 pages, 2 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-15",
      "selected": null,
      "title": "Grounding or Guesswork? Large Language Models are Presumptive Grounders",
      "urls": [
        "http://arxiv.org/pdf/2311.09144.pdf",
        "http://arxiv.org/abs/2311.09144v1",
        "http://arxiv.org/pdf/2311.09144v1"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate. A core ingredient in RLHF's success in aligning and improving large language models (LLMs) is its reward model, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using minimum Bayes risk decoding and reranking have succeeded in improving the final quality of translation. In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach. Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality. Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.",
      "authors": [
        "Ramos, Miguel Moura",
        "Fernandes, Patrick",
        "Farinhas, Ant\u00f3nio",
        "Martins, Andr\u00e9 F. T."
      ],
      "categories": null,
      "citations": null,
      "comments": "14 pages, work-in-progress",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-15",
      "selected": null,
      "title": "Aligning Neural Machine Translation Models: Human Feedback in Training and Inference",
      "urls": [
        "http://arxiv.org/pdf/2311.09132.pdf",
        "http://arxiv.org/pdf/2311.09132v1",
        "http://arxiv.org/abs/2311.09132v1"
      ]
    },
    {
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for enhancing model safety in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for semi-automatically constructing large-scale preference datasets. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. We evaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an expert model, generating approximately 10K preference samples. Finetuning an Alpaca model on this dataset demonstrates improved harmlessness while maintaining competitive performance on conversation and downstream tasks. Safer-Instruct addresses the challenges in preference data acquisition, advancing the development of safer and more responsible AI systems. Our code and data are available at https://github.com/uscnlp-lime/safer-instruct",
      "authors": [
        "Shi, Taiwei",
        "Chen, Kai",
        "Zhao, Jieyu"
      ],
      "categories": null,
      "citations": null,
      "comments": "11 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-15",
      "selected": null,
      "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data",
      "urls": [
        "http://arxiv.org/pdf/2311.08685v1",
        "http://arxiv.org/abs/2311.08685v1",
        "http://arxiv.org/pdf/2311.08685.pdf"
      ]
    },
    {
      "abstract": "Understanding people's preferences for facilities in sports stadiums is important to optimize the facility allocation and predict the crowd distribution for public safety purposes. Crowd preferences refer to people's priority in seeking food, game swags, restrooms, finding seats, and exploring the venue over time. Existing approaches include pre-event surveys and interviews, which are costly and prone to biases when the response rate is low. In recent years, the development of advanced sensing systems enables automatic tracking of crowd distributions in public spaces, enabling the characterization of human preferences based on sensor data. However, the main challenge of human preference characterization is that their behaviors are not static, but rather dynamic and context-dependent, leading to high uncertainties over time and space. To overcome this challenge, we present a novel method to infer the dynamic preferences of the crowd on facilities such as food stands, restrooms, and game swag stations using dynamic inverse reinforcement learning (DIRL). Our method leverages the facility layout in sports stadiums and learns a time- and context-dependent reward function of the crowd by observing their distribution around various facilities over time. We then infer the crowd's preference for facilities based on the weight of each type of facility in the reward function. To evaluate our approach, we conduct 6 real-world experiments for NCAA Pac-12 sports games at Stanford Maples Pavilion. We demonstrate how our method captures the changing crowd preferences on facilities over time and estimates the crowd distribution trend at 5 entry doors with 70% accuracy, allowing dynamic human preference characterization through sensor data. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Yiwen Dong",
        "Peide Huang",
        "Hae Young Noh"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3600100.3626272",
      "keywords": [
        "Human Preference",
        "Stadium Facility",
        "Crowd Management"
      ],
      "number_of_pages": 2,
      "pages": "305-306",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400702303",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "BuildSys 2023 - Proceedings of the10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation"
      },
      "publication_date": "2023-11-15",
      "selected": null,
      "title": "Characterizing Crowd Preferences on Stadium Facilities through Dynamic Inverse Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3600100.3626272",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179517167&origin=inward"
      ]
    },
    {
      "abstract": "Recently, diffusion-based deep generative models (e.g., Stable Diffusion) have shown impressive results in text-to-image synthesis. However, current text-to-image models often require multiple passes of prompt engineering by humans in order to produce satisfactory results for real-world applications. We propose BeautifulPrompt, a deep generative model to produce high-quality prompts from very simple raw descriptions, which enables diffusion-based models to generate more beautiful images. In our work, we first fine-tuned the BeautifulPrompt model over low-quality and high-quality collecting prompt pairs. Then, to ensure that our generated prompts can generate more beautiful images, we further propose a Reinforcement Learning with Visual AI Feedback technique to fine-tune our model to maximize the reward values of the generated prompts, where the reward values are calculated based on the PickScore and the Aesthetic Scores. Our results demonstrate that learning from visual AI feedback promises the potential to improve the quality of generated prompts and images significantly. We further showcase the integration of BeautifulPrompt to a cloud-native AI platform to provide better text-to-image generation service in the cloud.",
      "authors": [
        "Cao, Tingfeng",
        "Wang, Chengyu",
        "Liu, Bingyan",
        "Wu, Ziheng",
        "Zhu, Jinhui",
        "Huang, Jun"
      ],
      "categories": null,
      "citations": null,
      "comments": "emnlp 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-12",
      "selected": null,
      "title": "BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis",
      "urls": [
        "http://arxiv.org/pdf/2311.06752.pdf",
        "http://arxiv.org/abs/2311.06752v1",
        "http://arxiv.org/pdf/2311.06752v1"
      ]
    },
    {
      "abstract": "While recent advances have boosted LM proficiency in linguistic benchmarks, LMs consistently struggle to reason correctly on complex tasks like mathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a method with which to shape model reasoning processes. In particular, we explore two reward schemes, outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs), to optimize for logical reasoning. Our results show that the fine-grained reward provided by PRM-based methods enhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly, reducing performance in complex tasks (MATH). Furthermore, we show the critical role reward aggregation functions play in model performance. Providing promising avenues for future research, our study underscores the need for further exploration into fine-grained reward modeling for more reliable language models.",
      "authors": [
        "Pan, Sarah",
        "Lialin, Vladislav",
        "Muckatira, Sherin",
        "Rumshisky, Anna"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-10",
      "selected": null,
      "title": "Let's Reinforce Step by Step",
      "urls": [
        "http://arxiv.org/pdf/2311.05821.pdf",
        "http://arxiv.org/pdf/2311.05821v1",
        "http://arxiv.org/abs/2311.05821v1"
      ]
    },
    {
      "abstract": "Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student's current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised fine-tuning or \"single-step\" RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. In this work, we explore a new method for adapting LLMs with RL for such goal-directed dialogue. Our key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. Our algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirically, we show that our proposed approach achieves state-of-the-art performance in various goal-directed dialogue tasks that include teaching and preference elicitation.",
      "authors": [
        "Hong, Joey",
        "Levine, Sergey",
        "Dragan, Anca"
      ],
      "categories": null,
      "citations": null,
      "comments": "25 pages, 6 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-09",
      "selected": null,
      "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations",
      "urls": [
        "http://arxiv.org/pdf/2311.05584v1",
        "http://arxiv.org/abs/2311.05584v1",
        "http://arxiv.org/pdf/2311.05584.pdf"
      ]
    },
    {
      "abstract": "As large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback (RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. We may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. We further show that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that our fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Our results show the need for further research on protections on LLMs.",
      "authors": [
        "Zhan, Qiusi",
        "Fang, Richard",
        "Bindu, Rohan",
        "Gupta, Akul",
        "Hashimoto, Tatsunori",
        "Kang, Daniel"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-09",
      "selected": null,
      "title": "Removing RLHF Protections in GPT-4 via Fine-Tuning",
      "urls": [
        "http://arxiv.org/pdf/2311.05553.pdf",
        "http://arxiv.org/pdf/2311.05553v2",
        "http://arxiv.org/abs/2311.05553v2"
      ]
    },
    {
      "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.",
      "authors": [
        "Guo, Geyang",
        "Zhao, Ranchi",
        "Tang, Tianyi",
        "Zhao, Wayne Xin",
        "Wen, Ji-Rong"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-07",
      "selected": null,
      "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
      "urls": [
        "http://arxiv.org/pdf/2311.04072v1",
        "http://arxiv.org/abs/2311.04072v1",
        "http://arxiv.org/pdf/2311.04072.pdf"
      ]
    },
    {
      "abstract": "Many capable large language models (LLMs) are developed via self-supervised pre-training followed by a reinforcement-learning fine-tuning phase, often based on human or AI feedback. During this stage, models may be guided by their inductive biases to rely on simpler features which may be easier to extract, at a cost to robustness and generalisation. We investigate whether principles governing inductive biases in the supervised fine-tuning of LLMs also apply when the fine-tuning process uses reinforcement learning. Following Lovering et al (2021), we test two hypotheses: that features more $\\textit{extractable}$ after pre-training are more likely to be utilised by the final policy, and that the evidence for/against a feature predicts whether it will be utilised. Through controlled experiments on synthetic and natural language tasks, we find statistically significant correlations which constitute strong evidence for these hypotheses.",
      "authors": [
        "Cruz, Diogo",
        "Pona, Edoardo",
        "Holness-Tofts, Alex",
        "Schmied, Elias",
        "Alonso, V\u00edctor Abia",
        "Griffin, Charlie",
        "Cirstea, Bogdan-Ionut"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-07",
      "selected": null,
      "title": "Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features",
      "urls": [
        "http://arxiv.org/pdf/2311.04046.pdf",
        "http://arxiv.org/abs/2311.04046v1",
        "http://arxiv.org/pdf/2311.04046v1"
      ]
    },
    {
      "abstract": "Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) is the key technique for remote sensing image recognition. In this work, we propose a novel SAR ATR framework with reinforcement learning (RL) based human-in-the-loop. The proposed framework consists of an RL agent, followed by a GNN-based classifier. The RL agent is capable of learning from human feedback to identify the region of interest (RoI) in the SAR image. Then, a RoI input graph is constructed for the classifier to perform target classification. Through learning from human feedback, the RL agent can focus on the RoI and filter out irrelevant and distracting signals in the input SAR images. We evaluate the proposed framework for the moving and stationary target acquisition and recognition (MSTAR) dataset. The results show that incorporating human feedback can improve generalization of the proposed model. By visualizing the results, we observe that the RL agent can effectively identify the RoI with human-in-the-loop training. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Bingyi Zhang",
        "Sasindu Wijeratne",
        "Tian Ye",
        "Rajgopal Kannan",
        "Viktor Prasanna",
        "Carl Busart",
        "Lance Kaplan"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/RADAR54928.2023.10371087",
      "keywords": [
        "Human-in-the-loop",
        "Synthetic Aperture Radar",
        "Automatic Target Recognition",
        "Graph Neural Network"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8279-0",
        "issn": "10975764",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE International Radar Conference (RADAR)"
      },
      "publication_date": "2023-11-06",
      "selected": null,
      "title": "How can Human-in-the-loop Improve the Performance of SAR ATR? A Reinforcement Learning Based Approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182743159&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10371087"
      ]
    },
    {
      "abstract": "Reinforcement Learning (RL) plays an important role in the robotic manipulation domain since it allows self-learning from trial-and-error interactions with the environment. Still, sample efficiency and reward specification seriously limit its potential. One possible solution involves learning from expert guidance. However, obtaining a human expert is impractical due to the high cost of supervising an RL agent, and developing an automatic supervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate remarkable abilities to provide human-like feedback on user inputs in natural language. Nevertheless, they are not designed to directly control low-level robotic motions, as their pretraining is based on vast internet data rather than specific robotics data. In this paper, we introduce the Lafite-RL (Language agent feedback interactive Reinforcement Learning) framework, which enables RL agents to learn robotic tasks efficiently by taking advantage of LLMs' timely feedback. Our experiments conducted on RLBench tasks illustrate that, with simple prompt design in natural language, the Lafite-RL agent exhibits improved learning capabilities when guided by an LLM. It outperforms the baseline in terms of both learning efficiency and success rate, underscoring the efficacy of the rewards provided by an LLM.",
      "authors": [
        "Chu, Kun",
        "Zhao, Xufeng",
        "Weber, Cornelius",
        "Li, Mengdi",
        "Wermter, Stefan"
      ],
      "categories": null,
      "citations": null,
      "comments": "CoRL 2023 Workshop (oral)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-04",
      "selected": null,
      "title": "Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models",
      "urls": [
        "http://arxiv.org/abs/2311.02379v1",
        "http://arxiv.org/pdf/2311.02379v1",
        "http://arxiv.org/pdf/2311.02379.pdf"
      ]
    },
    {
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) can be used to capture complex and nuanced properties of text generation quality. As a result, the task of text summarization has been identified as a good candidate for this process. In this paper, we explore how preference agreement impacts the efficacy of RLHF for summarization. We show that sampling human preferences to include a range of annotator agreement results in (1) higher accuracy reward models and (2) alters the characteristics of quality captured. We additionally show improvements in downstream generation when using a reward model trained with a range of preference agreements. Our contributions have implications for the design of synthetic datasets as well as the importance of considering quality differentials in comparison-based data.",
      "authors": [
        "Gooding, Sian",
        "Mansoor, Hassan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-02",
      "selected": null,
      "title": "The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization",
      "urls": [
        "http://arxiv.org/abs/2311.04919v1",
        "http://arxiv.org/pdf/2311.04919v1",
        "http://arxiv.org/pdf/2311.04919.pdf"
      ]
    },
    {
      "abstract": "The development of trustworthy conversational information-seeking systems relies on dialogue models that can generate faithful and accurate responses based on relevant knowledge texts. However, two main challenges hinder this task. Firstly, language models may generate hallucinations due to data biases present in their pretraining corpus. Secondly, knowledge texts often contain redundant and irrelevant information that distracts the model's attention from the relevant text span. Previous works use additional data annotations on the knowledge texts to learn a knowledge identification module in order to bypass irrelevant information, but collecting such high-quality span annotations can be costly. In this work, we leverage reinforcement learning algorithms to overcome the above challenges by introducing a novel reward function. Our reward function combines an accuracy metric and a faithfulness metric to provide a balanced quality judgment of generated responses, which can be used as a cost-effective approximation to a human preference reward model when only a few preference annotations are available. Empirical experiments on two conversational information-seeking datasets demonstrate that our method can compete with other strong supervised learning baselines.",
      "authors": [
        "Du, Wanyu",
        "Ji, Yangfeng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-02",
      "selected": null,
      "title": "Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation",
      "urls": [
        "http://arxiv.org/abs/2311.00953v1",
        "http://arxiv.org/pdf/2311.00953.pdf",
        "http://arxiv.org/pdf/2311.00953v1"
      ]
    },
    {
      "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.",
      "authors": [
        "Deng, Yang",
        "Zhang, Wenxuan",
        "Lam, Wai",
        "Ng, See-Kiong",
        "Chua, Tat-Seng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-11-01",
      "selected": null,
      "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents",
      "urls": [
        "http://arxiv.org/pdf/2311.00262.pdf",
        "http://arxiv.org/abs/2311.00262v1",
        "http://arxiv.org/pdf/2311.00262v1"
      ]
    },
    {
      "abstract": "Reward and punishment have long been recognized as potent modulators of human behavior. Although reinforcement learning is a significant motor learning process, the exact mechanisms underlying how the brain learns movements through reward and punishment are not yet fully understood. Beyond the memory of specific examples, investigating the ability to generalize to new situations offers a better understanding of motor learning. This study hypothesizes that reward and punishment engage qualitatively different motivational systems with different neurochemical and neuroanatomical substrates, which would have differential effects on reinforcement-based motor learning and generalization. To test this hypothesis, two groups of participants learn a motor task in one direction and then relearn the same task in a new direction, receiving only performance-based reward or punishment score feedback. Our findings support our hypothesis, showing that reward led to slower learning but promoted generalization. On the other hand, punishment led to faster learning but impaired generalization. These behavioral differences may be due to different tendencies of movement variability in each group. The punishment group tended to explore more actively than the reward group during the initial learning phase, possibly due to loss aversion. In contrast, the reward group tended to explore more actively than the initial learning phase during the generalization test phase, seemingly recalling the strategy that led to the reward. These results suggest that reward and punishment may engage different neural mechanisms during reinforcement-based motor learning and generalization, with important implications for practical applications such as sports training and motor rehabilitation.NEW & NOTEWORTHY Although reinforcement learning is a significant motor learning process, the mechanisms underlying how the brain learns movements through reward and punishment are not fully understood. We modified a well-established motor adaptation task and used savings (faster relearning) to measure generalization. We found reward led to slower learning but promoted generalization, whereas punishment led to faster learning but impaired generalization, suggesting that reward and punishment may engage different neural mechanisms during reinforcement-based motor learning and generalization.",
      "authors": [
        "Yin, C.",
        "Li, B.",
        "Gao, T."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00242.2023",
      "keywords": [
        "reward",
        "motor generalization",
        "reinforcement learning",
        "punishment",
        "motor learning"
      ],
      "number_of_pages": 12,
      "pages": "1150-1161",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of neurophysiology"
      },
      "publication_date": "2023-11-01",
      "selected": null,
      "title": "Differential effects of reward and punishment on reinforcement-based motor learning and generalization",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175585310&origin=inward"
      ]
    },
    {
      "abstract": "The emergence of cooperation is a central issue in understanding collective behavior and evolution. The eco-evolutionary game model introduces a human\u00e2\u0080\u0093environment coupling mechanism, revealing that the feedback between strategies and the relevant environment is a key element in sustaining long-term cooperation. Previous theoretical studies have observed periodic oscillations between cooperative and defective actions under certain conditions. However, such investigations assume cooperators hold a benefit advantage over defectors, which does not fundamentally illuminate how cooperation emerges. Our paper emphasizes that understanding this issue requires considering inherent human memory characteristics. We refine the eco-evolutionary game model using reinforcement learning, constructing a multi-agent system that couples environment and memory-based decision-making. Comprehensive analyses encompass collective and individual perspectives. Our findings show that with the memory mechanism, oscillations between collective cooperation and defection can still occur, even if defection remains a strict Nash equilibrium. Cooperation emerges from the group's random exploratory actions in depleted environments, altering the environment's trends. A positive feedback loop forms among the environment, individual rewards, and actions, stabilizing cooperation as a favorable individual strategy at that point. However, established group cooperation leads individuals seeking optimal behavior to transition from cooperators to defectors through exploration, resulting in cooperation collapse. Subsequently, the memory mechanism reengages, diluting defectors\u00e2\u0080\u0099 expected payoffs and initiating a new round of exploratory behavior within the group. Our results unveil the micro-level mechanisms driving cyclic oscillations, enhancing our understanding of the environment-strategy interplay. \u00c2\u00a9 2023 The Author(s)",
      "authors": [
        "Di, C.",
        "Zhou, Q.",
        "Shen, J.",
        "Wang, J.",
        "Zhou, R.",
        "Wang, T."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.chaos.2023.114138",
      "keywords": [
        "Eco-evolutionary game",
        "Social dilemma",
        "Reinforcement learning",
        "Emergence of cooperation"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 11.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09600779",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.393,
        "snip": 1.988,
        "subject_areas": [
          "Physics and Astronomy (all)",
          "Mathematical Physics",
          "Applied Mathematics",
          "Statistical and Nonlinear Physics"
        ],
        "title": "Chaos, Solitons and Fractals"
      },
      "publication_date": "2023-11-01",
      "selected": null,
      "title": "The coupling effect between the environment and strategies drives the emergence of group cooperation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173215104&origin=inward"
      ]
    },
    {
      "abstract": "Asset Liability Management (ALM) is an essential risk management technique in Quantitative Finance and Actuarial Science. It aims to maximise a risk-taker's ability to fulfil future liabilities. ALM is especially critical in environments of elevated interest rate changes, as has been experienced globally between 2021 and 2023. Traditional ALM implementation is still heavily dependent on the judgement of professionals such as Quants, Actuaries or Investment Managers. This over-reliance on human input critically limits ALM performance due to restricted automation, human irrationality and restricted scope for multi-objective optimisation. This paper addressed these limitations by applying Deep Reinforcement Learning (DRL), which optimises through trial, and error and continuous feedback from the environment. We defined the Reinforcement Learning (RL) components for the ALM application: the RL decision-making Agent, Environment, Actions, States and Reward Functions. The results demonstrated that DRL ALM can achieve duration-matching outcomes within 1% of the theoretical ALM at a 95% confidence level. Furthermore, compared to a benchmark weekly rebalancing traditional ALM regime, DRL ALM achieved superior outcomes of net portfolios which are, on average, 3 times less sensitive to interest rate changes. DRL also allows for increased automation, flexibility, and multi-objective optimisation in ALM, reducing the negative impact of human limitations and improving risk management outcomes. The findings and principles presented in this study apply to various institutional risk-takers, including insurers, banks, pension funds, and asset managers. Overall, DRL ALM provides a promising Artificial Intelligence (AI) avenue for improving risk management outcomes compared to the traditional approaches. \u00c2\u00a9 2023 The Author(s)",
      "authors": [
        "Wekwete, T.A.",
        "Kufakunesu, R.",
        "van Zyl, G."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.iswa.2023.200286",
      "keywords": [
        "Deep hedging",
        "Deep learning",
        "Redington immunisation",
        "Deep reinforcement learning",
        "Reinforcement learning",
        "Asset liability management",
        "Duration matching"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "26673053",
        "publisher": "Elsevier B.V.",
        "sjr": 1.126,
        "snip": 2.279,
        "subject_areas": [
          "Artificial Intelligence",
          "Signal Processing",
          "Computer Science Applications",
          "Computer Science (miscellaneous)",
          "Computer Vision and Pattern Recognition"
        ],
        "title": "Intelligent Systems with Applications"
      },
      "publication_date": "2023-11-01",
      "selected": null,
      "title": "Application of deep reinforcement learning in asset liability management",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174440325&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) more capable in complex settings. RLHF proceeds as collecting human preference data, training a reward model on said data, and optimizing a base ML model with respect to said reward for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many assumptions about how the various pieces fit together, such as a reward model capturing human preferences and an RL optimizer extracting the right signal from a reward model. As the RLHF process involves many distinct design decisions, it is easy to assume that multiple processes are correlated and therefore numerically linked. This apparent correlation is often not true, where reward models are easily overoptimized or RL optimizers can reduce performance on tasks not modeled in the data. Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model training, RL scores, and downstream performance drives these issues, which we describe as an objective mismatch. In this paper, we illustrate the causes of this issue, reviewing relevant literature from model-based reinforcement learning, and argue for solutions. By solving objective mismatch in RLHF, the ML models of the future will be more precisely aligned to user instructions for both safety and helpfulness.",
      "authors": [
        "Lambert, Nathan",
        "Calandra, Roberto"
      ],
      "categories": null,
      "citations": null,
      "comments": "11 pages, 5 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-31",
      "selected": null,
      "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/abs/2311.00168v2",
        "http://arxiv.org/pdf/2311.00168.pdf",
        "http://arxiv.org/pdf/2311.00168v2"
      ]
    },
    {
      "abstract": "AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights. We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat. We employ low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 per model and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve a refusal rate below 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method retains general performance, which we validate by comparing our fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, we present a selection of harmful outputs produced by our models. While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate and adapt to new environments. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights.",
      "authors": [
        "Lermen, Simon",
        "Rogers-Smith, Charlie",
        "Ladish, Jeffrey"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-31",
      "selected": null,
      "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
      "urls": [
        "http://arxiv.org/abs/2310.20624v1",
        "http://arxiv.org/pdf/2310.20624.pdf",
        "http://arxiv.org/pdf/2310.20624v1"
      ]
    },
    {
      "abstract": "Ideally, we would place a robot in a real-world environment and leave it there improving on its own by gathering more experience autonomously. However, algorithms for autonomous robotic learning have been challenging to realize in the real world. While this has often been attributed to the challenge of sample complexity, even sample-efficient techniques are hampered by two major challenges - the difficulty of providing well \"shaped\" rewards, and the difficulty of continual reset-free training. In this work, we describe a system for real-world reinforcement learning that enables agents to show continual improvement by training directly in the real world without requiring painstaking effort to hand-design reward functions or reset mechanisms. Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration while leveraging a simple self-supervised learning algorithm for goal-directed policy learning. We show that in the absence of resets, it is particularly important to account for the current \"reachability\" of the exploration policy when deciding which regions of the space to explore. Based on this insight, we instantiate a practical learning system - GEAR, which enables robots to simply be placed in real-world environments and left to train autonomously without interruption. The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of binary comparative feedback. We evaluate this system on a suite of robotic tasks in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.",
      "authors": [
        "Balsells, Max",
        "Torne, Marcel",
        "Wang, Zihan",
        "Desai, Samedh",
        "Agrawal, Pulkit",
        "Gupta, Abhishek"
      ],
      "categories": null,
      "citations": null,
      "comments": "Project website\n  https://guided-exploration-autonomous-rl.github.io/GEAR/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-31",
      "selected": null,
      "title": "Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.20608.pdf",
        "http://arxiv.org/pdf/2310.20608v1",
        "http://arxiv.org/abs/2310.20608v1"
      ]
    },
    {
      "abstract": "Learning from preference-based feedback has recently gained considerable traction as a promising approach to align generative models with human interests. Instead of relying on numerical rewards, the generative models are trained using reinforcement learning with human feedback (RLHF). These approaches first solicit feedback from human labelers typically in the form of pairwise comparisons between two possible actions, then estimate a reward model using these comparisons, and finally employ a policy based on the estimated reward model. An adversarial attack in any step of the above pipeline might reveal private and sensitive information of human labelers. In this work, we adopt the notion of label differential privacy (DP) and focus on the problem of reward estimation from preference-based feedback while protecting privacy of each individual labelers. Specifically, we consider the parametric Bradley-Terry-Luce (BTL) model for such pairwise comparison feedback involving a latent reward parameter $\\theta^* \\in \\mathbb{R}^d$. Within a standard minimax estimation framework, we provide tight upper and lower bounds on the error in estimating $\\theta^*$ under both local and central models of DP. We show, for a given privacy budget $\\epsilon$ and number of samples $n$, that the additional cost to ensure label-DP under local model is $\\Theta \\big(\\frac{1}{ e^\\epsilon-1}\\sqrt{\\frac{d}{n}}\\big)$, while it is $\\Theta\\big(\\frac{\\text{poly}(d)}{\\epsilon n} \\big)$ under the weaker central model. We perform simulations on synthetic data that corroborate these theoretical results.",
      "authors": [
        "Chowdhury, Sayak Ray",
        "Zhou, Xingyu",
        "Natarajan, Nagarajan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-30",
      "selected": null,
      "title": "Differentially Private Reward Estimation with Preference Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.19733.pdf",
        "http://arxiv.org/abs/2310.19733v1",
        "http://arxiv.org/pdf/2310.19733v1"
      ]
    },
    {
      "abstract": "Survivor bias in observational data leads the optimization of recommender systems towards local optima. Currently most solutions re-mines existing human-system collaboration patterns to maximize longer-term satisfaction by reinforcement learning. However, from the causal perspective, mitigating survivor effects requires answering a counterfactual problem, which is generally unidentifiable and inestimable. In this work, we propose a neural causal model to achieve counterfactual inference. Specifically, we first build a learnable structural causal model based on its available graphical representations which qualitatively characterizes the preference transitions. Mitigation of the survivor bias is achieved though counterfactual consistency. To identify the consistency, we use the Gumbel-max function as structural constrains. To estimate the consistency, we apply reinforcement optimizations, and use Gumbel-Softmax as a trade-off to get a differentiable function. Both theoretical and empirical studies demonstrate the effectiveness of our solution.",
      "authors": [
        "Liu, Jialin",
        "Su, Xinyan",
        "Zhou, Peng",
        "Zhao, Xiangyu",
        "Li, Jun"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-30",
      "selected": null,
      "title": "A General Neural Causal Model for Interactive Recommendation",
      "urls": [
        "http://arxiv.org/abs/2310.19519v1",
        "http://arxiv.org/pdf/2310.19519v1",
        "http://arxiv.org/pdf/2310.19519.pdf"
      ]
    },
    {
      "abstract": "Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles -- where alignment with human expectations can be imperative. In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels. Instead of labels, we use simple heuristics to mimic human feedback. More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, i.e., boxes containing objects are scored higher than those without. We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates. Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery.",
      "authors": [
        "Luo, Katie Z",
        "Liu, Zhenzhen",
        "Chen, Xiangyu",
        "You, Yurong",
        "Benaim, Sagie",
        "Phoo, Cheng Perng",
        "Campbell, Mark",
        "Sun, Wen",
        "Hariharan, Bharath",
        "Weinberger, Kilian Q."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-29",
      "selected": null,
      "title": "Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery",
      "urls": [
        "http://arxiv.org/pdf/2310.19080v2",
        "http://arxiv.org/abs/2310.19080v2",
        "http://arxiv.org/pdf/2310.19080.pdf"
      ]
    },
    {
      "abstract": "In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 39% reduction in real memory usage but also ran 75% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 37%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.",
      "authors": [
        "Peng, Houwen",
        "Wu, Kan",
        "Wei, Yixuan",
        "Zhao, Guoshuai",
        "Yang, Yuxiang",
        "Liu, Ze",
        "Xiong, Yifan",
        "Yang, Ziyue",
        "Ni, Bolin",
        "Hu, Jingcheng",
        "Li, Ruihang",
        "Zhang, Miaosen",
        "Li, Chen",
        "Ning, Jia",
        "Wang, Ruizhe",
        "Zhang, Zheng",
        "Liu, Shuguang",
        "Chau, Joe",
        "Hu, Han",
        "Cheng, Peng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-27",
      "selected": null,
      "title": "FP8-LM: Training FP8 Large Language Models",
      "urls": [
        "http://arxiv.org/abs/2310.18313v2",
        "http://arxiv.org/pdf/2310.18313.pdf",
        "http://arxiv.org/pdf/2310.18313v2"
      ]
    },
    {
      "abstract": "Quality of Experience (QoE)-driven adaptive bitrate (ABR) algorithms are typically optimized using QoE models that are based on the mean opinion score (MOS), while such principles may not account for user heterogeneity on rating scales, resulting in unexpected behaviors. In this paper, we propose Jade, which leverages reinforcement learning with human feedback(RLHF) technologies to better align the users' opinion scores. Jade's rank-based QoE model considers relative values of user ratings to interpret the subjective perception of video sessions. We implement linear-based and Deep Neural Network (DNN)-based architectures for satisfying both accuracy and generalization ability. We further propose entropy-aware reinforced mechanisms for training policies with the integration of the proposed QoE models. Experimental results demonstrate that Jade performs favorably on conventional metrics, such as quality and stall ratio, and improves QoE by 8.09%-38.13% in different network conditions, emphasizing the importance of user heterogeneity in QoE modeling and the potential of combining linear-based and DNN-based models for performance improvement. \u00c2\u00a9 2023 Owner/Author.",
      "authors": [
        "Tianchi Huang",
        "Rui-Xiao Zhang",
        "Chenglei Wu",
        "Lifeng Sun"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3581783.3611771",
      "keywords": [
        "reinforcement learning",
        "adaptive video streaming"
      ],
      "number_of_pages": 12,
      "pages": "1707-1718",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400701085",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "MM 2023 - Proceedings of the 31st ACM International Conference on Multimedia"
      },
      "publication_date": "2023-10-27",
      "selected": null,
      "title": "Optimizing Adaptive Video Streaming with Human Feedback",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3581783.3611771",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179553225&origin=inward"
      ]
    },
    {
      "abstract": "Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement learning that leverages the expert demonstrations by KL-regularization for a policy learned by behavior cloning. Our findings reveal that using $N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite and $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$ is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight convergence guarantees for the behaviour cloning procedure under general assumptions on the policy classes. Additionally, we establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF). In this respect, we provide theoretical evidence showing the benefits of KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, thus setting our approach apart from the prior works.",
      "authors": [
        "Tiapkin, Daniil",
        "Belomestny, Denis",
        "Calandriello, Daniele",
        "Moulines, Eric",
        "Naumov, Alexey",
        "Perrault, Pierre",
        "Valko, Michal",
        "Menard, Pierre"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-26",
      "selected": null,
      "title": "Demonstration-Regularized RL",
      "urls": [
        "http://arxiv.org/pdf/2310.17303v1",
        "http://arxiv.org/pdf/2310.17303.pdf",
        "http://arxiv.org/abs/2310.17303v1"
      ]
    },
    {
      "abstract": "Many recent studies have shown the ability of large language models (LLMs) to achieve state-of-the-art performance on many NLP tasks, such as question answering, text summarization, coding, and translation. In some cases, the results provided by LLMs are on par with those of human experts. These models' most disruptive innovation is their ability to perform tasks via zero-shot or few-shot prompting. This capability has been successfully exploited to train instructed LLMs, where reinforcement learning with human feedback is used to guide the model to follow the user's requests directly. In this paper, we investigate the ability of instructed LLMs to improve conversational search effectiveness by rewriting user questions in a conversational setting. We study which prompts provide the most informative rewritten utterances that lead to the best retrieval performance. Reproducible experiments are conducted on publicly-available TREC CAST datasets. The results show that rewriting conversational utterances with instructed LLMs achieves significant improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and 11.5% in Recall@500 over state-of-the-art techniques. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Elnara Galimzhanova",
        "Cristina Ioana Muntean",
        "Franco Maria Nardini",
        "Raffaele Perego",
        "Guido Rocchietti"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/WI-IAT59888.2023.00014",
      "keywords": [
        "query rewriting",
        "LLMs",
        "conversational systems",
        "information retrieval",
        "ChatGPT"
      ],
      "number_of_pages": 8,
      "pages": "56-63",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-0919-5",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2023 22nd IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2023"
      },
      "publication_date": "2023-10-26",
      "selected": null,
      "title": "Rewriting Conversational Utterances with Instructed Large Language Models",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10350178",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182526506&origin=inward"
      ]
    },
    {
      "abstract": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
      "authors": [
        "Wu, Fan",
        "Inan, Huseyin A.",
        "Backurs, Arturs",
        "Chandrasekaran, Varun",
        "Kulkarni, Janardhan",
        "Sim, Robert"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-25",
      "selected": null,
      "title": "Privately Aligning Language Models with Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2310.16960.pdf",
        "http://arxiv.org/abs/2310.16960v1",
        "http://arxiv.org/pdf/2310.16960v1"
      ]
    },
    {
      "abstract": "In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce \"MimicTouch\", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instructing robots through imitation learning using multi-modal sensor data and retargeted human motions. To further mitigate the embodiment gap between humans and robots, we employ online residual reinforcement learning on the physical robot. Through comprehensive experiments, we validate the safety of MimicTouch in transferring a latent policy learned through imitation learning from human to robot. This ongoing work will pave the way for a broader spectrum of tactile-guided robotic applications.",
      "authors": [
        "Yu, Kelin",
        "Han, Yunhai",
        "Zhu, Matthew",
        "Zhao, Ye"
      ],
      "categories": null,
      "citations": null,
      "comments": "Presented at CoRL 2023 Deployable Workshop and NIPS 2023 Touch\n  Processing Workshop",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-25",
      "selected": null,
      "title": "MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.16917v2",
        "http://arxiv.org/abs/2310.16917v2",
        "http://arxiv.org/pdf/2310.16917.pdf"
      ]
    },
    {
      "abstract": "In urban planning, land use readjustment plays a pivotal role in aligning land use configurations with the current demands for sustainable urban development. However, present-day urban planning practices face two main issues. Firstly, land use decisions are predominantly dependent on human experts. Besides, while resident engagement in urban planning can promote urban sustainability and livability, it is challenging to reconcile the diverse interests of stakeholders. To address these challenges, we introduce a Consensus-based Multi-Agent Reinforcement Learning framework for real-world land use readjustment. This framework serves participatory urban planning, allowing diverse intelligent agents as stakeholder representatives to vote for preferred land use types. Within this framework, we propose a novel consensus mechanism in reward design to optimize land utilization through collective decision making. To abstract the structure of the complex urban system, the geographic information of cities is transformed into a spatial graph structure and then processed by graph neural networks. Comprehensive experiments on both traditional top-down planning and participatory planning methods from real-world communities indicate that our computational framework enhances global benefits and accommodates diverse interests, leading to improved satisfaction across different demographic groups. By integrating Multi-Agent Reinforcement Learning, our framework ensures that participatory urban planning decisions are more dynamic and adaptive to evolving community needs and provides a robust platform for automating complex real-world urban planning processes.",
      "authors": [
        "Qian, Kejiang",
        "Mao, Lingjun",
        "Liang, Xin",
        "Ding, Yimin",
        "Gao, Jin",
        "Wei, Xinran",
        "Guo, Ziyi",
        "Li, Jiajie"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-25",
      "selected": null,
      "title": "AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2310.16772.pdf",
        "http://arxiv.org/abs/2310.16772v2",
        "http://arxiv.org/pdf/2310.16772v2"
      ]
    },
    {
      "abstract": "While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training. Here, we focus on two prevalent methods used to align these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). SFT is simple and robust, powering a host of open-source models, while RLHF is a more sophisticated method used in top-tier models like ChatGPT but also suffers from instability and susceptibility to reward hacking. We propose a novel approach, Supervised Iterative Learning from Human Feedback (SuperHF), which seeks to leverage the strengths of both methods. Our hypothesis is two-fold: that the reward model used in RLHF is critical for efficient data use and model generalization and that the use of Proximal Policy Optimization (PPO) in RLHF may not be necessary and could contribute to instability issues. SuperHF replaces PPO with a simple supervised loss and a Kullback-Leibler (KL) divergence prior. It creates its own training data by repeatedly sampling a batch of model outputs and filtering them through the reward model in an online learning regime. We then break down the reward optimization problem into three components: robustly optimizing the training rewards themselves, preventing reward hacking-exploitation of the reward model that degrades model performance-as measured by a novel METEOR similarity metric, and maintaining good performance on downstream evaluations. Our experimental results show SuperHF exceeds PPO-based RLHF on the training objective, easily and favorably trades off high reward with low reward hacking, improves downstream calibration, and performs the same on our GPT-4 based qualitative evaluation scheme all the while being significantly simpler to implement, highlighting SuperHF's potential as a competitive language model alignment technique.",
      "authors": [
        "Mukobi, Gabriel",
        "Chatain, Peter",
        "Fong, Su",
        "Windesheim, Robert",
        "Kutyniok, Gitta",
        "Bhatia, Kush",
        "Alberti, Silas"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to the Socially Responsible Language Modelling Research\n  (SoLaR) workshop at NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-25",
      "selected": null,
      "title": "SuperHF: Supervised Iterative Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.16763.pdf",
        "http://arxiv.org/abs/2310.16763v1",
        "http://arxiv.org/pdf/2310.16763v1"
      ]
    },
    {
      "abstract": "Language models have seen significant growth in the size of their corpus, leading to notable performance improvements. Yet, there has been limited progress in developing models that handle smaller, more human-like datasets. As part of the BabyLM shared task, this study explores the impact of reinforcement learning from human feedback (RLHF) on language models pretrained from scratch with a limited training corpus. Comparing two GPT-2 variants, the larger model performs better in storytelling tasks after RLHF fine-tuning. These findings suggest that RLHF techniques may be more advantageous for larger models due to their higher learning and adaptation capacity, though more experiments are needed to confirm this finding. These insights highlight the potential benefits of RLHF fine-tuning for language models within limited data, enhancing their ability to maintain narrative focus and coherence while adhering better to initial instructions in storytelling tasks. The code for this work is publicly at https://github.com/Zephyr1022/BabyStories-UTSA.",
      "authors": [
        "Zhao, Xingmeng",
        "Wang, Tongnian",
        "Osborn, Sheri",
        "Rios, Anthony"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to BabyLM workshop at CoNLL",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-25",
      "selected": null,
      "title": "BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?",
      "urls": [
        "http://arxiv.org/pdf/2310.16681.pdf",
        "http://arxiv.org/abs/2310.16681v1",
        "http://arxiv.org/pdf/2310.16681v1"
      ]
    },
    {
      "abstract": "Language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. Reinforcement learning from human feedback (RLHF) with algorithms like PPO is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. Recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the RL framework with supervised fine-tuning, but they are costly due to the need for annotated data. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from AI feedback. The common practices, which unidirectionally distill the instruction-following responses from LLMs, are constrained by their bottleneck. Thus we introduce CycleAlign to distill alignment capabilities from parameter-invisible LLMs (black-box) to a parameter-visible model (white-box) in an iterative manner. With in-context learning (ICL) as the core of the cycle, the black-box models are able to rank the model-generated responses guided by human-craft instruction and demonstrations about their preferences. During iterative interaction, the white-box models also have a judgment about responses generated by them. Consequently, the agreement ranking could be viewed as a pseudo label to dynamically update the in-context demonstrations and improve the preference ranking ability of black-box models. Through multiple interactions, the CycleAlign framework could align the white-box model with the black-box model effectively in a low-resource way. Empirical results illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing methods, and achieves the state-of-the-art performance in alignment with human value.",
      "authors": [
        "Hong, Jixiang",
        "Tu, Quan",
        "Chen, Changyu",
        "Gao, Xing",
        "Zhang, Ji",
        "Yan, Rui"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-25",
      "selected": null,
      "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment",
      "urls": [
        "http://arxiv.org/pdf/2310.16271v1",
        "http://arxiv.org/abs/2310.16271v1",
        "http://arxiv.org/pdf/2310.16271.pdf"
      ]
    },
    {
      "abstract": "Aligning AI agents to human intentions and values is a key bottleneck in building safe and deployable AI applications. But whose values should AI agents be aligned with? Reinforcement learning with human feedback (RLHF) has emerged as the key framework for AI alignment. RLHF uses feedback from human reinforcers to fine-tune outputs; all widely deployed large language models (LLMs) use RLHF to align their outputs to human values. It is critical to understand the limitations of RLHF and consider policy challenges arising from these limitations. In this paper, we investigate a specific challenge in building RLHF systems that respect democratic norms. Building on impossibility results in social choice theory, we show that, under fairly broad assumptions, there is no unique voting protocol to universally align AI systems using RLHF through democratic processes. Further, we show that aligning AI agents with the values of all individuals will always violate certain private ethical preferences of an individual user i.e., universal AI alignment using RLHF is impossible. We discuss policy implications for the governance of AI systems built using RLHF: first, the need for mandating transparent voting rules to hold model builders accountable. Second, the need for model builders to focus on developing AI agents that are narrowly aligned to specific user groups.",
      "authors": [
        "Mishra, Abhilash"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages, no figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-24",
      "selected": null,
      "title": "AI Alignment and Social Choice: Fundamental Limitations and Policy Implications",
      "urls": [
        "http://arxiv.org/pdf/2310.16048v1",
        "http://arxiv.org/abs/2310.16048v1",
        "http://arxiv.org/pdf/2310.16048.pdf"
      ]
    },
    {
      "abstract": "The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability with RLHF to learn from unlabeled data, making it flexible for continual preference learning. Our experimental results show that COPF outperforms strong Continuous learning (CL) baselines when it comes to consistently aligning with human preferences on different tasks and domains.",
      "authors": [
        "Zhang, Han",
        "Gui, Lin",
        "Zhai, Yuanzhao",
        "Wang, Hui",
        "Lei, Yu",
        "Xu, Ruifeng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-24",
      "selected": null,
      "title": "COPF: Continual Learning Human Preference through Optimal Policy Fitting",
      "urls": [
        "http://arxiv.org/pdf/2310.15694.pdf",
        "http://arxiv.org/pdf/2310.15694v4",
        "http://arxiv.org/abs/2310.15694v4"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.",
      "authors": [
        "Freedman, Rachel",
        "Svegliato, Justin",
        "Wray, Kyle",
        "Russell, Stuart"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-23",
      "selected": null,
      "title": "Active teacher selection for reinforcement learning from human feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.15288.pdf",
        "http://arxiv.org/pdf/2310.15288v1",
        "http://arxiv.org/abs/2310.15288v1"
      ]
    },
    {
      "abstract": "Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, by using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by the idea of Thompson sampling. Our algorithm minimizes Bayesian regret bound and query complexity, again achieving a near-optimal tradeoff between these two quantities. Computation-wise, similar to the prior Thompson sampling algorithms under the regular RL setting, the main computation primitives of our algorithm are Bayesian supervised learning oracles which have been heavily investigated on the empirical side when applying Thompson sampling algorithms to RL benchmark problems.",
      "authors": [
        "Wu, Runzhe",
        "Sun, Wen"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-23",
      "selected": null,
      "title": "Making RL with Preference-based Feedback Efficient via Randomization",
      "urls": [
        "http://arxiv.org/pdf/2310.14554.pdf",
        "http://arxiv.org/pdf/2310.14554v1",
        "http://arxiv.org/abs/2310.14554v1"
      ]
    },
    {
      "abstract": "Recent advancements in natural language processing by large language models\n(LLMs), such as GPT-4, have been suggested to approach Artificial General\nIntelligence. And yet, it is still under dispute whether LLMs possess similar\nreasoning abilities to humans. This study evaluates GPT-4 and various other\nLLMs in judging the profoundness of mundane, motivational, and pseudo-profound\nstatements. We found a significant statement-to-statement correlation between\nthe LLMs and humans, irrespective of the type of statements and the prompting\ntechnique used. However, LLMs systematically overestimate the profoundness of\nnonsensical statements, with the exception of Tk-instruct, which uniquely\nunderestimates the profoundness of statements. Only few-shot learning prompts,\nas opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.\nFurthermore, this work provides insights into the potential biases induced by\nReinforcement Learning from Human Feedback (RLHF), inducing an increase in the\nbias to overestimate the profoundness of statements.",
      "authors": [
        "Eugenio Herrera-Berg",
        "Tom\u00e1s Browne",
        "Pablo Le\u00f3n-Villagr\u00e1",
        "Marc-Llu\u00eds Vives",
        "Cristian Calderon"
      ],
      "categories": null,
      "citations": null,
      "comments": "5 pages, 3 figures",
      "databases": [
        "arXiv"
      ],
      "doi": "10.18653/v1/2023.emnlp-main.599",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Computation and Language"
        ],
        "title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
      },
      "publication_date": "2023-10-22",
      "selected": null,
      "title": "Large Language Models are biased to overestimate profoundness",
      "urls": [
        "http://dx.doi.org/10.18653/v1/2023.emnlp-main.599",
        "https://aclanthology.org/2023.emnlp-main.599.pdf",
        "http://arxiv.org/abs/2310.14422v1",
        "http://arxiv.org/pdf/2310.14422v1"
      ]
    },
    {
      "abstract": "Sound recognition tools have wide-ranging impacts for deaf and hard of hearing (DHH) people from being informed of safety-critical information (e.g., fire alarms, sirens) to more mundane but still useful information (e.g., door knock, microwave beeps). However, prior sound recognition systems use models that are pre-trained on generic sound datasets and do not adapt well to diverse variations of real-world sounds. We introduce AdaptiveSound, a real-time system for portable devices (e.g., smartphones) that allows DHH users to provide corrective feedback to the sound recognition model to adapt the model to diverse acoustic environments. AdaptiveSound is informed by prior surveys of sound recognition systems, where DHH users strongly desired the ability to provide feedback to a pre-trained sound recognition model to fine-tune it to their environments. Through quantitative experiments and field evaluations with 12 DHH users, we show that AdaptiveSound can achieve a significantly higher accuracy (+14.6%) than prior state-of-the art systems in diverse real-world locations (e.g., homes, parks, streets, and malls) with little end-user effort (about 10 minutes of feedback). \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Do, H.",
        "Dang, Q.",
        "Huang, J.Z.",
        "Jain, D."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3597638.3608390",
      "keywords": [
        "applied machine learning",
        "audio event detection",
        "Human-AI",
        "hard of hearing",
        "reinforcement learning",
        "Deaf",
        "sound awareness",
        "Accessibility",
        "incremental learning",
        "deaf",
        "human-in-the-loop"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400702204",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ASSETS 2023 - Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility"
      },
      "publication_date": "2023-10-22",
      "selected": null,
      "title": "AdaptiveSound: An Interactive Feedback-Loop System to Improve Sound Recognition for Deaf and Hard of Hearing Users",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177891359&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.",
      "authors": [
        "Hejna, Joey",
        "Rafailov, Rafael",
        "Sikchi, Harshit",
        "Finn, Chelsea",
        "Niekum, Scott",
        "Knox, W. Bradley",
        "Sadigh, Dorsa"
      ],
      "categories": null,
      "citations": null,
      "comments": "Code released at https://github.com/jhejna/cpl. Edited 10/23 only to\n  fix typo in the title",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-20",
      "selected": null,
      "title": "Contrastive Preference Learning: Learning from Human Feedback without RL",
      "urls": [
        "http://arxiv.org/abs/2310.13639v2",
        "http://arxiv.org/pdf/2310.13639v2",
        "http://arxiv.org/pdf/2310.13639.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to use and more effective. A core piece of the RLHF process is the training and utilization of a model of human preferences that acts as a reward function for optimization. This approach, which operates at the intersection of many stakeholders and academic disciplines, remains poorly understood. RLHF reward models are often cited as being central to achieving performance, yet very few descriptors of capabilities, evaluations, training methods, or open-source models exist. Given this lack of information, further study and transparency is needed for learned RLHF reward models. In this paper, we illustrate the complex history of optimizing preferences, and articulate lines of inquiry to understand the sociotechnical context of reward models. In particular, we highlight the ontological differences between costs, rewards, and preferences at stake in RLHF's foundations, related methodological tensions, and possible research directions to improve general understanding of how reward models function.",
      "authors": [
        "Lambert, Nathan",
        "Gilbert, Thomas Krendl",
        "Zick, Tom"
      ],
      "categories": null,
      "citations": null,
      "comments": "14 pages, 3 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-20",
      "selected": null,
      "title": "The History and Risks of Reinforcement Learning and Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.13595.pdf",
        "http://arxiv.org/pdf/2310.13595v2",
        "http://arxiv.org/abs/2310.13595v2"
      ]
    },
    {
      "abstract": "Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel \\textit{probabilistic ranking} and \\textit{contextual ranking} approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the instruction-tuned LLM. The resulting model, which we call \\textbf{Tuna}, consistently improves the performance on Super Natural Instructions (119 test tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results than several strong reinforcement learning baselines. Our code and data are available at \\url{ https://github.com/microsoft/LMOps}.",
      "authors": [
        "Li, Haoran",
        "Liu, Yiran",
        "Zhang, Xingxing",
        "Lu, Wei",
        "Wei, Furu"
      ],
      "categories": null,
      "citations": 0,
      "comments": "EMNLP 2023, code and data are available at\n  https://github.com/microsoft/LMOps",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 18,
      "pages": "15146-15163",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798891760615",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Findings of the Association for Computational Linguistics: EMNLP 2023"
      },
      "publication_date": "2023-10-20",
      "selected": null,
      "title": "Tuna: Instruction Tuning using Feedback from Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2310.13385v1",
        "http://arxiv.org/pdf/2310.13385.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183299517&origin=inward",
        "http://arxiv.org/abs/2310.13385v1"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
      "authors": [
        "Ma, Yecheng Jason",
        "Liang, William",
        "Wang, Guanzhi",
        "Huang, De-An",
        "Bastani, Osbert",
        "Jayaraman, Dinesh",
        "Zhu, Yuke",
        "Fan, Linxi",
        "Anandkumar, Anima"
      ],
      "categories": null,
      "citations": null,
      "comments": "Project website and open-source code:\n  https://eureka-research.github.io/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-19",
      "selected": null,
      "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2310.12931.pdf",
        "http://arxiv.org/abs/2310.12931v1",
        "http://arxiv.org/pdf/2310.12931v1"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second ``baseline'' prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
      "authors": [
        "Rocamonde, Juan",
        "Montesinos, Victoriano",
        "Nava, Elvis",
        "Perez, Ethan",
        "Lindner, David"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-19",
      "selected": null,
      "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2310.12921v1",
        "http://arxiv.org/pdf/2310.12921.pdf",
        "http://arxiv.org/abs/2310.12921v1"
      ]
    },
    {
      "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.",
      "authors": [
        "Dai, Josef",
        "Pan, Xuehai",
        "Sun, Ruiyang",
        "Ji, Jiaming",
        "Xu, Xinbo",
        "Liu, Mickel",
        "Wang, Yizhou",
        "Yang, Yaodong"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-19",
      "selected": null,
      "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.12773v1",
        "http://arxiv.org/pdf/2310.12773.pdf",
        "http://arxiv.org/abs/2310.12773v1"
      ]
    },
    {
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown potential in qualitative tasks where clear objectives are lacking. However, its effectiveness is not fully realized when it is conceptualized merely as a tool to optimize average human preferences, especially in generative tasks that demand diverse model responses. Meanwhile, Quality Diversity (QD) algorithms excel at identifying diverse and high-quality solutions but often rely on manually crafted diversity metrics. This paper introduces Quality Diversity through Human Feedback (QDHF), a novel approach integrating human feedback into the QD framework. QDHF infers diversity metrics from human judgments of similarity among solutions, thereby enhancing the applicability and effectiveness of QD algorithms. Our empirical studies show that QDHF significantly outperforms state-of-the-art methods in automatic diversity discovery and matches the efficacy of using manually crafted metrics for QD on standard benchmarks in robotics and reinforcement learning. Notably, in a latent space illumination task, QDHF substantially enhances the diversity in images generated by a diffusion model and was more favorably received in user studies. We conclude by analyzing QDHF's scalability and the quality of its derived diversity metrics, emphasizing its potential to improve exploration and diversity in complex, open-ended optimization tasks. Source code is available on GitHub: https://github.com/ld-ing/qdhf.",
      "authors": [
        "Ding, Li",
        "Zhang, Jenny",
        "Clune, Jeff",
        "Spector, Lee",
        "Lehman, Joel"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS 2023: ALOE Workshop (Spotlight)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-18",
      "selected": null,
      "title": "Quality Diversity through Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.12103.pdf",
        "http://arxiv.org/pdf/2310.12103v2",
        "http://arxiv.org/abs/2310.12103v2"
      ]
    },
    {
      "abstract": "The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation. In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\\Psi$PO by setting $\\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.",
      "authors": [
        "Azar, Mohammad Gheshlaghi",
        "Rowland, Mark",
        "Piot, Bilal",
        "Guo, Daniel",
        "Calandriello, Daniele",
        "Valko, Michal",
        "Munos, R\u00e9mi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-18",
      "selected": null,
      "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
      "urls": [
        "http://arxiv.org/pdf/2310.12036v2",
        "http://arxiv.org/pdf/2310.12036.pdf",
        "http://arxiv.org/abs/2310.12036v2"
      ]
    },
    {
      "abstract": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.",
      "authors": [
        "Zheng, Rui",
        "Shen, Wei",
        "Hua, Yuan",
        "Lai, Wenbin",
        "Dou, Shihan",
        "Zhou, Yuhao",
        "Xi, Zhiheng",
        "Wang, Xiao",
        "Huang, Haoran",
        "Gui, Tao",
        "Zhang, Qi",
        "Huang, Xuanjing"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-18",
      "selected": null,
      "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
      "urls": [
        "http://arxiv.org/pdf/2310.11971v3",
        "http://arxiv.org/abs/2310.11971v3",
        "http://arxiv.org/pdf/2310.11971.pdf"
      ]
    },
    {
      "abstract": "While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.",
      "authors": [
        "Jang, Joel",
        "Kim, Seungone",
        "Lin, Bill Yuchen",
        "Wang, Yizhong",
        "Hessel, Jack",
        "Zettlemoyer, Luke",
        "Hajishirzi, Hannaneh",
        "Choi, Yejin",
        "Ammanabrolu, Prithviraj"
      ],
      "categories": null,
      "citations": null,
      "comments": "Preprint",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-17",
      "selected": null,
      "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging",
      "urls": [
        "http://arxiv.org/pdf/2310.11564.pdf",
        "http://arxiv.org/abs/2310.11564v1",
        "http://arxiv.org/pdf/2310.11564v1"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).",
      "authors": [
        "Shayegani, Erfan",
        "Mamun, Md Abdullah Al",
        "Fu, Yu",
        "Zaree, Pedram",
        "Dong, Yue",
        "Abu-Ghazaleh, Nael"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-16",
      "selected": null,
      "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
      "urls": [
        "http://arxiv.org/pdf/2310.10844.pdf",
        "http://arxiv.org/abs/2310.10844v1",
        "http://arxiv.org/pdf/2310.10844v1"
      ]
    },
    {
      "abstract": "A recently popular approach to solving reinforcement learning is with data from human preferences. In fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (OPE). Such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. Despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any low-dimensional manifold structure in the Markov decision process and obtain a sample-efficient estimator without suffering from the curse of high data ambient dimensionality. Under the assumption of high reward smoothness, our results \\textit{almost align with the classical OPE results with observable reward data}. To the best of our knowledge, this is the first result that establishes a \\textit{provably efficient} guarantee for off-policy evaluation with RLHF.",
      "authors": [
        "Li, Zihao",
        "Ji, Xiang",
        "Chen, Minshuo",
        "Wang, Mengdi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-16",
      "selected": null,
      "title": "Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks",
      "urls": [
        "http://arxiv.org/pdf/2310.10556.pdf",
        "http://arxiv.org/abs/2310.10556v1",
        "http://arxiv.org/pdf/2310.10556v1"
      ]
    },
    {
      "abstract": "Alignment is crucial for training large language models. The predominant strategy is Reinforcement Learning from Human Feedback (RLHF), with Proximal Policy Optimization (PPO) as the de-facto algorithm. Yet, PPO is known to struggle with computational inefficiency, a challenge that this paper aims to address. We identify three important properties of RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on these properties, we develop ReMax, a new algorithm tailored for RLHF. The design of ReMax builds on the celebrated algorithm REINFORCE but is enhanced with a new variance-reduction technique. ReMax offers threefold advantages over PPO: first, it is simple to implement with just 6 lines of code. It further eliminates more than 4 hyper-parameters in PPO, which are laborious to tune. Second, ReMax reduces memory usage by about 50%. To illustrate, PPO runs out of memory when fine-tuning a Llama2-7B model on A100-80GB GPUs, whereas ReMax can support the training. Even though memory-efficient techniques (e.g., ZeRO and offload) are employed for PPO to afford training, ReMax can utilize a larger batch size to increase throughput. Third, in terms of wall-clock time, PPO is about twice as slow as ReMax per iteration. Importantly, these improvements do not sacrifice task performance. We hypothesize that these advantages can be maintained in larger-scale models.",
      "authors": [
        "Li, Ziniu",
        "Xu, Tian",
        "Zhang, Yushun",
        "Lin, Zhihang",
        "Yu, Yang",
        "Sun, Ruoyu",
        "Luo, Zhi-Quan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-16",
      "selected": null,
      "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2310.10505v3",
        "http://arxiv.org/pdf/2310.10505.pdf",
        "http://arxiv.org/abs/2310.10505v3"
      ]
    },
    {
      "abstract": "In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.",
      "authors": [
        "Saito, Keita",
        "Wachi, Akifumi",
        "Wataoka, Koki",
        "Akimoto, Youhei"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-16",
      "selected": null,
      "title": "Verbosity Bias in Preference Labeling by Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2310.10076.pdf",
        "http://arxiv.org/pdf/2310.10076v1",
        "http://arxiv.org/abs/2310.10076v1"
      ]
    },
    {
      "abstract": "Assistant AI agents should be capable of rapidly acquiring novel skills and adapting to new user preferences. Traditional frameworks like imitation learning and reinforcement learning do not facilitate this capability because they support only low-level, inefficient forms of communication. In contrast, humans communicate with progressive efficiency by defining and sharing abstract intentions. Reproducing similar capability in AI agents, we develop a novel learning framework named Communication-Efficient Interactive Learning (CEIL). By equipping a learning agent with an abstract, dynamic language and an intrinsic motivation to learn with minimal communication effort, CEIL leads to emergence of a human-like pattern where the learner and the teacher communicate progressively efficiently by exchanging increasingly more abstract intentions. CEIL demonstrates impressive performance and communication efficiency on a 2D MineCraft domain featuring long-horizon decision-making tasks. Agents trained with CEIL quickly master new tasks, outperforming non-hierarchical and hierarchical imitation learning by up to 50% and 20% in absolute success rate, respectively, given the same number of interactions with the teacher. Especially, the framework performs robustly with teachers modeled after human pragmatic communication behavior.",
      "authors": [
        "Zheng, Ruijie",
        "Nguyen, Khanh",
        "Daum\u00e9 III, Hal",
        "Huang, Furong",
        "Narasimhan, Karthik"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-13",
      "selected": null,
      "title": "Progressively Efficient Learning",
      "urls": [
        "http://arxiv.org/pdf/2310.13004v1",
        "http://arxiv.org/pdf/2310.13004.pdf",
        "http://arxiv.org/abs/2310.13004v1"
      ]
    },
    {
      "abstract": "Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. Inspired by previous work which identified that feed-forward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of FFNs} by knowledge enhancement and alignment respectively. We first propose \\textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs' expressions in responses by aligning with gold knowledge for the factual consistency preference. To comprehensively assess the factual consistency and dialogue quality of responses, we employ extensive automatic measures and human evaluations including sophisticated fine-grained NLI-based metrics. Experimental results on WoW and CMU\\_DoG datasets demonstrate that our methods efficiently enhance the ability of the FFN module to convey factual knowledge, validating the efficacy of improving factual consistency for knowledge-grounded dialogue systems.",
      "authors": [
        "Xue, Boyang",
        "Wang, Weichao",
        "Wang, Hongru",
        "Mi, Fei",
        "Wang, Rui",
        "Wang, Yasheng",
        "Shang, Lifeng",
        "Jiang, Xin",
        "Liu, Qun",
        "Wong, Kam-Fai"
      ],
      "categories": null,
      "citations": 0,
      "comments": "EMNLP2023 Findings",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 16,
      "pages": "7829-7844",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798891760615",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Findings of the Association for Computational Linguistics: EMNLP 2023"
      },
      "publication_date": "2023-10-12",
      "selected": null,
      "title": "Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment",
      "urls": [
        "http://arxiv.org/pdf/2310.08372.pdf",
        "http://arxiv.org/abs/2310.08372v3",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183300575&origin=inward",
        "http://arxiv.org/pdf/2310.08372v3"
      ]
    },
    {
      "abstract": "Large language models (LLMs) fine-tuned by reinforcement learning from human feedback (RLHF) are becoming more widely deployed. We coin the term $\\textit{Implicit Reward Model}$ (IRM) to refer to the changes that occur to an LLM during RLHF that result in high-reward generations. We interpret IRMs, and measure their divergence from the RLHF reward model used in the fine-tuning process that induced them. By fitting a linear function to an LLM's IRM, a reward model with the same type signature as the RLHF reward model is constructed, allowing for direct comparison. Additionally, we validate our construction of the IRM through cross-comparison with classifications of features generated by an LLM based on their relevance to the RLHF reward model. Better comprehending IRMs can help minimize discrepencies between LLM behavior and training objectives, which we believe to be an essential component of the $\\textit{safety}$ and $\\textit{alignment}$ of LLMs.",
      "authors": [
        "Marks, Luke",
        "Abdullah, Amir",
        "Neo, Clement",
        "Arike, Rauno",
        "Torr, Philip",
        "Barez, Fazl"
      ],
      "categories": null,
      "citations": null,
      "comments": "19 pages, 5 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-12",
      "selected": null,
      "title": "Beyond Training Objectives: Interpreting Reward Model Divergence in Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2310.08164v4",
        "http://arxiv.org/abs/2310.08164v4",
        "http://arxiv.org/pdf/2310.08164.pdf"
      ]
    },
    {
      "abstract": "Dialogue generation task is one of the popular research topics in the field of natural language processing. However, how to improve the quality of model generated responses with the user feedback in the dialogue generation task is still one of the difficulties in the...",
      "authors": [
        "Cai, Mingxiu",
        "Wang, Daling",
        "Feng, Shi",
        "Zhang, Yifei"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-3-031-44699-3_34",
      "keywords": [
        "Dialogue Generation",
        "Commonsense Inference",
        "Contrastive Search",
        "User Feedback",
        "RLHF"
      ],
      "number_of_pages": 12,
      "pages": "376-387",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-3-031-44698-6",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12\u201315, 2023, Proceedings, Part III"
      },
      "publication_date": "2023-10-12",
      "selected": null,
      "title": "Generating Better Responses from\u00a0User Feedback via\u00a0Reinforcement Learning and\u00a0Commonsense Inference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174532270&origin=inward",
        "https://dl.acm.org/doi/10.1007/978-3-031-44699-3_34",
        "https://link.springer.com/content/pdf/10.1007/978-3-031-44699-3_34.pdf"
      ]
    },
    {
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.",
      "authors": [
        "Fu, Jiayi",
        "Lin, Lei",
        "Gao, Xiaoyang",
        "Liu, Pengli",
        "Chen, Zhengzong",
        "Yang, Zhirui",
        "Zhang, Shengnan",
        "Zheng, Xue",
        "Li, Yan",
        "Liu, Yuliang",
        "Ye, Xucheng",
        "Liao, Yiqiao",
        "Liao, Chao",
        "Chen, Bin",
        "Song, Chengru",
        "Wan, Junchen",
        "Lin, Zijia",
        "Zhang, Fuzheng",
        "Wang, Zhongyuan",
        "Zhang, Di",
        "Gai, Kun"
      ],
      "categories": null,
      "citations": null,
      "comments": "technical report. arXiv admin note: text overlap with\n  arXiv:2306.16636 by other authors",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-11",
      "selected": null,
      "title": "KwaiYiiMath: Technical Report",
      "urls": [
        "http://arxiv.org/abs/2310.07488v2",
        "http://arxiv.org/pdf/2310.07488.pdf",
        "http://arxiv.org/pdf/2310.07488v2"
      ]
    },
    {
      "abstract": "Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we develop an immediate human reward (IHR) reconstruction approach, regularized by environmental knowledge distilled in a latent space that captures the underlying dynamics of state transitions as well as issuing HF signals. Our approach has been tested over two real-world experiments, adaptive in-vivo neurostimulation and intelligent tutoring, as well as in a simulation environment (visual Q&A). Results show that our approach significantly improves the performance toward estimating HF signals accurately, compared to directly applying (variants of) existing OPE methods.",
      "authors": [
        "Gao, Qitong",
        "Gao, Ge",
        "Dong, Juncheng",
        "Tarokh, Vahid",
        "Chi, Min",
        "Pajic, Miroslav"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-11",
      "selected": null,
      "title": "Off-Policy Evaluation for Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.07123v2",
        "http://arxiv.org/pdf/2310.07123.pdf",
        "http://arxiv.org/abs/2310.07123v2"
      ]
    },
    {
      "abstract": "Diversity plays a significant role in many problems, such as ensemble learning, reinforcement learning, and combinatorial optimization. How to define the diversity measure is a longstanding problem. Many methods rely on expert experience to define a proper behavior space and then obtain the diversity measure, which is, however, challenging in many scenarios. In this paper, we propose the problem of learning a behavior space from human feedback and present a general method called Diversity from Human Feedback (DivHF) to solve it. DivHF learns a behavior descriptor consistent with human preference by querying human feedback. The learned behavior descriptor can be combined with any distance measure to define a diversity measure. We demonstrate the effectiveness of DivHF by integrating it with the Quality-Diversity optimization algorithm MAP-Elites and conducting experiments on the QDax suite. The results show that DivHF learns a behavior space that aligns better with human requirements compared to direct data-driven approaches and leads to more diverse solutions under human preference. Our contributions include formulating the problem, proposing the DivHF method, and demonstrating its effectiveness through experiments.",
      "authors": [
        "Wang, Ren-Jian",
        "Xue, Ke",
        "Wang, Yutong",
        "Yang, Peng",
        "Fu, Haobo",
        "Fu, Qiang",
        "Qian, Chao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-10",
      "selected": null,
      "title": "Diversity from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.06648v2",
        "http://arxiv.org/pdf/2310.06648.pdf",
        "http://arxiv.org/abs/2310.06648v2"
      ]
    },
    {
      "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",
      "authors": [
        "Kirk, Robert",
        "Mediratta, Ishita",
        "Nalmpantis, Christoforos",
        "Luketina, Jelena",
        "Hambro, Eric",
        "Grefenstette, Edward",
        "Raileanu, Roberta"
      ],
      "categories": null,
      "citations": null,
      "comments": "Code available here: https://github.com/facebookresearch/rlfh-gen-div",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-10",
      "selected": null,
      "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
      "urls": [
        "http://arxiv.org/pdf/2310.06452v2",
        "http://arxiv.org/pdf/2310.06452.pdf",
        "http://arxiv.org/abs/2310.06452v2"
      ]
    },
    {
      "abstract": "The edge-cloud network serves as a fundamental infrastructure for deploying Ubiquitous Edge Intelligence (UEI) applications, presenting critical demands of energy conservation and job acceleration. Current solutions often rely on complete prior knowledge of static resource information, which limits their adaptation to the dynamic online environment of UEI deployment. This challenge motivates us to stand from a novel perspective by jointly considering application-level properties and energy-level requirements during UEI task runtime. Our primary objective is to optimize the coordination strategy of model partition and function assignment, which can be formulated as a green workload coordination problem. Following this target, we propose GreenEdge, an innovative framework that simultaneously improves job processing speed and energy efficiency. The key insight presented by GreenEdge is the utilization of Reinforcement Learning with Human Feedback (RLHF) methodology, which draws inspiration from the human-in-the-loop philosophy. Evaluations with real-world applications demonstrate that our GreenEdge framework exhibits remarkable superiority over previous methods, promising significant advancements in implementing efficient UEI frameworks and applications. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Tina Ziting Xu",
        "Adolf K. Y. Ng"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/VTC2023-Fall60731.2023.10333403",
      "keywords": [
        "Ubiquitous Edge Intelligence",
        "Edge-cloud Networks",
        "Green Workload Coordination",
        "Large Language Models"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-2929-2",
        "issn": "1090-3038",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE 98th Vehicular Technology Conference (VTC2023-Fall)"
      },
      "publication_date": "2023-10-10",
      "selected": null,
      "title": "GreenEdge: Neural-enhanced Green Workload Coordination for Ubiquitous Edge Intelligence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85181170297&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10333403"
      ]
    },
    {
      "abstract": "It is of great potential for robots to replace human workers in manual finishing tasks using the deep reinforcement learning (DRL) technique. However, due to the high cost of the trial-and-error learning process in the real world, it is important to pre-train virtual...",
      "authors": [
        "Xiao, Mubang",
        "Luo, Xiao",
        "Ding, Ye"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-981-99-6486-4_44",
      "keywords": [
        "deep reinforcement learning",
        "contact modeling",
        "Robotic polishing"
      ],
      "number_of_pages": 12,
      "pages": "521-532",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-981-99-6485-7",
        "issn": "03029743",
        "publisher": "Springer-Verlag",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Intelligent Robotics and Applications: 16th International Conference, ICIRA 2023, Hangzhou, China, July 5\u20137, 2023, Proceedings, Part II"
      },
      "publication_date": "2023-10-10",
      "selected": null,
      "title": "Contact Force and\u00a0Material Removal Simulation for\u00a0a\u00a0Virtual Robotic Polishing Platform",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-981-99-6486-4_44.pdf",
        "https://dl.acm.org/doi/10.1007/978-981-99-6486-4_44",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175987862&origin=inward"
      ]
    },
    {
      "abstract": "Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research. Highlighted Takeaways: 1. RLHF is Online Inverse RL with Offline Demonstration Data. 2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error. 3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive. 4. The policy learning in RLHF is more challenging than conventional problems studied in IRL due to their high action dimensionality and feedback sparsity. 5. The main superiority of PPO over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.",
      "authors": [
        "Sun, Hao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-09",
      "selected": null,
      "title": "Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond",
      "urls": [
        "http://arxiv.org/abs/2310.06147v1",
        "http://arxiv.org/pdf/2310.06147v1",
        "http://arxiv.org/pdf/2310.06147.pdf"
      ]
    },
    {
      "abstract": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",
      "authors": [
        "Sun, Zhiqing",
        "Shen, Yikang",
        "Zhang, Hongxin",
        "Zhou, Qinhong",
        "Chen, Zhenfang",
        "Cox, David",
        "Yang, Yiming",
        "Gan, Chuang"
      ],
      "categories": null,
      "citations": null,
      "comments": "Project page: https://github.com/IBM/SALMON",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-09",
      "selected": null,
      "title": "SALMON: Self-Alignment with Principle-Following Reward Models",
      "urls": [
        "http://arxiv.org/pdf/2310.05910v1",
        "http://arxiv.org/pdf/2310.05910.pdf",
        "http://arxiv.org/abs/2310.05910v1"
      ]
    },
    {
      "abstract": "In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM. Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model. Extensive experiments on two human-centric NLG tasks, i.e., emotional support conversation and integrity \"Rule-of-Thumb\" generation, show that our method consistently exceeds previous SOTA models in both automatic and human evaluations.",
      "authors": [
        "Wang, Jiashuo",
        "Wang, Haozhao",
        "Sun, Shichao",
        "Li, Wenjie"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-09",
      "selected": null,
      "title": "Aligning Language Models with Human Preferences via a Bayesian Approach",
      "urls": [
        "http://arxiv.org/pdf/2310.05782v3",
        "http://arxiv.org/pdf/2310.05782.pdf",
        "http://arxiv.org/abs/2310.05782v3"
      ]
    },
    {
      "abstract": "Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B",
      "authors": [
        "Dong, Yi",
        "Wang, Zhilin",
        "Sreedhar, Makesh Narsimhan",
        "Wu, Xianchao",
        "Kuchaiev, Oleksii"
      ],
      "categories": null,
      "citations": 0,
      "comments": "Findings of EMNLP 2023",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 14,
      "pages": "11275-11288",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798891760615",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Findings of the Association for Computational Linguistics: EMNLP 2023"
      },
      "publication_date": "2023-10-09",
      "selected": null,
      "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183304167&origin=inward",
        "http://arxiv.org/abs/2310.05344v1",
        "http://arxiv.org/pdf/2310.05344.pdf",
        "http://arxiv.org/pdf/2310.05344v1"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bias-focused expert, disrupting the flow of semantic information. Experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.",
      "authors": [
        "Shen, Wei",
        "Zheng, Rui",
        "Zhan, Wenyu",
        "Zhao, Jun",
        "Dou, Shihan",
        "Gui, Tao",
        "Zhang, Qi",
        "Huang, Xuanjing"
      ],
      "categories": null,
      "citations": 0,
      "comments": "EMNLP 2023 findings, Length Bias in RLHF, Mitigate bias in reward\n  modeling",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 15,
      "pages": "2859-2873",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798891760615",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Findings of the Association for Computational Linguistics: EMNLP 2023"
      },
      "publication_date": "2023-10-08",
      "selected": null,
      "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.05199v5",
        "http://arxiv.org/pdf/2310.05199.pdf",
        "http://arxiv.org/abs/2310.05199v5",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183289237&origin=inward"
      ]
    },
    {
      "abstract": "Large language models are typically aligned with human preferences by optimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally expressed by Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.",
      "authors": [
        "Moskovitz, Ted",
        "Singh, Aaditya K.",
        "Strouse, DJ",
        "Sandholm, Tuomas",
        "Salakhutdinov, Ruslan",
        "Dragan, Anca D.",
        "McAleer, Stephen"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-06",
      "selected": null,
      "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
      "urls": [
        "http://arxiv.org/pdf/2310.04373v2",
        "http://arxiv.org/abs/2310.04373v2",
        "http://arxiv.org/pdf/2310.04373.pdf"
      ]
    },
    {
      "abstract": "Recommender systems trained on offline historical user behaviors are embracing conversational techniques to online query user preference. Unlike prior conversational recommendation approaches that systemically combine conversational and recommender parts through a reinforcement learning framework, we propose CORE, a new offline-training and online-checking paradigm that bridges a COnversational agent and REcommender systems via a unified uncertainty minimization framework. It can benefit any recommendation platform in a plug-and-play style. Here, CORE treats a recommender system as an offline relevance score estimator to produce an estimated relevance score for each item; while a conversational agent is regarded as an online relevance score checker to check these estimated scores in each session. We define uncertainty as the summation of unchecked relevance scores. In this regard, the conversational agent acts to minimize uncertainty via querying either attributes or items. Based on the uncertainty minimization framework, we derive the expected certainty gain of querying each attribute and item, and develop a novel online decision tree algorithm to decide what to query at each turn. Experimental results on 8 industrial datasets show that CORE could be seamlessly employed on 9 popular recommendation approaches. We further demonstrate that our conversational agent could communicate as a human if empowered by a pre-trained large language model.",
      "authors": [
        "Jin, Jiarui",
        "Chen, Xianyu",
        "Ye, Fanghua",
        "Yang, Mengyue",
        "Feng, Yue",
        "Zhang, Weinan",
        "Yu, Yong",
        "Wang, Jun"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-06",
      "selected": null,
      "title": "Lending Interaction Wings to Recommender Systems with Conversational Agents",
      "urls": [
        "http://arxiv.org/pdf/2310.04230v1",
        "http://arxiv.org/pdf/2310.04230.pdf",
        "http://arxiv.org/abs/2310.04230v1"
      ]
    },
    {
      "abstract": "Reward design for reinforcement learning agents can be difficult in situations where one not only wants the agent to achieve some effect in the world but where one also cares about how that effect is achieved. For example, we might wish for an agent to adhere to a tacit understanding of commonsense, align itself to a preference for how to behave for purposes of safety, or take on a particular role in an interactive game. Storytelling is a mode for communicating tacit procedural knowledge. We introduce a technique, Story Shaping, in which a reinforcement learning agent infers tacit knowledge from an exemplar story of how to accomplish a task and intrinsically rewards itself for performing actions that make its current environment adhere to that of the inferred story world. Specifically, Story Shaping infers a knowledge graph representation of the world state from observations, and also infers a knowledge graph from the exemplar story. An intrinsic reward is generated based on the similarity between the agent\u00e2\u0080\u0099s inferred world state graph and the inferred story world graph. We conducted experiments in text-based games requiring commonsense reasoning and shaping the behaviors of agents as virtual game characters. Copyright \u00c2\u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "authors": [
        "Peng, X.",
        "Cui, C.",
        "Zhou, W.",
        "Jia, R.",
        "Riedl, M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 11,
      "pages": "326-336",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "157735883X",
        "issn": "2326909X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - AAAI Artificial Intelligence and Interactive Digital Entertainment Conference, AIIDE"
      },
      "publication_date": "2023-10-06",
      "selected": null,
      "title": "Story Shaping: Teaching Agents Human-Like Behavior with Stories",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175403188&origin=inward"
      ]
    },
    {
      "abstract": "Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more \"helpful\" for tasks like web question answering, summarization, and multi-turn dialogue. When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can achieve the same downstream improvements as RLHF without increasing length. While our interventions mitigate length increases, they aren't uniformly effective across settings. Furthermore, we find that even running RLHF with a reward based solely on length can reproduce most of the downstream improvements over the initial policy model, showing that reward models in these settings have a long way to go.",
      "authors": [
        "Singhal, Prasann",
        "Goyal, Tanya",
        "Xu, Jiacheng",
        "Durrett, Greg"
      ],
      "categories": null,
      "citations": null,
      "comments": "20 pages, 12 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-05",
      "selected": null,
      "title": "A Long Way to Go: Investigating Length Correlations in RLHF",
      "urls": [
        "http://arxiv.org/pdf/2310.03716v1",
        "http://arxiv.org/pdf/2310.03716.pdf",
        "http://arxiv.org/abs/2310.03716v1"
      ]
    },
    {
      "abstract": "A single language model (LM), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct reward models (RMs) for each dimension (e.g., helpfulness, harmlessness, or honesty). Different LMs can then be optimized for different preferences using multi-objective RLHF (MORLHF) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives with minimal overheads. Essentially, MODPO folds language modeling directly into reward modeling, training LMs as implicit collective reward models (cRMs) that combine all objectives with specific weightings. While theoretically guaranteed to produce the same optimal solutions as MORLHF, MODPO is practically more stable and computationally efficient. Empirical results from safety alignment and long-form question answering confirm that MODPO matches or outperforms existing methods, consistently producing a Pareto front of LMs that cater to diverse preferences with 3 times less computational resources compared to MORLHF.",
      "authors": [
        "Zhou, Zhanhui",
        "Liu, Jie",
        "Yang, Chao",
        "Shao, Jing",
        "Liu, Yu",
        "Yue, Xiangyu",
        "Ouyang, Wanli",
        "Qiao, Yu"
      ],
      "categories": null,
      "citations": null,
      "comments": "Multi-Objective Direct Preference Optimization for LLMs",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-05",
      "selected": null,
      "title": "Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization",
      "urls": [
        "http://arxiv.org/abs/2310.03708v3",
        "http://arxiv.org/pdf/2310.03708v3",
        "http://arxiv.org/pdf/2310.03708.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the \"true\" reward, these learned reward models are susceptible to \\textit{overoptimization}. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger \"gold\" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.",
      "authors": [
        "Coste, Thomas",
        "Anwar, Usman",
        "Kirk, Robert",
        "Krueger, David"
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages, 12 figures (excluding appendix). Submitted to ICLR 2024",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-04",
      "selected": null,
      "title": "Reward Model Ensembles Help Mitigate Overoptimization",
      "urls": [
        "http://arxiv.org/abs/2310.02743v1",
        "http://arxiv.org/pdf/2310.02743.pdf",
        "http://arxiv.org/pdf/2310.02743v1"
      ]
    },
    {
      "abstract": "We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, $\\hat{A^*_r}$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of $\\hat{A^*_r}$ is less desirable than the appropriate and simpler approach of greedy maximization of $\\hat{A^*_r}$. From the perspective of the regret preference model, we also provide a clearer interpretation of fine tuning contemporary large language models with RLHF. This paper overall provides insight regarding why learning under the partial return preference model tends to work so well in practice, despite it conforming poorly to how humans give preferences.",
      "authors": [
        "Knox, W. Bradley",
        "Hatgis-Kessell, Stephane",
        "Adalgeirsson, Sigurdur Orn",
        "Booth, Serena",
        "Dragan, Anca",
        "Stone, Peter",
        "Niekum, Scott"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages (16 pages with references and appendix), 11 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-03",
      "selected": null,
      "title": "Learning Optimal Advantage from Preferences and Mistaking it for Reward",
      "urls": [
        "http://arxiv.org/pdf/2310.02456v1",
        "http://arxiv.org/pdf/2310.02456.pdf",
        "http://arxiv.org/abs/2310.02456v1"
      ]
    },
    {
      "abstract": "Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director for preference aligning at the inference phase. We evaluate AlignDiff on various locomotion tasks and demonstrate its superior performance on preference matching, switching, and covering compared to other baselines. Its capability of completing unseen downstream tasks under human instructions also showcases the promising potential for human-AI collaboration. More visualization videos are released on https://aligndiff.github.io/.",
      "authors": [
        "Dong, Zibin",
        "Yuan, Yifu",
        "Hao, Jianye",
        "Ni, Fei",
        "Mu, Yao",
        "Zheng, Yan",
        "Hu, Yujing",
        "Lv, Tangjie",
        "Fan, Changjie",
        "Hu, Zhipeng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-03",
      "selected": null,
      "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model",
      "urls": [
        "http://arxiv.org/abs/2310.02054v2",
        "http://arxiv.org/pdf/2310.02054v2",
        "http://arxiv.org/pdf/2310.02054.pdf"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is \"could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.",
      "authors": [
        "Zhang, Hangfan",
        "Guo, Zhimeng",
        "Zhu, Huaisheng",
        "Cao, Bochuan",
        "Lin, Lu",
        "Jia, Jinyuan",
        "Chen, Jinghui",
        "Wu, Dinghao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-02",
      "selected": null,
      "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?",
      "urls": [
        "http://arxiv.org/pdf/2310.01581v1",
        "http://arxiv.org/pdf/2310.01581.pdf",
        "http://arxiv.org/abs/2310.01581v1"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) has become a pivot technique in aligning large language models (LLMs) with human preferences. In RLHF practice, preference data plays a crucial role in bridging human proclivity and LLMs. However, the scarcity of diverse, naturalistic datasets of human preferences on LLM outputs at scale poses a great challenge to RLHF as well as feedback learning research within the open-source community. Current preference datasets, either proprietary or limited in size and prompt variety, result in limited RLHF adoption in open-source models and hinder further exploration. In this study, we propose ULTRAFEEDBACK, a large-scale, high-quality, and diversified preference dataset designed to overcome these limitations and foster RLHF development. To create ULTRAFEEDBACK, we compile a diverse array of instructions and models from multiple sources to produce comparative data. We meticulously devise annotation instructions and employ GPT-4 to offer detailed feedback in both numerical and textual forms. ULTRAFEEDBACK establishes a reproducible and expandable preference data construction pipeline, serving as a solid foundation for future RLHF and feedback learning research. Utilizing ULTRAFEEDBACK, we train various models to demonstrate its effectiveness, including the reward model UltraRM, chat language model UltraLM-13B-PPO, and critique model UltraCM. Experimental results indicate that our models outperform existing open-source models, achieving top performance across multiple benchmarks. Our data and models are available at https://github.com/thunlp/UltraFeedback.",
      "authors": [
        "Cui, Ganqu",
        "Yuan, Lifan",
        "Ding, Ning",
        "Yao, Guanming",
        "Zhu, Wei",
        "Ni, Yuan",
        "Xie, Guotong",
        "Liu, Zhiyuan",
        "Sun, Maosong"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-02",
      "selected": null,
      "title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
      "urls": [
        "http://arxiv.org/pdf/2310.01377v1",
        "http://arxiv.org/abs/2310.01377v1",
        "http://arxiv.org/pdf/2310.01377.pdf"
      ]
    },
    {
      "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named \\name, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We anticipate that this publicly available dataset will facilitate and inspire further research advancements in the field.",
      "authors": [
        "Li, Lei",
        "Chai, Yekun",
        "Wang, Shuohuan",
        "Sun, Yu",
        "Tian, Hao",
        "Zhang, Ningyu",
        "Wu, Hua"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-02",
      "selected": null,
      "title": "Tool-Augmented Reward Modeling",
      "urls": [
        "http://arxiv.org/abs/2310.01045v1",
        "http://arxiv.org/pdf/2310.01045.pdf",
        "http://arxiv.org/pdf/2310.01045v1"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.",
      "authors": [
        "Wang, Ziqi",
        "Hou, Le",
        "Lu, Tianjian",
        "Wu, Yuexin",
        "Li, Yunxuan",
        "Yu, Hongkun",
        "Ji, Heng"
      ],
      "categories": null,
      "citations": null,
      "comments": "28 pages, 5 figures, 4 tables",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-02",
      "selected": null,
      "title": "Enable Language Models to Implicitly Learn Self-Improvement From Data",
      "urls": [
        "http://arxiv.org/pdf/2310.00898v2",
        "http://arxiv.org/pdf/2310.00898.pdf",
        "http://arxiv.org/abs/2310.00898v2"
      ]
    },
    {
      "abstract": "Learning from human feedback is an effective way to improve robotic learning in exploration-heavy tasks. Compared to the wide application of binary human feedback, scalar human feedback has been used less because it is believed to be noisy and unstable. In this paper, we compare scalar and binary feedback, and demonstrate that scalar feedback benefits learning when properly handled. We collected binary or scalar feedback respectively from two groups of crowdworkers on a robot task. We found that when considering how consistently a participant labeled the same data, scalar feedback led to less consistency than binary feedback; however, the difference vanishes if small mismatches are allowed. Additionally, scalar and binary feedback show no significant differences in their correlations with key Reinforcement Learning targets. We then introduce Stabilizing TEacher Assessment DYnamics (STEADY) to improve learning from scalar feedback. Based on the idea that scalar feedback is muti-distributional, STEADY reconstructs underlying positive and negative feedback distributions and re-scales scalar feedback based on feedback statistics. We show that models trained with scalar feedback + STEADY outperform baselines, including binary feedback and raw scalar feedback, in a robot reaching task with non-expert human feedback. Our results show that both binary feedback and scalar feedback are dynamic, and scalar feedback is a promising signal for use in interactive Reinforcement Learning. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Hang Yu",
        "Reuben M. Aronson",
        "Katherine H. Allen",
        "Elaine Schaertl Short"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10342458",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "4121-4128",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "From \u201cThumbs Up\u201d to \u201c10 out of 10\u201d: Reconsidering Scalar Feedback in Interactive Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342458",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182524732&origin=inward"
      ]
    },
    {
      "abstract": "Advanced Driver Assistance Systems (ADAS) are increasingly important in improving driving safety and comfort, with Adaptive Cruise Control (ACC) being one of the most widely used. However, pre-defined ACC settings may not always align with driver's preferences and habits, leading to discomfort and potential safety issues. Personalized ACC (P-ACC) has been proposed to address this problem, but most existing research uses historical driving data to imitate behaviors that conform to driver preferences, neglecting real-time driver feedback. To bridge this gap, we propose a cloud-vehicle collaborative P-ACC framework that incorporates driver feedback adaptation in real time. The framework is divided into offline and online parts. The offline component records the driver's naturalistic car-following trajectory and uses inverse reinforcement learning (IRL) to train the model on the cloud. In the online component, driver feedback is used to update the driving gap preference in real time. The model is then retrained on the cloud with driver's takeover trajectories, achieving incremental learning to better match driver's preference. Human-in-the-loop (HuiL) simulation experiments demonstrate that our proposed method significantly reduces driver intervention in automatic control systems by up to 62.8%. By incorporating real-time driver feedback, our approach enhances the comfort and safety of P-ACC, providing a personalized and adaptable driving experience.",
      "authors": [
        "Zhouqiao Zhao",
        "Xishun Liao",
        "Amr Abdelraouf",
        "Kyungtae Han",
        "Rohit Gupta",
        "Matthew J. Barth",
        "Guoyuan Wu"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/SMC53992.2023.10394260",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "4675-4682",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3703-7",
        "issn": "1062-922X",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Real-Time Learning of Driving Gap Preference for Personalized Adaptive Cruise Control",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10394260"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of a large amount of interactive feedback. This paper presents a new method that uses scores provided by humans instead of pairwise preferences to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by humans negatively impacting the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method for robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Shukai Liu",
        "Chenming Wu",
        "Ying Li",
        "Liangjun Zhang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10341990",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "7561-7567",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341990",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182525943&origin=inward"
      ]
    },
    {
      "abstract": "Human-in-the-loop reinforcement learning (RL) methods actively integrate human knowledge to create reward functions for various robotic tasks. Learning from preferences shows promise as alleviates the requirement of demonstrations by querying humans on state-action sequences. However, the limited granularity of sequence-based approaches complicates temporal credit assignment. The amount of human querying is contingent on query quality, as redundant queries result in excessive human involvement. This paper addresses the often-overlooked aspect of query selection, which is closely related to active learning (AL). We propose a novel query selection approach that leverages variational autoencoder (VAE) representations of state sequences. In this manner, we formulate queries that are diverse in nature while simultaneously taking into account reward model estimations. We compare our approach to the current state-of-the-art query selection methods in preference-based RL, and find ours to be either on-par or more sample efficient through extensive benchmarking on simulated environments relevant to robotics. Lastly, we conduct an online study to verify the effectiveness of our query selection approach with real human feedback and examine several metrics related to human effort. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Daniel Marta",
        "Simon Holk",
        "Christian Pek",
        "Jana Tumova",
        "Iolanda Leite"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10341795",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7878-7885",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "VARIQuery: VAE Segment-Based Active Learning for Query Selection in Preference-Based Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182523595&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341795"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation tasks with varying levels of complexity. Our results show that SEED significantly outperforms state-of-the-art RL algorithms in sample efficiency and safety. In addition, SEED also exhibits a substantial reduction of human effort compared to other RLHF methods. Further details and video results can be found at https://seediros23.github.io/. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Ayano Hiranaka",
        "Minjune Hwang",
        "Sharon Lee",
        "Chen Wang",
        "Li Fei-Fei",
        "Jiajun Wu",
        "Ruohan Zhang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10341912",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7817-7824",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Primitive Skill-Based Robot Learning from Human Evaluative Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182523791&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341912"
      ]
    },
    {
      "abstract": "Developing robotic technologies for use in human society requires ensuring the safety of robots' navigation behaviors while adhering to pedestrians' expectations and social norms. However, understanding complex human-robot interactions (HRI) to infer potential cooperation and response among robots and pedestrians for cooperative collision avoid-ance is challenging. To address these challenges, we propose a novel socially-aware navigation benchmark called NaviS Tar, which utilizes a hybrid Spatio- Temporal grAph tRansformer to understand interactions in human-rich environments fusing crowd multi-modal dynamic features. We leverage an off-policy reinforcement learning algorithm with preference learning to train a policy and a reward function network with supervi-sor guidance. Additionally, we design a social score function to evaluate the overall performance of social navigation. To compare, we train and test our algorithm with other state-of-the-art methods in both simulator and real-world scenarios independently. Our results show that NaviSTAR outperforms previous methods with outstanding performance11The source code and experiment videos of this work are available at: https://sites.google.com/view/san-navistar \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Weizheng Wang",
        "Ruiqi Wang",
        "Le Mao",
        "Byung-Cheol Min"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10341395",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "11348-11355",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182524332&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341395"
      ]
    },
    {
      "abstract": "Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.",
      "authors": [
        "Xue, Tianci",
        "Wang, Ziqi",
        "Ji, Heng"
      ],
      "categories": null,
      "citations": null,
      "comments": "21 pages, 11 figures, 5 tables",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Parameter-Efficient Tuning Helps Language Model Alignment",
      "urls": [
        "http://arxiv.org/pdf/2310.00819.pdf",
        "http://arxiv.org/abs/2310.00819v1",
        "http://arxiv.org/pdf/2310.00819v1"
      ]
    },
    {
      "abstract": "Attics are one of the largest sources of energy loss in residential homes, but they are uncomfortable and dangerous for human workers to conduct air sealing and insulation. Hexapod robots are potentially suitable for carrying out those tasks in tight attic spaces since they are stable, compact, and lightweight. For hexapods to succeed in these tasks, they must be able to navigate inside tight attic spaces of single-family residential homes in the U.S., which typically contain rows of approximately 6 or 8-inch tall joists placed 16 inches apart from each other. Climbing over such obstacles is challenging for autonomous robotics systems. In this work, we develop a perceptive walking model for legged hexapods that can traverse over terrain with random joist structures using egocentric vision. Our method can be used on low-cost hardware not requiring real-time joint state feedback. We train our model in a teacher-student fashion with 2 phases: In phase 1, we use reinforcement learning with access to privileged information such as local elevation maps and joint feedback. In phase 2, we use supervised learning to distill the model into one with access to only onboard observations, consisting of egocentric depth images and robot orientation captured by a tracking camera. We demonstrate zero-shot sim-to-real transfer on a Hiwonder[1] SpiderPi robot, equipped with a depth camera onboard, climbing over joist courses we construct to simulate the environment in the field. Our proposed method achieves nearly 100% success rate climbing over the test courses, significantly outperforming the model without perception and the controller provided by the manufacturer. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Zixian Zang",
        "Maxime Kawawa-Beaudan",
        "Wenhao Yu",
        "Tingnan Zhang",
        "Avideh Zakhor"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10341957",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "2738-2745",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Perceptive Hexapod Legged Locomotion for Climbing Joist Environments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182524114&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341957"
      ]
    },
    {
      "abstract": "Healthy human locomotion functions with good gait symmetry depend on rhythmic coordination of the left and right legs, which can be deteriorated by neurological disorders like stroke and spinal cord injury. Powered exoskeletons are promising devices to improve impaired people's locomotion functions, like gait symmetry. However, given higher uncertainties and the time-varying nature of human-robot interaction, providing personalized robotic assistance from exoskeletons to achieve the best gait symmetry is challenging, especially for people with neurological disorders. In this paper, we propose a hierarchical control framework for a bilateral hip exoskeleton to provide the adaptive optimal hip joint assistance with a control objective of imposing the desired gait symmetry during walking. Three control levels are included in the hierarchical framework, including the high-level control to tune three control parameters based on a policy iteration reinforcement learning approach, the middle-level control to define the desired assistive torque profile based on a delayed output feedback control method, and the low-level control to achieve a good torque trajectory tracking performance. To evaluate the feasibility of the proposed control framework, five healthy young participants are recruited for treadmill walking experiments, where an artificial gait asymmetry is imitated as the hemiparesis post-stroke, and only the 'paretic' hip joint is controlled with the proposed framework. The pilot experimental studies demonstrate that the hierarchical control framework for the hip exoskeleton successfully (asymmetry index from 8.8% to - 0.5%) and efficiently (less than 4 minutes) achieved the desired gait symmetry by providing adaptive optimal assistance on the 'paretic' hip joint. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Qiang Zhang",
        "Xikai Tu",
        "Jennie Si",
        "Michael D. Lewek",
        "He Huang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10341440",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "6125-6132",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "A Robotic Assistance Personalization Control Approach of Hip Exoskeletons for Gait Symmetry Improvement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182524626&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341440"
      ]
    },
    {
      "abstract": "Though mopping the floor is a mundane and tedious daily task, enabling robots to perform it comparably to humans remains a challenge. Hand-coding desired mopping behaviors for variable surfaces and situations is particularly difficult. In this paper, we develop a robotic system for mopping the floor by mimicking the human behavior demonstrated in videos. Our baseline robotic system uses traditional computer vision techniques for tracking and inverse kinematics. Our proposed robot mop learning system comprises advanced computer vision techniques, Time Contrastive Network (TCN), and reinforcement learning. Using these, we devise a reward function for the mopping task. We use a Universal 10e robotic arm attached to a mop to perform the mopping task and a first-person camera attached on top of the robotic arm to provide feedback for robotic learning. We evaluate our proposed robot mop learning system's imitative similarity using optical flow, distance in mop location, and force applied to the floor, as well as cleaning efficiency using a white glove method. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Sanket Gaurav",
        "Aaron Crookes",
        "David Hoying",
        "Vignesh Narayanaswamy",
        "Harish Venkataraman",
        "Matthew Barker",
        "Venugopal Vasudevan",
        "Brian Ziebart"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS55552.2023.10342231",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "9947-9954",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Robot Learning to Mop Like Humans Using Video Demonstrations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182524713&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342231"
      ]
    },
    {
      "abstract": "Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.",
      "authors": [
        "Yu, Ping",
        "Xu, Hua",
        "Hu, Xia",
        "Deng, Chao"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/healthcare11202776",
      "keywords": [
        "medicine",
        "generative artificial intelligence",
        "LLM",
        "large language models",
        "ethics",
        "healthcare",
        "generative AI"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 2.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2227-9032",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.55,
        "snip": 0.873,
        "subject_areas": [
          "Health Policy",
          "Leadership and Management",
          "Health Information Management",
          "Health Informatics"
        ],
        "title": "Healthcare (Switzerland)"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175380287&origin=inward",
        "https://www.mdpi.com/2227-9032/11/20/2776/pdf?version=1698387162"
      ]
    },
    {
      "abstract": "Solid research depends on systematic, verifiable and repeatable scientometric analysis. However, scientometric analysis is difficult in the current research landscape characterized by the increasing number of publications per year, intersections between research domains, and the diversity of stakeholders involved in research projects. To address this problem, we propose SciCrowd, a hybrid human\u2013AI mixed-initiative system, which supports the collaboration between Artificial Intelligence services and crowdsourcing services. This work discusses the design and evaluation of SciCrowd. The evaluation is focused on attitudes, concerns and intentions towards use. This study contributes a nuanced understanding of the interplay between algorithmic and human tasks in the process of conducting scientometric analysis.",
      "authors": [
        "Correia, Ant\u00f3nio",
        "Grover, Andrea",
        "Jameel, Shoaib",
        "Schneider, Daniel",
        "Antunes, Pedro",
        "Fonseca, Benjamim"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10462-023-10548-7",
      "keywords": [
        "Scientometrics",
        "Human\u00e2\u0080\u0093AI interaction",
        "Artificial intelligence",
        "Bibliometric-enhanced information retrieval",
        "Crowdsourcing",
        "Reinforcement learning from human feedback"
      ],
      "number_of_pages": 28,
      "pages": "983-1010",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02692821",
        "publisher": "Springer Netherlands",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Artificial Intelligence Review"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "A hybrid human\u2013AI tool for scientometric analysis",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10462-023-10548-7.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164471709&origin=inward"
      ]
    },
    {
      "abstract": "Sense of agency, the feeling of being in control of one's actions and their effects, is particularly relevant during goal-directed actions. During feedback learning, action effects provide information about the best course of action to reinforce positive and prevent negative outcomes. However, it is unclear whether agency experience selectively affects the processing of negative or positive feedback during the performance of goal-directed actions. As an important marker of feedback processing, we examined agency-related changes in midfrontal oscillatory activity in response to performance feedback using electroencephalography. Thirty-three participants completed a reinforcement learning task during which they received positive (monetary gain) or negative (monetary loss) feedback following item choices made either by themselves (free-choice) or by the computer (forced-choice). Independent of choice context, midfrontal theta activity was more enhanced for negative than positive feedback. In addition, free, compared to forced choices increased midfrontal theta power for both gain and loss feedback. These results indicate that freedom of choice in a motivationally salient learning task leads to a general enhancement in the processing of affective action outcomes. Our findings contribute to an understanding of the neuronal mechanisms underlying agency-related changes during action regulation and indicate midfrontal theta activity as a neurophysiological marker important for the monitoring of affective action outcomes, irrespective of feedback valence. \u00c2\u00a9 2023 Elsevier B.V.",
      "authors": [
        "Giersiepen, M.",
        "Sch\u00c3\u00bctz-Bosbach, S.",
        "Kaiser, J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2023.108659",
      "keywords": [
        "EEG",
        "Affective feedback processing",
        "Sense of agency",
        "Freedom of choice",
        "Reinforcement learning",
        "Midfrontal oscillatory activity"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Freedom of choice boosts midfrontal theta power during affective feedback processing of goal-directed actions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168606528&origin=inward"
      ]
    },
    {
      "abstract": "ChatGPT (chat generative pre-trained transformer) is a new technology direction developed in the field of artificial intelligence in recent years, which covers digital functions such as device digital twin, device management, platform operation, etc., and is more characterized by versatility and generative human-machine dialogue. This paper firstly introduces the development status of ChatGPT, as well as the power equipment ChatGPT-type model and core technology architecture. It is illustrated that the large model has outstanding features such as excellent generalization ability, logical reasoning ability, and multimodal data analysis and generation ability. Then, the key technologies involved in ChatGPT-type large model for electric power equipment are analyzed from five aspects as follows: high arithmetic AI chip, corpus sample system construction, transformer-based generative pre-training model, multimodal algorithm embedded in big language model, and human feedback-based reinforcement learning technology. Finally, the feasibility and technical solutions for ChatGPT for power equipment to be carried out in the power industry are proposed, and the challenges and development directions of ChatGPT for power equipment in the future are summarized. \u00c2\u00a9 2023 Science Press. All rights reserved.",
      "authors": [
        "Jiang, X.",
        "Zang, Y.",
        "Liu, Y.",
        "Sheng, G.",
        "Xu, Y.",
        "Qian, Q."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.13336/j.1003-6520.hve.20231419",
      "keywords": [
        "digital twin",
        "power equipment ChatGPT",
        "reinforcement learning from human feedback",
        "transformer models",
        "general artificial intelligence"
      ],
      "number_of_pages": 13,
      "pages": "4033-4045",
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10036520",
        "publisher": "Science Press",
        "sjr": 0.62,
        "snip": 1.15,
        "subject_areas": [
          "Energy Engineering and Power Technology",
          "Electrical and Electronic Engineering"
        ],
        "title": "Gaodianya Jishu/High Voltage Engineering"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Power Equipment ChatGPT-type Model and Key Technologies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175521899&origin=inward"
      ]
    },
    {
      "abstract": "Though an accurate measurement of entropy, or more generally uncertainty, is critical to the success of human\u2013machine teams, the evaluation of the accuracy of such metrics as a probability of machine correctness is often aggregated and not assessed as an iterative control process. The entropy of the decisions made by human\u2013machine teams may not be accurately measured under cold start or at times of data drift unless disagreements between the human and machine are immediately fed back to the classifier iteratively. In this study, we present a stochastic framework by which an uncertainty model may be evaluated iteratively as a probability of machine correctness. We target a novel problem, referred to as the threshold selection problem, which involves a user subjectively selecting the point at which a signal transitions to a low state. This problem is designed to be simple and replicable for human\u2013machine experimentation while exhibiting properties of more complex applications. Finally, we explore the potential of incorporating feedback of machine correctness into a baseline na\u00efve Bayes uncertainty model with a novel reinforcement learning approach. The approach refines a baseline uncertainty model by incorporating machine correctness at every iteration. Experiments are conducted over a large number of realizations to properly evaluate uncertainty at each iteration of the human\u2013machine team. Results show that our novel approach, called closed-loop uncertainty, outperforms the baseline in every case, yielding about 45% improvement on average.",
      "authors": [
        "Bishof, Zachary",
        "Scheuerman, Jaelle",
        "Michael, Chris J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/e25101443",
      "keywords": [
        "calibration",
        "Q-learning",
        "reinforcement learning",
        "human\u00e2\u0080\u0093machine teams",
        "uncertainty",
        "interactive machine learning",
        "confidence"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1099-4300",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Entropy"
      },
      "publication_date": "2023-10-01",
      "selected": null,
      "title": "Closed-Loop Uncertainty: The Evaluation and Calibration of Uncertainty for Human\u2013Machine Teams under Data Drift",
      "urls": [
        "https://www.mdpi.com/1099-4300/25/10/1443/pdf?version=1697102201",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175247854&origin=inward"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pairwise Proximal Policy Optimization (P3O) that operates directly on comparative rewards. We show theoretically that P3O is invariant to equivalent rewards and avoids the complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO in the KL-Reward trade-off and can align with human preferences as well as or better than prior methods. In summary, this work introduces a simpler yet effective approach for aligning LLMs to human preferences through relative feedback.",
      "authors": [
        "Wu, Tianhao",
        "Zhu, Banghua",
        "Zhang, Ruoyu",
        "Wen, Zhaojin",
        "Ramchandran, Kannan",
        "Jiao, Jiantao"
      ],
      "categories": null,
      "citations": null,
      "comments": "19 pages, 5 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-30",
      "selected": null,
      "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
      "urls": [
        "http://arxiv.org/pdf/2310.00212.pdf",
        "http://arxiv.org/pdf/2310.00212v3",
        "http://arxiv.org/abs/2310.00212v3"
      ]
    },
    {
      "abstract": "Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made without demonstrations. Finally, we show that Motif mostly generates intuitive human-aligned behaviors which can be steered easily through prompt modifications, while scaling well with the LLM size and the amount of information given in the prompt.",
      "authors": [
        "Klissarov, Martin",
        "D'Oro, Pierluca",
        "Sodhani, Shagun",
        "Raileanu, Roberta",
        "Bacon, Pierre-Luc",
        "Vincent, Pascal",
        "Zhang, Amy",
        "Henaff, Mikael"
      ],
      "categories": null,
      "citations": null,
      "comments": "The first two authors equally contributed - order decided by coin\n  flip",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-29",
      "selected": null,
      "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback",
      "urls": [
        "http://arxiv.org/abs/2310.00166v1",
        "http://arxiv.org/pdf/2310.00166.pdf",
        "http://arxiv.org/pdf/2310.00166v1"
      ]
    },
    {
      "abstract": "We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms.",
      "authors": [
        "Clark, Kevin",
        "Vicol, Paul",
        "Swersky, Kevin",
        "Fleet, David J"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-29",
      "selected": null,
      "title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards",
      "urls": [
        "http://arxiv.org/pdf/2309.17400v1",
        "http://arxiv.org/pdf/2309.17400.pdf",
        "http://arxiv.org/abs/2309.17400v1"
      ]
    },
    {
      "abstract": "Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.",
      "authors": [
        "Li, Tianyu",
        "Jung, Hyunyoung",
        "Gombolay, Matthew",
        "Cho, Yong Kwon",
        "Ha, Sehoon"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-29",
      "selected": null,
      "title": "CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2309.17046v1",
        "http://arxiv.org/pdf/2309.17046.pdf",
        "http://arxiv.org/pdf/2309.17046v1"
      ]
    },
    {
      "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",
      "authors": [
        "Bai, Jinze",
        "Bai, Shuai",
        "Chu, Yunfei",
        "Cui, Zeyu",
        "Dang, Kai",
        "Deng, Xiaodong",
        "Fan, Yang",
        "Ge, Wenbin",
        "Han, Yu",
        "Huang, Fei",
        "Hui, Binyuan",
        "Ji, Luo",
        "Li, Mei",
        "Lin, Junyang",
        "Lin, Runji",
        "Liu, Dayiheng",
        "Liu, Gao",
        "Lu, Chengqiang",
        "Lu, Keming",
        "Ma, Jianxin",
        "Men, Rui",
        "Ren, Xingzhang",
        "Ren, Xuancheng",
        "Tan, Chuanqi",
        "Tan, Sinan",
        "Tu, Jianhong",
        "Wang, Peng",
        "Wang, Shijie",
        "Wang, Wei",
        "Wu, Shengguang",
        "Xu, Benfeng",
        "Xu, Jin",
        "Yang, An",
        "Yang, Hao",
        "Yang, Jian",
        "Yang, Shusheng",
        "Yao, Yang",
        "Yu, Bowen",
        "Yuan, Hongyi",
        "Yuan, Zheng",
        "Zhang, Jianwei",
        "Zhang, Xingxuan",
        "Zhang, Yichang",
        "Zhang, Zhenru",
        "Zhou, Chang",
        "Zhou, Jingren",
        "Zhou, Xiaohuan",
        "Zhu, Tianhang"
      ],
      "categories": null,
      "citations": null,
      "comments": "59 pages, 5 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-28",
      "selected": null,
      "title": "Qwen Technical Report",
      "urls": [
        "http://arxiv.org/pdf/2309.16609.pdf",
        "http://arxiv.org/pdf/2309.16609v1",
        "http://arxiv.org/abs/2309.16609v1"
      ]
    },
    {
      "abstract": "The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).",
      "authors": [
        "Wang, Chaoqi",
        "Jiang, Yibo",
        "Yang, Chenghao",
        "Liu, Han",
        "Chen, Yuxin"
      ],
      "categories": null,
      "citations": null,
      "comments": "Preprint",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-28",
      "selected": null,
      "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
      "urls": [
        "http://arxiv.org/pdf/2309.16240v1",
        "http://arxiv.org/abs/2309.16240v1",
        "http://arxiv.org/pdf/2309.16240.pdf"
      ]
    },
    {
      "abstract": "Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model. In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training? We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is expected to rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on Contrast Instructions compared to average humans. To show that RM consistency can be improved efficiently without using extra training budget, we propose two techniques ConvexDA and RewardFusion, which enhance reward consistency through extrapolation during the RM training and inference stage, respectively. We show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process.",
      "authors": [
        "Shen, Lingfeng",
        "Chen, Sihao",
        "Song, Linfeng",
        "Jin, Lifeng",
        "Peng, Baolin",
        "Mi, Haitao",
        "Khashabi, Daniel",
        "Yu, Dong"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-28",
      "selected": null,
      "title": "The Trickle-down Impact of Reward (In-)consistency on RLHF",
      "urls": [
        "http://arxiv.org/abs/2309.16155v1",
        "http://arxiv.org/pdf/2309.16155v1",
        "http://arxiv.org/pdf/2309.16155.pdf"
      ]
    },
    {
      "abstract": "For human-like dialogue systems, it is significant to inject the empathetic ability or elicit the opposite's positive emotions, while existing studies mostly only focus on either of the above two research lines. In this work, we propose a novel and grafted task named Empathetic Emotion Elicitation Dialog to make a dialog system able to possess both aspects of ability simultaneously. We do not train an empathetic dialog system and an emotion elicitation dialog system separately and then simply concatenate the responses generated by these two systems, which will cause illogical and repetitive responses. Instead, we propose a unified solution: (1) To generate empathetic responses and emotion elicitation responses within the same semantic space, we design a unified framework. (2) The unified framework has three stages which first retrieve the empathetic and emotion elicitation exemplars as external knowledge, then fine-tune the emotion/action prediction on a pre-trained language model to enhance the empathetic ability, and finally model the user feedback by reinforcement learning to enhance the emotion elicitation ability. Experiments show that our method outperforms the baselines in the response generation quality and simultaneously empathizes with the user and elicits their positive emotions. \u00c2\u00a9 2023 The Authors.",
      "authors": [
        "Zhu, Ying",
        "Wang, Bo",
        "Zhao, Dongming",
        "Huang, Kun",
        "Jiang, Zhuoxuan",
        "He, Ruifang",
        "Hou, Yuexian"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/FAIA230634",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "3148-3155",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781643684369",
        "issn": "09226389",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Artificial Intelligence and Applications"
      },
      "publication_date": "2023-09-28",
      "selected": null,
      "title": "Grafting Fine-Tuning and Reinforcement Learning for Empathetic Emotion Elicitation in Dialog Generation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175849462&origin=inward",
        "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230634"
      ]
    },
    {
      "abstract": "This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable extractive TLS method on two of the three benchmark datasets, and participants prefer our method's summaries to those of both the extractive TLS method and the pretrained abstractive model. The method does not require expensive reference summaries and needs only a small number of preferences to align the generated summaries with human preferences. Code available at https://github.com/Haruhi07/PBRL-TLS. \u00c2\u00a9 2023 The Authors.",
      "authors": [
        "Ye, Yuxuan",
        "Simpson, Edwin"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/FAIA230601",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "2882-2889",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781643684369",
        "issn": "09226389",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Artificial Intelligence and Applications"
      },
      "publication_date": "2023-09-28",
      "selected": null,
      "title": "Towards Abstractive Timeline Summarisation Using Preference-Based Reinforcement Learning",
      "urls": [
        "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230601",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175851486&origin=inward"
      ]
    },
    {
      "abstract": "A well-defined reward function is crucial for successful training of an reinforcement learning (RL) agent. However, defining a suitable reward function is a notoriously challenging task, especially in complex, multi-objective environments. Developers often have to resort to starting with an initial, potentially misspecified reward function, and iteratively adjusting its parameters, based on observed learned behavior. In this work, we aim to automate this process by proposing ITERS, an iterative reward shaping approach using human feedback for mitigating the effects of a misspecified reward function. Our approach allows the user to provide trajectory-level feedback on agent's behavior during training, which can be integrated as a reward shaping signal in the following training iteration. We also allow the user to provide explanations of their feedback, which are used to augment the feedback and reduce user effort and feedback frequency. We evaluate ITERS in three environments and show that it can successfully correct misspecified reward functions. \u00c2\u00a9 2023 The Authors.",
      "authors": [
        "Gajcin, Jasmina",
        "McCarthy, James",
        "Nair, Rahul",
        "Marinescu, Radu",
        "Daly, Elizabeth",
        "Dusparic, Ivana"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/FAIA230345",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "788-794",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781643684369",
        "issn": "09226389",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Artificial Intelligence and Applications"
      },
      "publication_date": "2023-09-28",
      "selected": null,
      "title": "Iterative Reward Shaping Using Human Feedback for Correcting Reward Misspecification",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175831749&origin=inward",
        "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230345"
      ]
    },
    {
      "abstract": "A common assumption in most Inverse Reinforcement Learning (IRL) methods is that human demonstrations are drawn from an optimal policy. However, this assumption poses a problem in real-world applications, as humans may have varying aptitudes for different tasks. To address this, preference-based IRL methods, using human rankings of trajectories, have been explored as a potential solution. More recently, demonstrations from humans with different levels of expertise have also been utilized to relax this assumption. In this paper, we consider scenarios where there is no prior knowledge of the demonstrators' expertise levels and humans are not readily available to provide rankings. Therefore, we propose a new method called Inverse Reinforcement Learning from Varying Expertise Levels (IRL-VEL), which combines demonstrations from different demonstrators with different levels of expertise to learn a high performance policy from their proficiency scores - an indication of how optimal their demonstrations are. In particular, we solve an optimization problem to learn these scores and subsequently use them to efficiently learn a reward function that optimizes a policy through reinforcement learning. To assess the performance of our algorithm, we learn two separate policies from two reward functions: an initial reward function directly obtained from these combined demonstrations and a final reward function that incorporates the proficiency scores. We evaluate the proposed algorithm in two simulation environments, namely, the continuous mountain car and the MuJoCo swimmer. The simulation results demonstrate that the proposed method can effectively learn and leverage the expertise levels underlying the demonstrations to learn a policy, significantly outperforming the methods that do not account for the expertise levels. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Somtochukwu Oguchienti",
        "Mahsa Ghasemi"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/Allerton58177.2023.10313475",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-0505-0",
        "issn": "2474-0195",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 59th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2023"
      },
      "publication_date": "2023-09-26",
      "selected": null,
      "title": "Inverse Reinforcement Learning with Learning and Leveraging Demonstrators\u2019 Varying Expertise Levels",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10313475",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179507675&origin=inward"
      ]
    },
    {
      "abstract": "Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in \"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.",
      "authors": [
        "Sun, Zhiqing",
        "Shen, Sheng",
        "Cao, Shengcao",
        "Liu, Haotian",
        "Li, Chunyuan",
        "Shen, Yikang",
        "Gan, Chuang",
        "Gui, Liang-Yan",
        "Wang, Yu-Xiong",
        "Yang, Yiming",
        "Keutzer, Kurt",
        "Darrell, Trevor"
      ],
      "categories": null,
      "citations": null,
      "comments": "Preprint",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-25",
      "selected": null,
      "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
      "urls": [
        "http://arxiv.org/pdf/2309.14525.pdf",
        "http://arxiv.org/pdf/2309.14525v1",
        "http://arxiv.org/abs/2309.14525v1"
      ]
    },
    {
      "abstract": "Modern computational natural philosophy conceptualizes the universe in terms of information and computation, establishing a framework for the study of cognition and intelligence. Despite some critiques, this computational perspective has significantly influenced our understanding of the natural world, leading to the development of AI systems like ChatGPT based on deep neural networks. Advancements in this domain have been facilitated by interdisciplinary research, integrating knowledge from multiple fields to simulate complex systems. Large Language Models (LLMs), such as ChatGPT, represent this approach's capabilities, utilizing reinforcement learning with human feedback (RLHF). Current research initiatives aim to integrate neural networks with symbolic computing, introducing a new generation of hybrid computational models.",
      "authors": [
        "Dodig-Crnkovic, Gordana"
      ],
      "categories": null,
      "citations": null,
      "comments": "17 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-22",
      "selected": null,
      "title": "Computational Natural Philosophy: A Thread from Presocratics through Turing to ChatGPT",
      "urls": [
        "http://arxiv.org/abs/2309.13094v1",
        "http://arxiv.org/pdf/2309.13094v1",
        "http://arxiv.org/pdf/2309.13094.pdf"
      ]
    },
    {
      "abstract": "Robot multimodal locomotion encompasses the ability to transition between walking and flying, representing a significant challenge in robotics. This work presents an approach that enables automatic smooth transitions between legged and aerial locomotion. Leveraging the concept of Adversarial Motion Priors, our method allows the robot to imitate motion datasets and accomplish the desired task without the need for complex reward functions. The robot learns walking patterns from human-like gaits and aerial locomotion patterns from motions obtained using trajectory optimization. Through this process, the robot adapts the locomotion scheme based on environmental feedback using reinforcement learning, with the spontaneous emergence of mode-switching behavior. The results highlight the potential for achieving multimodal locomotion in aerial humanoid robotics through automatic control of walking and flying modes, paving the way for applications in diverse domains such as search and rescue, surveillance, and exploration missions. This research contributes to advancing the capabilities of aerial humanoid robots in terms of versatile locomotion in various environments.",
      "authors": [
        "L'Erario, Giuseppe",
        "Hanover, Drew",
        "Romero, Angel",
        "Song, Yunlong",
        "Nava, Gabriele",
        "Viceconte, Paolo Maria",
        "Pucci, Daniele",
        "Scaramuzza, Davide"
      ],
      "categories": null,
      "citations": null,
      "comments": "6 pages, 8 figures, submitted to ICRA 2024",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-22",
      "selected": null,
      "title": "Learning to Walk and Fly with Adversarial Motion Priors",
      "urls": [
        "http://arxiv.org/abs/2309.12784v1",
        "http://arxiv.org/pdf/2309.12784v1",
        "http://arxiv.org/pdf/2309.12784.pdf"
      ]
    },
    {
      "abstract": "Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io",
      "authors": [
        "Xie, Tianbao",
        "Zhao, Siheng",
        "Wu, Chen Henry",
        "Liu, Yitao",
        "Luo, Qian",
        "Zhong, Victor",
        "Yang, Yanchao",
        "Yu, Tao"
      ],
      "categories": null,
      "citations": null,
      "comments": "23 pages, 10 figures, update",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-20",
      "selected": null,
      "title": "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2309.11489.pdf",
        "http://arxiv.org/abs/2309.11489v2",
        "http://arxiv.org/pdf/2309.11489v2"
      ]
    },
    {
      "abstract": "Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat.",
      "authors": [
        "Wang, Guan",
        "Cheng, Sijie",
        "Zhan, Xianyuan",
        "Li, Xiangang",
        "Song, Sen",
        "Liu, Yang"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-20",
      "selected": null,
      "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
      "urls": [
        "http://arxiv.org/pdf/2309.11235v1",
        "http://arxiv.org/pdf/2309.11235.pdf",
        "http://arxiv.org/abs/2309.11235v1"
      ]
    },
    {
      "abstract": "To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural language feedback. (b) Better single-turn performance does not guarantee better multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.",
      "authors": [
        "Wang, Xingyao",
        "Wang, Zihan",
        "Liu, Jiateng",
        "Chen, Yangyi",
        "Yuan, Lifan",
        "Peng, Hao",
        "Ji, Heng"
      ],
      "categories": null,
      "citations": null,
      "comments": "Code is available on our project website:\n  https://xingyaoww.github.io/mint-bench",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-19",
      "selected": null,
      "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
      "urls": [
        "http://arxiv.org/pdf/2309.10691.pdf",
        "http://arxiv.org/abs/2309.10691v2",
        "http://arxiv.org/pdf/2309.10691v2"
      ]
    },
    {
      "abstract": "Understanding how humans process feedback, predictions, and prediction errors, as well as their relationship to reinforcement magnitude, is crucial both for elucidating the mechanisms driving human economic behavior and for informing the development of new models of neuroplasticity and learning-related brain reorganization. The aim of our study was to investigate learning-associated plastic changes by examining alterations in evoked potentials in response to auditory cues during one of the most used empirical trials in neuroeconomics - monetary incentive delay (MID) task. Our findings demonstrate that associating a previously neutral auditory stimulus with a specific monetary loss can induce plastic changes manifesting as context-dependent enhancement of auditory evoked potential amplitudes. These results contribute to existing models of learning and decision-making and add to our understanding of the neural mechanisms underlying reinforcement learning. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Anastasia Grigoreva",
        "Alla Kondratenko",
        "Aleksei Gorin",
        "A.N. Shestakova"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/CNN59923.2023.10275169",
      "keywords": [
        "auditory MID task",
        "decision-making",
        "neuroplasticity",
        "P2"
      ],
      "number_of_pages": 4,
      "pages": "23-26",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-4406-6",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 5th International Conference \"Neurotechnologies and Neurointerfaces\", CNN 2023"
      },
      "publication_date": "2023-09-18",
      "selected": null,
      "title": "Context-Dependent P2 Plasticity Dynamics in the Monetary Incentive Delay Task",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175462690&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10275169"
      ]
    },
    {
      "abstract": "Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. This degradation is especially pronounced for tasks \"closest\" to the fine-tuning distribution. We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities. Conjugate prompting artificially makes the task look farther from the fine-tuning distribution while requiring the same capability. We find that conjugate prompting systematically recovers some of the pretraining capabilities on our synthetic setup. We then apply conjugate prompting to real-world LLMs using the observation that fine-tuning distributions are typically heavily skewed towards English. We find that simply translating the prompts to different languages can cause the fine-tuned models to respond like their pretrained counterparts instead. This allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, to recover harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
      "authors": [
        "Kotha, Suhas",
        "Springer, Jacob Mitchell",
        "Raghunathan, Aditi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-18",
      "selected": null,
      "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
      "urls": [
        "http://arxiv.org/abs/2309.10105v1",
        "http://arxiv.org/pdf/2309.10105.pdf",
        "http://arxiv.org/pdf/2309.10105v1"
      ]
    },
    {
      "abstract": "Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad range of metrics. SYNDICOM achieves a relative improvement of 53% over ChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the time. We will publicly release the code and the full dataset.",
      "authors": [
        "Richardson, Christopher",
        "Sundar, Anirudh",
        "Heck, Larry"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published at SigDial 2023, Number 129",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-18",
      "selected": null,
      "title": "SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback",
      "urls": [
        "http://arxiv.org/pdf/2309.10015.pdf",
        "http://arxiv.org/abs/2309.10015v1",
        "http://arxiv.org/pdf/2309.10015v1"
      ]
    },
    {
      "abstract": "Recent years have witnessed many successful trials in the robot learning field. For contact-rich robotic tasks, it is challenging to learn coordinated motor skills by reinforcement learning. Imitation learning solves this problem by using a mimic reward to encourage the robot to track a given reference trajectory. However, imitation learning is not so efficient and may constrain the learned motion. In this paper, we propose instruction learning, which is inspired by the human learning process and is highly efficient, flexible, and versatile for robot motion learning. Instead of using a reference signal in the reward, instruction learning applies a reference signal directly as a feedforward action, and it is combined with a feedback action learned by reinforcement learning to control the robot. Besides, we propose the action bounding technique and remove the mimic reward, which is shown to be crucial for efficient and flexible learning. We compare the performance of instruction learning with imitation learning, indicating that instruction learning can greatly speed up the training process and guarantee learning the desired motion correctly. The effectiveness of instruction learning is validated through a bunch of motion learning examples for a biped robot and a quadruped robot, where skills can be learned typically within several million steps. Besides, we also conduct sim-to-real transfer and online learning experiments on a real quadruped robot. Instruction learning has shown great merits and potential, making it a promising alternative for imitation learning.",
      "authors": [
        "Ye, Linqi",
        "Li, Jiayi",
        "Cheng, Yi",
        "Wang, Xianhao",
        "Liang, Bin",
        "Peng, Yan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-17",
      "selected": null,
      "title": "From Knowing to Doing: Learning Diverse Motor Skills through Instruction Learning",
      "urls": [
        "http://arxiv.org/abs/2309.09167v2",
        "http://arxiv.org/pdf/2309.09167.pdf",
        "http://arxiv.org/pdf/2309.09167v2"
      ]
    },
    {
      "abstract": "With the rapid development of enterprise Learning Management Systems (LMS), more and more companies are trying to build enterprise training and course learning platforms for promoting the career development of employees. Indeed, through course learning, many employees have the opportunity to improve their knowledge and skills. For these systems, a major issue is how to recommend learning plans, i.e., a set of courses arranged in the order they should be learned, that can help employees improve their work performance. Existing studies mainly focus on recommending courses that users are most likely to click on by capturing their learning preferences. However, the learning preference of employees may not be the right fit for their career development, and thus it may not necessarily mean their work performance can be improved accordingly. Furthermore, how to capture the mutual correlation and sequential effects between courses, and ensure the rationality of the generated results, is also a major challenge. To this end, in this paper, we propose the Generative Learning plAn recommenDation (GLAD) framework, which can generate personalized learning plans for employees to help them improve their work performance. Specifically, we first design a performance predictor and a rationality discriminator, which have the same transformer-based model architecture, but with totally different parameters and functionalities. In particular, the performance predictor is trained for predicting the work performance of employees based on their work profiles and historical learning records, while the rationality discriminator aims to evaluate the rationality of the generated results. Then, we design a learning plan generator based on the gated transformer and the cross-attention mechanism for learning plan generation. We calculate the weighted sum of the output from the performance predictor and the rationality discriminator as the reward, and we use Self-Critical Sequence Training (SCST) based policy gradient methods to train the generator following the Generative Adversarial Network (GAN) paradigm. Finally, extensive experiments on real-world data clearly validate the effectiveness of our GLAD framework compared with state-of-the-art baseline methods and reveal some interesting findings for talent management. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Zheng, Z.",
        "Sun, Y.",
        "Song, X.",
        "Zhu, H.",
        "Xiong, H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3604915.3608795",
      "keywords": [
        "generative recommendation",
        "reinforcement learning",
        "learning management system"
      ],
      "number_of_pages": 12,
      "pages": "443-454",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400702419",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023"
      },
      "publication_date": "2023-09-14",
      "selected": null,
      "title": "Generative Learning Plan Recommendation for Employees: A Performance-aware Reinforcement Learning Approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165699557&origin=inward"
      ]
    },
    {
      "abstract": "Interactive recommendation enables users to provide verbal and non-verbal relevance feedback (such as natural-language critiques and likes/dislikes) when viewing a ranked list of recommendations (such as images of fashion products), in order to guide the recommender system towards their desired items (i.e. goals) across multiple interaction turns. Such a multi-modal interactive recommendation (MMIR) task has been successfully formulated with deep reinforcement learning (DRL) algorithms by simulating the interactions between an environment (i.e. a user) and an agent (i.e. a recommender system). However, it is typically challenging and unstable to optimise the agent to improve the recommendation quality associated with implicit learning of multi-modal representations in an end-to-end fashion in DRL. This is known as the coupling of policy optimisation and representation learning. To address this coupling issue, we propose a novel goal-oriented multi-modal interactive recommendation model (GOMMIR) that uses both verbal and non-verbal relevance feedback to effectively incorporate the users' preferences over time. Specifically, our GOMMIR model employs a multi-task learning approach to explicitly learn the multi-modal representations using a multi-modal composition network when optimising the recommendation agent. Moreover, we formulate the MMIR task using goal-oriented reinforcement learning and enhance the optimisation objective by leveraging non-verbal relevance feedback for hard negative sampling and providing extra goal-oriented rewards to effectively optimise the recommendation agent. Following previous work, we train and evaluate our GOMMIR model by using user simulators that can generate natural-language feedback about the recommendations as a surrogate for real human users. Experiments conducted on four well-known fashion datasets demonstrate that our proposed GOMMIR model yields significant improvements in comparison to the existing state-of-the-art baseline models. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Yaxiong Wu",
        "Craig Macdonald",
        "Iadh Ounis"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3604915.3608775",
      "keywords": [
        "multi-modal",
        "interactive recommendation",
        "reinforcement learning",
        "relevance feedback"
      ],
      "number_of_pages": 12,
      "pages": "362-373",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400702419",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023"
      },
      "publication_date": "2023-09-14",
      "selected": null,
      "title": "Goal-Oriented Multi-Modal Interactive Recommendation with Verbal and Non-Verbal Relevance Feedback",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3604915.3608775",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174541996&origin=inward"
      ]
    },
    {
      "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",
      "authors": [
        "Li, Yuhui",
        "Wei, Fangyun",
        "Zhao, Jinjing",
        "Zhang, Chao",
        "Zhang, Hongyang"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-13",
      "selected": null,
      "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
      "urls": [
        "http://arxiv.org/abs/2309.07124v2",
        "http://arxiv.org/pdf/2309.07124.pdf",
        "http://arxiv.org/pdf/2309.07124v2"
      ]
    },
    {
      "abstract": "Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted to sampling preference pairs only from the SFT policy. To address these limitations, we introduce a novel approach called Statistical Rejection Sampling Optimization (RSO) that aims to source preference data from the target optimal policy using rejection sampling, enabling a more accurate estimation of the optimal policy. We also propose a unified framework that enhances the loss functions used in both SLiC and DPO from a preference modeling standpoint. Through extensive experiments across three diverse tasks, we demonstrate that RSO consistently outperforms both SLiC and DPO on evaluations from both Large Language Model (LLM) and human raters.",
      "authors": [
        "Liu, Tianqi",
        "Zhao, Yao",
        "Joshi, Rishabh",
        "Khalman, Misha",
        "Saleh, Mohammad",
        "Liu, Peter J.",
        "Liu, Jialu"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted in ICLR 2024",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-13",
      "selected": null,
      "title": "Statistical Rejection Sampling Improves Preference Optimization",
      "urls": [
        "http://arxiv.org/pdf/2309.06657.pdf",
        "http://arxiv.org/pdf/2309.06657v2",
        "http://arxiv.org/abs/2309.06657v2"
      ]
    },
    {
      "abstract": "LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting, which is also known as the alignment tax. To empirically verify this hypothesis, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. On the other hand, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between reward maximization and forgetting mitigation. In light of the above pressing issue in aligning LLMs, in this paper we explore model averaging, which interpolates between pre and post RLHF model weights, to achieve a more efficient reward-tax Pareto front. To understand its effectiveness, We offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different reward-tax trade-offs, we propose Adaptive Model Averaging (AMA) to adaptively find various combination ratios of model layers. AMA seeks to maximize the alignment reward while incurring minimal alignment tax. Moreover, we validate AMA's performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B.",
      "authors": [
        "Lin, Yong",
        "Lin, Hangyu",
        "Xiong, Wei",
        "Diao, Shizhe",
        "Liu, Jianmeng",
        "Zhang, Jipeng",
        "Pan, Rui",
        "Wang, Haoxiang",
        "Hu, Wenbin",
        "Zhang, Hanning",
        "Dong, Hanze",
        "Pi, Renjie",
        "Zhao, Han",
        "Jiang, Nan",
        "Ji, Heng",
        "Yao, Yuan",
        "Zhang, Tong"
      ],
      "categories": null,
      "citations": null,
      "comments": "28 Pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-12",
      "selected": null,
      "title": "Mitigating the Alignment Tax of RLHF",
      "urls": [
        "http://arxiv.org/pdf/2309.06256v3",
        "http://arxiv.org/abs/2309.06256v3",
        "http://arxiv.org/pdf/2309.06256.pdf"
      ]
    },
    {
      "abstract": "We present the effect of adapting to human preferences on trust in a human-robot teaming task. The team performs a task in which the robot acts as an action recommender to the human. It is assumed that the behavior of the human and the robot is based on some reward function they try to optimize. We use a new human trust-behavior model that enables the robot to learn and adapt to the human's preferences in real-time during their interaction using Bayesian Inverse Reinforcement Learning. We present three strategies for the robot to interact with a human: a non-learner strategy, in which the robot assumes that the human's reward function is the same as the robot's, a non-adaptive learner strategy that learns the human's reward function for performance estimation, but still optimizes its own reward function, and an adaptive-learner strategy that learns the human's reward function for performance estimation and also optimizes this learned reward function. Results show that adapting to the human's reward function results in the highest trust in the robot.",
      "authors": [
        "Bhat, Shreyas",
        "Lyons, Joseph B.",
        "Shi, Cong",
        "Yang, X. Jessie"
      ],
      "categories": null,
      "citations": null,
      "comments": "6 pages, 6 figures, AAAI Fall Symposium on Agent Teaming in\n  Mixed-Motive Situations",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-11",
      "selected": null,
      "title": "Effect of Adapting to Human Preferences on Trust in Human-Robot Teaming",
      "urls": [
        "http://arxiv.org/pdf/2309.05179.pdf",
        "http://arxiv.org/abs/2309.05179v1",
        "http://arxiv.org/pdf/2309.05179v1"
      ]
    },
    {
      "abstract": "Advanced Driver Assistance Systems (ADAS) are increasingly important in improving driving safety and comfort, with Adaptive Cruise Control (ACC) being one of the most widely used. However, pre-defined ACC settings may not always align with driver's preferences and habits, leading to discomfort and potential safety issues. Personalized ACC (P-ACC) has been proposed to address this problem, but most existing research uses historical driving data to imitate behaviors that conform to driver preferences, neglecting real-time driver feedback. To bridge this gap, we propose a cloud-vehicle collaborative P-ACC framework that incorporates driver feedback adaptation in real time. The framework is divided into offline and online parts. The offline component records the driver's naturalistic car-following trajectory and uses inverse reinforcement learning (IRL) to train the model on the cloud. In the online component, driver feedback is used to update the driving gap preference in real time. The model is then retrained on the cloud with driver's takeover trajectories, achieving incremental learning to better match driver's preference. Human-in-the-loop (HuiL) simulation experiments demonstrate that our proposed method significantly reduces driver intervention in automatic control systems by up to 62.8%. By incorporating real-time driver feedback, our approach enhances the comfort and safety of P-ACC, providing a personalized and adaptable driving experience.",
      "authors": [
        "Zhao, Zhouqiao",
        "Liao, Xishun",
        "Abdelraouf, Amr",
        "Han, Kyungtae",
        "Gupta, Rohit",
        "Barth, Matthew J.",
        "Wu, Guoyuan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-10",
      "selected": null,
      "title": "Real-time Learning of Driving Gap Preference for Personalized Adaptive Cruise Control",
      "urls": [
        "http://arxiv.org/pdf/2309.05115v1",
        "http://arxiv.org/abs/2309.05115v1",
        "http://arxiv.org/pdf/2309.05115.pdf"
      ]
    },
    {
      "abstract": "Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; simply by ranking the importance of the reward factors. More specifically, we propose a new RL framework -- HERON, which compares trajectories using a hierarchical decision tree induced by the given ranking. These comparisons are used to train a preference-based reward model, which is then used for policy learning. We find that our framework can not only train high performing agents on a variety of difficult tasks, but also provide additional benefits such as improved sample efficiency and robustness. Our code is available at https://github.com/abukharin3/HERON.",
      "authors": [
        "Bukharin, Alexander",
        "Li, Yixiao",
        "He, Pengcheng",
        "Chen, Weizhu",
        "Zhao, Tuo"
      ],
      "categories": null,
      "citations": null,
      "comments": "28 Pages, 15 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-06",
      "selected": null,
      "title": "Deep Reinforcement Learning from Hierarchical Weak Preference Feedback",
      "urls": [
        "http://arxiv.org/pdf/2309.02632.pdf",
        "http://arxiv.org/abs/2309.02632v1",
        "http://arxiv.org/pdf/2309.02632v1"
      ]
    },
    {
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) has revolutionized language modeling by aligning models with human preferences. However, the RL stage, Proximal Policy Optimization (PPO), requires over 3x the memory of Supervised Fine-Tuning (SFT), making it infeasible to use for most practitioners. To address this issue, we present a comprehensive analysis the memory usage, performance, and training time of memory-savings techniques for PPO. We introduce Hydra-RLHF by first integrating the SFT and Reward models and then dynamically turning LoRA \"off\" during training. Our experiments show: 1. Using LoRA during PPO reduces its memory usage to be smaller than SFT while improving alignment across four public benchmarks, and 2. Hydra-PPO reduces the latency per sample of LoRA-PPO by up to 65% while maintaining its performance. Our results demonstrate that Hydra-PPO is a simple and promising solution for enabling more widespread usage of RLHF.",
      "authors": [
        "Santacroce, Michael",
        "Lu, Yadong",
        "Yu, Han",
        "Li, Yuanzhi",
        "Shen, Yelong"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "Efficient RLHF: Reducing the Memory Usage of PPO",
      "urls": [
        "http://arxiv.org/pdf/2309.00754v1",
        "http://arxiv.org/abs/2309.00754v1",
        "http://arxiv.org/pdf/2309.00754.pdf"
      ]
    },
    {
      "abstract": "In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.",
      "authors": [
        "Cao, Yulong",
        "Ivanovic, Boris",
        "Xiao, Chaowei",
        "Pavone, Marco"
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages, 4 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "Reinforcement Learning with Human Feedback for Realistic Traffic Simulation",
      "urls": [
        "http://arxiv.org/pdf/2309.00709v1",
        "http://arxiv.org/pdf/2309.00709.pdf",
        "http://arxiv.org/abs/2309.00709v1"
      ]
    },
    {
      "abstract": "Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.",
      "authors": [
        "Xu, Haotian"
      ],
      "categories": null,
      "citations": null,
      "comments": "still in progress",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
      "urls": [
        "http://arxiv.org/pdf/2309.03224v3",
        "http://arxiv.org/abs/2309.03224v3",
        "http://arxiv.org/pdf/2309.03224.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences. However, gathering high-quality human preference labels can be a time-consuming and expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al., offers a promising alternative that leverages a powerful off-the-shelf LLM to generate preferences in lieu of human annotators. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, RLAIF achieves comparable or superior performance to RLHF, as rated by human evaluators. Furthermore, RLAIF demonstrates the ability to outperform a supervised fine-tuned baseline even when the LLM preference labeler is the same size as the policy. In another experiment, directly prompting the LLM for reward scores achieves superior performance to the canonical RLAIF setup, where LLM preference labels are first distilled into a reward model. Finally, we conduct extensive studies on techniques for generating aligned AI preferences. Our results suggest that RLAIF can achieve human-level performance, offering a potential solution to the scalability limitations of RLHF.",
      "authors": [
        "Lee, Harrison",
        "Phatale, Samrat",
        "Mansoor, Hassan",
        "Mesnard, Thomas",
        "Ferret, Johan",
        "Lu, Kellie",
        "Bishop, Colton",
        "Hall, Ethan",
        "Carbune, Victor",
        "Rastogi, Abhinav",
        "Prakash, Sushant"
      ],
      "categories": null,
      "citations": null,
      "comments": "Added two more tasks and many more experiments and analyses (e.g.\n  same-size RLAIF, direct RLAIF, cost analysis)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
      "urls": [
        "http://arxiv.org/pdf/2309.00267v2",
        "http://arxiv.org/pdf/2309.00267.pdf",
        "http://arxiv.org/abs/2309.00267v2"
      ]
    },
    {
      "abstract": "The Reward Positivity (RewP) is an event-related potential component with a delta band spectral representation that is elicited by reward receipt. Evidence suggests that RewP is modulated by both reward probability as well as affective valuation (\u00e2\u0080\u009cliking\u00e2\u0080\u009d). Here we determined whether RewP is a marker of enhanced hedonic salience of alcohol images in hazardous drinkers. We recruited 54 participants (Hazardous Drinkers = 28, Control = 26) who completed a reinforcement learning task with affective versus alcohol imagery during feedback. The learning task used images of puppies vs. alcohol paired with reinforcing feedback. Both groups rated categories of affective images (puppies, scenery, babies, neutral) similarly, but the hazardous drinking group rated alcohol significantly higher. There were no group differences in performance or in RewP amplitudes, even as a function of alcohol imagery. Contrary to prior findings, we did not observe a significant correlation between alcohol image rating and alcohol-specific RewP amplitude, although we did observe this relationship with the alcohol-specific delta band spectral representation of RewP. Within hazardous drinking group, there was significant correlation between hazardous drinking (AUDIT score) and alcohol-specific RewP indicating an inter-individual influence of drinking habits on affect specific RewP. These findings suggest a domain-specific enhancement of reward responsiveness in hazardous drinkers. \u00c2\u00a9 2023",
      "authors": [
        "Singh, G.",
        "Campbell, E.M.",
        "Hogeveen, J.",
        "Witkiewitz, K.",
        "Claus, E.D.",
        "Cavanagh, J.F."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.pscychresns.2023.111685",
      "keywords": [
        "Incentive salience",
        "Reward positivity",
        "Alcohol",
        "Reinforcement learning",
        "Affect"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09254927",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 0.813,
        "snip": 0.707,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Psychiatry and Mental Health",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Psychiatry Research - Neuroimaging"
      },
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "Affective imagery boosts the reward related delta power in hazardous drinkers",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165959635&origin=inward"
      ]
    },
    {
      "abstract": "The basal ganglia (BG), and more specifically the striatum, have long been proposed to play an essential role in action-selection based on a reinforcement learning (RL) paradigm. However, some recent findings, such as striatal spike-timing-dependent plasticity (STDP) or striatal lateral connectivity, require further research and modelling as their respective roles are still not well understood. Theoretical models of spiking neurons with homeostatic mechanisms, lateral connectivity, and reward-modulated STDP have demonstrated a remarkable capability to learn sensorial patterns that statistically correlate with a rewarding signal. In this article, we implement a functional and biologically inspired network model of the striatum, where learning is based on a previously proposed learning rule called spike-timing-dependent eligibility (STDE), which captures important experimental features in the striatum. The proposed computational model can recognize complex input patterns and consistently choose rewarded actions to respond to such sensorial inputs. Moreover, we assess the role different neuronal and network features, such as homeostatic mechanisms and lateral inhibitory connections, play in action-selection with the proposed model. The homeostatic mechanisms make learning more robust (in terms of suitable parameters) and facilitate recovery after rewarding policy swapping, while lateral inhibitory connections are important when multiple input patterns are associated with the same rewarded action. Finally, according to our simulations, the optimal delay between the action and the dopaminergic feedback is obtained around 300 ms, as demonstrated in previous studies of RL and in biological studies. \u00c2\u00a9 2023 The Author(s)",
      "authors": [
        "Gonz\u00c3\u00a1lez-Redondo, \u00c3\u0081.",
        "Garrido, J.",
        "Naveros Arrabal, F.",
        "Hellgren Kotaleski, J.",
        "Grillner, S.",
        "Ros, E."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neucom.2023.126377",
      "keywords": [
        "Dopamine",
        "Spike-timing-dependent plasticity",
        "Reinforcement learning",
        "Spiking neural network",
        "Eligibility trace",
        "Striatum"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 10.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09252312",
        "publisher": "Elsevier B.V.",
        "sjr": 1.481,
        "snip": 1.853,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neurocomputing"
      },
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "Reinforcement learning in a spiking neural model of striatum plasticity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161709132&origin=inward"
      ]
    },
    {
      "abstract": "The memories for past autobiographical experiences that we share can influence relationship formation and consolidation with important implications for our mental health. However, little is known about how people's responses to our memories can influence subsequent memory sharing. Previous research examined how operant processes (i.e., punishment with aversive sounds) influence the sharing of memories for specific events from our past. Understanding the (social) mechanisms associated with difficulty sharing specific autobiographical memories is important given the association between these difficulties and a range of psychiatric diagnoses. We investigate the effects of verbal and non-verbal social operants on the willingness to share specific autobiographical memories. Participants shared memories with a confederate who coded their memories as specific or non-specific and responded in either an engaged/attentive, dismissive manner or gave no feedback depending on participants' assigned condition. Participants who were reinforced for sharing specific memories and punished for sharing non-specific memories, were more likely to share specific than non-specific memories compared to those who received no feedback. Reinforcement alone was not sufficient for modifying specificity. The ways that we respond to people when they share memories with us can influence their subsequent willingness to share specific events from their past. \u00c2\u00a9 2023 The Authors",
      "authors": [
        "Adelina, N.",
        "Chiu, C.H.M.",
        "Lam, K.",
        "Takano, K.",
        "Barry, T.J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.brat.2023.104385",
      "keywords": [
        "Autobiographical memory",
        "Operant conditioning",
        "Reinforcement learning",
        "Memory specificity",
        "Social sharing"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00057967",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.787,
        "snip": 1.66,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Behaviour Research and Therapy"
      },
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "Social operant conditioning of autobiographical memory sharing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168011285&origin=inward"
      ]
    },
    {
      "abstract": "This article studies the multi- H\u00e2\u0088\u009e controls for the input-interference nonlinear systems via adaptive dynamic programming (ADP) method, which allows for multiple inputs to have the individual selfish component of the strategy to resist weighted interference. In this line, the ADP scheme is used to learn the Nash-optimization solutions of the input-interference nonlinear system such that multiple H\u00e2\u0088\u009einfty performance indices can reach the defined Nash equilibrium. First, the input-interference nonlinear system is given and the Nash equilibrium is defined. An adaptive neural network (NN) observer is introduced to identify the input-interference nonlinear dynamics. Then, the critic NNs are used to learn the multiple H\u00e2\u0088\u009eperformance indices. A novel adaptive law is designed to update the critic NN weights by minimizing the Hamiltonian-Jacobi-Isaacs (HJI) equation, which can be used to directly calculate the multi- H\u00e2\u0088\u009e controls effectively by using input-output data such that the actor structure is avoided. Moreover, the control system stability and updated parameter convergence are proved. Finally, two numerical examples are simulated to verify the proposed ADP scheme for the input-interference nonlinear system. \u00c2\u00a9 2021 IEEE.",
      "authors": [
        "Lv, Y.",
        "Na, J.",
        "Zhao, X.",
        "Huang, Y.",
        "Ren, X."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2021.3130092",
      "keywords": [
        "neural networks (NNs)",
        "H\u00e2\u0088\u009econtrol",
        "nonlinear system",
        "Adaptive dynamic programming (ADP)",
        "multi-input system"
      ],
      "number_of_pages": 13,
      "pages": "5601-5613",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "Multi-H\u221e Controls for Unknown Input-Interference Nonlinear System With Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121352257&origin=inward"
      ]
    },
    {
      "abstract": "Binary feedback, providing information solely about task success or failure, can be sufficient to drive motor learning. While binary feedback can induce explicit adjustments in movement strategy, it remains unclear if this type of feedback also induces implicit learning. We examined this question in a center-out reaching task by gradually moving an invisible reward zone away from a visual target to a final rotation of 7.5\u00b0 or 25\u00b0 in a between-group design. Participants received binary feedback, indicating if the movement intersected the reward zone. By the end of the training, both groups modified their reach angle by about 95% of the rotation. We quantified implicit learning by measuring performance in a subsequent no-feedback aftereffect phase, in which participants were told to forgo any adopted movement strategies and reach directly to the visual target. The results showed a small, but robust (2\u20133\u00b0) aftereffect in both groups, highlighting that binary feedback elicits implicit learning. Notably, for both groups, reaches to two flanking generalization targets were biased in the same direction as the aftereffect. This pattern is at odds with the hypothesis that implicit learning is a form of use-dependent learning. Rather, the results suggest that binary feedback can be sufficient to recalibrate a sensorimotor map.",
      "authors": [
        "van Mastrigt, Nina M.",
        "Tsay, Jonathan S.",
        "Wang, Tianhe",
        "Avraham, Guy",
        "Abram, Sabrina J.",
        "van der Kooij, Katinka",
        "Smeets, Jeroen B. J.",
        "Ivry, Richard B."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00221-023-06683-w",
      "keywords": [
        "Implicit learning",
        "Reward",
        "Visuomotor rotation",
        "Use-dependent learning",
        "Reinforcement learning",
        "Reward-based motor learning"
      ],
      "number_of_pages": 12,
      "pages": "2287-2298",
      "publication": {
        "category": "Journal",
        "cite_score": 3.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00144819",
        "publisher": "Springer Verlag",
        "sjr": 0.662,
        "snip": 0.85,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Experimental Brain Research"
      },
      "publication_date": "2023-09-01",
      "selected": null,
      "title": "Implicit reward-based motor learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167900101&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s00221-023-06683-w.pdf"
      ]
    },
    {
      "abstract": "A well-defined reward function is crucial for successful training of an reinforcement learning (RL) agent. However, defining a suitable reward function is a notoriously challenging task, especially in complex, multi-objective environments. Developers often have to resort to starting with an initial, potentially misspecified reward function, and iteratively adjusting its parameters, based on observed learned behavior. In this work, we aim to automate this process by proposing ITERS, an iterative reward shaping approach using human feedback for mitigating the effects of a misspecified reward function. Our approach allows the user to provide trajectory-level feedback on agent's behavior during training, which can be integrated as a reward shaping signal in the following training iteration. We also allow the user to provide explanations of their feedback, which are used to augment the feedback and reduce user effort and feedback frequency. We evaluate ITERS in three environments and show that it can successfully correct misspecified reward functions.",
      "authors": [
        "Gajcin, Jasmina",
        "McCarthy, James",
        "Nair, Rahul",
        "Marinescu, Radu",
        "Daly, Elizabeth",
        "Dusparic, Ivana"
      ],
      "categories": null,
      "citations": null,
      "comments": "7 pages, 2 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-30",
      "selected": null,
      "title": "Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification",
      "urls": [
        "http://arxiv.org/pdf/2308.15969.pdf",
        "http://arxiv.org/abs/2308.15969v1",
        "http://arxiv.org/pdf/2308.15969v1"
      ]
    },
    {
      "abstract": "<p>Age-related impairments in value representations and updating during decision-making and reward-based learning are often related to age-related attenuation in the catecholamine system such as dopamine (DA) and norepinephrine (NE). However, it is unclear to what extent age-related declines in NE functioning in humans affect reward-based decision-making. We conducted a probabilistic decision-making task and applied a Q-learning model to investigate participants\u2019 anticipatory values and value sensitivities. Task-related pupil dilations and locus coeruleus (LC) magnetic resonance imaging (MRI) contrast, which served as a potential window of the LC-NE functions, were assessed in younger and older adults. Results showed that in both choice and feedback phases, younger adults\u2019 (<i>N</i> = 42, 22 males) pupil dilations negatively correlated with anticipatory values, indicating uncertainty about outcome probabilities. Uncertainty-evoked pupil dilations in older adults (<i>N</i> = 41, 27 males) were smaller, indicating age-related impairments in value estimation and updating. In both age groups, participants who showed a larger uncertainty-evoked pupil dilation exhibited a higher value sensitivity as reflected in the \u03b2 parameter of the reinforcement Q-learning model. Furthermore, older adults (<i>N</i> = 34, 29 males) showed a lower LC-MRI contrast than younger adults (<i>N</i> = 25, 15 males). The LC-MRI contrast positively correlated with value sensitivity only in older but not in younger adults. These findings suggest that task-related pupillary responses can reflect age-related deficits in value estimation and updating during reward-based decision-making. Our evidence with the LC-MRI contrast further showed the age-related decline of the LC structure in modulating value representations during reward-based learning.</p><p><b>SIGNIFICANCE STATEMENT</b> Age-related impairments in value representation and updating during reward-based learning are associated with declines in the catecholamine modulation with age. However, it is unclear how age-related declines in the LC-NE system may affect reward-based learning. Here, we show that compared with younger adults, older adults exhibited reduced uncertainty-induced pupil dilations, suggesting age-related deficits in value estimation and updating. Older adults showed a lower structural MRI of the LC contrast than younger adults, indicating age-related degeneration of the LC structure. The association between the LC-MRI contrast and value sensitivity was only observed in older adults. Our findings may demonstrate a pioneering model to unravel the role of the LC-NE system in reward-based learning in aging.</p>",
      "authors": [
        "Hsiang-Yu Chen",
        "Michael Marxen",
        "Martin J. Dahl",
        "Franka Gl\u00f6ckner"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.2006-22.2023",
      "keywords": [
        "locus coeruleus",
        "norepinephrine",
        "reinforcement learning",
        "decision-making",
        "pupil dilation",
        "aging"
      ],
      "number_of_pages": 12,
      "pages": "6185-6196",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2023-08-30",
      "selected": null,
      "title": "Effects of Adult Age and Functioning of the Locus Coeruleus Norepinephrinergic System on Reward-Based Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169291981&origin=inward"
      ]
    },
    {
      "abstract": "Autonomous navigation of Unmanned Aerial Vehicles (UAVs) has real-life applications in remote sensing, wildlife surveillance, search and rescue operations. A popular training paradigm to learn optimal actions for navigating such complex, dynamic, and uncertain environments is Reinforcement Learning (RL), where the optimal decisions are learnt over time through a reward-feedback received from the environment. However, manually constructing a feedback function that can help guide the UAV to accomplish the desired objective is often very hard. Preference-based Reinforcement Learning (PbRL) is an emerging sub-field of RL where the manual construction of reward function is replaced with human feedback. In this setting, a human is presented with a pair of trajectories followed by the RL agent to elicit the subject\u2019s preference for one over the other. A PbRL algorithm would then compute an optimal sequence of actions using just the set of preferences collected over different trajectories. In this work, we consider PbRL for UAV navigation and follow an ensemble approach to enhance navigation performance. We demonstrate the efficacy of the proposed algorithm through experiments on a range of complex environments and tasks. Ours is the first work that uses human preferences to solve the UAV navigation problem to the best of our knowledge.",
      "authors": [
        "Sambhu H. Karumanchi",
        "Raghuram Bharadwaj Diddigi",
        "K.J. Prabuchandran",
        "Shalabh Bhatnagar"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/RO-MAN57019.2023.10309494",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "499-506",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2023-08-28",
      "selected": null,
      "title": "Autonomous UAV Navigation in Complex Environments using Human Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10309494"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably. \u00c2\u00a9 2023 Copyright held by the owner/author(s).",
      "authors": [
        "Hadas Kotek",
        "Rikker Dockum",
        "David Q. Sun"
      ],
      "categories": null,
      "citations": 1,
      "comments": "ACM Collective Intelligence",
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1145/3582269.3615599",
      "keywords": [
        "stereotypes",
        "bias",
        "explanations",
        "large language models",
        "ethics",
        "gender",
        "occupations"
      ],
      "number_of_pages": 13,
      "pages": "12-24",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400701139",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Computation and Language",
          "Learning",
          "Computers and Society"
        ],
        "title": "In Collective Intelligence Conference (CI '23), November 06-09,\n  2023, Delft, Netherlands. ACM, New York, NY, USA (2023)"
      },
      "publication_date": "2023-08-28",
      "selected": null,
      "title": "Gender bias and stereotypes in Large Language Models",
      "urls": [
        "http://dx.doi.org/10.1145/3582269.3615599",
        "http://arxiv.org/abs/2308.14921v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179687325&origin=inward",
        "https://dl.acm.org/doi/10.1145/3582269.3615599",
        "http://arxiv.org/pdf/2308.14921v1"
      ]
    },
    {
      "abstract": "In recent years, the idea of teleoperating robots to perform manipulation tasks has gained popularity. Being able to take remote control of a robotic system performing an assembly task provides exciting possibilities for industry, not only operating the robot in hazardous environments, but also facilitating the accessibility of human workers, requesting help from a remote expert, or even teaching skills. With the rise of teleoperated manipulators, it is expected that tools that facilitate the interaction of humans with the remote environment will be more prevalent in industry, leading to a greater availability of them. This is the case with Virtual Fixtures (VFs), which are collections of abstract sensory information overlaid on top of reflected sensory feedback from a remote environment. Other ever more prevalent tools are Digital Twins (DTs), virtual representations of systems that facilitate bidirectional communication between the real and the virtual worlds. Being more likely than ever that factories have both an implemented Digital Twin of a system and VFs for a particular manipulation system, we propose a method to leverage both tools. In this paper, methods to integrate already existing VFs in a Reinforcement Learning (RL) pipeline and to use RL to construct an optimal VF to aid the human user in a telemanipulation task are proposed, tackling the cumbersome problems of reward function and VF design. The results show that this approach is able to correctly estimate the right parameters of predefined VFs and open the possibility to future work in this topic. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Diego Fernandez Prado",
        "Eckehard Steinbach"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/CASE56687.2023.10260408",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-0357-0",
        "issn": "2161-8070",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)"
      },
      "publication_date": "2023-08-26",
      "selected": null,
      "title": "Estimating Virtual Fixture Parameters in Digital Twin Environments for Robot Manipulation Tasks using Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10260408",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174384386&origin=inward"
      ]
    },
    {
      "abstract": "Adapting the User Interface (UI) of software systems to user requirements and the context of use is challenging. The main difficulty consists of suggesting the right adaptation at the right time in the right place in order to make it valuable for end-users. We believe that recent progress in Machine Learning techniques provides useful ways in which to support adaptation more effectively. In particular, Reinforcement learning (RL) can be used to personalise interfaces for each context of use in order to improve the user experience (UX). However, determining the reward of each adaptation alternative is a challenge in RL for UI adaptation. Recent research has explored the use of reward models to address this challenge, but there is currently no empirical evidence on this type of model. In this paper, we propose a confirmatory study design that aims to investigate the effectiveness of two different approaches for the generation of reward models in the context of UI adaptation using RL: (1) by employing a reward model derived exclusively from predictive Human-Computer Interaction (HCI) models (HCI), and (2) by employing predictive HCI models augmented by Human Feedback (HCI&HF). The controlled experiment will use an AB/BA crossover design with two treatments: HCI and HCI&HF. We shall determine how the manipulation of these two treatments will affect the UX when interacting with adaptive user interfaces (AUI). The UX will be measured in terms of user engagement and user satisfaction, which will be operationalized by means of predictive HCI models and the Questionnaire for User Interaction Satisfaction (QUIS), respectively. By comparing the performance of two reward models in terms of their ability to adapt to user preferences with the purpose of improving the UX, our study contributes to the understanding of how reward modelling can facilitate UI adaptation using RL.",
      "authors": [
        "Gaspar-Figueiredo, Daniel",
        "Abrah\u00e3o, Silvia",
        "Fern\u00e1ndez-Diego, Marta",
        "Insfran, Emilio"
      ],
      "categories": null,
      "citations": null,
      "comments": "6 pages + 1 refs. 2 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-26",
      "selected": null,
      "title": "A Comparative Study on Reward Models for UI Adaptation with Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2308.13937v2",
        "http://arxiv.org/pdf/2308.13937v2",
        "http://arxiv.org/pdf/2308.13937.pdf"
      ]
    },
    {
      "abstract": "With the development of deep learning techniques, supervised learning has achieved performances surpassing those of humans. Researchers have designed numerous corresponding models for different data modalities, achieving excellent results in supervised tasks. However, with the exponential increase of data in multiple fields, the recognition and classification of unlabeled data have gradually become a hot topic. In this paper, we employed a Reinforcement Learning framework to simulate the cognitive processes of humans for effectively addressing novel class discovery in the Open-set domain. We deployed a Member-to-Leader Multi-Agent framework to extract and fuse features from multi-modal information, aiming to acquire a more comprehensive understanding of the feature space. Furthermore, this approach facilitated the incorporation of self-supervised learning to enhance model training. We employed a clustering method with varying constraint conditions, ranging from strict to loose, allowing for the generation of dependable labels for a subset of unlabeled data during the training phase. This iterative process is similar to human exploratory learning of unknown data. These mechanisms collectively update the network parameters based on rewards received from environmental feedback. This process enables effective control over the extent of exploration learning, ensuring the accuracy of learning in unknown data categories. We demonstrate the performance of our approach in both the 3D and 2D domains by employing the OS-MN40, OS-MN40-Miss, and Cifar10 datasets. Our approach achieves competitive competitive results.",
      "authors": [
        "Li, Qiang",
        "Ma, Qiuyang",
        "Nie, Weizhi",
        "Liu, Anan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-26",
      "selected": null,
      "title": "Reinforcement Learning Based Multi-modal Feature Fusion Network for Novel Class Discovery",
      "urls": [
        "http://arxiv.org/pdf/2308.13801.pdf",
        "http://arxiv.org/abs/2308.13801v1",
        "http://arxiv.org/pdf/2308.13801v1"
      ]
    },
    {
      "abstract": "Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our approach is based on the Markov balance equation and introduces a novel conditional kernel density estimation-based imitation learning framework. It involves estimating the environment's transition dynamics using conditional kernel density estimators and seeks to satisfy the probabilistic balance equations for the environment. We establish that our estimators satisfy basic asymptotic consistency requirements. Through a series of numerical experiments on continuous state benchmark environments, we show consistently superior empirical performance over many state-of-the-art IL algorithms.",
      "authors": [
        "Agrawal, Rishabh",
        "Dahlin, Nathan",
        "Jain, Rahul",
        "Nayyar, Ashutosh"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-24",
      "selected": null,
      "title": "Conditional Kernel Imitation Learning for Continuous State Environments",
      "urls": [
        "http://arxiv.org/abs/2308.12573v1",
        "http://arxiv.org/pdf/2308.12573.pdf",
        "http://arxiv.org/pdf/2308.12573v1"
      ]
    },
    {
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference. Creating a smaller yet potent language model for text rewriting presents a formidable challenge because it requires balancing the need for a small size with the need to retain the emergent capabilities of the LLM, that requires costly data collection. To address the above challenge, we introduce a new instruction tuning approach for building a mobile-centric text rewriting model. Our strategies enable the generation of high quality training data without any human labeling. In addition, we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data. To further bridge the performance gap with the larger server-side model, we propose an effective approach that combines the mobile rewrite agent with the server model using a cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce MessageRewriteEval, a benchmark that focuses on text rewriting for messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size. Notably, we show that our proposed cascading approach improves model performance.",
      "authors": [
        "Zhu, Yun",
        "Liu, Yinxiao",
        "Stahlberg, Felix",
        "Kumar, Shankar",
        "Chen, Yu-hui",
        "Luo, Liangchen",
        "Shu, Lei",
        "Liu, Renjie",
        "Chen, Jindong",
        "Meng, Lei"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-22",
      "selected": null,
      "title": "Towards an On-device Agent for Text Rewriting",
      "urls": [
        "http://arxiv.org/pdf/2308.11807.pdf",
        "http://arxiv.org/pdf/2308.11807v1",
        "http://arxiv.org/abs/2308.11807v1"
      ]
    },
    {
      "abstract": "Recently, deep reinforcement learning (RL) algorithms have been advanced, and especially, Importance Weighted Actor-Learner Architecture (IMPALA) outperformed human expert scores in some of the Atari-2600 games. However, in the Bowling of Atari games where there is a serious sparse reward problem, IMPALA has poor performance. The sparse reward problem, which arises from the requirement of a sequence of actions, is a significant challenge in reinforcement learning. To address this problem, we propose human interactive learning with intrinsic reward as a solution. Combining human interactive learning and intrinsic reward into an RL algorithm, we build a sequential action guiding system in the training agent. As a result of combining the feedback neural network and intrinsic reward, experimental results show efficient convergence to a higher score than the baseline algorithm in Bowling. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Sung-Yun Park",
        "Seung-Jin Hong",
        "Sang-Kwang Lee"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/CoG57401.2023.10333217",
      "keywords": [
        "intrinsic reward",
        "deep reinforcement learning",
        "Human interactive learning"
      ],
      "number_of_pages": 4,
      "pages": "1-4",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-2278-1",
        "issn": "2325-4270",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Conference on Computatonal Intelligence and Games, CIG"
      },
      "publication_date": "2023-08-21",
      "selected": null,
      "title": "Human Interactive Learning with Intrinsic Reward",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180550737&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10333217"
      ]
    },
    {
      "abstract": "Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking the activation differences which result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicate the effect on Llama-13B and GPT-J-6B. Our approach yields inference-time control over high-level properties of output & preserves performance on off-target topics. The method requires far less compute and implementation effort than finetuning and RLHF, allows for natural language specification by users, and its overhead scales naturally with model size.",
      "authors": [
        "Turner, Alexander Matt",
        "Thiergart, Lisa",
        "Udell, David",
        "Leech, Gavin",
        "Mini, Ulisse",
        "MacDiarmid, Monte"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-20",
      "selected": null,
      "title": "Activation Addition: Steering Language Models Without Optimization",
      "urls": [
        "http://arxiv.org/pdf/2308.10248.pdf",
        "http://arxiv.org/abs/2308.10248v3",
        "http://arxiv.org/pdf/2308.10248v3"
      ]
    },
    {
      "abstract": "Large language models (LLMs) have showcased remarkable potential across various tasks by conditioning on prompts. However, the quality of different human-written prompts leads to substantial discrepancies in LLMs' performance, and improving prompts usually necessitates considerable human effort and expertise. To this end, this paper proposes Prompt with Actor-Critic Editing (PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from the actor-critic algorithm in reinforcement learning, PACE leverages LLMs as the dual roles of actors and critics, conceptualizing prompt as a type of policy. PACE refines prompt, taking into account the feedback from both actors performing prompt and critics criticizing response. This process helps LLMs better align prompt to a specific task, thanks to real responses and thinking from LLMs. We conduct extensive experiments on 24 instruction induction tasks and 21 big-bench tasks. Experimental results indicate that PACE elevates the relative performance of medium/low-quality human-written prompts by up to 98\\%, which has comparable performance to high-quality human-written prompts. Moreover, PACE also exhibits notable efficacy for prompt generation.",
      "authors": [
        "Dong, Yihong",
        "Luo, Kangcheng",
        "Jiang, Xue",
        "Jin, Zhi",
        "Li, Ge"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-19",
      "selected": null,
      "title": "PACE: Improving Prompt with Actor-Critic Editing for Large Language Model",
      "urls": [
        "http://arxiv.org/abs/2308.10088v1",
        "http://arxiv.org/pdf/2308.10088.pdf",
        "http://arxiv.org/pdf/2308.10088v1"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.",
      "authors": [
        "Gulcehre, Caglar",
        "Paine, Tom Le",
        "Srinivasan, Srivatsan",
        "Konyushkova, Ksenia",
        "Weerts, Lotte",
        "Sharma, Abhishek",
        "Siddhant, Aditya",
        "Ahern, Alex",
        "Wang, Miaosen",
        "Gu, Chenjie",
        "Macherey, Wolfgang",
        "Doucet, Arnaud",
        "Firat, Orhan",
        "de Freitas, Nando"
      ],
      "categories": null,
      "citations": null,
      "comments": "23 pages, 16 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-17",
      "selected": null,
      "title": "Reinforced Self-Training (ReST) for Language Modeling",
      "urls": [
        "http://arxiv.org/abs/2308.08998v2",
        "http://arxiv.org/pdf/2308.08998v2",
        "http://arxiv.org/pdf/2308.08998.pdf"
      ]
    },
    {
      "abstract": "Affective distress (as observed in anxiety and depression) has been observed to be related to insufficient sensitivity to changing reinforcement during operant learning. Whether such findings are specific to anxiety or depression is unclear given a wider literature relating negative affect to abnormal learning and the possibility that relationships are not consistent across incentive types (i.e., punishment and reward) and outcomes (i.e., positive or negative). In two separate samples (n1 = 100; n2 = 88), participants completed an operant learning task with positive or negative, and neutral socio-affective feedback, designed to assess adaptive responses to changing environmental volatility. Individual parameter estimates were generated with hierarchical Bayesian modelling. Effects of manipulations were modelled by decomposing parameters into a linear combination of effects on the logit scale. While effects tended to support prior work, neither general affective distress nor anxiety or depression were consistently related to a decrease in the adaptive adjustment of learning-rates in response to changing environmental volatility (Sample 1: \u00ce\u00b2\u00ce\u00b1:volatility = \u00e2\u0088\u00920.01, 95 % HDI = \u00e2\u0088\u00920.14, 0.13; Sample 2: \u00ce\u00b2\u00ce\u00b1:volatility = \u00e2\u0088\u00920.15, 95 % HDI = \u00e2\u0088\u00920.37, 0.05). Interaction effects in Sample 1 suggested that while distress was associated with decrements in adaptive learning under punishment-maximisation, it was associated with improvements under reward-maximisation. While our results are broadly consistent with prior work, they suggest that the role of anxiety or depression in volatility learning, if present, is subtle and difficult to detect. Inconsistencies between our samples, along with issues of parameter identifiability complicated interpretation. \u00c2\u00a9 2023 The Authors",
      "authors": [
        "Hammond, D.",
        "Xu, P.",
        "Ai, H.",
        "Van Dam, N.T."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jad.2023.05.021",
      "keywords": [
        "Computational psychiatry",
        "Anxiety",
        "Learning",
        "Social feedback",
        "Reinforcement-learning",
        "Depression"
      ],
      "number_of_pages": 10,
      "pages": "322-331",
      "publication": {
        "category": "Journal",
        "cite_score": 9.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01650327",
        "publisher": "Elsevier B.V.",
        "sjr": 1.988,
        "snip": 1.877,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Journal of Affective Disorders"
      },
      "publication_date": "2023-08-15",
      "selected": null,
      "title": "Anxiety and depression related abnormalities in socio-affective learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159640404&origin=inward"
      ]
    },
    {
      "abstract": "The question of how the brain represents reward prediction errors is central to reinforcement learning and adaptive, goal-directed behavior. Previous studies have revealed prediction error representations in multiple electrophysiological signatures, but it remains elusive whether these electrophysiological correlates underlying prediction errors are sensitive to valence (in a signed form) or to salience (in an unsigned form). One possible reason concerns the loose correspondence between objective probability and subjective prediction resulting from the optimistic bias, that is, the tendency to overestimate the likelihood of encountering positive future events. In the present electroencephalography (EEG) study, we approached this question by directly measuring participants' idiosyncratic, trial-to-trial prediction errors elicited by subjective and objective probabilities across two experiments. We adopted monetary gain and loss feedback in Experiment 1 and positive and negative feedback as communicated by the same zero-value feedback in Experiment 2. We provided electrophysiological evidence in time and time-frequency domains supporting both reward and salience prediction error signals. Moreover, we showed that these electrophysiological signatures were highly flexible and sensitive to an optimistic bias and various forms of salience. Our findings shed new light on multiple presentations of prediction error in the human brain, which differ in format and functional role. \u00c2\u00a9 2023 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.",
      "authors": [
        "Zheng, Y.",
        "Mei, S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/hbm.26398",
      "keywords": [
        "optimistic bias",
        "reward prediction error",
        "salience prediction error",
        "EEG dynamics"
      ],
      "number_of_pages": 16,
      "pages": "4545-4560",
      "publication": {
        "category": "Journal",
        "cite_score": 9.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10659471",
        "publisher": "Wiley-Liss Inc.",
        "sjr": 1.688,
        "snip": 1.395,
        "subject_areas": [
          "Radiological and Ultrasound Technology",
          "Anatomy",
          "Neurology",
          "Neurology (clinical)",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Human Brain Mapping"
      },
      "publication_date": "2023-08-15",
      "selected": null,
      "title": "Neural dissociation between reward and salience prediction errors through the lens of optimistic bias",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162138151&origin=inward"
      ]
    },
    {
      "abstract": "Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.",
      "authors": [
        "Yuan, Youliang",
        "Jiao, Wenxiang",
        "Wang, Wenxuan",
        "Huang, Jen-tse",
        "He, Pinjia",
        "Shi, Shuming",
        "Tu, Zhaopeng"
      ],
      "categories": null,
      "citations": null,
      "comments": "13 pages, 4 figures, 9 tables",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-12",
      "selected": null,
      "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
      "urls": [
        "http://arxiv.org/pdf/2308.06463v1",
        "http://arxiv.org/pdf/2308.06463.pdf",
        "http://arxiv.org/abs/2308.06463v1"
      ]
    },
    {
      "abstract": "In this work, we address the problem of directing the text generation of a language model (LM) towards a desired behavior, aligning the generated text with the preferences of the human operator. We propose using another, instruction-tuned language model as a critic reward model in a zero-shot way thanks to the prompt of a Yes-No question that represents the user preferences, without requiring further labeled data. This zero-shot reward model provides the learning signal to further fine-tune the base LM using Reinforcement Learning from AI Feedback (RLAIF); yet our approach is also compatible in other contexts such as quality-diversity search. Extensive evidence of the capabilities of the proposed ZYN framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. Code available at \\url{https://github.com/vicgalle/zero-shot-reward-models/}.",
      "authors": [
        "Gallego, Victor"
      ],
      "categories": null,
      "citations": null,
      "comments": "pre-print, work in progress",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-11",
      "selected": null,
      "title": "ZYN: Zero-Shot Reward Models with Yes-No Questions for RLAIF",
      "urls": [
        "http://arxiv.org/abs/2308.06385v2",
        "http://arxiv.org/pdf/2308.06385v2",
        "http://arxiv.org/pdf/2308.06385.pdf"
      ]
    },
    {
      "abstract": "Autonomous system refers to a computer system that can operate independently, control itself and make decisions by itself. It does not rely on the support of external systems or software, and can complete various tasks and operations autonomously. Autonomous systems are often highly intelligent and adaptive in that they can acquire environmental information through sensors, analyze and process that information to make appropriate responses, and perform tasks without human intervention. Autonomous systems have a wide range of applications in artificial intelligence, autonomous systems, automated control and other fields, which can improve production efficiency, reduce costs, improve safety and reliability. However, most of the existing methods of autonomous systems are based on traditional machine learning or reinforcement learning. This paper proposes an algorithm based on deep reinforcement learning to improve the learning efficiency of autonomous systems - LHU algorithm. The simulation dialogue experience is obtained from the interaction between autonomous systems and trainable user models. A posterior-session experience is the synthesis of successful session data by the posterior-module by acting on those failed session fragments and user goals. The dialogue experiences obtained from the user model and the posterior module provide more dialogue examples and positive feedback on dialogue strategy learning respectively. Finally, the reliability of the model is verified by experiments. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Qingqing Yang",
        "Mingyao An",
        "Ding Chen"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICIPCA59209.2023.10257710",
      "keywords": [
        "LHU algorithm",
        "innovative research",
        "autonomous system"
      ],
      "number_of_pages": 6,
      "pages": "1478-1483",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-1468-7",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE International Conference on Image Processing and Computer Applications, ICIPCA 2023"
      },
      "publication_date": "2023-08-11",
      "selected": null,
      "title": "Innovative Research on Improving Efficiency of LHU Algorithm in Autonomous System",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10257710",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174386233&origin=inward"
      ]
    },
    {
      "abstract": "The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in shaping the impact of large language models (LLMs), contributing significantly to controlling output toxicity and selecting output styles, particularly as LLMs often harbor misleading content, highlighting the urgency to align them with human values for secure AI systems. The RLHF, characterized by complexity, instability, and sensitivity to hyperparameters, makes the evaluation of the reward model for complex tasks challenging, thereby further complicating the use of Proximal Policy Optimization (PPO). In this paper, we introduce a simple task designed to employ Gloden as a reward model that validates the effectiveness of PPO and inspires it, primarily explaining the task of utilizing PPO to manipulate the tokenizer length of the output generated by the model. Experiments confirm that PPO is not only effective in manipulating the output tokenizer length to a certain extent in this type of task but also exhibits facilitated training once the influence of the reward model effect is excluded, making it an exciting development.",
      "authors": [
        "Fan, Miao",
        "Hu, Chen",
        "Zhou, Shuchang"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-10",
      "selected": null,
      "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length",
      "urls": [
        "http://arxiv.org/abs/2308.05585v1",
        "http://arxiv.org/pdf/2308.05585.pdf",
        "http://arxiv.org/pdf/2308.05585v1"
      ]
    },
    {
      "abstract": "To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RLHF-Blender. More information is available at https://rlhfblender.info/.",
      "authors": [
        "Metz, Yannick",
        "Lindner, David",
        "Baur, Rapha\u00ebl",
        "Keim, Daniel",
        "El-Assady, Mennatallah"
      ],
      "categories": null,
      "citations": null,
      "comments": "14 pages, 3 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Learning",
          "Human-Computer Interaction"
        ],
        "title": "ICML2023 Interactive Learning from Implicit Human Feedback\n  Workshop"
      },
      "publication_date": "2023-08-08",
      "selected": null,
      "title": "RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2308.04332.pdf",
        "http://arxiv.org/pdf/2308.04332v1",
        "http://arxiv.org/abs/2308.04332v1"
      ]
    },
    {
      "abstract": "Quality of Experience~(QoE)-driven adaptive bitrate (ABR) algorithms are typically optimized using QoE models that are based on the mean opinion score~(MOS), while such principles may not account for user heterogeneity on rating scales, resulting in unexpected behaviors. In this paper, we propose Jade, which leverages reinforcement learning with human feedback~(RLHF) technologies to better align the users' opinion scores. Jade's rank-based QoE model considers relative values of user ratings to interpret the subjective perception of video sessions. We implement linear-based and Deep Neural Network (DNN)-based architectures for satisfying both accuracy and generalization ability. We further propose entropy-aware reinforced mechanisms for training policies with the integration of the proposed QoE models. Experimental results demonstrate that Jade performs favorably on conventional metrics, such as quality and stall ratio, and improves QoE by 8.09%-38.13% in different network conditions, emphasizing the importance of user heterogeneity in QoE modeling and the potential of combining linear-based and DNN-based models for performance improvement.",
      "authors": [
        "Huang, Tianchi",
        "Zhang, Rui-Xiao",
        "Wu, Chenglei",
        "Sun, Lifeng"
      ],
      "categories": null,
      "citations": null,
      "comments": "ACM Multimedia 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-08",
      "selected": null,
      "title": "Optimizing Adaptive Video Streaming with Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2308.04132.pdf",
        "http://arxiv.org/pdf/2308.04132v2",
        "http://arxiv.org/abs/2308.04132v2"
      ]
    },
    {
      "abstract": "The proceedings contain 94 papers. The topics discussed include: changing distributions and preferences in learning systems; protecting children from online exploitation: can a trained model detect harmful communication strategies?; analysis of climate campaigns on social media using Bayesian model averaging; analysis of climate campaigns on social media using Bayesian model averaging; from preference elicitation to participatory ML: a critical survey & guidelines for future research; how does value similarity affect human reliance in AI-assisted ethical decision making?; user tampering in reinforcement learning recommender systems; beyond the ML model: applying safety engineering frameworks to text-to-image development; a systematic review of ethical concerns with voice assistants; the ethical implications of generative audio models: a systematic literature review; and learning optimal fair decision trees: trade-offs between interpretability, fairness, and accuracy.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400702310",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publication_date": "2023-08-08",
      "selected": null,
      "title": "AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173789347&origin=inward"
      ]
    },
    {
      "abstract": "Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot align responses with experts' intentions. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from continuous pre-training, SFT, to Reinforcement Learning from Human Feedback (RLHF). Additionally, we construct a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We also define a refined annotation rule and evaluation criteria given the unique characteristics of the biomedical domain. Extensive experimental results show that Zhongjing outperforms baselines in various capacities and matches the performance of ChatGPT in some abilities, despite the 100x parameters. Ablation studies also demonstrate the contributions of each component: pre-training enhances medical knowledge, and RLHF further improves instruction-following ability and safety. Our code, datasets, and models are available at https://github.com/SupritYoung/Zhongjing.",
      "authors": [
        "Yang, Songhua",
        "Zhao, Hanjie",
        "Zhu, Senbin",
        "Zhou, Guangyu",
        "Xu, Hongfei",
        "Jia, Yuxiang",
        "Zan, Hongying"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-07",
      "selected": null,
      "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue",
      "urls": [
        "http://arxiv.org/pdf/2308.03549v3",
        "http://arxiv.org/pdf/2308.03549.pdf",
        "http://arxiv.org/abs/2308.03549v3"
      ]
    },
    {
      "abstract": "Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (\\textit{e.g.,} BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences. This is a computational challenge as presented by the practice of sequence generation problems, such as machine translation, where we often deal with a large action space (\\textit{e.g.,} a vocabulary) and a long action sequence (\\textit{e.g.,} a translation). In this work, we introduce two-stage sampling and dynamic sampling approaches to improve the sampling efficiency during training sequence generation models via RL. We experiment with our approaches on the traditional sequence generation tasks, including machine translation and abstractive summarization. Furthermore, we evaluate our approaches in RL from human feedback (RLHF) through training a large language model using the reward model. Experimental results show that the efficient sampling-based RL, referred to as ESRL, can outperform all baselines in terms of both training efficiency and memory consumption. Notably, ESRL yields consistent performance gains over the strong REINFORCE, minimum risk training, and proximal policy optimization methods.",
      "authors": [
        "Wang, Chenglong",
        "Zhou, Hang",
        "Hu, Yimin",
        "Huo, Yifu",
        "Li, Bei",
        "Liu, Tongran",
        "Xiao, Tong",
        "Zhu, Jingbo"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-04",
      "selected": null,
      "title": "ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation",
      "urls": [
        "http://arxiv.org/pdf/2308.02223v1",
        "http://arxiv.org/abs/2308.02223v1",
        "http://arxiv.org/pdf/2308.02223.pdf"
      ]
    },
    {
      "abstract": "In microservice systems, the identification of root causes of anomalies is imperative for service reliability and business impact. This process is typically divided into two phases: (i)constructing a service dependency graph that outlines the sequence and structure of system components that are invoked, and (ii) localizing the root cause components using the graph, traces, logs, and Key Performance Indicators (KPIs) such as latency. However, both phases are not straightforward due to the highly dynamic and complex nature of the system, particularly in large-scale commercial architectures like Microsoft Exchange. In this paper, we propose a new framework that employs Hierarchical Reinforcement Learning from Human Feedback (HRLHF) to address these challenges. Our framework leverages the static topology of the microservice system and efficiently employs the feedback of engineers to reduce uncertainty in the discovery of the service dependency graph. The framework utilizes reinforcement learning to reduce the number of queries required from O(N2) to O(1), enabling the construction of the dependency graph with high accuracy and minimal human effort. Additionally, we extend the discovered dependency graphs to window causal graphs that capture the characteristics of time series over a specified time period, resulting in improved root cause analysis accuracy and robustness. Evaluations on both real datasets from Microsoft Exchange and synthetic datasets with injected anomalies demonstrate superior performance on various metrics compared to state-of-the-art methods. It is worth mentioning that, our framework has been integrated as a crucial component in Microsoft M365 Exchange service. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Lu Wang",
        "Chaoyun Zhang",
        "Ruomeng Ding",
        "Yong Xu",
        "Qihang Chen",
        "Wentao Zou",
        "Qingjun Chen",
        "Meng Zhang",
        "Xuedong Gao",
        "Hao Fan",
        "Saravan Rajmohan",
        "Qingwei Lin",
        "Dongmei Zhang"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3580305.3599934",
      "keywords": [
        "root cause analysis",
        "reinforcement learning from human feedback",
        "causal discovery"
      ],
      "number_of_pages": 10,
      "pages": "5116-5125",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400701030",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
      },
      "publication_date": "2023-08-04",
      "selected": null,
      "title": "Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171324149&origin=inward",
        "https://dl.acm.org/doi/10.1145/3580305.3599934"
      ]
    },
    {
      "abstract": "Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users' historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they do not require any expert knowledge. With PrefRec, we can fully exploit the advantages of RL in optimizing long-term goals, while avoiding complex reward engineering. PrefRec uses the preferences to automatically train a reward function in an end-to-end manner. The reward function is then used to generate learning signals to train the recommendation policy. Furthermore, we design an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance. We conduct experiments on a variety of long-term user engagement optimization tasks. The results show that PrefRec significantly outperforms previous state-of-the-art methods in all the tasks. \u00c2\u00a9 2023 Owner/Author.",
      "authors": [
        "Wanqi Xue",
        "Qingpeng Cai",
        "Zhenghai Xue",
        "Shuo Sun",
        "Shuchang Liu",
        "Dong Zheng",
        "Peng Jiang",
        "Kun Gai",
        "Bo An"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3580305.3599473",
      "keywords": [
        "reinforcement learning with human preferences",
        "recommender systems",
        "long-term user engagement"
      ],
      "number_of_pages": 11,
      "pages": "2874-2884",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400701030",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
      },
      "publication_date": "2023-08-04",
      "selected": null,
      "title": "PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3580305.3599473",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171379753&origin=inward"
      ]
    },
    {
      "abstract": "ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost. With this development, DeepSpeed-Chat paves the way for broader access to advanced RLHF training, even for data scientists with limited resources, thereby fostering innovation and further development in the field of AI.",
      "authors": [
        "Yao, Zhewei",
        "Aminabadi, Reza Yazdani",
        "Ruwase, Olatunji",
        "Rajbhandari, Samyam",
        "Wu, Xiaoxia",
        "Awan, Ammar Ahmad",
        "Rasley, Jeff",
        "Zhang, Minjia",
        "Li, Conglong",
        "Holmes, Connor",
        "Zhou, Zhongzhu",
        "Wyatt, Michael",
        "Smith, Molly",
        "Kurilenko, Lev",
        "Qin, Heyang",
        "Tanaka, Masahiro",
        "Che, Shuai",
        "Song, Shuaiwen Leon",
        "He, Yuxiong"
      ],
      "categories": null,
      "citations": null,
      "comments": "14 pages, 7 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-08-02",
      "selected": null,
      "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales",
      "urls": [
        "http://arxiv.org/abs/2308.01320v1",
        "http://arxiv.org/pdf/2308.01320v1",
        "http://arxiv.org/pdf/2308.01320.pdf"
      ]
    },
    {
      "abstract": "Based on the growing evidence on caring and enduring relationships displayed by species across the evolutionary ladder, the ubiquity and importance of environmental uncertainty faced by all organisms, and the adaptational principle that learning may involve preference learning besides instrumental reinforcement learning, this paper proposes a novel information theoretic model of affective bonding, focusing on humans. A special case of the proposed \u00e2\u0080\u009cinformational affective tie mechanism\u00e2\u0080\u009d (iATM) turns out to be the model of Bault, Fahrenfort, Pelloux, Ridderinkhof, and van Winden: An affective social tie mechanism, Journal of Economic Psychology, 2017, 61, 152\u00e2\u0080\u0093175. In further contrast to the latter model, the iATM model allows for the role of multiple contexts and distributed attention. Moreover, it provides a dynamic, context related, endogenous representation of the well-known social value orientation construct, facilitating the propagation of caring as observed in the literature. Empirical support is provided along different dimensions. Although the model is not estimated in full detail, a necessary condition regarding its parameters is shown to be fulfilled. Furthermore, experimental findings concerning various well-known games can be tracked under plausible calibration. In addition, the mechanism can be linked to neurobiological evidence concerning maternal (and paternal) care \u00e2\u0080\u0093 as the presumed primordial caregiving system \u00e2\u0080\u0093 and the signaling role of oxytocin. Finally, the evidence concerning non-human species is addressed, as well as the role of norms and reciprocity. \u00c2\u00a9 2023 The Author(s)",
      "authors": [
        "van Winden, F."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.joep.2023.102625",
      "keywords": [
        "Uncertainty-based model",
        "Public good",
        "Empirical support",
        "Social preference learning",
        "Affective ties"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01674870",
        "publisher": "Elsevier B.V.",
        "sjr": 1.601,
        "snip": 1.527,
        "subject_areas": [
          "Economics and Econometrics",
          "Applied Psychology",
          "Sociology and Political Science"
        ],
        "title": "Journal of Economic Psychology"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "The informational affective tie mechanism: on the role of uncertainty, context, and attention in caring",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152131274&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) has become widely adopted in robot control. Despite many successes, one major persisting problem can be very low data efficiency. One solution is interactive feedback, which has been shown to speed up RL considerably. As a result, there is an abundance of different strategies, which are, however, primarily tested on discrete grid-world and small scale optimal control scenarios. In the literature, there is no consensus about which feedback frequency is optimal or at which time the feedback is most beneficial. To resolve these discrepancies we isolate and quantify the effect of feedback frequency in robotic tasks with continuous state and action spaces. The experiments encompass inverse kinematics learning for robotic manipulator arms of different complexity. We show that seemingly contradictory reported phenomena occur at different complexity levels. Furthermore, our results suggest that no single ideal feedback frequency exists. Rather that feedback frequency should be changed as the agent\u2019s proficiency in the task increases.",
      "authors": [
        "Harnack, Daniel",
        "Pivin-Bachler, Julie",
        "Navarro-Guerrero, Nicol\u00e1s"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00521-022-07949-0",
      "keywords": [
        "Guided exploration",
        "Intrinsic feedback homology",
        "Human-aligned reinforcement learning",
        "Interactive reinforcement learning"
      ],
      "number_of_pages": 13,
      "pages": "16931-16943",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09410643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "Quantifying the effect of feedback frequency in interactive reinforcement learning for robotic tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143315822&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s00521-022-07949-0.pdf"
      ]
    },
    {
      "abstract": "When deciding between options that do or do not lead to future choices, humans often choose to choose. We studied choice seeking by asking subjects to first decide between a choice opportunity or performing a computer-selected action, after which they either chose freely or performed the forced action. Subjects preferred choice when these options were equally rewarded, even deterministically, and traded extrinsic rewards for opportunities to choose. We explained individual variability in choice seeking using reinforcement learning models incorporating risk sensitivity and overvaluation of rewards obtained through choice. Model fits revealed that 28% of subjects were sensitive to the worst possible outcome associated with free choice, and this pessimism reduced their choice preference with increasing risk. Moreover, outcome overvaluation was necessary to explain patterns of individual choice preference across levels of risk. We also manipulated the degree to which subjects controlled stimulus outcomes. We found that degrading coherence between their actions and stimulus outcomes diminished choice preference following forced actions, although willingness to repeat selection of choice opportunities remained high. When subjects chose freely during these repeats, they were sensitive to rewards when actions were controllable but ignored outcomes\u2013even positive ones\u2013associated with reduced controllability. Our results show that preference for choice can be modulated by extrinsic reward properties including reward probability and risk as well as by controllability of the environment.",
      "authors": [
        "J\u00e9r\u00f4me Munuera",
        "Marta Ribes Agost",
        "David Bendetowicz",
        "Adrien Kerebel",
        "Val\u00e9rian Chambon",
        "Brian Lau"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1010551",
      "keywords": [
        "Experimental design",
        "Learning",
        "Fractals",
        "Behavior",
        "Decision making",
        "Motivation",
        "Reaction time",
        "Psychological attitudes"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "Intrinsic motivation for choice varies with individual risk attitudes and the controllability of the environment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169848291&origin=inward",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1010551&type=printable"
      ]
    },
    {
      "abstract": "The growing development of autonomous systems is driving the application of mobile robots in crowded environments. These scenarios often require robots to satisfy multiple conflicting objectives with different relative preferences, such as work efficiency, safety, and smoothness, which inherently cause robots\u2019 poor exploration in seeking policies optimizing several performance criteria. In this paper, we propose a multi-objective deep reinforcement learning framework for crowd-aware robot navigation problems to learn policies over multiple competing objectives whose relative importance preference is dynamic to the robot. First, a two-stream structure is introduced to separately extract the spatial and temporal features of pedestrian motion characteristics. Second, to learn navigation policies for each possible preference, a multi-objective deep reinforcement learning method is proposed to maximize a weighted-sum scalarization of different objective functions. We consider path planning and path tracking tasks, which focus on conflicting objectives of collision avoidance, target reaching, and path following. Experimental results demonstrate that our method can effectively navigate through crowds in simulated environments while satisfying different task requirements.",
      "authors": [
        "Cheng, Guangran",
        "Wang, Yuanda",
        "Dong, Lu",
        "Cai, Wenzhe",
        "Sun, Changyin"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00521-023-08385-4",
      "keywords": [
        "Mobile robot",
        "Path planning",
        "Multi-objective deep reinforcement learning",
        "Path tracking",
        "Crowd-aware navigation"
      ],
      "number_of_pages": 19,
      "pages": "16247-16265",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09410643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "Multi-objective deep reinforcement learning for crowd-aware robot navigation with dynamic human preference",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00521-023-08385-4.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161891668&origin=inward"
      ]
    },
    {
      "abstract": "In order to deploy robots that could be adapted by non-expert users, interactive imitation learning (IIL) methods must be flexible regarding the interaction preferences of the teacher and avoid assumptions of perfect teachers (oracles), while considering they make mistakes influenced by diverse human factors. In this work, we propose an IIL method that improves the human\u2013robot interaction for non-expert and imperfect teachers in two directions. First, uncertainty estimation is included to endow the agents with a lack of knowledge awareness (epistemic uncertainty) and demonstration ambiguity awareness (aleatoric uncertainty), such that the robot can request human input when it is deemed more necessary. Second, the proposed method enables the teachers to train with the flexibility of using corrective demonstrations, evaluative reinforcements, and implicit positive feedback. The experimental results show an improvement in learning convergence with respect to other learning methods when the agent learns from highly ambiguous teachers. Additionally, in a user study, it was found that the components of the proposed method improve the teaching experience and the data efficiency of the learning process.",
      "authors": [
        "Celemin, Carlos",
        "Kober, Jens"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00521-022-08118-z",
      "keywords": [
        "Uncertainty",
        "Human reinforcement",
        "Interactive imitation learning",
        "Corrective demonstrations",
        "Active learning"
      ],
      "number_of_pages": 19,
      "pages": "16821-16839",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09410643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "Knowledge- and ambiguity-aware robot learning from corrective and evaluative feedback",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00521-022-08118-z.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146280338&origin=inward"
      ]
    },
    {
      "abstract": "For an Artificially Intelligent (AI) system to maintain alignment between human desires and its behaviour, it is important that the AI account for human preferences. This paper proposes and empirically evaluates the first approach to aligning agent behaviour to human preference via an apologetic framework. In practice, an apology may consist of an acknowledgement, an explanation and an intention for the improvement of future behaviour. We propose that such an apology, provided in response to recognition of undesirable behaviour, is one way in which an AI agent may both be transparent and trustworthy to a human user. Furthermore, that behavioural adaptation as part of apology is a viable approach to correct against undesirable behaviours. The Act-Assess-Apologise framework potentially could address both the practical and social needs of a human user, to recognise and make reparations against prior undesirable behaviour and adjust for the future. Applied to a dual-auxiliary impact minimisation problem, the apologetic agent had a near perfect determination and apology provision accuracy in several non-trivial configurations. The agent subsequently demonstrated behaviour alignment with success that included up to complete avoidance of the impacts described by these objectives in some scenarios.",
      "authors": [
        "Harland, Hadassah",
        "Dazeley, Richard",
        "Nakisa, Bahareh",
        "Cruz, Francisco",
        "Vamplew, Peter"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00521-023-08586-x",
      "keywords": [
        "AI apology",
        "Multi-objective reinforcement learning",
        "AI safety",
        "Impact minimisation",
        "Human alignment"
      ],
      "number_of_pages": 14,
      "pages": "16917-16930",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09410643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "AI apology: interactive multi-objective reinforcement learning for human-aligned AI",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00521-023-08586-x.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153283220&origin=inward"
      ]
    },
    {
      "abstract": "Robot motor skills can be acquired by deep reinforcement learning as neural networks to reflect state\u2013action mapping. The selection of states has been demonstrated to be crucial for successful robot motor learning. However, because of the complexity of neural networks, human insights and engineering efforts are often required to select appropriate states through qualitative approaches, such as ablation studies, without a quantitative analysis of the state importance. Here we present a systematic saliency analysis that quantitatively evaluates the relative importance of different feedback states for motor skills learned through deep reinforcement learning. Our approach provides a guideline to identify the most essential feedback states for robot motor learning. By using only the important states including joint positions, gravity vector and base linear and angular velocities, we demonstrate that a simulated quadruped robot can learn various robust locomotion skills. We find that locomotion skills learned only with important states can achieve task performance comparable to the performance of those with more states. This work provides quantitative insights into the impacts of state observations on specific types of motor skills, enabling the learning of a wide range of motor skills with minimal sensing dependencies. Traditional feedback-state selection in robot learning is empirical and requires substantial engineering efforts. Yu et al. develop a quantitative and systematic state-importance analysis, revealing crucial feedback signals for learning locomotion skills.",
      "authors": [
        "Yu, Wanming",
        "Yang, Chuanyu",
        "McGreavy, Christopher",
        "Triantafyllidis, Eleftherios",
        "Bellegarda, Guillaume",
        "Shafiee, Milad",
        "Ijspeert, Auke Jan",
        "Li, Zhibin"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s42256-023-00701-w",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "919-932",
      "publication": {
        "category": "Journal",
        "cite_score": 32.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2522-5839",
        "publisher": "Springer International Publishing",
        "sjr": 6.21,
        "snip": 6.723,
        "subject_areas": [
          "Artificial Intelligence",
          "Software",
          "Computer Networks and Communications",
          "Human-Computer Interaction",
          "Computer Vision and Pattern Recognition"
        ],
        "title": "Nature Machine Intelligence"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "Identifying important sensory feedback for learning locomotion skills",
      "urls": [
        "https://www.nature.com/articles/s42256-023-00701-w.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168542639&origin=inward"
      ]
    },
    {
      "abstract": "Reward Positivity (RewP) is a feedback-locked event-related potential component that is specifically elicited by rewarding feedback and scales with positive reward prediction error, a hallmark of reinforcement learning models. The RewP is also diminished in depression, suggesting that it may be a novel marker of anhedonia. Here, we examined if a sad mood induction offered an opportunity to causally induce a mood-related alteration of the RewP and reward-related learning. In Experiment 1 (N\u00c2 =\u00c2 50 total), participants were randomly assigned to previously established sad or neutral mood induction procedures before a probabilistic selection task. This manipulation failed to induce changes in affect, suggesting that standard methods are inadequate. In Experiment 2 (N\u00c2 =\u00c2 50 total), participants were randomly assigned to newly developed happy versus sad mood manipulations, which successfully induced large changes in affect. While the RewP was unaffected by mood induction, positive mood moderated the relationship between prediction error encoding in the RewP and reward learning, such that low positive mood and low prediction error encoding resulted in poorer reward learning. These findings provide a mechanistic example of how reduced positive affect moderates reward learning via poorer information encoding in the RewP. \u00c2\u00a9 2023 Society for Psychophysiological Research.",
      "authors": [
        "Jackson, T.C.J.",
        "Cavanagh, J.F."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.14276",
      "keywords": [
        "Reward Positivity",
        "mood induction",
        "reward learning",
        "reward prediction error"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "Reduced positive affect alters reward learning via reduced information encoding in the Reward Positivity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148452815&origin=inward"
      ]
    },
    {
      "abstract": "This study focuses on the home health care routing problem (HHCRP) in the scenario of high population density areas where many elders live closely together. This study considers two main objectives. The first is to reduce travel and wait times for nurses or elders. The second concerns socially related objectives in scheduling problems, such as \u00e2\u0080\u0098quality of life\u00e2\u0080\u0099 and empowerment, by considering assumptions related to the acquaintanceship and mutual preferences of nurses and elders. This study models the effects of mutual preferences and acquaintanceship on service time in HHCRP. We use the Markov decision process and chance-constrained programming (CCP) to model the system to conserve the sequential service provision parameters and better represent the influence of stochastic service times. Because traditional deterministic algorithms cannot solve such a model, we apply a model-free reinforcement learning algorithm, Q-learning (QL), as well as the ant colony optimisation (ACO) algorithm. Thus, we tackle this problem by developing a model and algorithm to solve complex, large-scale systems. This study's theoretical and practical contributions are verified by feedback from researchers and practitioners. \u00c2\u00a9 2023 The Author(s)",
      "authors": [
        "Ting Zhang",
        "Yang Liu",
        "Xintong Yang",
        "Jingjing Chen",
        "Jiaming Huang"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.cie.2023.109332",
      "keywords": [
        "Q-learning",
        "Home health care",
        "Vehicle routing problem",
        "Mixed time windows",
        "Stochastic service time",
        "Ant-colony optimisation"
      ],
      "number_of_pages": 14,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 11.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0360-8352",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.76,
        "snip": 2.238,
        "subject_areas": [
          "Computer Science (all)",
          "Engineering (all)"
        ],
        "title": "Computers and Industrial Engineering"
      },
      "publication_date": "2023-08-01",
      "selected": null,
      "title": "Home health care routing and scheduling in densely populated communities considering complex human behaviours",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.cie.2023.109332",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163064962&origin=inward"
      ]
    },
    {
      "abstract": "The trustworthiness of machine learning has emerged as a critical topic in the field, encompassing various applications and research areas such as robustness, security, interpretability, and fairness. The last decade saw the development of numerous methods addressing these challenges. In this survey, we systematically review these advancements from a data-centric perspective, highlighting the shortcomings of traditional empirical risk minimization (ERM) training in handling challenges posed by the data. Interestingly, we observe a convergence of these methods, despite being developed independently across trustworthy machine learning subfields. Pearl's hierarchy of causality offers a unifying framework for these techniques. Accordingly, this survey presents the background of trustworthy machine learning development using a unified set of concepts, connects this language to Pearl's causal hierarchy, and finally discusses methods explicitly inspired by causality literature. We provide a unified language with mathematical vocabulary to link these methods across robustness, adversarial robustness, interpretability, and fairness, fostering a more cohesive understanding of the field. Further, we explore the trustworthiness of large pretrained models. After summarizing dominant techniques like fine-tuning, parameter-efficient fine-tuning, prompting, and reinforcement learning with human feedback, we draw connections between them and the standard ERM. This connection allows us to build upon the principled understanding of trustworthy methods, extending it to these new techniques in large pretrained models, paving the way for future methods. Existing methods under this perspective are also reviewed. Lastly, we offer a brief summary of the applications of these methods and discuss potential future aspects related to our survey. For more information, please visit http://trustai.one.",
      "authors": [
        "Liu, Haoyang",
        "Chaudhary, Maheep",
        "Wang, Haohan"
      ],
      "categories": null,
      "citations": null,
      "comments": "47 pages, 13 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-31",
      "selected": null,
      "title": "Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives",
      "urls": [
        "http://arxiv.org/pdf/2307.16851v1",
        "http://arxiv.org/abs/2307.16851v1",
        "http://arxiv.org/pdf/2307.16851.pdf"
      ]
    },
    {
      "abstract": "This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.",
      "authors": [
        "White, Devin",
        "Wu, Mingkang",
        "Novoseller, Ellen",
        "Lawhern, Vernon J.",
        "Waytowich, Nicholas",
        "Cao, Yongcan"
      ],
      "categories": null,
      "citations": null,
      "comments": "This is an extended version of the paper \"Rating-based Reinforcement\n  Learning\" accepted to the 38th Annual AAAI Conference on Artificial\n  Intelligence",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-30",
      "selected": null,
      "title": "Rating-based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2307.16348v2",
        "http://arxiv.org/pdf/2307.16348.pdf",
        "http://arxiv.org/abs/2307.16348v2"
      ]
    },
    {
      "abstract": "Incorporating Autonomous Vehicles (AVs) into existing transportation systems necessitates examining their coexistence with Human-driven Vehicles (HVs) in mixed traffic environments. Central to this coexistence is the AVs' ability to emulate human-like interaction intentions within traffic scenarios. We introduce a novel framework for planning unprotected left-turn trajectories for AVs, designed to mirror human driving behaviors and effectively communicate social intentions. This framework consists of three phases: trajectory generation, evaluation, and selection.In the trajectory generation phase, we utilize real human-driving trajectory data to establish constraints for a predicted trajectory space, creating candidate motion trajectories that reflect intent. The evaluation phase incorporates maximum entropy inverse reinforcement learning (ME-IRL) to gauge human trajectory preferences, considering aspects like traffic efficiency, driving comfort, and interactive safety. During the selection phase, a Boltzmann distribution-based approach is employed to assign rewards and probabilities to the candidate trajectories, promoting human-like decision-making. We validate our framework using an authentic trajectory dataset and conduct a comparative analysis with various baseline methods. Our results, derived from simulator tests and human-in-the-loop driving experiments, affirm our framework's superiority in mimicking human-like driving, expressing intent, and computational efficiency. For additional information of this research, please visit https://shorturl.at/jqu35.",
      "authors": [
        "Liu, Jiaqi",
        "Qi, Xiao",
        "Ni, Ying",
        "Sun, Jian",
        "Hang, Peng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-29",
      "selected": null,
      "title": "Teaching Autonomous Vehicles to Express Interaction Intent during Unprotected Left Turns: A Human-Driving-Prior-Based Trajectory Planning Approach",
      "urls": [
        "http://arxiv.org/pdf/2307.15950v2",
        "http://arxiv.org/abs/2307.15950v2",
        "http://arxiv.org/pdf/2307.15950.pdf"
      ]
    },
    {
      "abstract": "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.",
      "authors": [
        "Lai, Viet Dac",
        "Van Nguyen, Chien",
        "Ngo, Nghia Trung",
        "Nguyen, Thuat",
        "Dernoncourt, Franck",
        "Rossi, Ryan A.",
        "Nguyen, Thien Huu"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-29",
      "selected": null,
      "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2307.16039v2",
        "http://arxiv.org/pdf/2307.16039.pdf",
        "http://arxiv.org/abs/2307.16039v2"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation tasks with varying levels of complexity. Our results show that SEED significantly outperforms state-of-the-art RL algorithms in sample efficiency and safety. In addition, SEED also exhibits a substantial reduction of human effort compared to other RLHF methods. Further details and video results can be found at https://seediros23.github.io/.",
      "authors": [
        "Hiranaka, Ayano",
        "Hwang, Minjune",
        "Lee, Sharon",
        "Wang, Chen",
        "Fei-Fei, Li",
        "Wu, Jiajun",
        "Zhang, Ruohan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-28",
      "selected": null,
      "title": "Primitive Skill-based Robot Learning from Human Evaluative Feedback",
      "urls": [
        "http://arxiv.org/abs/2307.15801v2",
        "http://arxiv.org/pdf/2307.15801v2",
        "http://arxiv.org/pdf/2307.15801.pdf"
      ]
    },
    {
      "abstract": "Natural language instruction following is paramount to enable collaboration between artificial agents and human beings. Natural language-conditioned reinforcement learning (RL) agents have shown how natural languages' properties, such as compositionality, can provide a strong inductive bias to learn complex policies. Previous architectures like HIGhER combine the benefit of language-conditioning with Hindsight Experience Replay (HER) to deal with sparse rewards environments. Yet, like HER, HIGhER relies on an oracle predicate function to provide a feedback signal highlighting which linguistic description is valid for which state. This reliance on an oracle limits its application. Additionally, HIGhER only leverages the linguistic information contained in successful RL trajectories, thus hurting its final performance and data-efficiency. Without early successful trajectories, HIGhER is no better than DQN upon which it is built. In this paper, we propose the Emergent Textual Hindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses both of its limitations by means of (i) a discriminative visual referential game, commonly studied in the subfield of Emergent Communication (EC), used here as an unsupervised auxiliary task and (ii) a semantic grounding scheme to align the emergent language with the natural language of the instruction-following benchmark. We show that the referential game's agents make an artificial language emerge that is aligned with the natural-like language used to describe goals in the BabyAI benchmark and that it is expressive enough so as to also describe unsuccessful RL trajectories and thus provide feedback to the RL agent to leverage the linguistic, structured information contained in all trajectories. Our work shows that EC is a viable unsupervised auxiliary task for RL and provides missing pieces to make HER more widely applicable.",
      "authors": [
        "Denamgana\u00ef, Kevin",
        "Hernandez, Daniel",
        "Vardal, Ozan",
        "Missaoui, Sondess",
        "Walker, James Alfred"
      ],
      "categories": null,
      "citations": null,
      "comments": "work in progress",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-28",
      "selected": null,
      "title": "ETHER: Aligning Emergent Communication for Hindsight Experience Replay",
      "urls": [
        "http://arxiv.org/pdf/2307.15494v2",
        "http://arxiv.org/abs/2307.15494v2",
        "http://arxiv.org/pdf/2307.15494.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.",
      "authors": [
        "Casper, Stephen",
        "Davies, Xander",
        "Shi, Claudia",
        "Gilbert, Thomas Krendl",
        "Scheurer, J\u00e9r\u00e9my",
        "Rando, Javier",
        "Freedman, Rachel",
        "Korbak, Tomasz",
        "Lindner, David",
        "Freire, Pedro",
        "Wang, Tony",
        "Marks, Samuel",
        "Segerie, Charbel-Rapha\u00ebl",
        "Carroll, Micah",
        "Peng, Andi",
        "Christoffersen, Phillip",
        "Damani, Mehul",
        "Slocum, Stewart",
        "Anwar, Usman",
        "Siththaranjan, Anand",
        "Nadeau, Max",
        "Michaud, Eric J.",
        "Pfau, Jacob",
        "Krasheninnikov, Dmitrii",
        "Chen, Xin",
        "Langosco, Lauro",
        "Hase, Peter",
        "B\u0131y\u0131k, Erdem",
        "Dragan, Anca",
        "Krueger, David",
        "Sadigh, Dorsa",
        "Hadfield-Menell, Dylan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-27",
      "selected": null,
      "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2307.15217v2",
        "http://arxiv.org/pdf/2307.15217.pdf",
        "http://arxiv.org/abs/2307.15217v2"
      ]
    },
    {
      "abstract": "We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.",
      "authors": [
        "Yang, Kevin",
        "Klein, Dan",
        "Celikyilmaz, Asli",
        "Peng, Nanyun",
        "Tian, Yuandong"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-24",
      "selected": null,
      "title": "RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment",
      "urls": [
        "http://arxiv.org/pdf/2307.12950.pdf",
        "http://arxiv.org/abs/2307.12950v2",
        "http://arxiv.org/pdf/2307.12950v2"
      ]
    },
    {
      "abstract": "In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.",
      "authors": [
        "Leo Gao",
        "John Schulman",
        "Jacob Hilton"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3618408.3618845",
      "keywords": [],
      "number_of_pages": 32,
      "pages": "10835-10866",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMLR.org",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 40th International Conference on Machine Learning"
      },
      "publication_date": "2023-07-23",
      "selected": null,
      "title": "Scaling laws for reward model overoptimization",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3618408.3618845"
      ]
    },
    {
      "abstract": "Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying reinforcement learning algorithm is complex and requires additional training for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning. The implementation of HIR is available at https://github.com/tianjunz/HIR.",
      "authors": [
        "Tianjun Zhang",
        "Fangchen Liu",
        "Justin Wong",
        "Pieter Abbeel",
        "Joseph E. Gonzalez"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3618408.3620145",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "41414-41428",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMLR.org",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 40th International Conference on Machine Learning"
      },
      "publication_date": "2023-07-23",
      "selected": null,
      "title": "The wisdom of hindsight makes language models better instruction followers",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3618408.3620145"
      ]
    },
    {
      "abstract": "We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the K-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and Max Entropy Inverse Reinforcement Learning, and provide the first sample complexity bound for both problems.",
      "authors": [
        "Banghua Zhu",
        "Michael I. Jordan",
        "Jiantao Jiao"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3618408.3620222",
      "keywords": [],
      "number_of_pages": null,
      "pages": "46037-43067",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMLR.org",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 40th International Conference on Machine Learning"
      },
      "publication_date": "2023-07-23",
      "selected": null,
      "title": "Principled reinforcement learning with human feedback from pairwise or K-wise comparisons",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3618408.3620222"
      ]
    },
    {
      "abstract": "This study focuses on the topic of offline preference-based reinforcement learning (PbRL), a variant of conventional reinforcement learning that dispenses with the need for online interaction or specification of reward functions. Instead, the agent is provided with fixed offline trajectories and human preferences between pairs of trajectories to extract the dynamics and task information, respectively. Since the dynamics and task information are orthogonal, a naive approach would involve using preference-based reward learning followed by an off-the-shelf offline RL algorithm. However, this requires the separate learning of a scalar reward function, which is assumed to be an information bottleneck of the learning process. To address this issue, we propose the offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function. OPPO achieves this by introducing an offline hindsight information matching objective for optimizing a contextual policy and a preference modeling objective for finding the optimal context. OPPO further integrates a well-performing decision policy by optimizing the two objectives iteratively. Our empirical results demonstrate that OPPO effectively models offline preferences and outperforms prior competing baselines, including offline RL algorithms performed over either true or pseudo reward function specifications. Our code is available on the project website: https://sites.google.com/view/oppo-icml-2023.",
      "authors": [
        "Yachen Kang",
        "Diyuan Shi",
        "Jinxin Liu",
        "Li He",
        "Donglin Wang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3618408.3619053",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "15753-15768",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMLR.org",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 40th International Conference on Machine Learning"
      },
      "publication_date": "2023-07-23",
      "selected": null,
      "title": "Beyond reward: offline preference-guided policy optimization",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3618408.3619053"
      ]
    },
    {
      "abstract": "Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally optimal objective but that different divergences present different alignment and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good balance between these objectives, and frequently outperforms forward KL divergence by a wide margin, leading to significant improvements over prior work. These distinguishing characteristics between divergences persist as the model size increases, highlighting the importance of selecting appropriate divergence objectives.",
      "authors": [
        "Dongyoung Go",
        "Tomasz Korbak",
        "Germ\u00e1n Kruszewski",
        "Jos Rozen",
        "Nahyeon Ryu",
        "Marc Dymetman"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3618408.3618871",
      "keywords": [],
      "number_of_pages": 38,
      "pages": "11546-11583",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMLR.org",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 40th International Conference on Machine Learning"
      },
      "publication_date": "2023-07-23",
      "selected": null,
      "title": "Aligning language models with preferences through f-divergence minimization",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3618408.3618871"
      ]
    },
    {
      "abstract": "In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspired by our previous work on combining demonstrations and pairwise preferences in Minecraft, which was awarded a research prize at the 2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in Minecraft. Example trajectory rollouts of DIP-RL and baselines are located at https://sites.google.com/view/dip-rl.",
      "authors": [
        "Novoseller, Ellen",
        "Goecks, Vinicius G.",
        "Watkins, David",
        "Miller, Josh",
        "Waytowich, Nicholas"
      ],
      "categories": null,
      "citations": null,
      "comments": "Paper accepted at The Many Facets of Preference Learning Workshop at\n  the International Conference on Machine Learning (ICML), Honolulu, Hawaii,\n  USA, 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-22",
      "selected": null,
      "title": "DIP-RL: Demonstration-Inferred Preference Learning in Minecraft",
      "urls": [
        "http://arxiv.org/abs/2307.12158v1",
        "http://arxiv.org/pdf/2307.12158v1",
        "http://arxiv.org/pdf/2307.12158.pdf"
      ]
    },
    {
      "abstract": "Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.",
      "authors": [
        "Mehta, Viraj",
        "Neopane, Ojash",
        "Das, Vikramjeet",
        "Lin, Sen",
        "Schneider, Jeff",
        "Neiswanger, Willie"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-21",
      "selected": null,
      "title": "Kernelized Offline Contextual Dueling Bandits",
      "urls": [
        "http://arxiv.org/pdf/2307.11288v1",
        "http://arxiv.org/pdf/2307.11288.pdf",
        "http://arxiv.org/abs/2307.11288v1"
      ]
    },
    {
      "abstract": "BackgroundsValue-based decision-making impairment in depression is a complex phenomenon: while some studies did find evidence of blunted reward learning and reward-related signals in the brain, others indicate no effect. Here we test whether such reward sensitivity deficits are dependent on the overall value of the decision problem.MethodsWe used a two-armed bandit task with two different contexts: one \u2018rich\u2019, one \u2018poor\u2019 where both options were associated with an overall positive, negative expected value, respectively. We tested patients (N = 30) undergoing a major depressive episode and age, gender and socio-economically matched controls (N = 26). Learning performance followed by a transfer phase, without feedback, were analyzed to distangle between a decision or a value-update process mechanism. Finally, we used computational model simulation and fitting to link behavioral patterns to learning biases.ResultsControl subjects showed similar learning performance in the \u2018rich\u2019 and the \u2018poor\u2019 contexts, while patients displayed reduced learning in the \u2018poor\u2019 context. Analysis of the transfer phase showed that the context-dependent impairment in patients generalized, suggesting that the effect of depression has to be traced to the outcome encoding. Computational model-based results showed that patients displayed a higher learning rate for negative compared to positive outcomes (the opposite was true in controls).ConclusionsOur results illustrate that reinforcement learning performances in depression depend on the value of the context. We show that depressive patients have a specific trouble in contexts with an overall negative state value, which in our task is consistent with a negativity bias at the learning rates level.",
      "authors": [
        "Henri Vandendriessche",
        "Amel Demmou",
        "Sophie Bavard",
        "Julien Yadak",
        "C\u00e9dric Lemogne",
        "Thomas Mauras",
        "Stefano Palminteri"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1017/S0033291722001593",
      "keywords": [
        "negativity bias",
        "depression",
        "reinforcement learning",
        "Context dependency",
        "reward processing"
      ],
      "number_of_pages": 11,
      "pages": "4696-4706",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00332917",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychological Medicine"
      },
      "publication_date": "2023-07-21",
      "selected": null,
      "title": "Contextual influence of reinforcement learning performance of depression: evidence for a negativity bias?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167827022&origin=inward"
      ]
    },
    {
      "abstract": "Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning. Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses. Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy. HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification. The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies. This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses. HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users. Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors.",
      "authors": [
        "Torne, Marcel",
        "Balsells, Max",
        "Wang, Zihan",
        "Desai, Samedh",
        "Chen, Tao",
        "Agrawal, Pulkit",
        "Gupta, Abhishek"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-20",
      "selected": null,
      "title": "Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback",
      "urls": [
        "http://arxiv.org/pdf/2307.11049v1",
        "http://arxiv.org/abs/2307.11049v1",
        "http://arxiv.org/pdf/2307.11049.pdf"
      ]
    },
    {
      "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.",
      "authors": [
        "Wang, Rongsheng",
        "Duan, Yaofei",
        "Lam, ChanTong",
        "Chen, Jiexi",
        "Xu, Jiangsheng",
        "Chen, Haoming",
        "Liu, Xiaohong",
        "Pang, Patrick Cheong-Iao",
        "Tan, Tao"
      ],
      "categories": null,
      "citations": null,
      "comments": "5 pages, 3 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-20",
      "selected": null,
      "title": "IvyGPT: InteractiVe Chinese pathwaY language model in medical domain",
      "urls": [
        "http://arxiv.org/abs/2307.10512v1",
        "http://arxiv.org/pdf/2307.10512v1",
        "http://arxiv.org/pdf/2307.10512.pdf"
      ]
    },
    {
      "abstract": "Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.",
      "authors": [
        "Singh, Ashish",
        "Agarwal, Prateek",
        "Huang, Zixuan",
        "Singh, Arpita",
        "Yu, Tong",
        "Kim, Sungchul",
        "Bursztyn, Victor",
        "Vlassis, Nikos",
        "Rossi, Ryan A."
      ],
      "categories": null,
      "citations": null,
      "comments": "19 pages, 4 figures. Benchmark Documentation:\n  https://figcapshf.github.io/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-20",
      "selected": null,
      "title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2307.10867v1",
        "http://arxiv.org/abs/2307.10867v1",
        "http://arxiv.org/pdf/2307.10867.pdf"
      ]
    },
    {
      "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available, and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, Financial Generative Pre-trained Transformer (FinGPT), that automates the collection and curation of real-time financial data from 34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes have been open-sourced.",
      "authors": [
        "Liu, Xiao-Yang",
        "Wang, Guoxuan",
        "Yang, Hongyang",
        "Zha, Daochen"
      ],
      "categories": null,
      "citations": null,
      "comments": "43 pages, 8 tables, and 2 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-19",
      "selected": null,
      "title": "FinGPT: Democratizing Internet-scale Data for Financial Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2307.10485.pdf",
        "http://arxiv.org/abs/2307.10485v2",
        "http://arxiv.org/pdf/2307.10485v2"
      ]
    },
    {
      "abstract": "Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.",
      "authors": [
        "Kang, Yachen",
        "He, Li",
        "Liu, Jinxin",
        "Zhuang, Zifeng",
        "Wang, Donglin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-19",
      "selected": null,
      "title": "STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization",
      "urls": [
        "http://arxiv.org/abs/2307.09692v1",
        "http://arxiv.org/pdf/2307.09692v1",
        "http://arxiv.org/pdf/2307.09692.pdf"
      ]
    },
    {
      "abstract": "Given a textual sentence provided by a user, the Temporal Language Grounding (TLG) task is defined as the process of finding a semantically relevant video moment or clip from an untrimmed video. In recent years, localization-based TLG methods have been explored, which adopt reinforcement learning to locate a clip from the video. However, these methods are not stable enough due to the stochastic exploration mechanism of reinforcement learning, which is sensitive to the reward. Therefore, providing a more flexible and reasonable reward has become a focus of attention for both academia and industry. Inspired by the training process of chatGPT, we innovatively adopt a vision-language pre-training (VLP) model as a reward model, which provides flexible rewards to help the localization-based TLG task converge. Specifically, a reinforcement learning-based localization module is introduced to predict the start and end timestamps in multi-modal scenarios. Thereafter, we fine-tune a reward model based on a VLP model, even introducing some human feedback, which provides a flexible reward score for the localization module. In this way, our model is able to capture subtle differences of the untrimmed video. Extensive experiments on two datasets have well verified the effectiveness of our proposed solution. \u00c2\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
      "authors": [
        "Yawen Zeng",
        "Keyu Pan",
        "Ning Han"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3539618.3592054",
      "keywords": [
        "temporal language grounding",
        "cross-modal moment retrieval"
      ],
      "number_of_pages": 5,
      "pages": "2344-2348",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450394086",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval"
      },
      "publication_date": "2023-07-18",
      "selected": null,
      "title": "RewardTLG: Learning to Temporally Language Grounding from Flexible Reward",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3539618.3592054",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168696847&origin=inward"
      ]
    },
    {
      "abstract": "ChatGPT and similar large language model (LLM) based conversational agents have brought shock waves to the research world. Although astonished by their human-like performance, we find they share a significant weakness with many other existing conversational agents in that they all take a passive approach in responding to user queries. This limits their capacity to understand the users and the task better and to offer recommendations based on a broader context than a given conversation. Proactiveness is still missing in these agents, including their ability to initiate a conversation, shift topics, or offer recommendations that take into account a more extensive context. To address this limitation, this tutorial reviews methods for equipping conversational agents with proactive interaction abilities. The full-day tutorial is divided into four parts, including multiple interactive exercises. We will begin the tutorial with an interactive exercise and cover the design of existing conversational systems architecture and challenges. The content includes coverage of LLM-based recent advancements such as ChatGPT and Bard, along with reinforcement learning with human feedback (RLHF) technique. Then we will introduce the concept of proactive conversation agents and preset recent advancements in proactiveness of conversational agents, including actively driving conversations by asking questions, topic shifting, and methods that support strategic planning of conversation. Next, we will discuss important issues in conversational responses' quality control, including safety, appropriateness, language detoxication, hallucination, and alignment. Lastly, we will launch another interactive exercise and discussion with the audience to arrive at concluding remarks, prospecting open challenges and new directions. By exploring new techniques for enhancing conversational agents' proactive behavior to improve user engagement, this tutorial aims to help researchers and practitioners develop more effective conversational agents that can better understand and respond to user needs proactively and safely. \u00c2\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
      "authors": [
        "Lizi Liao",
        "Grace Hui Yang",
        "Chirag Shah"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3539618.3594250",
      "keywords": [
        "conversational search",
        "proactive conversation",
        "conversational ai"
      ],
      "number_of_pages": 4,
      "pages": "3452-3455",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450394086",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval"
      },
      "publication_date": "2023-07-18",
      "selected": null,
      "title": "Proactive Conversational Agents in the Post-ChatGPT World",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3539618.3594250",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168664849&origin=inward"
      ]
    },
    {
      "abstract": "Temporal credit assignment is crucial for learning and skill development in natural and artificial intelligence. While computational methods like the TD approach in reinforcement learning have been proposed, it's unclear if they accurately represent how humans handle feedback delays. Cognitive models intend to represent the mental steps by which humans solve problems and perform a number of tasks, but limited research in cognitive science has addressed the credit assignment problem in humans and cognitive models. Our research uses a cognitive model based on a theory of decisions from experience, Instance-Based Learning Theory (IBLT), to test different credit assignment mechanisms in a goal-seeking navigation task with varying levels of decision complexity. Instance-Based Learning (IBL) models simulate the process of making sequential choices with different credit assignment mechanisms, including a new IBL-TD model that combines the IBL decision mechanism with the TD approach. We found that (1) An IBL model that gives equal credit assignment to all decisions is able to match human performance better than other models, including IBL-TD and Q-learning; (2) IBL-TD and Q-learning models underperform compared to humans initially, but eventually, they outperform humans; (3) humans are influenced by decision complexity, while models are not. Our study provides insights into the challenges of capturing human behavior and the potential opportunities to use these models in future AI systems to support human activities.",
      "authors": [
        "Nguyen, Thuy Ngoc",
        "McDonald, Chase",
        "Gonzalez, Cleotilde"
      ],
      "categories": null,
      "citations": null,
      "comments": "11 figures; 3 tables",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-16",
      "selected": null,
      "title": "Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents",
      "urls": [
        "http://arxiv.org/abs/2307.08171v1",
        "http://arxiv.org/pdf/2307.08171v1",
        "http://arxiv.org/pdf/2307.08171.pdf"
      ]
    },
    {
      "abstract": "The proceedings contain 180 papers. The topics discussed include: on the evolution of mechanisms for three-option collective decision-making in a swarm of simulated robots; the barrier tree benchmark: many basins and double funnels; aggregation through adaptive random walks in a minimalist robot swarm; factored particle swarm optimization for policy co-training in reinforcement learning; PID-inspired modifications in response threshold models in swarm intelligent systems; learning-based neural ant colony optimization; leveraging human feedback to evolve and discover novel emergent behaviors in robot swarms; swarms of artificial platelets for emergent hole detection and healing in wireless sensor networks; a study of ant-based pheromone spaces for generation perturbative hyper-heuristics; particle swarm optimization with ring topology for multi-modal multi-objective problems; and region-based evaluation particle swarm optimization with dual solution libraries for real-time traffic signal timing optimization.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400701191",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "GECCO 2023 - Proceedings of the 2023 Genetic and Evolutionary Computation Conference"
      },
      "publication_date": "2023-07-15",
      "selected": null,
      "title": "GECCO 2023 - Proceedings of the 2023 Genetic and Evolutionary Computation Conference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167690166&origin=inward"
      ]
    },
    {
      "abstract": "Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting and bi-gathering. To learn effective policies, we introduce appropriate reward functions for these tasks and propose a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a tactile sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world. Code and videos are available at https://sites.google.com/view/bi-touch/.",
      "authors": [
        "Lin, Yijiong",
        "Church, Alex",
        "Yang, Max",
        "Li, Haoran",
        "Lloyd, John",
        "Zhang, Dandan",
        "Lepora, Nathan F."
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted by IEEE Robotics and Automation Letters (RA-L)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-12",
      "selected": null,
      "title": "Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2307.06423.pdf",
        "http://arxiv.org/pdf/2307.06423v1",
        "http://arxiv.org/abs/2307.06423v1"
      ]
    },
    {
      "abstract": "Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design FAIRO to generalize to three types of HITL application setups that have the shared adaptation decision problem. Furthermore, we recognize that fairness-aware policies can sometimes conflict with the application's utility. To address this challenge, we provide a fairness-utility tradeoff in FAIRO, allowing system designers to balance the objectives of fairness and utility based on specific application requirements. Extensive evaluations of FAIRO on the three HITL applications demonstrate its generalizability and effectiveness in promoting fairness while accounting for human variability. On average, FAIRO can improve fairness compared with other methods across all three applications by 35.36%.",
      "authors": [
        "Zhao, Tianyu",
        "Taherisadr, Mojtaba",
        "Elmalaki, Salma"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-12",
      "selected": null,
      "title": "FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems",
      "urls": [
        "http://arxiv.org/pdf/2307.05857v2",
        "http://arxiv.org/pdf/2307.05857.pdf",
        "http://arxiv.org/abs/2307.05857v2"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of a large amount of interactive feedback. This paper presents a new method that uses scores provided by humans instead of pairwise preferences to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by humans negatively impacting the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method for robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL.",
      "authors": [
        "Liu, Shukai",
        "Wu, Chenming",
        "Li, Ying",
        "Zhang, Liangjun"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted by IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2023)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-11",
      "selected": null,
      "title": "Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores",
      "urls": [
        "http://arxiv.org/abs/2307.05405v2",
        "http://arxiv.org/pdf/2307.05405v2",
        "http://arxiv.org/pdf/2307.05405.pdf"
      ]
    },
    {
      "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.",
      "authors": [
        "Zheng, Rui",
        "Dou, Shihan",
        "Gao, Songyang",
        "Hua, Yuan",
        "Shen, Wei",
        "Wang, Binghai",
        "Liu, Yan",
        "Jin, Senjie",
        "Liu, Qin",
        "Zhou, Yuhao",
        "Xiong, Limao",
        "Chen, Lu",
        "Xi, Zhiheng",
        "Xu, Nuo",
        "Lai, Wenbin",
        "Zhu, Minghao",
        "Chang, Cheng",
        "Yin, Zhangyue",
        "Weng, Rongxiang",
        "Cheng, Wensen",
        "Huang, Haoran",
        "Sun, Tianxiang",
        "Yan, Hang",
        "Gui, Tao",
        "Zhang, Qi",
        "Qiu, Xipeng",
        "Huang, Xuanjing"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-11",
      "selected": null,
      "title": "Secrets of RLHF in Large Language Models Part I: PPO",
      "urls": [
        "http://arxiv.org/pdf/2307.04964v2",
        "http://arxiv.org/pdf/2307.04964.pdf",
        "http://arxiv.org/abs/2307.04964v2"
      ]
    },
    {
      "abstract": "In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.",
      "authors": [
        "Ji, Jiaming",
        "Liu, Mickel",
        "Dai, Juntao",
        "Pan, Xuehai",
        "Zhang, Chi",
        "Bian, Ce",
        "Zhang, Chi",
        "Sun, Ruiyang",
        "Wang, Yizhou",
        "Yang, Yaodong"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published at NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-10",
      "selected": null,
      "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
      "urls": [
        "http://arxiv.org/pdf/2307.04657.pdf",
        "http://arxiv.org/pdf/2307.04657v3",
        "http://arxiv.org/abs/2307.04657v3"
      ]
    },
    {
      "abstract": "Online reinforcement learning (RL) is increasingly used for realizing adaptive systems in the presence of design time uncertainty. Online RL facilitates learning from actual operational data and thereby leverages feedback only available at runtime. However, Online RL requires the definition of an effective and correct reward function, which quantifies the feedback to the RL algorithm and thereby guides learning. With Deep RL gaining interest, the learned knowledge is no longer explicitly represented, but is represented as a neural network. For a human, it becomes practically impossible to relate the parametrization of the neural network to concrete RL decisions. Deep RL thus essentially appears as a black box, which severely limits the debugging of adaptive systems. We previously introduced the explainable RL technique XRL-DINE, which provides visual insights into why certain decisions were made at important time points. Here, we introduce an empirical user study involving 54 software engineers from academia and industry to assess (1) the performance of software engineers when performing different tasks using XRL-DINE and (2) the perceived usefulness and ease of use of XRL-DINE.",
      "authors": [
        "Metzger, Andreas",
        "Laufer, Jan",
        "Feit, Felix",
        "Pohl, Klaus"
      ],
      "categories": null,
      "citations": null,
      "comments": "arXiv admin note: substantial text overlap with arXiv:2210.05931",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-09",
      "selected": null,
      "title": "A User Study on Explainable Online Reinforcement Learning for Adaptive Systems",
      "urls": [
        "http://arxiv.org/pdf/2307.04098.pdf",
        "http://arxiv.org/abs/2307.04098v1",
        "http://arxiv.org/pdf/2307.04098v1"
      ]
    },
    {
      "abstract": "Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI's ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment. \u00c2\u00a9 2023 Owner/Author.",
      "authors": [
        "Andreas Liesenfeld",
        "Alianda Lopez",
        "Mark Dingemanse"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1145/3571884.3604316",
      "keywords": [
        "chatGPT",
        "open source",
        "survey",
        "RLHF",
        "large language models"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400700149",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 5th International Conference on Conversational User Interfaces, CUI 2023"
      },
      "publication_date": "2023-07-08",
      "selected": null,
      "title": "Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators",
      "urls": [
        "http://arxiv.org/pdf/2307.05532v1",
        "https://dl.acm.org/doi/10.1145/3571884.3604316",
        "http://arxiv.org/abs/2307.05532v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167796655&origin=inward",
        "http://dx.doi.org/10.1145/3571884.3604316"
      ]
    },
    {
      "abstract": "In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the Prototypical Part Network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, it often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns human preferences and identify non-spurious prototypes. In place of a full RL update, we propose the Reweighed, Reselected, and Retrained Prototypical Part Network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The first two steps are reward-based reweighting and reselection, which align prototypes with human feedback. The final step is retraining to realign the model's features with the updated prototypes. We find that R3-ProtoPNet improves the overall meaningfulness of the prototypes, and maintains or improves individual model performance. When multiple trained R3-ProtoPNets are incorporated into an ensemble, we find increases in both interpretability and predictive performance.",
      "authors": [
        "Netzorg, Robin",
        "Li, Jiaxun",
        "Yu, Bin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-08",
      "selected": null,
      "title": "Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining",
      "urls": [
        "http://arxiv.org/pdf/2307.03887.pdf",
        "http://arxiv.org/abs/2307.03887v2",
        "http://arxiv.org/pdf/2307.03887v2"
      ]
    },
    {
      "abstract": "Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we present a novel risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk (CVaR) objective under both linear and general function approximations, enriched by human feedback. These new formulations provide a principled way to guarantee safety in each decision making step throughout the control process. Moreover, integrating human feedback into risk-sensitive RL framework bridges the gap between algorithmic decision-making and human participation, allowing us to also guarantee safety for human-in-the-loop systems. We propose provably sample-efficient algorithms for this Iterated CVaR RL and provide rigorous theoretical analysis. Furthermore, we establish a matching lower bound to corroborate the optimality of our algorithms in a linear context.",
      "authors": [
        "Chen, Yu",
        "Du, Yihan",
        "Hu, Pihe",
        "Wang, Siwei",
        "Wu, Desheng",
        "Huang, Longbo"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-06",
      "selected": null,
      "title": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2307.02842v3",
        "http://arxiv.org/pdf/2307.02842.pdf",
        "http://arxiv.org/abs/2307.02842v3"
      ]
    },
    {
      "abstract": "Object manipulation is a primary function expected of a robotic manipulator. The solution to the inverse kinematic problem is of significance for accomplishment of such tasks. There are inherent bottlenecks associated with arriving at these solutions. This is particularly true in the context of human-robot interaction scenarios, wherein the dynamics makes it challenging for traditional motion planning algorithms to define safe, collision-free path. Deep Reinforcement Learning is being increasingly applied to circumvent the bottlenecks associated with solution of inverse kinematics. In this paper, we enriched Deep Deterministic Policy Gradient with Hindsight Experience Replay to teach a robot to autonomously manipulate objects by trial and error interactions with the environment. To assess our proposal, we simulate a 4-DOF DOBOT robot manipulator via a physics simulation backed by MuJoCo to perform object manipulation tasks including push, pick and place, and slide. Experimental evaluation shows that the desired tasks can be satisfactorily achieved using the aforementioned policies. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Chakravadhanula, V.V.",
        "Agarwal, T.",
        "Hazarika, S.M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3610419.3610420",
      "keywords": [
        "Object manipulation",
        "Exploration-exploitation",
        "Hindsight Experience Replay",
        "Motion planning",
        "Robotic manipulator",
        "Supervised learning",
        "Trial and error",
        "Feedback mechanism",
        "Collisionfree path",
        "Serial manipulators",
        "Unsupervised learning",
        "Inverse kinematics",
        "Reinforcement learning",
        "Optimal policy",
        "Data generation",
        "Human-robot interaction"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450399807",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM International Conference Proceeding Series"
      },
      "publication_date": "2023-07-05",
      "selected": null,
      "title": "Motion Planning using Reinforcement Learning for Serial Manipulators",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179880475&origin=inward"
      ]
    },
    {
      "abstract": "Compiler bugs pose a significant threat to safety-critical applications, and promptly and effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches typically convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI to tame LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in LLM4CBI. (1) LLM4CBI utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. (2) LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. (3) A test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi), our evaluation demonstrates the advantages of LLM4CBI: It isolates more bugs, ranging from 13.6% to 90.9% in various settings, than the other approaches. Additionally, we demonstrate that LLM4CBI is extensible, allowing for easy integration with other LLMs.",
      "authors": [
        "Tu, Haoxin",
        "Zhou, Zhide",
        "Jiang, He",
        "Yusuf, Imam Nur Bani",
        "Li, Yuxian",
        "Jiang, Lingxiao"
      ],
      "categories": null,
      "citations": null,
      "comments": "Under review of IEEE Transactions on Software Engineering",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-02",
      "selected": null,
      "title": "LLM4CBI: Taming LLMs to Generate Effective Test Programs for Compiler Bug Isolation",
      "urls": [
        "http://arxiv.org/abs/2307.00593v1",
        "http://arxiv.org/pdf/2307.00593.pdf",
        "http://arxiv.org/pdf/2307.00593v1"
      ]
    },
    {
      "abstract": "Generative adversarial imitation learning (GAIL) has shown good results in several research areas by taking advantage of generative adversarial networks. However, GAIL lacks a reward mechanism and usually adopts a model-free approach based on stochastic policies, which is not ideal for solving complex, dynamically uncertain population intelligence problems, especially in the face of autonomous driving environments. In this paper, a policy framework is shaped by combining the human knowledge with GAIL (HKGAIL). HKGAIL embeds human decision models into the learning process to infer the underlying structure of expert demonstrations. The skills learned from expert demonstrations can directly guide the actions (policies) of the learning process of the agents, and the policies can be optimized through the feedback function of the discriminator. Experiments on both driving and landing tasks show that HKGAIL was able to better fit the policy close to the expert, and was 16.2% safer than GAIL for the driving task. \u00c2\u00a9 2022 The Authors. IET Intelligent Transport Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",
      "authors": [
        "Peng, Y.",
        "Tan, G.",
        "Si, H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1049/itr2.12313",
      "keywords": [
        "human knowledge",
        "policy shaping",
        "generative adversarial imitation learning",
        "reinforcement learning"
      ],
      "number_of_pages": 10,
      "pages": "1302-1311",
      "publication": {
        "category": "Journal",
        "cite_score": 6.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1751956X",
        "publisher": "John Wiley & Sons Inc.",
        "sjr": 0.681,
        "snip": 1.1,
        "subject_areas": [
          "Mechanical Engineering",
          "Environmental Science (all)",
          "Law",
          "Transportation"
        ],
        "title": "IET Intelligent Transport Systems"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "HKGAIL: Policy shaping via integrating human knowledge with generative adversarial imitation learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142119139&origin=inward"
      ]
    },
    {
      "abstract": "Generative Artificial Intelligence (GAI) is attracting the increasing attention of materials community for its excellent capability of generating required contents. With the introduction of Prompt paradigm and reinforcement learning from human feedback (RLHF), GAI shifts from the task-specific to general pattern gradually, enabling to tackle multiple complicated tasks involved in resolving the structure-activity relationships. Here, we review the development status of GAI comprehensively and analyze pros and cons of various generative models in the view of methodology. The applications of task-specific generative models involving materials inverse design and data augmentation are also dissected. Taking ChatGPT as an example, we explore the potential applications of general GAI in generating multiple materials content, solving differential equation as well as querying materials FAQs. Furthermore, we summarize six challenges encountered for the use of GAI in materials science and provide the corresponding solutions. This work paves the way for providing effective and explainable materials data generation and analysis approaches to accelerate the materials research and development. \u00c2\u00a9 2023 The Authors",
      "authors": [
        "Liu, Y.",
        "Yang, Z.",
        "Yu, Z.",
        "Liu, Z.",
        "Liu, D.",
        "Lin, H.",
        "Li, M.",
        "Ma, S.",
        "Avdeev, M.",
        "Shi, S."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jmat.2023.05.001",
      "keywords": [
        "Machine learning",
        "Artificial intelligence",
        "Novel materials discovery",
        "Deep learning",
        "Generative artificial intelligence",
        "Materials science"
      ],
      "number_of_pages": 19,
      "pages": "798-816",
      "publication": {
        "category": "Journal",
        "cite_score": 12.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "23528478",
        "publisher": "Chinese Ceramic Society",
        "sjr": 1.8,
        "snip": 1.723,
        "subject_areas": [
          "Metals and Alloys",
          "Electronic, Optical and Magnetic Materials",
          "Surfaces, Coatings and Films"
        ],
        "title": "Journal of Materiomics"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Generative artificial intelligence and its applications in materials science: Current situation and future perspectives",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163376203&origin=inward"
      ]
    },
    {
      "abstract": "Drawing from the resources of psychoanalysis and critical media studies, in this article we develop an analysis of large language models (LLMs) as \u00e2\u0080\u0098automated subjects\u00e2\u0080\u0099. We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which artificial intelligence (AI) behaviour, including its productions of bias and harm, can be analysed. First, we introduce language models, discuss their significance and risks, and outline our case for interpreting model design and outputs with support from psychoanalytic concepts. We trace a brief history of language models, culminating with the releases, in 2022, of systems that realise \u00e2\u0080\u0098state-of-the-art\u00e2\u0080\u0099 natural language processing performance. We engage with one such system, OpenAI's InstructGPT, as a case study, detailing the layers of its construction and conducting exploratory and semi-structured interviews with chatbots. These interviews probe the model's moral imperatives to be \u00e2\u0080\u0098helpful\u00e2\u0080\u0099, \u00e2\u0080\u0098truthful\u00e2\u0080\u0099 and \u00e2\u0080\u0098harmless\u00e2\u0080\u0099 by design. The model acts, we argue, as the condensation of often competing social desires, articulated through the internet and harvested into training data, which must then be regulated and repressed. This foundational structure can however be redirected via prompting, so that the model comes to identify with, and transfer, its commitments to the immediate human subject before it. In turn, these automated productions of language can lead to the human subject projecting agency upon the model, effecting occasionally further forms of countertransference. We conclude that critical media methods and psychoanalytic theory together offer a productive frame for grasping the powerful new capacities of AI-driven language systems. \u00c2\u00a9 The Author(s) 2023.",
      "authors": [
        "Magee, L.",
        "Arora, V.",
        "Munn, L."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/20539517231210273",
      "keywords": [
        "reinforcement learning from human feedback (RLHF)",
        "psychoanalysis",
        "AI",
        "large language models",
        "automated subjects",
        "chatbot interviews"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Big Data and Society"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Structured like a language model: Analysing AI as an automated subject",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176411085&origin=inward"
      ]
    },
    {
      "abstract": "Objective: To determine if ChatGPT can generate useful suggestions for improving clinical decision support (CDS) logic and to assess noninferiority compared to human-generated suggestions. Methods: We supplied summaries of CDS logic to ChatGPT, an artificial intelligence (AI) tool for question answering that uses a large language model, and asked it to generate suggestions. We asked human clinician reviewers to review the AI-generated suggestions as well as human-generated suggestions for improving the same CDS alerts, and rate the suggestions for their usefulness, acceptance, relevance, understanding, workflow, bias, inversion, and redundancy. Results: Five clinicians analyzed 36 AI-generated suggestions and 29 human-generated suggestions for 7 alerts. Of the 20 suggestions that scored highest in the survey, 9 were generated by ChatGPT. The suggestions generated by AI were found to offer unique perspectives and were evaluated as highly understandable and relevant, with moderate usefulness, low acceptance, bias, inversion, redundancy. Conclusion: AI-generated suggestions could be an important complementary part of optimizing CDS alerts, can identify potential improvements to alert logic and support their implementation, and may even be able to assist experts in formulating their own suggestions for CDS improvement. ChatGPT shows great potential for using large language models and reinforcement learning from human feedback to improve CDS alert logic and potentially other medical areas involving complex, clinical logic, a key step in the development of an advanced learning health system. \u00c2\u00a9 The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association.",
      "authors": [
        "Liu, S.",
        "Wright, A.P.",
        "Patterson, B.L.",
        "Wanderer, J.P.",
        "Turer, R.W.",
        "Nelson, S.D.",
        "McCoy, A.B.",
        "Sittig, D.F.",
        "Wright, A."
      ],
      "categories": null,
      "citations": 43,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/jamia/ocad072",
      "keywords": [
        "artificial intelligence",
        "large language model",
        "clinical decision support"
      ],
      "number_of_pages": 9,
      "pages": "1237-1245",
      "publication": {
        "category": "Journal",
        "cite_score": 11.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10675027",
        "publisher": "Oxford University Press",
        "sjr": 2.44,
        "snip": 2.379,
        "subject_areas": [
          "Health Informatics"
        ],
        "title": "Journal of the American Medical Informatics Association"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Using AI-generated suggestions from ChatGPT to optimize clinical decision support",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158997968&origin=inward"
      ]
    },
    {
      "abstract": "Currently, for most three-terminal neuromorphic devices, only the gate terminal is active. The inadequate modes and freedom of modulation in such devices greatly hinder the implementation of complex neural behaviors and brain-like thinking strategies in hardware systems. Taking advantage of the unique feature of co-existing in-plane (IP) and out-of-plane (OOP) ferroelectricity in two-dimensional (2D) ferroelectric \u00ce\u00b1-In2Se3, we construct a three-active-terminal neuromorphic device where any terminal can modulate the conductance state. Based on the co-operation mode, controlling food intake as a complex nervous system-level behavior is achieved to carry out positive and negative feedback. Specifically, reinforcement learning as a brain-like thinking strategy is implemented due to the coupling between polarizations in different directions. Compared to the single modulation mode, the chance of the agent successfully obtaining the reward in the Markov decision process is increased from 68% to 82% under the co-operation mode through the coupling effect between IP and OOP ferroelectricity in 2D \u00ce\u00b1-In2Se3 layers. Our work demonstrates the practicability of three-active-terminal neuromorphic devices in handling complex tasks and advances a significant step towards implementing brain-like learning strategies based on neuromorphic devices for dealing with real-world challenges. \u00c2\u00a9 2023 The Royal Society of Chemistry.",
      "authors": [
        "Guo, F.",
        "Io, W.F.",
        "Dang, Z.",
        "Ding, R.",
        "Pang, S.-Y.",
        "Zhao, Y.",
        "Hao, J."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1039/d3mh00714f",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "3719-3728",
      "publication": {
        "category": "Journal",
        "cite_score": 22.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "20516347",
        "publisher": "Royal Society of Chemistry",
        "sjr": 3.88,
        "snip": 2.101,
        "subject_areas": [
          "Mechanics of Materials",
          "Materials Science (all)",
          "Electrical and Electronic Engineering",
          "Process Chemistry and Technology"
        ],
        "title": "Materials Horizons"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Achieving reinforcement learning in a three-active-terminal neuromorphic device based on a 2D vdW ferroelectric material",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165305518&origin=inward"
      ]
    },
    {
      "abstract": "BatGPT is a large-scale language model designed and trained jointly by Wuhan University and Shanghai Jiao Tong University. It is capable of generating highly natural and fluent text in response to various types of input, including text prompts, images, and audio. In the modeling level, we employ a bidirectional autoregressive architecture that allows the model to efficiently capture the complex dependencies of natural language, making it highly effective in tasks such as language generation, dialog systems, and question answering. Moreover, the bidirectional autoregressive modeling not only operates from left to right but also from right to left, effectively reducing fixed memory effects and alleviating model hallucinations. In the training aspect, we propose a novel parameter expansion method for leveraging the pre-training of smaller models and employ reinforcement learning from both AI and human feedback, aimed at improving the model's alignment performance. Overall, these approaches significantly improve the effectiveness of BatGPT, and the model can be utilized for a wide range of natural language applications.",
      "authors": [
        "Li, Zuchao",
        "Zhang, Shitou",
        "Zhao, Hai",
        "Yang, Yifei",
        "Yang, Dongjie"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer",
      "urls": [
        "http://arxiv.org/abs/2307.00360v2",
        "http://arxiv.org/pdf/2307.00360v2",
        "http://arxiv.org/pdf/2307.00360.pdf"
      ]
    },
    {
      "abstract": "Event-related potentials that follow feedback in reinforcement learning tasks have been proposed to reflect neural encoding of prediction errors. Prior research has shown that in the interval of 240\u00e2\u0080\u0093340 ms multiple different prediction error encodings appear to co-occur, including a value signal carrying signed quantitative prediction error and a valence signal merely carrying sign. The effects used to identify these two encoders, respectively a sign main effect and a sign \u00c3\u0097 size interaction, do not reliably discriminate them. A full discrimination is made possible by comparing tasks in which the reinforcer available on a given trial is set to be either appetitive or aversive against tasks where a trial allows the possibility of either. This study presents a meta-analysis of reinforcement learning experiments, the majority of which presented the possibility of winning or losing money. Value and valence encodings were identified by conventional difference wave methodology but additionally by an analysis of their predicted behavior using a Bayesian analysis that incorporated nulls into the evidence for each encoder. The results suggest that a valence encoding, sensitive only to the available outcomes on the trial at hand precedes a later value encoding sensitive to the outcomes available in the wider experimental context. The implications of this for modeling computational processes of reinforcement learning in humans are discussed. \u00c2\u00a9 2023 The Authors. Psychophysiology published by Wiley Periodicals LLC on behalf of Society for Psychophysiological Research.",
      "authors": [
        "Stewardson, H.",
        "Sambrook, T.D."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.14266",
      "keywords": [
        "ERP",
        "RewP",
        "FRN",
        "adaptive scaling",
        "Bayesian",
        "reward prediction error"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Valence precedes value in neural encoding of prediction error",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148039735&origin=inward"
      ]
    },
    {
      "abstract": "Human-robot interaction (HRI) describes scenarios in which both human and robot work as partners, sharing the same environment or complementing each other on a joint task. HRI is characterized by the need for high adaptability and flexibility of robotic systems toward their human interaction partners. One of the major challenges in HRI is task planning with dynamic subtask assignment, which is particularly challenging when subtask choices of the human are not readily accessible by the robot. In the present work, we explore the feasibility of using electroencephalogram (EEG) based neuro-cognitive measures for online robot learning of dynamic subtask assignment. To this end, we demonstrate in an experimental human subject study, featuring a joint HRI task with a UR10 robotic manipulator, the presence of EEG measures indicative of a human partner anticipating a takeover situation from human to robot or vice-versa. The present work further proposes a reinforcement learning based algorithm employing these measures as a neuronal feedback signal from the human to the robot for dynamic learning of subtask-assignment. The efficacy of this algorithm is validated in a simulation-based study. The simulation results reveal that even with relatively low decoding accuracies, successful robot learning of subtask-assignment is feasible, with around 80% choice accuracy among four subtasks within 17 minutes of collaboration. The simulation results further reveal that scalability to more subtasks is feasible and mainly accompanied with longer robot learning times. These findings demonstrate the usability of EEG-based neuro-cognitive measures to mediate the complex and largely unsolved problem of human-robot collaborative task planning.",
      "authors": [
        "Stefan K. Ehrlich",
        "Emmanuel Dean-Leon",
        "Nicholas Tacca",
        "Simon Armleder",
        "Viktorija Dimova-Edeleva",
        "Gordon Cheng"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0287958",
      "keywords": [
        "Electroencephalography",
        "Robotics",
        "Robots",
        "Robotic behavior",
        "Learning",
        "Human learning",
        "Event-related potentials",
        "Learning curves"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Human-robot collaborative task planning using anticipatory brain responses",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164402865&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0287958&type=printable"
      ]
    },
    {
      "abstract": "To mitigate the challenges of operating through narrow incisions under image guidance, there is a desire to develop intelligent systems that assist decision making and spatial reasoning in minimally invasive surgery (MIS). In this context, machine learning-based systems for interventional image analysis are receiving considerable attention because of their flexibility and the opportunity to provide immediate, informative feedback to clinicians. It is further believed that learning-based image analysis may eventually form the foundation for semi- or fully automated delivery of surgical treatments. A significant bottleneck in developing such systems is the availability of annotated images with sufficient variability to train generalizable models, particularly the most recently favored deep convolutional neural networks or transformer architectures. A popular alternative to acquiring and manually annotating data from the clinical practice is the simulation of these data from human-based models. Simulation has many advantages, including the avoidance of ethical issues, precisely controlled environments, and the scalability of data collection. Here, we survey recent work that relies on in silico training of learning-based MIS systems, in which data are generated via computational simulation. For each imaging modality, we review available simulation tools in terms of compute requirements, image quality, and usability, as well as their applications for training intelligent systems. We further discuss open challenges for simulation-based development of MIS systems, such as the need for integrated imaging and physical modeling for non-optical modalities, as well as generative patient models not dependent on underlying computed tomography, MRI, or other patient data. In conclusion, as the capabilities of in silico training mature, with respect to sim-to-real transfer, computational efficiency, and degree of control, they are contributing toward the next generation of intelligent surgical systems. \u00c2\u00a9 2023 IOP Publishing Ltd.",
      "authors": [
        "Killeen, B.D.",
        "Cho, S.M.",
        "Armand, M.",
        "Taylor, R.H.",
        "Unberath, M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/2516-1091/acd28b",
      "keywords": [
        "ultrasound",
        "in silico virtual clinical trials",
        "minimally invasive surgery",
        "reinforcement learning",
        "machine learning",
        "x-ray",
        "endoscopy"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Progress in Biomedical Engineering"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "In silico simulation: a key enabling technology for next-generation intelligent surgical systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159783730&origin=inward"
      ]
    },
    {
      "abstract": "Evidence for positivity and optimism bias abounds in high-level belief updates. However, no consensus has been reached regarding whether learning asymmetries exist in more elementary forms of updates such as reinforcement learning (RL). In RL, the learning asymmetry concerns the sensitivity difference in incorporating positive and negative prediction errors (PE) into value estimation, namely the asymmetry of learning rates associated with positive and negative PEs. Although RL has been established as a canonical framework in characterizing interactions between agent and environment, the direction of learning asymmetry remains controversial. Here, we propose that part of the controversy stems from the fact that people may have different value expectations before entering the learning environment. Such a default value expectation influences how PEs are calculated and consequently biases subjects\u2019 choices. We test this hypothesis in two learning experiments with stable or varying reinforcement probabilities, across monetary gains, losses, and gain-loss mixed environments. Our results consistently support the model incorporating both asymmetric learning rates and the initial value expectation, highlighting the role of initial expectation in value updating and choice preference. Further simulation and model parameter recovery analyses confirm the unique contribution of initial value expectation in accessing learning rate asymmetry.",
      "authors": [
        "Yinmei Ni",
        "Jingwei Sun",
        "Jian Li"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1010751",
      "keywords": [
        "Experimental design",
        "Learning",
        "Behavior",
        "Human learning",
        "Simulation and modeling",
        "Decision making",
        "Psychological stress",
        "Psychology"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "The shadowing effect of initial expectation on learning asymmetry",
      "urls": [
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1010751&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166488967&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) methods can be used to develop a controller for the heating, ventilation, and air conditioning (HVAC) systems that both saves energy and ensures high occupants' thermal comfort levels. However, the existing works typically require on-policy data to train an RL agent, and the occupants' personalized thermal preferences are not considered, which is limited in the real-world scenarios. This paper designs a high-performance model-based offline RL algorithm for personalized HVAC systems. The proposed algorithm can quickly adapt to different occupants' thermal preferences with a few thermal feedbacks, guaranteeing the high occupants' personalized thermal comfort levels efficiently. First, we use a meta-supervised learning algorithm to train an occupant's thermal preference model. Then, we train an ensemble neural network to predict the thermal states of the considered zone. In addition, the obtained ensemble networks can indicate the regions in the state and action spaces covered by the offline dataset. With the personalized thermal preference model updated via meta-Testing, model-based RL is used to derive the optimal HVAC controller. Since the proposed algorithm only requires offline datasets and a few online thermal feedbacks for training, it contributes to a more practical deployment of the RL algorithm to HVAC systems. We use the ASHRAE database II to verify the effectiveness and advantage of the meta-learning algorithm for modeling different occupants' thermal preferences. Numerical simulations on the EnergyPlus environment demonstrate that the proposed algorithm can guarantee personalized thermal preferences with a slight increase of power consumption of 1.91% compared with the model-based RL algorithm with on-policy data aggregation. \u00c2\u00a9 2016 IEEE.",
      "authors": [
        "Chen, L.",
        "Meng, F.",
        "Zhang, Y."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TSUSC.2023.3251302",
      "keywords": [
        "model-based offline rein-forcement learning",
        "HVAC systems",
        "meta-learning",
        "human-in-The-loop control"
      ],
      "number_of_pages": 18,
      "pages": "504-521",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Sustainable Computing"
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Fast Human-in-The-Loop Control for HVAC Systems via Meta-Learning and Model-Based Offline Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149400322&origin=inward"
      ]
    },
    {
      "abstract": "Human in the loop, a reinforcement learning approach for refining search queries in systematic literature review.",
      "authors": [
        "Maisie Badami",
        "Boualem Benatallah",
        "Marcos Baez"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1016/j.is.2023.102231",
      "keywords": [
        "Systematic reviews",
        "Query adaptation",
        "Reinforcement learning",
        "Word embedding",
        "Query enrichment"
      ],
      "number_of_pages": 16,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 7.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0306-4379",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.976,
        "snip": 1.97,
        "subject_areas": [
          "Software",
          "Information Systems",
          "Hardware and Architecture"
        ],
        "title": "Inf. Syst."
      },
      "publication_date": "2023-07-01",
      "selected": null,
      "title": "Adaptive search query generation and refinement in systematic literature review",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.is.2023.102231"
      ]
    },
    {
      "abstract": "Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of $n$ responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms existing alignment algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations. Furthermore, we demonstrate that longer, more diverse, and higher-quality preference ranking sequences can consistently enhance the performance of human alignment.",
      "authors": [
        "Song, Feifan",
        "Yu, Bowen",
        "Li, Minghao",
        "Yu, Haiyang",
        "Huang, Fei",
        "Li, Yongbin",
        "Wang, Houfeng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-30",
      "selected": null,
      "title": "Preference Ranking Optimization for Human Alignment",
      "urls": [
        "http://arxiv.org/abs/2306.17492v1",
        "http://arxiv.org/pdf/2306.17492v1",
        "http://arxiv.org/pdf/2306.17492.pdf"
      ]
    },
    {
      "abstract": "Imitation learning (IL) has recently shown impressive performance in training a reinforcement learning agent with human demonstrations, eliminating the difficulty of designing elaborate reward functions in complex environments. However, most IL methods work under the assumption of the optimality of the demonstrations and thus cannot learn policies to surpass the demonstrators. Some methods have been investigated to obtain better-than-demonstration (BD) performance with inner human feedback or preference labels. In this paper, we propose a method to learn rewards from suboptimal demonstrations via a weighted preference learning technique (LERP). Specifically, we first formulate the suboptimality of demonstrations as the inaccurate estimation of rewards. The inaccuracy is modeled with a reward noise random variable following the Gumbel distribution. Moreover, we derive an upper bound of the expected return with different noise coefficients and propose a theorem to surpass the demonstrations. Unlike existing literature, our analysis does not depend on the linear reward constraint. Consequently, we develop a BD model with a weighted preference learning technique. Experimental results on continuous control and high-dimensional discrete control tasks show the superiority of our LERP method over other state-of-the-art BD methods. Copyright \u00c2\u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "authors": [
        "Huo, L.",
        "Wang, Z.",
        "Xu, M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "7953-7961",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358800",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023"
      },
      "publication_date": "2023-06-27",
      "selected": null,
      "title": "Learning Noise-Induced Reward Functions for Surpassing Demonstrations in Imitation Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168255157&origin=inward"
      ]
    },
    {
      "abstract": "Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan & Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. Project site with code and data: https://intercode-benchmark.github.io",
      "authors": [
        "Yang, John",
        "Prabhakar, Akshara",
        "Narasimhan, Karthik",
        "Yao, Shunyu"
      ],
      "categories": null,
      "citations": null,
      "comments": "Project site with code and data:\n  https://intercode-benchmark.github.io",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-26",
      "selected": null,
      "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
      "urls": [
        "http://arxiv.org/pdf/2306.14898v3",
        "http://arxiv.org/abs/2306.14898v3",
        "http://arxiv.org/pdf/2306.14898.pdf"
      ]
    },
    {
      "abstract": "Existing variable speed limit (VSL) control strategies have the disadvantages of poor flexibility, slow response time, and a heavy reliance on the compliance rate and traffic flow prediction models. It is also difficult to achieve effective control by solely relying on variable message signs (VMS) to post speed limits to drivers in the mixed traffic environment with both the connected and automated vehicles (CAVs) and human-driven vehicles (HDVs). This paper proposes a VSL control strategy based on the improved dueling double deep Q network (IPD3QN) under the mixed traffic flow environment, i.e., 1PD3QN-VSL. This strategy integrates the ability of deep reinforcement learning to automatically adapt to complex environments without establishing traffic flow prediction models, and the advantages of controllability of CAVs. The prioritized experience replay mechanism is introduced into the dueling double deep Q network (D3QN) framework of deep reinforcement learning to enhance the convergence speed and parameter update efficiency of the network. Meanwhile, an adaptive e - greedy algorithm is proposed to solve the problem of balance between exploration and utilization in D3QN's learning process. The proposed VSL control strategy aims to minimize the total time spent (TTS) of vehicles on the freeway section. Real-time traffic data and speed limits within the previous control cycle are used as inputs to the IPD3QN algorithm. Then, a reward function is constructed to guide the algorithm to generate the dynamic speed limit value executed in the VSL control area. The effectiveness of the 1PD3QN-VSL control strategy is verified under different conditions and compared to no control, feedback control, and D3QN-VSL control in terms of control performance. The results indicate that the proposed strategy can achieve remarkable control performance at a 30% penetration rate and effectively improve bottleneck traffic efficiency and reduce the spatiotemporal range of traffic congestion in both stable and fluctuating demand scenarios. Compared to the suboptimal D3QN-VSL control, the proposed strategy can achieve improvement of 14.46% on TTS in the stable scenario and the improvement of 10.36% in the fluctuating traffic demand scenario. \u00c2\u00a9 2023 Science Press. All rights reserved.",
      "authors": [
        "Han, L.",
        "Zhang, L.",
        "Guo, W.-A."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.16097/j.cnki.1009-6744.2023.03.013",
      "keywords": [
        "mixed traffic flow",
        "intelligent transportation",
        "variable speed limit control",
        "improved dueling double deep Q network",
        "deep reinforcement learning",
        "connected and automated vehicles"
      ],
      "number_of_pages": 13,
      "pages": "110-122",
      "publication": {
        "category": "Journal",
        "cite_score": 1.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10096744",
        "publisher": "Science Press",
        "sjr": 0.299,
        "snip": 0.831,
        "subject_areas": [
          "Modeling and Simulation",
          "Computer Science Applications",
          "Transportation",
          "Control and Systems Engineering"
        ],
        "title": "Jiaotong Yunshu Xitong Gongcheng Yu Xinxi/Journal of Transportation Systems Engineering and Information Technology"
      },
      "publication_date": "2023-06-25",
      "selected": null,
      "title": "Variable Speed Limit Control Based on Improved Dueling Double Deep Q Network Under Mixed Traffic Environment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165542412&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games with a restricted set of policies. The latter case can be further reduced to adversarial MDP when preferences only depend on the final state. We instantiate all reward-based RL subroutines by concrete provable algorithms, and apply our theory to a large class of models including tabular MDPs and MDPs with generic function approximation. We further provide guarantees when K-wise comparisons are available.",
      "authors": [
        "Wang, Yuanhao",
        "Liu, Qinghua",
        "Jin, Chi"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted for presentation at NeurIPS 2023; 29 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-25",
      "selected": null,
      "title": "Is RLHF More Difficult than Standard RL?",
      "urls": [
        "http://arxiv.org/abs/2306.14111v2",
        "http://arxiv.org/pdf/2306.14111v2",
        "http://arxiv.org/pdf/2306.14111.pdf"
      ]
    },
    {
      "abstract": "The quality of the artificial vision produced by visual prostheses has traditionally been evaluated by human psychophysical tests using images expressed with an array of phosphenes. However, such experiments involving human subjects are considerably time-consuming and labor-intensive. One potential solution may be implementing an efficient approach to assist or replace psychophysical experiments showing a short-term learning effect in human subjects. The present work developed a reinforcement learning (RL)-based feedback framework which built artificial agents to emulate the behavioral changes in the learning process of human subjects over the experimental trials. In our framework, we first trained an agent which can gradually learn to identify 720 faces presented in low-resolution phosphene images with feedback rewards received from the agreement in the perception of nine training human subjects. Then, in the automating stage of the framework, we created nine RL agents. By testing those agents, we found the RL agents mimicked the learning effects of nine test human subjects better than nine instances of a supervised learning (SL) model. Given the similar outcomes with human tests and the time efficiency of RL, our framework may expedite the development of visual prosthetic systems by at least partially replacing laborious human psychophysical experiments. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Na Min An",
        "Hyeonhee Roh",
        "Sein Kim",
        "Jae Hun Kim",
        "Maesoon Im"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IJCNN54540.2023.10191870",
      "keywords": [
        "Artificial Vision",
        "Human Psychophysical Test",
        "Agent-Based System",
        "Reinforcement Learning"
      ],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8868-6",
        "issn": "2161-4393",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Neural Networks"
      },
      "publication_date": "2023-06-18",
      "selected": null,
      "title": "Reinforcement Learning Framework to Simulate Short-Term Learning Effects of Human Psychophysical Experiments Assessing the Quality of Artificial Vision",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169617898&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10191870"
      ]
    },
    {
      "abstract": "Reinforcement learning for hard-exploration tasks remains challenging due to the long-term dependence and sparse-and-delay rewards in complex environments. In these challenging tasks, intrinsic motivation has become a dominant paradigm to enable the agent to explore the environment when no external reward feedback is available. In this work, inspired by studies from the human memory mechanism, we present a mnemonic dictionary learning (MDL) model for intrinsic motivation in reinforcement learning. The MDL model leverages sparse dictionary learning to incremental abstract the exploration histories into a compact memory-like dictionary, providing an excellent intrinsic motivation model. This mnemonic dictionary model not only drives the agent to explore novel stats in the environments indicated by the memory reconstruction error but also helps the agent to remember the key states and structure of the environments using its learned bases and reconstruction coefficients. The proposed MDL model can serve as a generative module for existing exploration methods. Extensive experimental results on typical sparse-reward tasks demonstrate its effectiveness and applicability over several competing algorithms. We will release the source code and trained models to facilitate further studies in this research direction. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Renye Yan",
        "Zhe Wu",
        "Yuan Zhan",
        "Pin Tao",
        "Zongwei Wang",
        "Yimao Cai",
        "Junliang Xing"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IJCNN54540.2023.10191424",
      "keywords": [
        "exploration",
        "sparse dictionary learning",
        "deep neural network",
        "Reinforcement learning",
        "intrinsic motivation"
      ],
      "number_of_pages": 7,
      "pages": "1-7",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8868-6",
        "issn": "2161-4393",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Neural Networks"
      },
      "publication_date": "2023-06-18",
      "selected": null,
      "title": "Mnemonic Dictionary Learning for Intrinsic Motivation in Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10191424",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169605074&origin=inward"
      ]
    },
    {
      "abstract": "Learning cooperation in sparse-reward multi-agent reinforcement learning is challenging, since agents need to explore in the large joint-state space with sparse feedback. However, in cooperative games, the cooperative target is often related to partial attributes, hence there is no need to treat the whole state space equally. Therefore, we propose Underexplored Subspace Mining (USM), a novel type of intrinsic reward that encourages agents to selectively explore partial attributes instead of wasting time on the whole state space to accelerate learning. Specially, considering that the target-related attributes are varying in different games and hard to predefine, we choose to focus on the underexplored subspace as an alternative, which is an automatic aggregation of the underexplored bottom-level dimensions without any human design or learning parameters. We evaluate our method in cooperative games with discrete and continuous state space separately. Results demonstrate that USM consistently outperforms existing state-of-the-art methods, and becomes the only method that has succeeded in sparse-reward games evaluated with larger state space or more complicated cooperation dynamics. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Yang Yu",
        "Qiyue Yin",
        "Junge Zhang",
        "Hao Chen",
        "Kaiqi Huang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IJCNN54540.2023.10191833",
      "keywords": [
        "sparse-reward cooperation",
        "reinforcement learning",
        "selective exploration",
        "multi-agent system"
      ],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8868-6",
        "issn": "2161-4393",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Neural Networks"
      },
      "publication_date": "2023-06-18",
      "selected": null,
      "title": "Underexplored Subspace Mining for Sparse-Reward Cooperative Multi-Agent Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169534173&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10191833"
      ]
    },
    {
      "abstract": "Constructing commonsense knowledge graphs (CKGs) has attracted wide research attention due to its significant importance in cognitive intelligence. Nevertheless, existing CKGs are typically oriented to English, limiting the research in non-English languages. Meanwhile, the emergence of foundation models like ChatGPT and GPT-4 has shown promising intelligence with the help of reinforcement learning from human feedback. Under the background, in this paper, we utilize foundation models to construct a Chinese CKG, named Snowman. Specifically, we distill different types of commonsense head items from ChatGPT, and continue to use it to collect tail items with respect to the head items and pre-defined relations. Based on the preliminary analysis, we find the negative commonsense knowledge distilled by ChatGPT achieves lower human acceptance compared to other knowledge. Therefore, we design a simple yet effective self-instruct filtering strategy to filter out invalid negative commonsense. Overall, the constructed Snowman covers more than ten million Chinese commonsense triples, making it the largest Chinese CKG. Moreover, human studies show the acceptance of Snowman achieves 90.6\\%, indicating the high-quality triples distilled by the cutting-edge foundation model. We also conduct experiments on commonsense knowledge models to show the usability and effectiveness of our Snowman.",
      "authors": [
        "Wang, Jiaan",
        "Qu, Jianfeng",
        "Liang, Yunlong",
        "Li, Zhixu",
        "Liu, An",
        "Liu, Guanfeng",
        "Zheng, Xin"
      ],
      "categories": null,
      "citations": null,
      "comments": "tech report",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-17",
      "selected": null,
      "title": "Snowman: A Million-scale Chinese Commonsense Knowledge Graph Distilled from Foundation Model",
      "urls": [
        "http://arxiv.org/abs/2306.10241v1",
        "http://arxiv.org/pdf/2306.10241.pdf",
        "http://arxiv.org/pdf/2306.10241v1"
      ]
    },
    {
      "abstract": "Learning and recognition can be improved by sorting novel items into categories and subcategories. Such hierarchical categorization is easy when it can be performed according to learned rules (e.g., \u00e2\u0080\u009cif car, then automatic or stick shift\u00e2\u0080\u009d or \u00e2\u0080\u009cif boat, then motor or sail\u00e2\u0080\u009d). Here, we present results showing that human participants acquire categorization rules for new visual hierarchies rapidly, and that, as they do, corresponding hierarchical representations of the categorized stimuli emerge in patterns of neural activation in the dorsal striatum and in posterior frontal and parietal cortex. Participants learned to categorize novel visual objects into a hierarchy with superordinate and subordinate levels based on the objects' shape features, without having been told the categorization rules for doing so. On each trial, participants were asked to report the category and subcategory of the object, after which they received feedback about the correctness of their categorization responses. Participants trained over the course of a one-hour-long session while their brain activation was measured using functional magnetic resonance imaging. Over the course of training, significant hierarchy learning took place as participants discovered the nested categorization rules, as evidenced by the occurrence of a learning trial, after which performance suddenly increased. This learning was associated with increased representational strength of the newly acquired hierarchical rules in a corticostriatal network including the posterior frontal and parietal cortex and the dorsal striatum. We also found evidence suggesting that reinforcement learning in the dorsal striatum contributed to hierarchical rule learning. \u00c2\u00a9 2023 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.",
      "authors": [
        "Frank, S.M.",
        "Maechler, M.R.",
        "Fogelson, S.V.",
        "Tse, P.U."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/hbm.26323",
      "keywords": [
        "representational similarity analysis",
        "dorsal striatum",
        "categorization learning",
        "reinforcement learning",
        "hierarchical rule learning",
        "corticostriatal loops",
        "hierarchical decision trees",
        "intraparietal sulcus",
        "frontal cortex",
        "fMRI"
      ],
      "number_of_pages": 16,
      "pages": "3897-3912",
      "publication": {
        "category": "Journal",
        "cite_score": 9.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10659471",
        "publisher": "Wiley-Liss Inc.",
        "sjr": 1.688,
        "snip": 1.395,
        "subject_areas": [
          "Radiological and Ultrasound Technology",
          "Anatomy",
          "Neurology",
          "Neurology (clinical)",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Human Brain Mapping"
      },
      "publication_date": "2023-06-15",
      "selected": null,
      "title": "Hierarchical categorization learning is associated with representational changes in the dorsal striatum and posterior frontal and parietal cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85154070208&origin=inward"
      ]
    },
    {
      "abstract": "This paper presents a fully decentralized approach for realtime non-cooperative multi-robot navigation in social mini-games, such as navigating through a narrow doorway or negotiating right of way at a corridor intersection. Our contribution is a new realtime bi-level optimization algorithm, in which the top-level optimization consists of computing a fair and collision-free ordering followed by the bottom-level optimization which plans optimal trajectories conditioned on the ordering. We show that, given such a priority order, we can impose simple kinodynamic constraints on each robot that are sufficient for it to plan collision-free trajectories with minimal deviation from their preferred velocities, similar to how humans navigate in these scenarios. We successfully deploy the proposed algorithm in the real world using F$1/10$ robots, a Clearpath Jackal, and a Boston Dynamics Spot as well as in simulation using the SocialGym 2.0 multi-agent social navigation simulator, in the doorway and corridor intersection scenarios. We compare with state-of-the-art social navigation methods using multi-agent reinforcement learning, collision avoidance algorithms, and crowd simulation models. We show that $(i)$ classical navigation performs $44\\%$ better than the state-of-the-art learning-based social navigation algorithms, $(ii)$ without a scheduling protocol, our approach results in collisions in social mini-games $(iii)$ our approach yields $2\\times$ and $5\\times$ fewer velocity changes than CADRL in doorways and intersections, and finally $(iv)$ bi-level navigation in doorways at a flow rate of $2.8 - 3.3$ (ms)$^{-1}$ is comparable to flow rate in human navigation at a flow rate of $4$ (ms)$^{-1}$.",
      "authors": [
        "Chandra, Rohan",
        "Menon, Rahul",
        "Sprague, Zayne",
        "Anantula, Arya",
        "Biswas, Joydeep"
      ],
      "categories": null,
      "citations": null,
      "comments": "Submitted to IROS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-15",
      "selected": null,
      "title": "Decentralized Social Navigation with Non-Cooperative Robots via Bi-Level Optimization",
      "urls": [
        "http://arxiv.org/pdf/2306.08815v1",
        "http://arxiv.org/pdf/2306.08815.pdf",
        "http://arxiv.org/abs/2306.08815v1"
      ]
    },
    {
      "abstract": "Handing objects to humans is an essential capability for collaborative\nrobots. Previous research works on human-robot handovers focus on facilitating\nthe performance of the human partner and possibly minimising the physical\neffort needed to grasp the object. However, altruistic robot behaviours may\nresult in protracted and awkward robot motions, contributing to unpleasant\nsensations by the human partner and affecting perceived safety and social\nacceptance. This paper investigates whether transferring the cognitive science\nprinciple that \"humans act coefficiently as a group\" (i.e. simultaneously\nmaximising the benefits of all agents involved) to human-robot cooperative\ntasks promotes a more seamless and natural interaction. Human-robot\ncoefficiency is first modelled by identifying implicit indicators of human\ncomfort and discomfort as well as calculating the robot energy consumption in\nperforming the desired trajectory. We then present a reinforcement learning\napproach that uses the human-robot coefficiency score as reward to adapt and\nlearn online the combination of robot interaction parameters that maximises\nsuch coefficiency. Results proved that by acting coefficiently the robot could\nmeet the individual preferences of most subjects involved in the experiments,\nimprove the human perceived comfort, and foster trust in the robotic partner.",
      "authors": [
        "Marta Lagomarsino",
        "Marta Lorenzini",
        "Merryn Dale Constable",
        "Elena De Momi",
        "Cristina Becchio",
        "Arash Ajoudani"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 6 figures, IEEE Robotics and Automation Letters",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1109/LRA.2023.3280752",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-12",
      "selected": null,
      "title": "Maximising Coefficiency of Human-Robot Handovers through Reinforcement Learning",
      "urls": [
        "http://dx.doi.org/10.1109/LRA.2023.3280752",
        "http://arxiv.org/abs/2306.07205v1",
        "http://arxiv.org/pdf/2306.07205v1"
      ]
    },
    {
      "abstract": "The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or 'data enrichment', has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Will Hawkins",
        "Brent Mittelstadt"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3593013.3593995",
      "keywords": [
        "research ethics",
        "artificial intelligence",
        "data enrichment"
      ],
      "number_of_pages": 10,
      "pages": "261-270",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400701924",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publication_date": "2023-06-12",
      "selected": null,
      "title": "The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163608191&origin=inward",
        "https://dl.acm.org/doi/10.1145/3593013.3593995"
      ]
    },
    {
      "abstract": "In this paper, we present an LLM-based code translation method and an associated tool called CoTran, that translates whole-programs from one high-level programming language to another. Current LLM-based code translation methods lack a training approach to ensure that the translated code reliably compiles or bears substantial functional equivalence to the input code. In our work, we train an LLM via reinforcement learning, by modifying the fine-tuning process to incorporate compiler feedback and symbolic execution (symexec)-based equivalence testing feedback that checks for functional equivalence between the input and output programs. The idea is to guide an LLM-in-training, via compiler and symexec-based testing feedback, by letting it know how far it is from producing perfect translations. We report on extensive experiments comparing CoTran with 14 other code translation tools that include human-written transpilers, LLM-based translation tools, and ChatGPT over a benchmark of more than 57,000 Java-Python equivalent pairs, and we show that CoTran outperforms them on relevant metrics such as compilation accuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example, our tool achieves 48.68% FEqAcc, 76.98% CompAcc for Python-to-Java translation, whereas the nearest competing tool (PLBART-base) only gets 38.26% and 75.77% resp. Also, built upon CodeT5, CoTran achieves +11.23%, +14.89% improvement on FEqAcc and +4.07%, +8.14% on CompAcc for Java-to-Python and Python-to-Java translation resp.",
      "authors": [
        "Jana, Prithwish",
        "Jha, Piyush",
        "Ju, Haoyang",
        "Kishore, Gautham",
        "Mahajan, Aryan",
        "Ganesh, Vijay"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-11",
      "selected": null,
      "title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution",
      "urls": [
        "http://arxiv.org/pdf/2306.06755v3",
        "http://arxiv.org/pdf/2306.06755.pdf",
        "http://arxiv.org/abs/2306.06755v3"
      ]
    },
    {
      "abstract": "Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability. Dataset and code of our analysis is available in https://github.com/tanny411/GPT3-Reliability-Check.",
      "authors": [
        "Khatun, Aisha",
        "Brown, Daniel G."
      ],
      "categories": null,
      "citations": 0,
      "comments": "Accepted in TrustNLP: Third Workshop on Trustworthy Natural Language\n  Processing, co-located with ACL 2023",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 23,
      "pages": "73-95",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2023-06-09",
      "selected": null,
      "title": "Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175479957&origin=inward",
        "http://arxiv.org/pdf/2306.06199.pdf",
        "http://arxiv.org/abs/2306.06199v1",
        "http://arxiv.org/pdf/2306.06199v1"
      ]
    },
    {
      "abstract": "Reinforcement learning implies learning from positive and negative feedback when an agent interacts with its environment. Studies mapped feedback-based teaching signals to how the human brain processes information using dopamine (among other neurotransmitters). Two main dopamine genes; DAT1 and COMT, have a key role in regulating dopamine levels in the brain. Each gene has two well-studied variations that modulate reinforcement learning differently, thus creating four DAT1-COMT interaction patterns. Every human being carries one out of the four patterns. Extracting genotype variations via biological sampling is costly and time-consuming. Here, we introduce a machine learning alternative for effectively predicting the DAT1-COMT genotype variations by training classifiers on data from a reinforcement learning task, namely: (1). We k-nearest neighbor, random forest, and neural classifiers on reinforcement learning task data from 146 subjects who also provided blood samples for genotyping. The results showed that random forest has the best performance in predicting individual gene variations, while neural networks showed the highest performance for predicting the DAT1-COMT combined genotypes compared to biological sampling. Our approach opens a new direction for using machine learning in the form of reinforcement learning tasks and classifiers to infer key biological information. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Ashar Y. Natsheh",
        "Rashid Jayousi",
        "Mohammad M. Herzallah"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/AIIoT58121.2023.10174308",
      "keywords": [
        "Machine learning",
        "feedback-based learning",
        "reinforcement learning",
        "data mining",
        "dopamine",
        "prediction"
      ],
      "number_of_pages": 7,
      "pages": "0678-0684",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3762-4",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE World AI IoT Congress, AIIoT 2023"
      },
      "publication_date": "2023-06-07",
      "selected": null,
      "title": "Data Mining Algorithms Predict DAT1 and COMT Dopamine Genotypes Based on Reinforcement Learning Task",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10174308",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166672318&origin=inward"
      ]
    },
    {
      "abstract": "Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.",
      "authors": [
        "Ram\u00e9, Alexandre",
        "Couairon, Guillaume",
        "Shukor, Mustafa",
        "Dancette, Corentin",
        "Gaya, Jean-Baptiste",
        "Soulier, Laure",
        "Cord, Matthieu"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-07",
      "selected": null,
      "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
      "urls": [
        "http://arxiv.org/abs/2306.04488v2",
        "http://arxiv.org/pdf/2306.04488.pdf",
        "http://arxiv.org/pdf/2306.04488v2"
      ]
    },
    {
      "abstract": "Preference-based Reinforcement Learning (PbRL) has demonstrated remarkable efficacy in aligning rewards with human intentions. However, a significant challenge lies in the need of substantial human labels, which is costly and time-consuming. Additionally, the expensive preference data obtained from prior tasks is not typically reusable for subsequent task learning, leading to extensive labeling for each new task. In this paper, we propose a novel zero-shot preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, eliminating the requirement for human queries. Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels. However, learning directly from inferred labels that contains a fraction of noisy labels will result in an inaccurate reward function, subsequently affecting policy performance. To this end, we introduce Robust Preference Transformer, which models the rewards as Gaussian distributions and incorporates reward uncertainty in addition to reward mean. The empirical results on robotic manipulation tasks of Meta-World and Robomimic show that our method has strong capabilities of transferring preferences between tasks and learns reward functions from noisy labels robustly. Furthermore, we reveal that our method attains near-oracle performance with a small proportion of scripted labels.",
      "authors": [
        "Liu, Runze",
        "Du, Yali",
        "Bai, Fengshuo",
        "Lyu, Jiafei",
        "Li, Xiu"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-06",
      "selected": null,
      "title": "Zero-shot Preference Learning for Offline RL via Optimal Transport",
      "urls": [
        "http://arxiv.org/pdf/2306.03615.pdf",
        "http://arxiv.org/pdf/2306.03615v1",
        "http://arxiv.org/abs/2306.03615v1"
      ]
    },
    {
      "abstract": "This research aims to build generative language models specialized for the legal domain. The manuscript presents the development of LexGPT models based on GPT-J models and pre-trained with Pile of Law. The foundation model built in this manuscript is the initial step for the development of future applications in the legal domain, such as further training with reinforcement learning from human feedback. Another objective of this manuscript is to assist legal professionals in utilizing language models through the ``No Code'' approach. By fine-tuning models with specialized data and without modifying any source code, legal professionals can create custom language models for downstream tasks with minimum effort and technical knowledge. The downstream task in this manuscript is to turn a LexGPT model into a classifier, although the performance is notably lower than the state-of-the-art result. How to enhance downstream task performance without modifying the model or its source code is a research topic for future exploration.",
      "authors": [
        "Lee, Jieh-Sheng"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages and 2 figures. To be published in the Proceedings of the\n  Seventeenth International Workshop on Juris-informatics (JURISIN 2023),\n  hosted by JSAI International Symposia on AI 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-05",
      "selected": null,
      "title": "LexGPT 0.1: pre-trained GPT-J models with Pile of Law",
      "urls": [
        "http://arxiv.org/abs/2306.05431v1",
        "http://arxiv.org/pdf/2306.05431.pdf",
        "http://arxiv.org/pdf/2306.05431v1"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also provide a theoretical justification supporting the design of our loss function.",
      "authors": [
        "Zhu, Banghua",
        "Sharma, Hiteshi",
        "Frujeri, Felipe Vieira",
        "Dong, Shi",
        "Zhu, Chenguang",
        "Jordan, Michael I.",
        "Jiao, Jiantao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-04",
      "selected": null,
      "title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment",
      "urls": [
        "http://arxiv.org/abs/2306.02231v3",
        "http://arxiv.org/pdf/2306.02231v3",
        "http://arxiv.org/pdf/2306.02231.pdf"
      ]
    },
    {
      "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.",
      "authors": [
        "Wu, Zeqiu",
        "Hu, Yushi",
        "Shi, Weijia",
        "Dziri, Nouha",
        "Suhr, Alane",
        "Ammanabrolu, Prithviraj",
        "Smith, Noah A.",
        "Ostendorf, Mari",
        "Hajishirzi, Hannaneh"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS 2023 camera-ready",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-06-02",
      "selected": null,
      "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
      "urls": [
        "http://arxiv.org/abs/2306.01693v2",
        "http://arxiv.org/pdf/2306.01693.pdf",
        "http://arxiv.org/pdf/2306.01693v2"
      ]
    },
    {
      "abstract": "This paper introduces a novel method for determining the best room to place an object in, for embodied scene rearrangement. While state-of-the-art approaches rely on large language models (LLMs) or reinforcement learned (RL) policies for this task, our approach, CLIPGraphs, efficiently combines commonsense domain knowledge, data-driven methods, and recent advances in multimodal learning. Specifically, it (a)encodes a knowledge graph of prior human preferences about the room location of different objects in home environments, (b) incorporates vision-language features to support multimodal queries based on images or text, and (c) uses a graph network to learn object-room affinities based on embeddings of the prior knowledge and the vision-language features. We demonstrate that our approach provides better estimates of the most appropriate location of objects from a benchmark set of object categories in comparison with state-of-the-art baselines",
      "authors": [
        "Agrawal, Ayush",
        "Arora, Raghav",
        "Datta, Ahana",
        "Banerjee, Snehasis",
        "Bhowmick, Brojeshwar",
        "Jatavallabhula, Krishna Murthy",
        "Sridharan, Mohan",
        "Krishna, Madhava"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Robotics"
        ],
        "title": "RO-MAN 2023 Conference"
      },
      "publication_date": "2023-06-02",
      "selected": null,
      "title": "CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities",
      "urls": [
        "http://arxiv.org/abs/2306.01540v1",
        "http://arxiv.org/pdf/2306.01540v1",
        "http://arxiv.org/pdf/2306.01540.pdf"
      ]
    },
    {
      "abstract": "Stereotyping is a ubiquitous feature of social cognition, yet surprisingly little is known about how group-related beliefs influence the acquisition of person knowledge. Accordingly, in combination with computational modeling (i.e., Reinforcement Learning Drift Diffusion Model analysis), here we used a probabilistic selection task to explore the extent to which gender stereotypes impact instrumental learning. Several theoretically interesting effects were observed. First, reflecting the impact of cultural socialization on person construal, an expectancy-based preference for stereotype-consistent (vs. stereotype-inconsistent) responses was observed. Second, underscoring the potency of unexpected information, learning rates were faster for counter-stereotypic compared to stereotypic individuals, both for negative and positive prediction errors. Collectively, these findings are consistent with predictive accounts of social perception and have implications for the conditions under which stereotyping can potentially be reduced. \u00c2\u00a9 2023 The Authors",
      "authors": [
        "Falb\u00c3\u00a9n, J.K.",
        "Golubickis, M.",
        "Tsamadi, D.",
        "Persson, L.M.",
        "Macrae, C.N."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cognition.2023.105386",
      "keywords": [
        "Drift diffusion model",
        "Stereotyping",
        "Reinforcement learning",
        "Person perception",
        "Prediction errors"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00100277",
        "publisher": "Elsevier B.V.",
        "sjr": 1.691,
        "snip": 1.813,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Cognitive Neuroscience",
          "Language and Linguistics",
          "Linguistics and Language"
        ],
        "title": "Cognition"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "The power of the unexpected: Prediction errors enhance stereotype-based learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147538627&origin=inward"
      ]
    },
    {
      "abstract": "Obsessive-compulsive disorder (OCD) has been suggested to be associated with impairment of model-based behavioral control. Meanwhile, recent work suggested shorter memory trace for negative than positive prediction errors (PEs) in OCD. We explored relations between these two suggestions through computational modeling. Based on the properties of cortico-basal ganglia pathways, we modeled human as an agent having a combination of successor representation (SR)-based system that enables model-based-like control and individual representation (IR)-based system that only hosts model-free control, with the two systems potentially learning from positive and negative PEs in different rates. We simulated the agent\u2019s behavior in the environmental model used in the recent work that describes potential development of obsession-compulsion cycle. We found that the dual-system agent could develop enhanced obsession-compulsion cycle, similarly to the agent having memory trace imbalance in the recent work, if the SR- and IR-based systems learned mainly from positive and negative PEs, respectively. We then simulated the behavior of such an opponent SR+IR agent in the two-stage decision task, in comparison with the agent having only SR-based control. Fitting of the agents\u2019 behavior by the model weighing model-based and model-free control developed in the original two-stage task study resulted in smaller weights of model-based control for the opponent SR+IR agent than for the SR-only agent. These results reconcile the previous suggestions about OCD, i.e., impaired model-based control and memory trace imbalance, raising a novel possibility that opponent learning in model(SR)-based and model-free controllers underlies obsession-compulsion. Our model cannot explain the behavior of OCD patients in punishment, rather than reward, contexts, but it could be resolved if opponent SR+IR learning operates also in the recently revealed non-canonical cortico-basal ganglia-dopamine circuit for threat/aversiveness, rather than reward, reinforcement learning, and the aversive SR + appetitive IR agent could actually develop obsession-compulsion if the environment is modeled differently.",
      "authors": [
        "Reo Sato",
        "Kanji Shimomura",
        "Kenji Morita"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1011206",
      "keywords": [
        "Obsessive-compulsive disorder",
        "Anxiolytics",
        "Agent-based modeling",
        "Learning",
        "Behavior",
        "Simulation and modeling",
        "Decision making",
        "Human learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "Opponent learning with different representations in the cortico-basal ganglia pathways can develop obsession-compulsion cycle",
      "urls": [
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1011206&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164069791&origin=inward"
      ]
    },
    {
      "abstract": "Estimating human preference is an essential capability of a social robot. Such a machine Theory of Mind for others\u2019 preferences is studied predominantly under the framework of inverse reinforcement learning, which however requires millions of approach-avoidance trajectories in a simulated grid world as training samples to achieve state-of-the-art results. Here, the present study explores the utility of taking spatially small-scale approach-avoidance behaviors in real life as predictive cues for efficiently estimating human preferences. Using a toy-playing scenario as an example (\u03a3N\u2009=\u200958 young and older adults), we found that a person\u2019s subjectively reported and objectively inferred levels of attention and happiness could well predict the person\u2019s subjective levels of toy preference. Importantly, attention and happiness can be rapidly estimated from gaze directions and emotional expressions by computer vision techniques in a learning-free manner. Compared with preference learning from spatially larger-scale movement behaviors, robot estimation of human preferences from these smaller-scale behavioral cues can be more efficient and generalizable to unlearned situations in real life.",
      "authors": [
        "Huang, Tsung-Ren",
        "Chen, Tzu-Chun",
        "Lin, Ting-Yu",
        "Goh, Joshua O. S.",
        "Chang, Yu-Ling",
        "Yeh, Su-Ling",
        "Fu, Li-Chen"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s12369-023-01007-y",
      "keywords": [
        "Social Intelligence",
        "Human-Robot Interaction",
        "Preference",
        "Theory of Mind"
      ],
      "number_of_pages": 8,
      "pages": "999-1006",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18754791",
        "publisher": "Springer Netherlands",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Social Robotics"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "Spatially Small-scale Approach-avoidance Behaviors Allow Learning-free Machine Inference of Object Preferences in Human Minds",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s12369-023-01007-y.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160278537&origin=inward"
      ]
    },
    {
      "abstract": "This research focuses on the application of artificial neural networks (ANNs) on parameters extraction of photovoltaic (PV) models. Extracting parameters of the PV models accurately is crucial to control and optimize PV systems. Although many algorithms have been proposed to address this issue, how to extract the parameters of the PV models accurately and reliably is still a great challenge. Neural network algorithm (NNA) is a recently reported metaheuristic algorithm. NNA is inspired by ANNs. Benefiting from the unique structure of ANNs, NNA shows excellent global search ability. However, NNA faces the challenge of slow convergence rate and local optima stagnation in solving complex optimization problems. This article presents an improved NNA, named neural network algorithm with reinforcement learning (RLNNA), for extracting parameters of the PV models. In RLNNA, three strategies, namely modification factor with reinforcement learning (RL), transfer operator with historical population, and feedback operator, are designed to overcome the challenge of NNA. To verify the performance of RLNNA, it is employed to extract the parameters of the three PV models. Experimental results show that RLNNA can extract the parameters of the considered PV models with higher accuracy and stronger stability compared with NNA and the other 12 powerful algorithms, which fully indicates the effectiveness of the improved strategies. \u00c2\u00a9 2012 IEEE.",
      "authors": [
        "Zhang, Y."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2021.3109565",
      "keywords": [
        "Artificial neural networks (ANNs)",
        "photovoltaic (PV) models",
        "neural network algorithm (NNA)",
        "reinforcement learning (RL)"
      ],
      "number_of_pages": 11,
      "pages": "2806-2816",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "Neural Network Algorithm With Reinforcement Learning for Parameters Extraction of Photovoltaic Models",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115172442&origin=inward"
      ]
    },
    {
      "abstract": "Social deficits are among the core and most striking psychiatric symptoms, present in most psychiatric disorders. Here, we introduce a novel social learning framework, which consists of neuro-computational models that combine reinforcement learning with various types of social knowledge structures. We outline how this social learning framework can help specify and quantify social psychopathology across disorders and provide an overview of the brain regions that may be involved in this type of social learning. We highlight how this framework can specify commonalities and differences in the social psychopathology of individuals with autism spectrum disorder (ASD), personality disorders (PD), and major depressive disorder (MDD) and improve treatments on an individual basis. We conjecture that individuals with psychiatric disorders rely on rigid social knowledge representations when learning about others, albeit the nature of their rigidity and the behavioral consequences can greatly differ. While non-clinical cohorts tend to efficiently adapt social knowledge representations to relevant environmental constraints, psychiatric cohorts may rigidly stick to their preconceived notions or overly coarse knowledge representations during learning. \u00c2\u00a9 2023 Elsevier Ltd",
      "authors": [
        "Rosenblau, G.",
        "Frolichs, K.",
        "Korn, C.W."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neubiorev.2023.105181",
      "keywords": [
        "Neuro-computational modelling",
        "Personality disorders",
        "Transdiagnostic",
        "Mental health",
        "Autism spectrum disorder",
        "Major depressive disorder",
        "Reinforcement learning",
        "Social learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 13.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01497634",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.599,
        "snip": 2.49,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Neuroscience and Biobehavioral Reviews"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "A neuro-computational social learning framework to facilitate transdiagnostic classification and treatment across psychiatric disorders",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153104823&origin=inward"
      ]
    },
    {
      "abstract": "This letter introduces ERRA, an embodied learning architecture that enables robots to jointly obtain three fundamental capabilities (reasoning, planning, and interaction) for solving long-horizon language-conditioned manipulation tasks. ERRA is based on tightly-coupled probabilistic inferences at two granularity levels. Coarse-resolution inference is formulated as sequence generation through a large language model, which infers action language from natural language instruction and environment state. The robot then zooms to the fine-resolution inference part to perform the concrete action corresponding to the action language. Fine-resolution inference is constructed as a Markov decision process, which takes action language and environmental sensing as observations and outputs the action. The results of action execution in environments provide feedback for subsequent coarse-resolution reasoning. Such coarse-to-fine inference allows the robot to decompose and achieve long-horizon tasks interactively. In extensive experiments, we show that ERRA can complete various long-horizon manipulation tasks specified by abstract language instructions. We also demonstrate successful generalization to the novel but similar natural language instructions. \u00c2\u00a9 2016 IEEE.",
      "authors": [
        "Zhao, C.",
        "Yuan, S.",
        "Jiang, C.",
        "Cai, J.",
        "Yu, H.",
        "Wang, M.Y.",
        "Chen, Q."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/LRA.2023.3265893",
      "keywords": [
        "human-robot interaction",
        "reinforcement learning",
        "Manipulation",
        "large language model (LLM)",
        "reasoning"
      ],
      "number_of_pages": 8,
      "pages": "3230-3237",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "ERRA: An Embodied Representation and Reasoning Architecture for Long-Horizon Language-Conditioned Manipulation Tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153351346&origin=inward"
      ]
    },
    {
      "abstract": "The technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
      "authors": [
        "Will Hawkins",
        "Brent Mittelstadt"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1145/3593013.3593995.",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Artificial Intelligence",
          "Computers and Society"
        ],
        "title": "2023 ACM Conference on Fairness, Accountability, and Transparency"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices",
      "urls": [
        "http://arxiv.org/abs/2306.01800v1",
        "http://dx.doi.org/10.1145/3593013.3593995.",
        "http://arxiv.org/pdf/2306.01800v1"
      ]
    },
    {
      "abstract": "Reinforcement learning models have been extensively studied for decision-making tasks with reward feedback. However, in designing an experiment to collect data for Q-learning models, the quantitative effect of a presented stimulus on the estimation precision of participant parameters has generally not been considered. That is, the lack of a mathematical framework has prevented researchers from designing an optimal experiment. To tackle this problem, this study analytically derives the Fisher information. Furthermore, this study formulates a stochastic representation of the Q-learning model, which is one of the most commonly applied reinforcement learning models. With this derivation, a two-step procedure is proposed to select the optimal stimuli in terms of estimation precision, in which low-cost Fisher information evaluation and more detailed finite-sample Monte Carlo simulation are combined. The simulation studies show that reward probability reversal leads to a high estimation precision for the learning rate parameter. By contrast, for the inverse temperature parameter, a larger difference in reward probability between options leads to higher estimation precision. These results reveal that the optimal experimental design is dependent on which trait parameters of the Q-learning model are of interest to researchers. Further, it is found that the use of undesirable stimuli in terms of trait parameter precision leads to a large bias in the correlation coefficient estimate. Based on the results, the approaches to designing experiments in the Q-learning model are discussed.",
      "authors": [
        "Fujita, Kazuya",
        "Okada, Kensuke",
        "Katahira, Kentaro"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s42113-022-00163-0",
      "keywords": [
        "Fisher information matrix",
        "Adaptive stimulus selection",
        "Reinforcement learning",
        "Computerized adaptive testing"
      ],
      "number_of_pages": 18,
      "pages": "262-279",
      "publication": {
        "category": "Journal",
        "cite_score": 4.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2522-087X",
        "publisher": "Springer Nature",
        "sjr": 0.968,
        "snip": 0.919,
        "subject_areas": [
          "Developmental and Educational Psychology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Computational Brain and Behavior"
      },
      "publication_date": "2023-06-01",
      "selected": null,
      "title": "Stimulus Selection in a Q-learning Model Using Fisher Information and Monte Carlo Simulation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146994625&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s42113-022-00163-0.pdf"
      ]
    },
    {
      "abstract": "<p>People experience instances of social feedback as interdependent with potential implications for their entire self-concept. How do people maintain positivity and coherence across the self-concept while updating self-views from feedback? We present a network model describing how the brain represents the semantic dependency relations among traits and uses this information to avoid an overall loss of positivity and coherence. Both male and female human participants received social feedback during a self-evaluation task while undergoing functional magnetic resonance imaging. We modeled self-belief updating by incorporating a reinforcement learning model within the network structure. Participants learned more rapidly from positive than negative feedback and were less likely to change self-views for traits with more dependencies in the network. Further, participants back propagated feedback across network relations while retrieving prior feedback on the basis of network similarity to inform ongoing self-views. Activation in ventromedial prefrontal cortex (vmPFC) reflected the constrained updating process such that positive feedback led to higher activation and negative feedback to less activation for traits with more dependencies. Additionally, vmPFC was associated with the novelty of a trait relative to previously self-evaluated traits in the network, and angular gyrus was associated with greater certainty for self-beliefs given the relevance of prior feedback. We propose that neural computations that selectively enhance or attenuate social feedback and retrieve past relevant experiences to guide ongoing self-evaluations may support an overall positive and coherent self-concept.</p><p><b>SIGNIFICANCE STATEMENT</b> We humans experience social feedback throughout our lives, but we do not dispassionately incorporate feedback into our self-concept. The implications of feedback for our entire self-concept plays a role in how we either change or retain our prior self-beliefs. In a neuroimaging study, we find that people are less likely to change their beliefs from feedback when the feedback has broader implications for the self-concept. This resistance to change is reflected in processing in the ventromedial prefrontal cortex, a region that is central to self-referential and social cognition. These results are broadly applicable given the role that maintaining a positive and coherent self-concept plays in promoting mental health and development throughout the lifespan.</p>",
      "authors": [
        "Jacob J. Elder",
        "Tyler H. Davis",
        "Brent L. Hughes"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1951-22.2023",
      "keywords": [
        "network analysis",
        "semantics",
        "reinforcement learning",
        "self-concept",
        "prefrontal cortex",
        "social feedback"
      ],
      "number_of_pages": 19,
      "pages": "4110-4128",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2023-05-31",
      "selected": null,
      "title": "A Fluid Self-Concept: How the Brain Maintains Coherence and Positivity across an Interconnected Self-Concept While Incorporating Social Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160966515&origin=inward"
      ]
    },
    {
      "abstract": "Autonomously learning diverse behaviors without an extrinsic reward signal has been a problem of interest in reinforcement learning. However, the nature of learning in such mechanisms is unconstrained, often resulting in the accumulation of several unusable, unsafe or misaligned skills. In order to avoid such issues and ensure the discovery of safe and human-aligned skills, it is necessary to incorporate humans into the unsupervised training process, which remains a largely unexplored research area. In this work, we propose Controlled Diversity with Preference (CDP) See code here: https://github.com/HussonnoisMaxence/CDP (https://github.com/HussonnoisMaxence/CDP) , a novel, collaborative human-guided mechanism for an agent to learn a set of skills that is diverse as well as desirable. The key principle is to restrict the discovery of skills to those regions that are deemed to be desirable as per a preference model trained using human preference labels on trajectory pairs. We evaluate our approach on 2D navigation and Mujoco environments and demonstrate the ability to discover diverse, yet desirable skills.",
      "authors": [
        "Maxence Hussonnois",
        "Thommen George Karimpanal",
        "Santu Rana"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3545946.3598755",
      "keywords": [
        "human preferences",
        "skill diversity",
        "reinforcement learning"
      ],
      "number_of_pages": 9,
      "pages": "1135-1143",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450394321",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems"
      },
      "publication_date": "2023-05-30",
      "selected": null,
      "title": "Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3545946.3598755"
      ]
    },
    {
      "abstract": "We investigate an approach for enabling a reinforcement learning agent to learn about dangerous states or constraints from stop-feedback preventing the agent from taking any further, potentially dangerous, actions. Such feedback could be provided by human supervisors overseeing the RL agent's behavior while carrying out some complex tasks. To enable the RL agent to learn from the supervisor's feedback, we propose a probabilistic model for approximating how the supervisor's feedback could have been generated and consider a Bayesian approach for inferring dangerous states. We evaluated our approach using an OpenAI Safety Gym environment and demonstrated that our agent can effectively infer the imposed safety constraints. Furthermore, we conducted a user study to validate our human-inspired feedback model and to obtain insights into the human provision of stop-feedback.",
      "authors": [
        "Silvia Poletti",
        "Alberto Testolin",
        "Sebastian Tschiatschek"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3545946.3598923",
      "keywords": [
        "human feedback",
        "safety",
        "constraint learning",
        "reinforcement learning"
      ],
      "number_of_pages": 3,
      "pages": "2328-2330",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450394321",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems"
      },
      "publication_date": "2023-05-30",
      "selected": null,
      "title": "Learning Constraints From Human Stop-Feedback in Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3545946.3598923"
      ]
    },
    {
      "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
      "authors": [
        "Rafailov, Rafael",
        "Sharma, Archit",
        "Mitchell, Eric",
        "Ermon, Stefano",
        "Manning, Christopher D.",
        "Finn, Chelsea"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-29",
      "selected": null,
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "urls": [
        "http://arxiv.org/pdf/2305.18290.pdf",
        "http://arxiv.org/abs/2305.18290v2",
        "http://arxiv.org/pdf/2305.18290v2"
      ]
    },
    {
      "abstract": "Is it possible to evaluate the moral cognition of complex artificial agents? In this work, we take a look at one aspect of morality: `doing the right thing for the right reasons.' We propose a behavior-based analysis of artificial moral cognition which could also be applied to humans to facilitate like-for-like comparison. Morally-motivated behavior should persist despite mounting cost; by measuring an agent's sensitivity to this cost, we gain deeper insight into underlying motivations. We apply this evaluation to a particular set of deep reinforcement learning agents, trained by memory-based meta-reinforcement learning. Our results indicate that agents trained with a reward function that includes other-regarding preferences perform helping behavior in a way that is less sensitive to increasing cost than agents trained with more self-interested preferences.",
      "authors": [
        "Mao, Yiran",
        "Reinecke, Madeline G.",
        "Kunesch, Markus",
        "Du\u00e9\u00f1ez-Guzm\u00e1n, Edgar A.",
        "Comanescu, Ramona",
        "Haas, Julia",
        "Leibo, Joel Z."
      ],
      "categories": null,
      "citations": null,
      "comments": "11 pages, 3 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-29",
      "selected": null,
      "title": "Doing the right thing for the right reason: Evaluating artificial moral cognition by probing cost insensitivity",
      "urls": [
        "http://arxiv.org/pdf/2305.18269.pdf",
        "http://arxiv.org/pdf/2305.18269v1",
        "http://arxiv.org/abs/2305.18269v1"
      ]
    },
    {
      "abstract": "Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While PbRL has demonstrated practical success in fine-tuning language models, existing theoretical work focuses on regret minimization and fails to capture most of the practical frameworks. In this study, we fill in such a gap between theoretical PbRL and practical algorithms by proposing a theoretical reward-agnostic PbRL framework where exploratory trajectories that enable accurate learning of hidden reward functions are acquired before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing theoretical literature. Specifically, our framework can incorporate linear and low-rank MDPs with efficient sample complexity. Additionally, we investigate reward-agnostic RL with action-based comparison feedback and introduce an efficient querying algorithm tailored to this scenario.",
      "authors": [
        "Zhan, Wenhao",
        "Uehara, Masatoshi",
        "Sun, Wen",
        "Lee, Jason D."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-29",
      "selected": null,
      "title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2305.18505v2",
        "http://arxiv.org/abs/2305.18505v2",
        "http://arxiv.org/pdf/2305.18505.pdf"
      ]
    },
    {
      "abstract": "In this paper, we study offline Reinforcement Learning with Human Feedback (RLHF) where we aim to learn the human's underlying reward and the MDP's optimal policy from a set of trajectories induced by human choices. RLHF is challenging for multiple reasons: large state space but limited human feedback, the bounded rationality of human decisions, and the off-policy distribution shift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for modeling and understanding human choices. DCC, rooted in econometrics and decision theory, is widely used to model a human decision-making process with forward-looking and bounded rationality. We propose a \\underline{D}ynamic-\\underline{C}hoice-\\underline{P}essimistic-\\underline{P}olicy-\\underline{O}ptimization (DCPPO) method. \\ The method involves a three-stage process: The first step is to estimate the human behavior policy and the state-action value function via maximum likelihood estimation (MLE); the second step recovers the human reward function via minimizing Bellman mean squared error using the learned value functions; the third step is to plug in the learned reward and invoke pessimistic value iteration for finding a near-optimal policy. With only single-policy coverage (i.e., optimal policy) of the dataset, we prove that the suboptimality of DCPPO almost matches the classical pessimistic offline RL algorithm in terms of suboptimality's dependency on distribution shift and dimension. To the best of our knowledge, this paper presents the first theoretical guarantees for off-policy offline RLHF with dynamic discrete choice model.",
      "authors": [
        "Li, Zihao",
        "Yang, Zhuoran",
        "Wang, Mengdi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-29",
      "selected": null,
      "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism",
      "urls": [
        "http://arxiv.org/abs/2305.18438v3",
        "http://arxiv.org/pdf/2305.18438.pdf",
        "http://arxiv.org/pdf/2305.18438v3"
      ]
    },
    {
      "abstract": "How do language models \"think\"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.",
      "authors": [
        "Nguyen, Khanh"
      ],
      "categories": null,
      "citations": null,
      "comments": "Proceedings of the First Workshop on Theory of Mind in Communicating\n  Agents at (TOM @ ICML 2023)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-28",
      "selected": null,
      "title": "Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective",
      "urls": [
        "http://arxiv.org/pdf/2305.17760.pdf",
        "http://arxiv.org/pdf/2305.17760v6",
        "http://arxiv.org/abs/2305.17760v6"
      ]
    },
    {
      "abstract": "Given the popular presupposition of human reasoning as the standard for learning and decision making, there have been significant efforts and a growing trend in research to replicate these innate human abilities in artificial systems. As such, topics including Game Theory, Theory of Mind, and Machine Learning, among others, integrate concepts that are assumed components of human reasoning. These serve as techniques to replicate and understand the behaviors of humans. In addition, next-generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, allowing them not only to replicate human models as a technique to \"learn\"but also to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this article is to provide a succinct yet systematic review of important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques that learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without necessarily learning via trial and error. \u00c2\u00a9 2023 Copyright held by the owner/author(s).",
      "authors": [
        "Andrew Fuchs",
        "Andrea Passarella",
        "Marco Conti"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3580492",
      "keywords": [
        "human behavior",
        "human-centric AI",
        "Artificial intelligence",
        "Machine Learning",
        "bias",
        "Human-AI Interaction",
        "cognition"
      ],
      "number_of_pages": 47,
      "pages": "1-47",
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1556-4665",
        "publisher": "Association for Computing Machinery (ACM)",
        "sjr": 0.487,
        "snip": 1.313,
        "subject_areas": [
          "Software",
          "Computer Science (miscellaneous)",
          "Control and Systems Engineering"
        ],
        "title": "ACM Transactions on Autonomous and Adaptive Systems"
      },
      "publication_date": "2023-05-28",
      "selected": null,
      "title": "Modeling, Replicating, and Predicting Human Behavior: A Survey",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152768694&origin=inward",
        "https://dl.acm.org/doi/10.1145/3580492"
      ]
    },
    {
      "abstract": "Preference-based reinforcement learning (PbRL) provides a natural way to align RL agents' behavior with human desired outcomes, but is often restrained by costly human feedback. To improve feedback efficiency, most existing PbRL methods focus on selecting queries to maximally improve the overall quality of the reward model, but counter-intuitively, we find that this may not necessarily lead to improved performance. To unravel this mystery, we identify a long-neglected issue in the query selection schemes of existing PbRL studies: Query-Policy Misalignment. We show that the seemingly informative queries selected to improve the overall quality of reward model actually may not align with RL agents' interests, thus offering little help on policy learning and eventually resulting in poor feedback efficiency. We show that this issue can be effectively addressed via near on-policy query and a specially designed hybrid experience replay, which together enforce the bidirectional query-policy alignment. Simple yet elegant, our method can be easily incorporated into existing approaches by changing only a few lines of code. We showcase in comprehensive experiments that our method achieves substantial gains in both human feedback and RL sample efficiency, demonstrating the importance of addressing query-policy misalignment in PbRL tasks.",
      "authors": [
        "Hu, Xiao",
        "Li, Jianxiong",
        "Zhan, Xianyuan",
        "Jia, Qing-Shan",
        "Zhang, Ya-Qin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-27",
      "selected": null,
      "title": "Query-Policy Misalignment in Preference-Based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2305.17400v2",
        "http://arxiv.org/pdf/2305.17400v2",
        "http://arxiv.org/pdf/2305.17400.pdf"
      ]
    },
    {
      "abstract": "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.",
      "authors": [
        "Fu, Yao",
        "Ou, Litu",
        "Chen, Mingyu",
        "Wan, Yuhao",
        "Peng, Hao",
        "Khot, Tushar"
      ],
      "categories": null,
      "citations": null,
      "comments": "Preprint. Code at https://github.com/FranxYao/chain-of-thought-hub",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-26",
      "selected": null,
      "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
      "urls": [
        "http://arxiv.org/pdf/2305.17306.pdf",
        "http://arxiv.org/abs/2305.17306v1",
        "http://arxiv.org/pdf/2305.17306v1"
      ]
    },
    {
      "abstract": "We propose a method to capture the handling abilities of fast jet pilots in a software model via reinforcement learning (RL) from human preference feedback. We use pairwise preferences over simulated flight trajectories to learn an interpretable rule-based model called a reward tree, which enables the automated scoring of trajectories alongside an explanatory rationale. We train an RL agent to execute high-quality handling behaviour by using the reward tree as the objective, and thereby generate data for iterative preference collection and further refinement of both tree and agent. Experiments with synthetic preferences show reward trees to be competitive with uninterpretable neural network reward models on quantitative and qualitative evaluations.",
      "authors": [
        "Bewley, Tom",
        "Lawry, Jonathan",
        "Richards, Arthur"
      ],
      "categories": null,
      "citations": null,
      "comments": "arXiv admin note: substantial text overlap with arXiv:2210.01007",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-26",
      "selected": null,
      "title": "Learning Interpretable Models of Aircraft Handling Behaviour by Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2305.16924.pdf",
        "http://arxiv.org/abs/2305.16924v1",
        "http://arxiv.org/pdf/2305.16924v1"
      ]
    },
    {
      "abstract": "Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning with respect to both image-text alignment and image quality. Our code is available at https://github.com/google-research/google-research/tree/master/dpok.",
      "authors": [
        "Fan, Ying",
        "Watkins, Olivia",
        "Du, Yuqing",
        "Liu, Hao",
        "Ryu, Moonkyung",
        "Boutilier, Craig",
        "Abbeel, Pieter",
        "Ghavamzadeh, Mohammad",
        "Lee, Kangwook",
        "Lee, Kimin"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-25",
      "selected": null,
      "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models",
      "urls": [
        "http://arxiv.org/pdf/2305.16381.pdf",
        "http://arxiv.org/abs/2305.16381v3",
        "http://arxiv.org/pdf/2305.16381v3"
      ]
    },
    {
      "abstract": "This study focuses on the topic of offline preference-based reinforcement learning (PbRL), a variant of conventional reinforcement learning that dispenses with the need for online interaction or specification of reward functions. Instead, the agent is provided with fixed offline trajectories and human preferences between pairs of trajectories to extract the dynamics and task information, respectively. Since the dynamics and task information are orthogonal, a naive approach would involve using preference-based reward learning followed by an off-the-shelf offline RL algorithm. However, this requires the separate learning of a scalar reward function, which is assumed to be an information bottleneck of the learning process. To address this issue, we propose the offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function. OPPO achieves this by introducing an offline hindsight information matching objective for optimizing a contextual policy and a preference modeling objective for finding the optimal context. OPPO further integrates a well-performing decision policy by optimizing the two objectives iteratively. Our empirical results demonstrate that OPPO effectively models offline preferences and outperforms prior competing baselines, including offline RL algorithms performed over either true or pseudo reward function specifications. Our code is available on the project website: https://sites.google.com/view/oppo-icml-2023 .",
      "authors": [
        "Kang, Yachen",
        "Shi, Diyuan",
        "Liu, Jinxin",
        "He, Li",
        "Wang, Donglin"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 16,
      "pages": "15753-15768",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-05-25",
      "selected": null,
      "title": "Beyond Reward: Offline Preference-guided Policy Optimization",
      "urls": [
        "http://arxiv.org/pdf/2305.16217v2",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174384953&origin=inward",
        "http://arxiv.org/pdf/2305.16217.pdf",
        "http://arxiv.org/abs/2305.16217v2"
      ]
    },
    {
      "abstract": "Translating natural language sentences to first-order logic (NL-FOL translation) is a longstanding challenge in the NLP and formal logic literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of directly translating natural language into FOL rules, which outperforms GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5, and can achieve similar performance as GPT-4 with a fraction of the cost. This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model. To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel gener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset of 34K high-quality and diverse sentence-level NL-FOL pairs collected from GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4 for pairs, and dynamically adjusts the prompts to ensure the collection of pairs with rich and diverse contexts at different levels of complexity, and verifies the validity of the generated FOL rules. Codes, weights, and data are available at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small \\text{https://github.com/gblackout/LogicLLaMA}}}$.",
      "authors": [
        "Yang, Yuan",
        "Xiong, Siheng",
        "Payani, Ali",
        "Shareghi, Ehsan",
        "Fekri, Faramarz"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-24",
      "selected": null,
      "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
      "urls": [
        "http://arxiv.org/pdf/2305.15541v1",
        "http://arxiv.org/abs/2305.15541v1",
        "http://arxiv.org/pdf/2305.15541.pdf"
      ]
    },
    {
      "abstract": "Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods na\\\"ively combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the $Q$-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released.",
      "authors": [
        "Hejna, Joey",
        "Sadigh, Dorsa"
      ],
      "categories": null,
      "citations": null,
      "comments": "Updated for NeurIPS 2023 Acceptance",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-24",
      "selected": null,
      "title": "Inverse Preference Learning: Preference-based RL without a Reward Function",
      "urls": [
        "http://arxiv.org/abs/2305.15363v2",
        "http://arxiv.org/pdf/2305.15363.pdf",
        "http://arxiv.org/pdf/2305.15363v2"
      ]
    },
    {
      "abstract": "In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both \\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors} in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at \\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is available at \\url{https://www.HuatuoGPT.cn/}.",
      "authors": [
        "Zhang, Hongbo",
        "Chen, Junying",
        "Jiang, Feng",
        "Yu, Fei",
        "Chen, Zhihong",
        "Li, Jianquan",
        "Chen, Guiming",
        "Wu, Xiangbo",
        "Zhang, Zhiyi",
        "Xiao, Qingying",
        "Wan, Xiang",
        "Wang, Benyou",
        "Li, Haizhou"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-24",
      "selected": null,
      "title": "HuatuoGPT, towards Taming Language Model to Be a Doctor",
      "urls": [
        "http://arxiv.org/pdf/2305.15075v1",
        "http://arxiv.org/abs/2305.15075v1",
        "http://arxiv.org/pdf/2305.15075.pdf"
      ]
    },
    {
      "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
      "authors": [
        "Tian, Katherine",
        "Mitchell, Eric",
        "Zhou, Allan",
        "Sharma, Archit",
        "Rafailov, Rafael",
        "Yao, Huaxiu",
        "Finn, Chelsea",
        "Manning, Christopher D."
      ],
      "categories": null,
      "citations": null,
      "comments": "EMNLP 2023 Camera Ready",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-24",
      "selected": null,
      "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
      "urls": [
        "http://arxiv.org/abs/2305.14975v2",
        "http://arxiv.org/pdf/2305.14975.pdf",
        "http://arxiv.org/pdf/2305.14975v2"
      ]
    },
    {
      "abstract": "In this paper, we investigate the problem of offline Preference-based Reinforcement Learning (PbRL) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coefficient. We also establish lower bounds that highlight the necessity of such concentrability and the difference from standard RL, where state-action-wise rewards are directly observed. We further extend and analyze our algorithm when the feedback is given over action pairs.",
      "authors": [
        "Zhan, Wenhao",
        "Uehara, Masatoshi",
        "Kallus, Nathan",
        "Lee, Jason D.",
        "Sun, Wen"
      ],
      "categories": null,
      "citations": null,
      "comments": "The first two authors contribute equally",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-24",
      "selected": null,
      "title": "Provable Offline Preference-Based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2305.14816v2",
        "http://arxiv.org/pdf/2305.14816.pdf",
        "http://arxiv.org/abs/2305.14816v2"
      ]
    },
    {
      "abstract": "Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL",
      "authors": [
        "Baheti, Ashutosh",
        "Lu, Ximing",
        "Brahman, Faeze",
        "Bras, Ronan Le",
        "Sap, Maarten",
        "Riedl, Mark"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-24",
      "selected": null,
      "title": "Improving Language Models with Advantage-based Offline Policy Gradients",
      "urls": [
        "http://arxiv.org/abs/2305.14718v3",
        "http://arxiv.org/pdf/2305.14718v3",
        "http://arxiv.org/pdf/2305.14718.pdf"
      ]
    },
    {
      "abstract": "Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon increases, or when data is obtained from human demonstrators. Based on the overall strength of Sequence Modeling, we also investigate architectural choices and scaling trends for DT on Atari and D4RL and make design recommendations. We find that scaling the amount of data for DT by 5x gives a 2.5x average score improvement on Atari.",
      "authors": [
        "Bhargava, Prajjwal",
        "Chitnis, Rohan",
        "Geramifard, Alborz",
        "Sodhani, Shagun",
        "Zhang, Amy"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-23",
      "selected": null,
      "title": "Sequence Modeling is a Robust Contender for Offline Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2305.14550v2",
        "http://arxiv.org/pdf/2305.14550.pdf",
        "http://arxiv.org/pdf/2305.14550v2"
      ]
    },
    {
      "abstract": "Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost",
      "authors": [
        "Kim, Sungdong",
        "Bae, Sanghwan",
        "Shin, Jamin",
        "Kang, Soyoung",
        "Kwak, Donghyun",
        "Yoo, Kang Min",
        "Seo, Minjoon"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to EMNLP 2023 main conference",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-23",
      "selected": null,
      "title": "Aligning Large Language Models through Synthetic Feedback",
      "urls": [
        "http://arxiv.org/abs/2305.13735v2",
        "http://arxiv.org/pdf/2305.13735.pdf",
        "http://arxiv.org/pdf/2305.13735v2"
      ]
    },
    {
      "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io .",
      "authors": [
        "Black, Kevin",
        "Janner, Michael",
        "Du, Yilun",
        "Kostrikov, Ilya",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": "23 pages, 16 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-22",
      "selected": null,
      "title": "Training Diffusion Models with Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2305.13301.pdf",
        "http://arxiv.org/abs/2305.13301v4",
        "http://arxiv.org/pdf/2305.13301v4"
      ]
    },
    {
      "abstract": "The ability to pick up on language signals in an ongoing interaction is crucial for future machine learning models to collaborate and interact with humans naturally. In this paper, we present an initial study that evaluates intra-episodic feedback given in a collaborative setting. We use a referential language game as a controllable example of a task-oriented collaborative joint activity. A teacher utters a referring expression generated by a well-known symbolic algorithm (the \"Incremental Algorithm\") as an initial instruction and then monitors the follower's actions to possibly intervene with intra-episodic feedback (which does not explicitly have to be requested). We frame this task as a reinforcement learning problem with sparse rewards and learn a follower policy for a heuristic teacher. Our results show that intra-episodic feedback allows the follower to generalize on aspects of scene complexity and performs better than providing only the initial statement.",
      "authors": [
        "Sadler, Philipp",
        "Hakimov, Sherzod",
        "Schlangen, David"
      ],
      "categories": null,
      "citations": null,
      "comments": "5 pages, Accepted at Findings of ACL 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-22",
      "selected": null,
      "title": "Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers",
      "urls": [
        "http://arxiv.org/pdf/2305.12880.pdf",
        "http://arxiv.org/pdf/2305.12880v1",
        "http://arxiv.org/abs/2305.12880v1"
      ]
    },
    {
      "abstract": "A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning. Our experiments demonstrate one concrete instance of this through efficient exploration based on the representation and resolution of epistemic uncertainty. In order to illustrate these ideas in a transparent manner, we restrict attention to a simple didactic data generating process and leave for future work extension to systems of practical scale.",
      "authors": [
        "Xu, Wanqiao",
        "Dong, Shi",
        "Arumugam, Dilip",
        "Van Roy, Benjamin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-19",
      "selected": null,
      "title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models",
      "urls": [
        "http://arxiv.org/pdf/2305.11455.pdf",
        "http://arxiv.org/pdf/2305.11455v1",
        "http://arxiv.org/abs/2305.11455v1"
      ]
    },
    {
      "abstract": "Optimizing for humans' latent preferences remains a grand challenge in route recommendation. Prior research has provided increasingly general techniques based on inverse reinforcement learning (IRL), yet no approach has been successfully scaled to world-sized routing problems with hundreds of millions of states and demonstration trajectories. In this paper, we provide methods for scaling IRL using graph compression, spatial parallelization, and problem initialization based on dominant eigenvectors. We revisit classic algorithms and study them in a large-scale setting, and make the key observation that there exists a trade-off between the use of cheap, deterministic planners and expensive yet robust stochastic policies. We leverage this insight in Receding Horizon Inverse Planning (RHIP), a new generalization of classic IRL algorithms that provides fine-grained control over performance trade-offs via its planning horizon. Our contributions culminate in a policy that achieves a 16-24% improvement in global route quality, and to the best of our knowledge, represents the largest instance of IRL in a real-world setting to date. Benchmark results show critical benefits to more sustainable modes of transportation, where factors beyond journey time play a substantial role. We conclude by conducting an ablation study of key components, presenting negative results from alternative eigenvalue solvers, and identifying opportunities to further improve scalability via IRL-specific batching strategies.",
      "authors": [
        "Barnes, Matt",
        "Abueg, Matthew",
        "Lange, Oliver F.",
        "Deeds, Matt",
        "Trader, Jason",
        "Molitor, Denali",
        "Wulfmeier, Markus",
        "O'Banion, Shawn"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-18",
      "selected": null,
      "title": "Massively Scalable Inverse Reinforcement Learning in Google Maps",
      "urls": [
        "http://arxiv.org/pdf/2305.11290.pdf",
        "http://arxiv.org/pdf/2305.11290v3",
        "http://arxiv.org/abs/2305.11290v3"
      ]
    },
    {
      "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
      "authors": [
        "Zhou, Chunting",
        "Liu, Pengfei",
        "Xu, Puxin",
        "Iyer, Srini",
        "Sun, Jiao",
        "Mao, Yuning",
        "Ma, Xuezhe",
        "Efrat, Avia",
        "Yu, Ping",
        "Yu, Lili",
        "Zhang, Susan",
        "Ghosh, Gargi",
        "Lewis, Mike",
        "Zettlemoyer, Luke",
        "Levy, Omer"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-18",
      "selected": null,
      "title": "LIMA: Less Is More for Alignment",
      "urls": [
        "http://arxiv.org/pdf/2305.11206.pdf",
        "http://arxiv.org/pdf/2305.11206v1",
        "http://arxiv.org/abs/2305.11206v1"
      ]
    },
    {
      "abstract": "Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.",
      "authors": [
        "Zhao, Yao",
        "Joshi, Rishabh",
        "Liu, Tianqi",
        "Khalman, Misha",
        "Saleh, Mohammad",
        "Liu, Peter J."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-17",
      "selected": null,
      "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2305.10425.pdf",
        "http://arxiv.org/abs/2305.10425v1",
        "http://arxiv.org/pdf/2305.10425v1"
      ]
    },
    {
      "abstract": "Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the generated program fails to solve the task. Prepended to this fine-tuning text, a binary reward token is used to differentiate correct and buggy solutions. On MBPP, a code generation dataset, LeTI substantially improves the performance of two base LMs of different scales. LeTI requires no ground-truth outputs for training and even outperforms a fine-tuned baseline that does. LeTI's strong performance generalizes to other datasets. Trained on MBPP, it achieves comparable or better performance than the base LMs on unseen problems in HumanEval. Furthermore, compared to binary feedback, we observe that textual feedback leads to improved generation quality and sample efficiency, achieving the same performance with fewer than half of the gradient steps. LeTI is equally applicable in natural language tasks when they can be formulated as code generation, which we empirically verified on event argument extraction.",
      "authors": [
        "Wang, Xingyao",
        "Peng, Hao",
        "Jabbarvand, Reyhaneh",
        "Ji, Heng"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-17",
      "selected": null,
      "title": "LeTI: Learning to Generate from Textual Interactions",
      "urls": [
        "http://arxiv.org/abs/2305.10314v1",
        "http://arxiv.org/pdf/2305.10314v1",
        "http://arxiv.org/pdf/2305.10314.pdf"
      ]
    },
    {
      "abstract": "Artificial limbs are sophisticated devices to assist people with tasks of daily living. Despite advanced robotic prostheses demonstrating similar motion capabilities to biological limbs, users report them difficult and non-intuitive to use. Providing more effective feedback from the device to the user has therefore become a topic of increased interest. In particular, prediction learning methods from the field of reinforcement learning -- specifically, an approach termed Pavlovian signalling -- have been proposed as one approach for better modulating feedback in prostheses since they can adapt during continuous use. One challenge identified in these learning methods is that they can forget previously learned predictions when a user begins to successfully act upon delivered feedback. The present work directly addresses this challenge, contributing new evidence on the impact of algorithmic choices, such as on- or off-policy methods and representation choices, on the Pavlovian signalling from a machine to a user during their control of a robotic arm. Two conditions of algorithmic differences were studied using different scenarios of controlling a robotic arm: an automated motion system and human participant piloting. Contrary to expectations, off-policy learning did not provide the expected solution to the forgetting problem. We instead identified beneficial properties of a look-ahead state representation that made existing approaches able to learn (and not forget) predictions in support of Pavlovian signalling. This work therefore contributes new insight into the challenges of providing learned predictive feedback from a prosthetic device, and demonstrates avenues for more dynamic signalling in future human-machine interactions.",
      "authors": [
        "Parker, Adam S. R.",
        "Dawson, Michael R.",
        "Pilarski, Patrick M."
      ],
      "categories": null,
      "citations": null,
      "comments": "12 pages inc. supplementary, 7 figures, 3 algorithms, Published the\n  NeurIPS Workshop on Human in the Loop Learning, Nov 28 - Dec 8 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-16",
      "selected": null,
      "title": "Continually Learned Pavlovian Signalling Without Forgetting for Human-in-the-Loop Robotic Control",
      "urls": [
        "http://arxiv.org/pdf/2305.14365.pdf",
        "http://arxiv.org/abs/2305.14365v1",
        "http://arxiv.org/pdf/2305.14365v1"
      ]
    },
    {
      "abstract": "Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to 10% in multiple text similarity metrics over other learned, retrieval-augmented or prompting-based critique generators.",
      "authors": [
        "Aky\u00fcrek, Afra Feyza",
        "Aky\u00fcrek, Ekin",
        "Madaan, Aman",
        "Kalyan, Ashwin",
        "Clark, Peter",
        "Wijaya, Derry",
        "Tandon, Niket"
      ],
      "categories": null,
      "citations": 0,
      "comments": "ACL 2023",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 18,
      "pages": "7716-7733",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2023-05-15",
      "selected": null,
      "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85169031675&origin=inward",
        "http://arxiv.org/pdf/2305.08844.pdf",
        "http://arxiv.org/abs/2305.08844v2",
        "http://arxiv.org/pdf/2305.08844v2"
      ]
    },
    {
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values through instruction tuning. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. A response to this downside is to fall back to supervised fine-tuning (SFT) with additional carefully selected expert demonstrations. However, while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead. In this study, we propose another alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative adversarial training style to enable the LLMs to learn useful human expert demonstrations without being directly exposed to the training examples, thus enabling good generalization capabilities while preserving sample efficiency. Our preliminary findings indicate that RLGAF can help align LLMs outputs with competitive performance against RLHF and SFT, while not suffering from their respective inherent restrictions, suggesting promising avenues for further research on automating AI alignment.",
      "authors": [
        "Yu, Zhang Ze",
        "Jaw, Lau Jia",
        "Jiang, Wong Qin",
        "Hui, Zhang",
        "Low, Bryan Kian Hsiang"
      ],
      "categories": null,
      "citations": null,
      "comments": "18 pages, 7 figures, 11 tables",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-09",
      "selected": null,
      "title": "Fine-tuning Language Models with Generative Adversarial Feedback",
      "urls": [
        "http://arxiv.org/pdf/2305.06176.pdf",
        "http://arxiv.org/pdf/2305.06176v2",
        "http://arxiv.org/abs/2305.06176v2"
      ]
    },
    {
      "abstract": "The goal of DARPA's Symbiotic Design of Cyber Physical Systems (SDCPS) program is to develop tools for \"correct-by-synthesis\"design of cyber physical systems (CPS) and reduce the time from concept to deployment from years to months. Achieving this goal poses several hard challenges. Design spaces are high-dimensional cross-products of discrete and continuous spaces. It can take minutes to hours to evaluate the performance of a design. The human designer's intent is often not concretely articulated. Sometimes designs are not created from scratch but rather by completing or repairing existing ones. This paper outlines how the AIMED system addresses these challenges. AIMED consists of three core technologies. The first is \"deformable connector\"that eliminates an important type of discreteness from design spaces. Thus, not only is the design space vastly simplified, efficient optimization engines for purely continuous spaces can be used in the search for a design. The second core technology is Inverse Specification, based on inverse reinforcement learning that infers human intent by asking the human a small number of simple preference questions. The third core technology is Gaussian Mixture Models that allows completion and repair of designs and finds not just one but a diversity of solutions. AIMED is illustrated in the context of Unmanned Airborne Vehicles (UAVs) although it was also applied to the design of Unmanned Underwater Vehicles (UUVs). AIMED was used to automatically discover high-scoring, novel UAVs, unencumbered by biases of planarity and symmetry: a UAV with non-coplanar propellers and another with asymmetric wings. We expect our experience will apply to design of other CPS. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Sanjai Narain",
        "Dana Chee",
        "Pranav Iyer",
        "Emily Mak",
        "Ricardo Valdez",
        "Manli Zhu",
        "Niraj Jha",
        "Jaime Fisac",
        "Kai-Chieh Hsu",
        "Prerit Terway",
        "Kishore Pochiraju",
        "Brendan Englot",
        "Emil Pitz",
        "Sean Rooney",
        "Yewei Huang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3576914.3589205",
      "keywords": [
        "Connector",
        "Exploration",
        "Deformable",
        "Learning",
        "Mixture",
        "Reinforcement",
        "Gaussian",
        "AI",
        "Space",
        "Inverse",
        "Models",
        "Optimization",
        "Design",
        "Bayesian"
      ],
      "number_of_pages": 5,
      "pages": "136-140",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400700491",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Cyber-Physical Systems and Internet of Things Week 2023"
      },
      "publication_date": "2023-05-09",
      "selected": null,
      "title": "AIMED: AI-Mediated Exploration of Design: An Experience Report",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159773045&origin=inward",
        "https://dl.acm.org/doi/10.1145/3576914.3589205"
      ]
    },
    {
      "abstract": "Unit testing is essential in detecting bugs in functionally-discrete program units. Manually writing high-quality unit tests is time-consuming and laborious. Although traditional techniques can generate tests with reasonable coverage, they exhibit low readability and cannot be directly adopted by developers. Recent work has shown the large potential of large language models (LLMs) in unit test generation, which can generate more human-like and meaningful test code. ChatGPT, the latest LLM incorporating instruction tuning and reinforcement learning, has performed well in various domains. However, It remains unclear how effective ChatGPT is in unit test generation. In this work, we perform the first empirical study to evaluate ChatGPT's capability of unit test generation. Specifically, we conduct a quantitative analysis and a user study to systematically investigate the quality of its generated tests regarding the correctness, sufficiency, readability, and usability. The tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures. Still, the passing tests generated by ChatGPT resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we propose ChatTESTER, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTESTER incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTESTER by generating 34.3% more compilable tests and 18.7% more tests with correct assertions than the default ChatGPT.",
      "authors": [
        "Yuan, Zhiqiang",
        "Lou, Yiling",
        "Liu, Mingwei",
        "Ding, Shiji",
        "Wang, Kaixin",
        "Chen, Yixuan",
        "Peng, Xin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-07",
      "selected": null,
      "title": "No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation",
      "urls": [
        "http://arxiv.org/abs/2305.04207v2",
        "http://arxiv.org/pdf/2305.04207.pdf",
        "http://arxiv.org/pdf/2305.04207v2"
      ]
    },
    {
      "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
      "authors": [
        "Sun, Zhiqing",
        "Shen, Yikang",
        "Zhou, Qinhong",
        "Zhang, Hongxin",
        "Chen, Zhenfang",
        "Cox, David",
        "Yang, Yiming",
        "Gan, Chuang"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted at NeurIPS 2023 (Spotlight). Project page:\n  https://github.com/IBM/Dromedary",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-05-04",
      "selected": null,
      "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
      "urls": [
        "http://arxiv.org/pdf/2305.03047v2",
        "http://arxiv.org/abs/2305.03047v2",
        "http://arxiv.org/pdf/2305.03047.pdf"
      ]
    },
    {
      "abstract": "[No abstract available]",
      "authors": [
        "Huang, J.",
        "Yeung, A.M.",
        "Kerr, D.",
        "Klonoff, D.C."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/19322968231161095",
      "keywords": [
        "diabetes",
        "artificial intelligence",
        "search",
        "diabetes technology",
        "chatbot"
      ],
      "number_of_pages": 2,
      "pages": "853-854",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Diabetes Science and Technology"
      },
      "publication_date": "2023-05-01",
      "selected": null,
      "title": "Using ChatGPT to Predict the Future of Diabetes Technology",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148416395&origin=inward"
      ]
    },
    {
      "abstract": "While frontal midline theta (FM\u00ce\u00b8) has been associated with threat processing, with cognitive control in the context of anxiety, and with reinforcement learning, most reinforcement learning studies on FM\u00ce\u00b8 have used reward rather than threat-related stimuli as reinforcer. Accordingly, the role of FM\u00ce\u00b8 in threat-related reinforcement learning is largely unknown. Here, n\u00c2 =\u00c2 23 human participants underwent one reward-, and one punishment-, based reversal learning task, which differed only with regard to the kind of reinforcers that feedback was tied to (i.e., monetary gain vs. loud noise burst, respectively). In addition to single-trial EEG, we assessed single-trial feedback expectations based on both a reinforcement learning computational model and trial-by-trial subjective feedback expectation ratings. While participants' performance and feedback expectations were comparable between the reward and punishment tasks, FM\u00ce\u00b8 was more reliably amplified to negative vs. positive feedback in the reward vs. punishment task. Regressions with feedback valence, computationally derived, and self-reported expectations as predictors and FM\u00ce\u00b8 as criterion further revealed that trial-by-trial variations in FM\u00ce\u00b8 specifically relate to reward-related feedback-valence and not to threat-related feedback or to violated expectations/prediction errors. These findings suggest that FM\u00ce\u00b8 as measured in reinforcement learning tasks may be less sensitive to the processing of events with direct relevance for fear and anxiety. \u00c2\u00a9 2022 The Authors. Psychophysiology published by Wiley Periodicals LLC on behalf of Society for Psychophysiological Research.",
      "authors": [
        "Stolz, C.",
        "Pickering, A.D.",
        "Mueller, E.M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.14235",
      "keywords": [
        "EEG",
        "computational model",
        "theta",
        "punishment avoidance",
        "reinforcement learning",
        "prediction error"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2023-05-01",
      "selected": null,
      "title": "Dissociable feedback valence effects on frontal midline theta during reward gain versus threat avoidance learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144206899&origin=inward"
      ]
    },
    {
      "abstract": "The functional interplay between the corticolimbic GABAergic and opioidergic systems plays a crucial role in regulating the reward system and cognitive aspects of motivational behaviors leading to the development of addictive behaviors and disorders. This review provides a summary of the shared mechanisms of GABAergic and opioidergic transmission, which modulate the activity of dopaminergic neurons located in the ventral tegmental area (VTA), the central hub of the reward mechanisms. This review comprehensively covers the neuroanatomical and neurobiological aspects of corticolimbic inhibitory neurons that express opioid receptors, which act as modulators of corticolimbic GABAergic transmission. The presence of opioid and GABA receptors on the same neurons allows for the modulation of the activity of dopaminergic neurons in the ventral tegmental area, which plays a key role in the reward mechanisms of the brain. This colocalization of receptors and their immunochemical markers can provide a comprehensive understanding for clinicians and researchers, revealing the neuronal circuits that contribute to the reward system. Moreover, this review highlights the importance of GABAergic transmission-induced neuroplasticity under the modulation of opioid receptors. It discusses their interactive role in reinforcement learning, network oscillation, aversive behaviors, and local feedback or feedforward inhibitions in reward mechanisms. Understanding the shared mechanisms of these systems may lead to the development of new therapeutic approaches for addiction, reward-related disorders, and drug-induced cognitive impairment.",
      "authors": [
        "Hosseinzadeh Sahafi, Oveis",
        "Sardari, Maryam",
        "Alijanpour, Sakineh",
        "Rezayof, Ameneh"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/brainsci13050815",
      "keywords": [
        "signaling pathways",
        "brain regions",
        "opioids",
        "reward system",
        "GABAergic interneurons"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2076-3425",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.752,
        "snip": 0.938,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Brain Sciences"
      },
      "publication_date": "2023-05-01",
      "selected": null,
      "title": "Shared Mechanisms of GABAergic and Opioidergic Transmission Regulate Corticolimbic Reward Systems and Cognitive Aspects of Motivational Behaviors",
      "urls": [
        "https://www.mdpi.com/2076-3425/13/5/815/pdf?version=1684462583",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160242486&origin=inward"
      ]
    },
    {
      "abstract": "Controllability, or the influence one has over their surroundings, is crucial for decision-making and mental health. Traditionally, controllability is operationalized in sensorimotor terms as one's ability to exercise their actions to achieve an intended outcome (also termed \u00e2\u0080\u009cagency\u00e2\u0080\u009d). However, recent social neuroscience research suggests that humans also assess if and how they can exert influence over other people (i.e., their actions, outcomes, beliefs) to achieve desired outcomes (\"social controllability\u00e2\u0080\u009d). In this review, we will synthesize empirical findings and neurocomputational frameworks related to social controllability. We first introduce the concepts of contextual and perceived controllability and their respective relevance for decision-making. Then, we outline neurocomputational frameworks that can be used to model social controllability, with a focus on behavioral economic paradigms and reinforcement learning approaches. Finally, we discuss the implications of social controllability for computational psychiatry research, using delusion and obsession-compulsion as examples. Taken together, we propose that social controllability could be a key area of investigation in future social neuroscience and computational psychiatry research. \u00c2\u00a9 2023 Elsevier Ltd",
      "authors": [
        "Na, S.",
        "Rhoads, S.A.",
        "Yu, A.N.C.",
        "Fiore, V.G.",
        "Gu, X."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neubiorev.2023.105139",
      "keywords": [
        "Computational psychiatry",
        "Model-based learning",
        "Model-free learning",
        "Social controllability",
        "Cognitive map",
        "Reinforcement learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 13.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01497634",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.599,
        "snip": 2.49,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Neuroscience and Biobehavioral Reviews"
      },
      "publication_date": "2023-05-01",
      "selected": null,
      "title": "Towards a neurocomputational account of social controllability: From models to mental health",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151009762&origin=inward"
      ]
    },
    {
      "abstract": "In inverse reinforcement learning (RL), there are two agents. An expert target agent has a performance cost function and exhibits control and state behaviors to a learner. The learner agent does not know the expert's performance cost function but seeks to reconstruct it by observing the expert's behaviors and tries to imitate these behaviors optimally by its own response. In this article, we formulate an imitation problem where the optimal performance intent of a discrete-time (DT) expert target agent is unknown to a DT Learner agent. Using only the observed expert's behavior trajectory, the learner seeks to determine a cost function that yields the same optimal feedback gain as the expert's, and thus, imitates the optimal response of the expert. We develop an inverse RL approach with a new scheme to solve the behavior imitation problem. The approach consists of a cost function update based on an extension of RL policy iteration and inverse optimal control, and a control policy update based on optimal control. Then, under this scheme, we develop an inverse reinforcement Q-learning algorithm, which is an extension of RL Q-learning. This algorithm does not require any knowledge of agent dynamics. Proofs of stability, convergence, and optimality are given. A key property about the nonunique solution is also shown. Finally, simulation experiments are presented to show the effectiveness of the new approach. \u00c2\u00a9 2012 IEEE.",
      "authors": [
        "Xue, W.",
        "Lian, B.",
        "Fan, J.",
        "Kolaric, P.",
        "Chai, T.",
        "Lewis, F.L."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2021.3106635",
      "keywords": [
        "inverse reinforcement learning (RL)",
        "inverse optimal control (IOC)",
        "Data-based control",
        "Q-learning"
      ],
      "number_of_pages": 14,
      "pages": "2386-2399",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2023-05-01",
      "selected": null,
      "title": "Inverse Reinforcement Q-Learning Through Expert Imitation for Discrete-Time Systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115141793&origin=inward"
      ]
    },
    {
      "abstract": "Whereas the effects of the early stages of acute stress seem to improve learning and increase loss aversion in decision making, in later stages, the opposite has been found, an impairment in decision making probably due to higher reward-attraction, as the STARS approach suggests. This study aims to investigate the effects of the later stages of acute stress on decision making and its underlying processes using a computational model. We hypothesized that stress would affect underlying cognitive strategies during decision making. Ninety-five participants were randomly distributed into two groups, experimental (N = 46) and control (N = 49). A virtual version of The Trier Social Stress Test (TSST) was used as a laboratory stressor. After 20 min, decision making was assessed by using the Iowa Gambling Task (IGT). The Value-Plus-Preservation (VPP) RL computational model was used to extract decision-making components. As expected, the stressed participants showed deficits in IGT performance on reinforcement-learning and feedback sensitivity. However, there was no gains attraction. These results are discussed by considering that decision making in later stages of acute stress could be based on impairments in prefrontal cortex functioning. \u00c2\u00a9 2023 The Authors",
      "authors": [
        "Ben Hassen, N.",
        "Molins, F.",
        "Paz, M.",
        "Serrano, M.-\u00c3\u0081."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2023.108585",
      "keywords": [
        "Stress",
        "Iowa Gambling Task",
        "Reinforcement-learning",
        "Decision making",
        "Computational model"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2023-05-01",
      "selected": null,
      "title": "Later stages of acute stress impair reinforcement-learning and feedback sensitivity in decision making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159298277&origin=inward"
      ]
    },
    {
      "abstract": "Evaluating the performance of low-light image enhancement (LLE) is highly subjective, thus making integrating human preferences into image enhancement a necessity. Existing methods fail to consider this and present a series of potentially valid heuristic criteria for training enhancement models. In this paper, we propose a new paradigm, i.e., aesthetics-guided low-light image enhancement (ALL-E), which introduces aesthetic preferences to LLE and motivates training in a reinforcement learning framework with an aesthetic reward. Each pixel, functioning as an agent, refines itself by recursive actions, i.e., its corresponding adjustment curve is estimated sequentially. Extensive experiments show that integrating aesthetic assessment improves both subjective experience and objective evaluation. Our results on various benchmarks demonstrate the superiority of ALL-E over state-of-the-art methods.",
      "authors": [
        "Li, Ling",
        "Liang, Dong",
        "Gao, Yuanhang",
        "Huang, Sheng-Jun",
        "Chen, Songcan"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1062-1070",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781956792034",
        "issn": "10450823",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IJCAI International Joint Conference on Artificial Intelligence"
      },
      "publication_date": "2023-04-28",
      "selected": null,
      "title": "ALL-E: Aesthetics-guided Low-light Image Enhancement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170373299&origin=inward",
        "http://arxiv.org/pdf/2304.14610v2",
        "http://arxiv.org/abs/2304.14610v2",
        "http://arxiv.org/pdf/2304.14610.pdf"
      ]
    },
    {
      "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.",
      "authors": [
        "Moghaddam, Shima Rahimi",
        "Honey, Christopher J."
      ],
      "categories": null,
      "citations": null,
      "comments": "27 pages, 4 main figures, 2 supplementary figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-22",
      "selected": null,
      "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
      "urls": [
        "http://arxiv.org/abs/2304.11490v3",
        "http://arxiv.org/pdf/2304.11490v3",
        "http://arxiv.org/pdf/2304.11490.pdf"
      ]
    },
    {
      "abstract": "Nanowire networks (NWNs) mimic the brain\u00e2\u0080\u0099s neurosynaptic connectivity and emergent dynamics. Consequently, NWNs may also emulate the synaptic processes that enable higher-order cognitive functions such as learning and memory. A quintessential cognitive task used to measure human working memory is the n-back task. In this study, task variations inspired by the n-back task are implemented in a NWN device, and external feedback is applied to emulate brain-like supervised and reinforcement learning. NWNs are found to retain information in working memory to at least n = 7 steps back, remarkably similar to the originally proposed \u00e2\u0080\u009cseven plus or minus two\u00e2\u0080\u009d rule for human subjects. Simulations elucidate how synapse-like NWN junction plasticity depends on previous synaptic modifications, analogous to \u00e2\u0080\u009csynaptic metaplasticity\u00e2\u0080\u009d in the brain, and how memory is consolidated via strengthening and pruning of synaptic conductance pathways. Copyright \u00c2\u00a9 2023 The Authors, some rights reserved.",
      "authors": [
        "Loeffler, A.",
        "Diaz-Alvarez, A.",
        "Zhu, R.",
        "Ganesh, N.",
        "Shine, J.M.",
        "Nakayama, T.",
        "Kuncic, Z."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1126/sciadv.adg3289",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Science Advances"
      },
      "publication_date": "2023-04-21",
      "selected": null,
      "title": "Neuromorphic learning, working memory, and metaplasticity in nanowire networks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153551396&origin=inward"
      ]
    },
    {
      "abstract": "An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.",
      "authors": [
        "Wolf, Yotam",
        "Wies, Noam",
        "Avnery, Oshri",
        "Levine, Yoav",
        "Shashua, Amnon"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-19",
      "selected": null,
      "title": "Fundamental Limitations of Alignment in Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2304.11082v5",
        "http://arxiv.org/pdf/2304.11082.pdf",
        "http://arxiv.org/abs/2304.11082v5"
      ]
    },
    {
      "abstract": "Real-world decision-making problems are often partially observable, and many can be formulated as a Partially Observable Markov Decision Process (POMDP). When we apply reinforcement learning (RL) algorithms to the POMDP, reasonable estimation of the hidden states can help solve the problems. Furthermore, explainable decision-making is preferable, considering their application to real-world tasks such as autonomous driving cars. We proposed an RL algorithm that estimates the hidden states by end-to-end training, and visualize the estimation as a state-transition graph. Experimental results demonstrated that the proposed algorithm can solve simple POMDP problems and that the visualization makes the agent's behavior interpretable to humans.",
      "authors": [
        "Nishimori, Soichiro",
        "Koyamada, Sotetsu",
        "Ishii, Shin"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pagee, 6 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-19",
      "selected": null,
      "title": "End-to-End Policy Gradient Method for POMDPs and Explainable Agents",
      "urls": [
        "http://arxiv.org/pdf/2304.09769v1",
        "http://arxiv.org/pdf/2304.09769.pdf",
        "http://arxiv.org/abs/2304.09769v1"
      ]
    },
    {
      "abstract": "In this paper, we introduce an AI-mediated framework that can provide intelligent feedback to augment human cognition. Specifically, we leverage deep reinforcement learning (DRL) to provide adaptive time pressure feedback to improve user performance in a math arithmetic task. Time pressure feedback could either improve or deteriorate user performance by regulating user attention and anxiety. Adaptive time pressure feedback controlled by a DRL policy according to users' real-time performance could potentially solve this trade-off problem. However, the DRL training and hyperparameter tuning may require large amounts of data and iterative user studies. Therefore, we propose a dual-DRL framework that trains a regulation DRL agent to regulate user performance by interacting with another simulation DRL agent that mimics user cognition behaviors from an existing dataset. Our user study demonstrates the feasibility and effectiveness of the dual-DRL framework in augmenting user performance, in comparison to the baseline group. \u00c2\u00a9 2023 Owner/Author.",
      "authors": [
        "Songlin Xu",
        "Xinyu Zhang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3544548.3580905",
      "keywords": [
        "human-AI integration",
        "time pressure",
        "machine learning",
        "Human augmentation",
        "deep reinforcement learning",
        "cognition regulation"
      ],
      "number_of_pages": 16,
      "pages": "1-16",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450394215",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems"
      },
      "publication_date": "2023-04-19",
      "selected": null,
      "title": "Augmenting Human Cognition with an AI-Mediated Intelligent Visual Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160024073&origin=inward",
        "https://dl.acm.org/doi/10.1145/3544548.3580905"
      ]
    },
    {
      "abstract": "An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires too much feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an active-learning-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorithm guarantees to provide a nearly optimal policy for the task with high probability. We show that, even with the presence of random noise in the feedback, the algorithm only takes $\\widetilde{O}(H{{\\dim_{R}^2}})$ queries on the reward function to provide an $\\epsilon$-optimal policy for any $\\epsilon > 0$. Here $H$ is the horizon of the RL environment, and $\\dim_{R}$ specifies the complexity of the function class representing the reward function. In contrast, standard RL algorithms require to query the reward function for at least $\\Omega(\\operatorname{poly}(d, 1/\\epsilon))$ state-action pairs where $d$ depends on the complexity of the environmental transition.",
      "authors": [
        "Kong, Dingwen",
        "Yang, Lin F."
      ],
      "categories": null,
      "citations": null,
      "comments": "36th Conference on Neural Information Processing Systems (NeurIPS\n  2022)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-18",
      "selected": null,
      "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning",
      "urls": [
        "http://arxiv.org/pdf/2304.08944.pdf",
        "http://arxiv.org/pdf/2304.08944v1",
        "http://arxiv.org/abs/2304.08944v1"
      ]
    },
    {
      "abstract": "Modern very-large-scale integrated (VLSI) circuit placement with huge state space is a critical task for achieving layouts with high performance. Recently, reinforcement learning (RL) algorithms have made a promising breakthrough to dramatically save design time than human effort. However, the previous RL-based works either require a large dataset of chip placements for pre-training or produce illegal final placement solutions. In this paper, DeepTH, a three-head policy gradient placer, is proposed to learn from scratch without the need of pre-training, and generate superior chip floorplans. Graph neural network is initially adopted to extract the features from nodes and nets of chips for estimating the policy and value. To efficiently improve the quality of floorplans, a reconstruction head is employed in the RL network to recover the visual representation of the current placement, by enriching the extracted features of placement embedding. Besides, the reconstruction error is used as a bonus during training to encourage exploration while alleviating the sparse reward problem. Furthermore, the expert knowledge of floorplanning preference is embedded into the decision process to narrow down the potential action space. Experiment results on the ISPD 2005 benchmark have shown that our method achieves 19.02% HPWL improvement than the analytic placer DREAMPlace and 19.89% improvement at least than the state-of-the-art RL algorithms.",
      "authors": [
        "Dengwei Zhao",
        "Shuai Yuan",
        "Yanan Sun",
        "Shikui Tu",
        "Lei Xu"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.23919/DATE56975.2023.10137100",
      "keywords": [
        "Visual reconstruction",
        "Chip placement",
        "Three head",
        "Intrin-sic reward",
        "Reinforcement learning"
      ],
      "number_of_pages": 2,
      "pages": "1-2",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-9624-9",
        "issn": "1530-1591",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
      },
      "publication_date": "2023-04-17",
      "selected": null,
      "title": "DeepTH: Chip Placement with Deep Reinforcement Learning Using a Three-Head Policy Network",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162704850&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10137100"
      ]
    },
    {
      "abstract": "In the field of motion simulation, the level of immersion strongly depends on the motion cueing algorithm (MCA), as it transfers the reference motion of the simulated vehicle to a motion of the motion simulation platform (MSP). The challenge for the MCA is to reproduce the motion perception of a real vehicle driver as accurately as possible without exceeding the limits of the workspace of the MSP in order to provide a realistic virtual driving experience. In case of a large discrepancy between the perceived motion signals and the optical cues, motion sickness may occur with the typical symptoms of nausea, dizziness, headache and fatigue. Existing approaches either produce non-optimal results, e.g., due to filtering, linearization, or simplifications, or the required computational time exceeds the real-time requirements of a closed-loop application. In this work a new solution is presented, where not a human designer specifies the principles of the MCA but an artificial intelligence (AI) learns the optimal motion by trial and error in an interaction with the MSP. To achieve this, deep reinforcement learning (RL) is applied, where an agent interacts with an environment formulated as a Markov decision process~(MDP). This allows the agent to directly control a simulated MSP to obtain feedback on its performance in terms of platform workspace usage and the motion acting on the simulator user. The RL algorithm used is proximal policy optimization (PPO), where the value function and the policy corresponding to the control strategy are learned and both are mapped in artificial neural networks (ANN). This approach is implemented in Python and the functionality is demonstrated by the practical example of pre-recorded lateral maneuvers. The subsequent validation on a standardized double lane change shows that the RL algorithm is able to learn the control strategy and improve the quality of...",
      "authors": [
        "Scheidel, Hendrik",
        "Asadi, Houshyar",
        "Bellmann, Tobias",
        "Seefried, Andreas",
        "Mohamed, Shady",
        "Nahavandi, Saeid"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-15",
      "selected": null,
      "title": "A novel approach of a deep reinforcement learning based motion cueing algorithm for vehicle driving simulation",
      "urls": [
        "http://arxiv.org/pdf/2304.07600.pdf",
        "http://arxiv.org/pdf/2304.07600v1",
        "http://arxiv.org/abs/2304.07600v1"
      ]
    },
    {
      "abstract": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.",
      "authors": [
        "K\u00f6pf, Andreas",
        "Kilcher, Yannic",
        "von R\u00fctte, Dimitri",
        "Anagnostidis, Sotiris",
        "Tam, Zhi-Rui",
        "Stevens, Keith",
        "Barhoum, Abdullah",
        "Duc, Nguyen Minh",
        "Stanley, Oliver",
        "Nagyfi, Rich\u00e1rd",
        "ES, Shahul",
        "Suri, Sameer",
        "Glushkov, David",
        "Dantuluri, Arnav",
        "Maguire, Andrew",
        "Schuhmann, Christoph",
        "Nguyen, Huu",
        "Mattick, Alexander"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published in NeurIPS 2023 Datasets and Benchmarks",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-14",
      "selected": null,
      "title": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment",
      "urls": [
        "http://arxiv.org/abs/2304.07327v2",
        "http://arxiv.org/pdf/2304.07327v2",
        "http://arxiv.org/pdf/2304.07327.pdf"
      ]
    },
    {
      "abstract": "In recent years, a proliferation of cyber-security threats and diversity has been on the rise culminating in an increase in their reporting and analysis. To counter that, many non-profit organizations have emerged in this domain, such as MITRE and OSWAP, which have been actively tracking vulnerabilities, and publishing defense recommendations in standardized formats. As producing data in such formats manually is very time-consuming, there have been some proposals to automate the process. Unfortunately, a major obstacle to adopting supervised machine learning for this problem has been the lack of publicly available specialized datasets. Here, we aim to bridge this gap. In particular, we focus on mapping CVE records into MITRE CWE Weaknesses, and we release to the research community a manually annotated dataset of 4,012 records for this task. With a human-in-the-loop framework in mind, we approach the problem as a ranking task and aim to incorporate reinforced learning to make use of the human feedback in future work. Our experimental results using fine-tuned deep learning models, namely Sentence-BERT and rankT5, show sizable performance gains over BM25, BERT, and RoBERTa, which demonstrates the need for an architecture capable of good semantic understanding for this task.",
      "authors": [
        "Haddad, Ashraf",
        "Aaraj, Najwa",
        "Nakov, Preslav",
        "Mare, Septimiu Fabian"
      ],
      "categories": null,
      "citations": null,
      "comments": "cybersecurity, MITRE, CVE, CWE",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-13",
      "selected": null,
      "title": "Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses",
      "urls": [
        "http://arxiv.org/pdf/2304.11130.pdf",
        "http://arxiv.org/abs/2304.11130v1",
        "http://arxiv.org/pdf/2304.11130v1"
      ]
    },
    {
      "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.",
      "authors": [
        "Dong, Hanze",
        "Xiong, Wei",
        "Goyal, Deepanshu",
        "Zhang, Yihan",
        "Chow, Winnie",
        "Pan, Rui",
        "Diao, Shizhe",
        "Zhang, Jipeng",
        "Shum, Kashun",
        "Zhang, Tong"
      ],
      "categories": null,
      "citations": null,
      "comments": "29 pages, 12 figures, Published in Transactions on Machine Learning\n  Research (TMLR)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-13",
      "selected": null,
      "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
      "urls": [
        "http://arxiv.org/abs/2304.06767v4",
        "http://arxiv.org/pdf/2304.06767v4",
        "http://arxiv.org/pdf/2304.06767.pdf"
      ]
    },
    {
      "abstract": "Deep reinforcement learning (DRL) has demonstrated its potential in solving complex manufacturing decision-making problems, especially in a context where the system learns over time with actual operation in the absence of training data. One interesting and challenging application for such methods is the assembly sequence planning (ASP) problem. In this paper, we propose an approach to the implementation of DRL methods in ASP. The proposed approach introduces in the RL environment parametric actions to improve training time and sample efficiency and uses two different reward signals: (1) user\u2019s preferences and (2) total assembly time duration. The user\u2019s preferences signal addresses the difficulties and non-ergonomic properties of the assembly faced by the human and the total assembly time signal enforces the optimization of the assembly. Three of the most powerful deep RL methods were studied, Advantage Actor-Critic (A2C), Deep Q-Learning (DQN), and Rainbow, in two different scenarios: a stochastic and a deterministic one. Finally, the performance of the DRL algorithms was compared to tabular Q-Learnings performance. After 10,000 episodes, the system achieved near optimal behaviour for the algorithms tabular Q-Learning, A2C, and Rainbow. Though, for more complex scenarios, the algorithm tabular Q-Learning is expected to underperform in comparison to the other 2 algorithms. The results support the potential for the application of deep reinforcement learning in assembly sequence planning problems with human interaction.",
      "authors": [
        "Neves, Miguel",
        "Neto, Pedro"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": "10.1007/s00170-022-09877-8",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1433-3015",
        "publisher": "Springer London",
        "sjr": 0.774,
        "snip": 1.417,
        "subject_areas": [
          "Mechanical Engineering",
          "Software",
          "Learning",
          "Computer Science Applications",
          "Industrial and Manufacturing Engineering",
          "Control and Systems Engineering"
        ],
        "title": "The International Journal of Advanced Manufacturing Technology"
      },
      "publication_date": "2023-04-13",
      "selected": null,
      "title": "Deep reinforcement learning applied to an assembly sequence planning problem with user preferences",
      "urls": [
        "http://dx.doi.org/10.1007/s00170-022-09877-8",
        "https://link.springer.com/content/pdf/10.1007/s00170-022-09877-8.pdf",
        "http://arxiv.org/abs/2304.06567v1",
        "http://arxiv.org/pdf/2304.06567v1"
      ]
    },
    {
      "abstract": "One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination performance in human evaluations in Hanabi.",
      "authors": [
        "Hu, Hengyuan",
        "Sadigh, Dorsa"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-13",
      "selected": null,
      "title": "Language Instructed Reinforcement Learning for Human-AI Coordination",
      "urls": [
        "http://arxiv.org/pdf/2304.07297.pdf",
        "http://arxiv.org/abs/2304.07297v2",
        "http://arxiv.org/pdf/2304.07297v2"
      ]
    },
    {
      "abstract": "Developing robotic technologies for use in human society requires ensuring the safety of robots' navigation behaviors while adhering to pedestrians' expectations and social norms. However, maintaining real-time communication between robots and pedestrians to avoid collisions can be challenging. To address these challenges, we propose a novel socially-aware navigation benchmark called NaviSTAR, which utilizes a hybrid Spatio-Temporal grAph tRansformer (STAR) to understand interactions in human-rich environments fusing potential crowd multi-modal information. We leverage off-policy reinforcement learning algorithm with preference learning to train a policy and a reward function network with supervisor guidance. Additionally, we design a social score function to evaluate the overall performance of social navigation. To compare, we train and test our algorithm and other state-of-the-art methods in both simulator and real-world scenarios independently. Our results show that NaviSTAR outperforms previous methods with outstanding performance\\footnote{The source code and experiment videos of this work are available at: https://sites.google.com/view/san-navistar",
      "authors": [
        "Wang, Weizheng",
        "Wang, Ruiqi",
        "Mao, Le",
        "Min, Byung-Cheol"
      ],
      "categories": null,
      "citations": null,
      "comments": "To appear in IROS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-12",
      "selected": null,
      "title": "NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning",
      "urls": [
        "http://arxiv.org/pdf/2304.05979.pdf",
        "http://arxiv.org/abs/2304.05979v2",
        "http://arxiv.org/pdf/2304.05979v2"
      ]
    },
    {
      "abstract": "Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.",
      "authors": [
        "Boiko, Daniil A.",
        "MacKnight, Robert",
        "Gomes, Gabe"
      ],
      "categories": null,
      "citations": null,
      "comments": "Version 1, April 11, 2023. 48 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-11",
      "selected": null,
      "title": "Emergent autonomous scientific research capabilities of large language models",
      "urls": [
        "http://arxiv.org/abs/2304.05332v1",
        "http://arxiv.org/pdf/2304.05332v1",
        "http://arxiv.org/pdf/2304.05332.pdf"
      ]
    },
    {
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling. Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-n learner. Codes available at https://github.com/GanjinZero/RRHF.",
      "authors": [
        "Yuan, Zheng",
        "Yuan, Hongyi",
        "Tan, Chuanqi",
        "Wang, Wei",
        "Huang, Songfang",
        "Huang, Fei"
      ],
      "categories": null,
      "citations": null,
      "comments": "ArXiv version For NeurIPS 2023 accepted paper: RRHF: Rank Responses\n  to Align Language Models with Human Feedback",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-11",
      "selected": null,
      "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears",
      "urls": [
        "http://arxiv.org/abs/2304.05302v3",
        "http://arxiv.org/pdf/2304.05302v3",
        "http://arxiv.org/pdf/2304.05302.pdf"
      ]
    },
    {
      "abstract": "Generating human-like behavior on robots is a great challenge especially in dexterous manipulation tasks with robotic hands. Scripting policies from scratch is intractable due to the high-dimensional control space, and training policies with reinforcement learning (RL) and manual reward engineering can also be hard and lead to unnatural motions. Leveraging the recent progress on RL from Human Feedback, we propose a framework that learns a universal human prior using direct human preference feedback over videos, for efficiently tuning the RL policies on 20 dual-hand robot manipulation tasks in simulation, without a single human demonstration. A task-agnostic reward model is trained through iteratively generating diverse polices and collecting human preference over the trajectories; it is then applied for regularizing the behavior of polices in the fine-tuning stage. Our method empirically demonstrates more human-like behaviors on robot hands in diverse tasks including even unseen tasks, indicating its generalization capability.",
      "authors": [
        "Ding, Zihan",
        "Chen, Yuanpei",
        "Ren, Allen Z.",
        "Gu, Shixiang Shane",
        "Wang, Qianxu",
        "Dong, Hao",
        "Jin, Chi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-10",
      "selected": null,
      "title": "Learning a Universal Human Prior for Dexterous Manipulation from Human Preference",
      "urls": [
        "http://arxiv.org/pdf/2304.04602v2",
        "http://arxiv.org/abs/2304.04602v2",
        "http://arxiv.org/pdf/2304.04602.pdf"
      ]
    },
    {
      "abstract": "Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.",
      "authors": [
        "Ge, Yingqiang",
        "Hua, Wenyue",
        "Mei, Kai",
        "Ji, Jianchao",
        "Tan, Juntao",
        "Xu, Shuyuan",
        "Li, Zelong",
        "Zhang, Yongfeng"
      ],
      "categories": null,
      "citations": null,
      "comments": "In NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-10",
      "selected": null,
      "title": "OpenAGI: When LLM Meets Domain Experts",
      "urls": [
        "http://arxiv.org/pdf/2304.04370v6",
        "http://arxiv.org/abs/2304.04370v6",
        "http://arxiv.org/pdf/2304.04370.pdf"
      ]
    },
    {
      "abstract": "Importance: Despite high antidepressant placebo response rates, the mechanisms underlying the persistence of antidepressant placebo effects are still poorly understood. Objective: To investigate the neurobehavioral mechanisms underlying the evolution of antidepressant placebo effects using a reinforcement learning (RL) framework. Design, Setting, and Participants: In this acute within-patient cross-sectional study of antidepressant placebos, patients aged 18 to 55 years not receiving medication for major depressive disorder (MDD) were recruited at the University of Pittsburgh between February 21, 2017, to March 1, 2021. Interventions: The antidepressant placebo functional magnetic resonance imaging task manipulates placebo-associated expectancies using visually cued fast-acting antidepressant infusions and controls their reinforcement with sham visual neurofeedback while assessing expected and experienced mood improvement. Main Outcomes and Measures: The trial-by-trial evolution of expectancies and mood was examined using multilevel modeling and RL, relating model-predicted signals to spatiotemporal dynamics of blood oxygenation level-dependent (BOLD) response. Results: A bayesian RL model comparison in 60 individuals (mean [SE] age, 24.5 [0.8] years; 51 females [85%]) with MDD revealed that antidepressant placebo trial-wise expectancies were updated by composite learning signals multiplexing sensory evidence (neurofeedback) and trial-wise mood (bayesian omnibus risk <0.001; exceedance probability = 97%). Placebo expectancy, neurofeedback manipulations, and composite learning signals modulated the visual cortex and dorsal attention network (threshold-free cluster enhancement [TFCE] = 1 - P >.95). As participants anticipated antidepressant infusions, learned placebo expectancies modulated the salience network (SN, TFCE = 1 - P >.95), positively scaling with depression severity. Conclusions and Relevance: Results of this cross-sectional study suggest that on a timescale of minutes, antidepressant placebo effects were maintained by positive feedback loops between expectancies and mood improvement. During learning, representations of placebos and their perceived effects were enhanced in primary and secondary sensory cortices. Latent learned placebo expectancies were encoded in the SN. \u00c2\u00a9 2023 American Medical Association. All rights reserved.",
      "authors": [
        "Peci\u00c3\u00b1a, M.",
        "Chen, J.",
        "Karp, J.F.",
        "Dombrovski, A.Y."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1001/jamapsychiatry.2023.0010",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "389-398",
      "publication": {
        "category": "Journal",
        "cite_score": 31.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2168622X",
        "publisher": "American Medical Association",
        "sjr": 6.578,
        "snip": 6.04,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "JAMA Psychiatry"
      },
      "publication_date": "2023-04-05",
      "selected": null,
      "title": "Dynamic Feedback between Antidepressant Placebo Expectancies and Mood",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152174010&origin=inward"
      ]
    },
    {
      "abstract": "This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and\nGPT-4) research, state-of-the-art large language models (LLM) from the GPT\nseries, and their prospective applications across diverse domains. Indeed, key\ninnovations such as large-scale pre-training that captures knowledge across the\nentire world wide web, instruction fine-tuning and Reinforcement Learning from\nHuman Feedback (RLHF) have played significant roles in enhancing LLMs'\nadaptability and performance. We performed an in-depth analysis of 194 relevant\npapers on arXiv, encompassing trend analysis, word cloud representation, and\ndistribution analysis across various application domains. The findings reveal a\nsignificant and increasing interest in ChatGPT-related research, predominantly\ncentered on direct natural language processing applications, while also\ndemonstrating considerable potential in areas ranging from education and\nhistory to mathematics, medicine, and physics. This study endeavors to furnish\ninsights into ChatGPT's capabilities, potential implications, ethical concerns,\nand offer direction for future advancements in this field.",
      "authors": [
        "Yiheng Liu",
        "Tianle Han",
        "Siyuan Ma",
        "Jiayue Zhang",
        "Yuanyuan Yang",
        "Jiaming Tian",
        "Hao He",
        "Antong Li",
        "Mengshen He",
        "Zhengliang Liu",
        "Zihao Wu",
        "Lin Zhao",
        "Dajiang Zhu",
        "Xiang Li",
        "Ning Qiang",
        "Dingang Shen",
        "Tianming Liu",
        "Bao Ge"
      ],
      "categories": null,
      "citations": null,
      "comments": "21 pages, 4 figures, accepted by Meta-Radiology",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1016/j.metrad.2023.100017",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Computation and Language"
        ],
        "title": "Meta-Radiology (2023)100017"
      },
      "publication_date": "2023-04-04",
      "selected": null,
      "title": "Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models",
      "urls": [
        "http://arxiv.org/pdf/2304.01852v4",
        "http://dx.doi.org/10.1016/j.metrad.2023.100017",
        "http://arxiv.org/abs/2304.01852v4"
      ]
    },
    {
      "abstract": "The remarkable performance of large language models (LLMs) on complex linguistic tasks has sparked a lively debate on the nature of their capabilities. Unlike humans, these models learn language exclusively from textual data, without direct interaction with the real world. Nevertheless, they can generate seemingly meaningful text about a wide range of topics. This impressive accomplishment has rekindled interest in the classical 'Symbol Grounding Problem,' which questioned whether the internal representations and outputs of classical symbolic AI systems could possess intrinsic meaning. Unlike these systems, modern LLMs are artificial neural networks that compute over vectors rather than symbols. However, an analogous problem arises for such systems, which we dub the Vector Grounding Problem. This paper has two primary objectives. First, we differentiate various ways in which internal representations can be grounded in biological or artificial systems, identifying five distinct notions discussed in the literature: referential, sensorimotor, relational, communicative, and epistemic grounding. Unfortunately, these notions of grounding are often conflated. We clarify the differences between them, and argue that referential grounding is the one that lies at the heart of the Vector Grounding Problem. Second, drawing on theories of representational content in philosophy and cognitive science, we propose that certain LLMs, particularly those fine-tuned with Reinforcement Learning from Human Feedback (RLHF), possess the necessary features to overcome the Vector Grounding Problem, as they stand in the requisite causal-historical relations to the world that underpin intrinsic meaning. We also argue that, perhaps unexpectedly, multimodality and embodiment are neither necessary nor sufficient conditions for referential grounding in artificial systems.",
      "authors": [
        "Mollo, Dimitri Coelho",
        "Milli\u00e8re, Rapha\u00ebl"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-04",
      "selected": null,
      "title": "The Vector Grounding Problem",
      "urls": [
        "http://arxiv.org/abs/2304.01481v1",
        "http://arxiv.org/pdf/2304.01481.pdf",
        "http://arxiv.org/pdf/2304.01481v1"
      ]
    },
    {
      "abstract": "Western diets rich in fat and sugar promote excess calorie intake and weight gain; however, the underlying mechanisms are unclear. Despite a well-documented association between obesity and altered brain dopamine function, it remains elusive whether these alterations are (1) pre-existing, increasing the individual susceptibility to weight gain, (2) secondary to obesity, or (3) directly attributable to repeated exposure to western diet. To close this gap, we performed a randomized, controlled study (NCT05574660) with normal-weight participants exposed to a high-fat/high-sugar snack or a low-fat/low-sugar snack for 8 weeks in addition to their regular diet. The high-fat/high-sugar intervention decreased the preference for low-fat food while increasing brain response to food and associative learning independent of food cues or reward. These alterations were independent of changes in body weight and metabolic parameters, indicating a direct effect of high-fat, high-sugar foods on neurobehavioral adaptations that may increase the risk for overeating and weight gain. \u00c2\u00a9 2023 The Author(s)",
      "authors": [
        "Edwin Thanarajah, S.",
        "DiFeliceantonio, A.G.",
        "Albus, K.",
        "Kuzmanovic, B.",
        "Rigoux, L.",
        "Iglesias, S.",
        "Han\u00c3\u009fen, R.",
        "Schlamann, M.",
        "Cornely, O.A.",
        "Br\u00c3\u00bcning, J.C.",
        "Tittgemeyer, M.",
        "Small, D.M."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cmet.2023.02.015",
      "keywords": [
        "reward",
        "taste",
        "reinforcement learning",
        "neural plasticity",
        "obesity",
        "dopamine",
        "value",
        "preference",
        "high-fat diet",
        "fMRI",
        "prediction error"
      ],
      "number_of_pages": null,
      "pages": "571-584.e6",
      "publication": {
        "category": "Journal",
        "cite_score": 49.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15504131",
        "publisher": "Cell Press",
        "sjr": 10.037,
        "snip": 4.686,
        "subject_areas": [
          "Physiology",
          "Molecular Biology",
          "Cell Biology"
        ],
        "title": "Cell Metabolism"
      },
      "publication_date": "2023-04-04",
      "selected": null,
      "title": "Habitual daily intake of a sweet and fatty snack modulates reward processing in humans",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151350191&origin=inward"
      ]
    },
    {
      "abstract": "Spatial crowdsourcing (SC) has proven as a promising paradigm to employ human workers to collect data from diverse Point-of-Interests (PoIs) in a given area. Different from using human participants, we propose a novel air-ground SC scenario to fully take advantage of benefits brought by unmanned vehicles (UVs), including unmanned aerial vehicles (UAVs) with controllable high mobility and unmanned ground vehicles (UGVs) with abundant sensing resources. The objective is to maximize the amount of collected data, geographical fairness among all PoIs, and minimize the data loss and energy consumption, integrated as one single metric called \"efficiency\". We explicitly explore both individuality and cooperation natures of UAVs and UGVs by proposing a multi-agent deep reinforcement learning (MADRL) framework called \"h/i-MADRL\". Compatible with all multi-agent actor-critic methods, h/i-MADRL adds two novel plug-in modules: (a) h-CoPO, which models the cooperation preference among heterogeneous UAVs and UGVs; and (b) i-EOI, which extracts the UV's individuality and encourages a better spatial division of work by adding intrinsic reward. Extensive experimental results on two real-world datasets on Purdue and NCSU campuses confirm that h/i-MADRL achieves a better exploration of both individuality and cooperation simultaneously, resulting in a better performance in terms of efficiency compared with five baselines. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Yuxiao Ye",
        "Chi Harold Liu",
        "Zipeng Dai",
        "Jianxin Zhao",
        "Ye Yuan",
        "Guoren Wang",
        "Jian Tang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICDE55515.2023.00023",
      "keywords": [
        "Intrinsic reward",
        "Multi-agent deep reinforcement learning",
        "Air-ground spatial crowdsourcing"
      ],
      "number_of_pages": 13,
      "pages": "205-217",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-2228-6",
        "issn": "1063-6382",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE 39th International Conference on Data Engineering (ICDE)"
      },
      "publication_date": "2023-04-03",
      "selected": null,
      "title": "Exploring both Individuality and Cooperation for Air-Ground Spatial Crowdsourcing by Multi-Agent Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167652811&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10184585"
      ]
    },
    {
      "abstract": "Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a \"User,\" a \"Therapist,\" and a \"Critic.\" We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human values. By incorporating psychotherapy and reinforcement learning techniques, the framework enables AI chatbots to learn and adapt to human preferences and values in a safe and ethical way, contributing to the development of a more human-centric and responsible AI.",
      "authors": [
        "Lin, Baihan",
        "Bouneffouf, Djallel",
        "Cecchi, Guillermo",
        "Varshney, Kush R."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-02",
      "selected": null,
      "title": "Towards Healthy AI: Large Language Models Need Therapists Too",
      "urls": [
        "http://arxiv.org/pdf/2304.00416v1",
        "http://arxiv.org/abs/2304.00416v1",
        "http://arxiv.org/pdf/2304.00416.pdf"
      ]
    },
    {
      "abstract": "Although pair trading is the simplest hedging strategy for an investor to eliminate market risk, it is still a great challenge for reinforcement learning (RL) methods to perform pair trading as human expertise. It requires RL methods to make thousands of correct actions that nevertheless have no obvious relations to the overall trading profit, and to reason over infinite states of the time-varying market most of which have never appeared in history. However, existing RL methods ignore the temporal connections between asset price movements and the risk of the performed trading. These lead to frequent tradings with high transaction costs and potential losses, which barely reach the human expertise level of trading. Therefore, we introduce CREDIT, a risk-aware agent capable of learning to exploit long-term trading opportunities in pair trading similar to a human expert. CREDIT is the first to apply bidirectional GRU along with the temporal attention mechanism to fully consider the temporal correlations embedded in the states, which allows CREDIT to capture long-term patterns of the price movements of two assets to earn higher profit. We also design the risk-aware reward inspired by the economic theory, that models both the profit and risk of the tradings during the trading period. It helps our agent to master pair trading with a robust trading preference that avoids risky trading with possible high returns and losses. Experiments show that it outperforms existing reinforcement learning methods in pair trading and achieves a significant profit over five years of U.S. stock data.",
      "authors": [
        "Han, Weiguang",
        "Huang, Jimin",
        "Xie, Qianqian",
        "Zhang, Boyi",
        "Lai, Yanzhao",
        "Peng, Min"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 5 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-04-01",
      "selected": null,
      "title": "Mastering Pair Trading with Risk-Aware Recurrent Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2304.00364.pdf",
        "http://arxiv.org/abs/2304.00364v1",
        "http://arxiv.org/pdf/2304.00364v1"
      ]
    },
    {
      "abstract": "Humans often demonstrate diverse behaviors due to their personal preferences, for instance, related to their individual execution style or personal margin for safety. In this paper, we consider the problem of integrating both path and velocity preferences into trajectory planning for robotic manipulators. We first learn reward functions that represent the user path and velocity preferences from kinesthetic demonstration. We then optimize the trajectory in two steps, first the path and then the velocity, to produce trajectories that adhere to both task requirements and user preferences. We design a set of parameterized features that capture the fundamental preferences in a pick-and-place type of object transportation task, both in the shape and timing of the motion. We demonstrate that our method is capable of generalizing such preferences to new scenarios. We implement our algorithm on a Franka Emika 7-DoF robot arm and validate the functionality and flexibility of our approach in a user study. The results show that non-expert users are able to teach the robot their preferences with just a few iterations of feedback.",
      "authors": [
        "Avaei, Armin",
        "van der Spaa, Linda",
        "Peternel, Luka",
        "Kober, Jens"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/robotics12020061",
      "keywords": [
        "learning from demonstration",
        "coactive learning",
        "incremental inverse reinforcement learning",
        "physical human\u00e2\u0080\u0093robot interaction",
        "human preferences"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2218-6581",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.812,
        "snip": 1.612,
        "subject_areas": [
          "Artificial Intelligence",
          "Mechanical Engineering",
          "Control and Optimization"
        ],
        "title": "Robotics"
      },
      "publication_date": "2023-04-01",
      "selected": null,
      "title": "An Incremental Inverse Reinforcement Learning Approach for Motion Planning with Separated Path and Velocity Preferences",
      "urls": [
        "https://www.mdpi.com/2218-6581/12/2/61/pdf?version=1682059317",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153718431&origin=inward"
      ]
    },
    {
      "abstract": "Precisely charting the maturation of core neurocognitive functions such as reinforcement learning (RL) and flexible adaptation to changing action-outcome contingencies is key for developmental neuroscience and adjacent fields like developmental psychiatry. However, research in this area is both sparse and conflicted, especially regarding potentially asymmetric development of learning for different motives (obtain wins vs avoid losses) and learning from valenced feedback (positive vs negative). In the current study, we investigated the development of RL from adolescence to adulthood, using a probabilistic reversal learning task modified to experimentally separate motivational context and feedback valence, in a sample of 95 healthy participants between 12 and 45. We show that adolescence is characterized by enhanced novelty seeking and response shifting especially after negative feedback, which leads to poorer returns when reward contingencies are stable. Computationally, this is accounted for by reduced impact of positive feedback on behavior. We also show, using fMRI, that activity of the medial frontopolar cortex reflecting choice probability is attenuated in adolescence. We argue that this can be interpreted as reflecting diminished confidence in upcoming choices. Interestingly, we find no age-related differences between learning in win and loss contexts. \u00c2\u00a9 2023 The Authors",
      "authors": [
        "Waltmann, M.",
        "Herzog, N.",
        "Reiter, A.M.F.",
        "Villringer, A.",
        "Horstmann, A.",
        "Deserno, L."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.dcn.2023.101226",
      "keywords": [
        "Reversal learning",
        "Computational modelling",
        "Adolescence",
        "Reinforcement learning",
        "fMRI"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 8.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18789293",
        "publisher": "Elsevier B.V.",
        "sjr": 1.792,
        "snip": 1.42,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Developmental Cognitive Neuroscience"
      },
      "publication_date": "2023-04-01",
      "selected": null,
      "title": "Diminished reinforcement sensitivity in adolescence is associated with enhanced response switching and reduced coding of choice probability in the medial frontal pole",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149743880&origin=inward"
      ]
    },
    {
      "abstract": "[No abstract available]",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ejwf.2023.03.001",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "39-40",
      "publication": {
        "category": "Journal",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "22124438",
        "publisher": "Elsevier Inc.",
        "sjr": 0.477,
        "snip": 0.757,
        "subject_areas": [
          "Orthodontics"
        ],
        "title": "Journal of the World Federation of Orthodontists"
      },
      "publication_date": "2023-04-01",
      "selected": null,
      "title": "Conversing on orthodontics with an AI Chatbot",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151306721&origin=inward"
      ]
    },
    {
      "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
      "authors": [
        "Madaan, Aman",
        "Tandon, Niket",
        "Gupta, Prakhar",
        "Hallinan, Skyler",
        "Gao, Luyu",
        "Wiegreffe, Sarah",
        "Alon, Uri",
        "Dziri, Nouha",
        "Prabhumoye, Shrimai",
        "Yang, Yiming",
        "Gupta, Shashank",
        "Majumder, Bodhisattwa Prasad",
        "Hermann, Katherine",
        "Welleck, Sean",
        "Yazdanbakhsh, Amir",
        "Clark, Peter"
      ],
      "categories": null,
      "citations": null,
      "comments": "Code, data, and demo at https://selfrefine.info/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-30",
      "selected": null,
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "urls": [
        "http://arxiv.org/pdf/2303.17651v2",
        "http://arxiv.org/pdf/2303.17651.pdf",
        "http://arxiv.org/abs/2303.17651v2"
      ]
    },
    {
      "abstract": "In this paper, a novel switching pushing skill algorithm is proposed to improve the efficiency of planar non-prehensile manipulation, which draws inspiration from human pushing actions and comprises two sub-problems, i.e., discrete decision-making of pushing point and continuous feedback control of pushing action. In order to solve the sub-problems above, a combination of Model Predictive Control (MPC) and Deep Reinforcement Learning (DRL) method is employed. Firstly, the selection of pushing point is modeled as a Markov decision process,and an off-policy DRL method is used by reshaping the reward function to train the decision-making model for selecting pushing point from a pre-constructed set based on the current state. Secondly, a motion constraint region (MCR) is constructed for the specific pushing point based on the distance from the target, followed by utilizing the MPC controller to regulate the motion of the object within the MCR towards the target pose. The trigger condition for switching the pushing point occurs when the object reaches the boundary of the MCR under the pushing action. Subsequently, the pushing point and the controller are updated iteratively until the target pose is reached. We conducted pushing experiments on four distinct object shapes in both simulated and physical environments to evaluate our method. The results indicate that our method achieves a significantly higher training efficiency, with a training time that is only about 20% of the baseline method while maintaining around the same success rate. Moreover, our method outperforms the baseline method in terms of both training and execution efficiency of pushing operations, allowing for rapid learning of robot pushing skills.",
      "authors": [
        "Zhang, Bo",
        "Huang, Cong",
        "Zhang, Haixu",
        "Bai, Xiaoshan"
      ],
      "categories": null,
      "citations": null,
      "comments": "21 pages, 10 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-30",
      "selected": null,
      "title": "Switching Pushing Skill Combined MPC and Deep Reinforcement Learning for Planar Non-prehensile Manipulation",
      "urls": [
        "http://arxiv.org/abs/2303.17379v1",
        "http://arxiv.org/pdf/2303.17379.pdf",
        "http://arxiv.org/pdf/2303.17379v1"
      ]
    },
    {
      "abstract": "Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.",
      "authors": [
        "Scheurer, J\u00e9r\u00e9my",
        "Campos, Jon Ander",
        "Korbak, Tomasz",
        "Chan, Jun Shern",
        "Chen, Angelica",
        "Cho, Kyunghyun",
        "Perez, Ethan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-28",
      "selected": null,
      "title": "Training Language Models with Language Feedback at Scale",
      "urls": [
        "http://arxiv.org/pdf/2303.16755v2",
        "http://arxiv.org/pdf/2303.16755.pdf",
        "http://arxiv.org/abs/2303.16755v2"
      ]
    },
    {
      "abstract": "Indoor Environment Quality (IEQ) is one of the most important goals for smart spaces. Thermal comfort is typically considered the most emphasized factor in IEQ that depends on personalized thermal preference. In this paper, we explore technical challenges to deploying a robot-driven personalized thermal control system that uses a mobile robot for learning user-state-specific preference efficiently. We conduct a few experiments that give a clue to overcome such challenges (i.e. low image recognition) when the system is deployed in a real world. We present future directions to improve robot-driven preference learning from the exploration. \u00c2\u00a9 2023 ACM.",
      "authors": [
        "Kim, G.",
        "Kim, H.",
        "Kim, Y.",
        "Lee, D."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3555776.3577760",
      "keywords": [
        "reinforcement learning",
        "clothing recognition",
        "smart homes",
        "preference learning",
        "human activity recognition",
        "mobile robot"
      ],
      "number_of_pages": 8,
      "pages": "724-731",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450395175",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the ACM Symposium on Applied Computing"
      },
      "publication_date": "2023-03-27",
      "selected": null,
      "title": "Towards Deployment of Mobile Robot driven Preference Learning for User-State-Specific Thermal Control in A Real-World Smart Space",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85162921963&origin=inward"
      ]
    },
    {
      "abstract": "Conversational agent (CA) for psychotherapy and behavioral intervention has great potential to provide solutions that can benefit human health. However, most CA for behavior intervention and healthcare are based on pre-scripted conversations and rules instead of generative models, because the generative model is not stable enough to be used in the highly sensitive domain like behavioral intervention. Based on the fact that generative models and reinforcement learning techniques have been widely used in various domains, a CA integrating generative models for behavioral interventions is proposed in this work and the approach is expected to continuously improve the generative model and the agent itself based on collected human feedback from both client and therapist during the interaction. The approach involves techniques, such as few-shot generation by language models, prompt engineering, and reinforcement learning from human feedback (RLHF) as the Human-in-the-Loop interaction. We expect that this approach can enable the generative models to be used in highly sensitive fields such as mental healthcare and behavioral intervention. \u00c2\u00a9 2023 Owner/Author.",
      "authors": [
        "Xin Sun",
        "Jos A. Bosch",
        "Jan De Wit",
        "Emiel Krahmer"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3581754.3584142",
      "keywords": [
        "Human-in-the-Loop",
        "Conversational agents",
        "Behavioral intervention;",
        "Natural language generation"
      ],
      "number_of_pages": 3,
      "pages": "99-101",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400701078",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Companion Proceedings of the 28th International Conference on Intelligent User Interfaces"
      },
      "publication_date": "2023-03-27",
      "selected": null,
      "title": "Human-in-the-Loop Interaction for continuously Improving Generative Model in Conversational Agent for Behavioral Intervention",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151920611&origin=inward",
        "https://dl.acm.org/doi/10.1145/3581754.3584142"
      ]
    },
    {
      "abstract": "ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.",
      "authors": [
        "Aiyappa, Rachith",
        "An, Jisun",
        "Kwak, Haewoon",
        "Ahn, Yong-Yeol"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "47-54",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2023-03-22",
      "selected": null,
      "title": "Can we trust the evaluation on ChatGPT?",
      "urls": [
        "http://arxiv.org/pdf/2303.12767v1",
        "http://arxiv.org/pdf/2303.12767.pdf",
        "http://arxiv.org/abs/2303.12767v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173285077&origin=inward"
      ]
    },
    {
      "abstract": "Reliable localization is crucial for autonomous robots to navigate efficiently and safely. Some navigation methods can plan paths with high localizability (which describes the capability of acquiring reliable localization). By following these paths, the robot can access the sensor streams that facilitate more accurate location estimation results by the localization algorithms. However, most of these methods require prior knowledge and struggle to adapt to unseen scenarios or dynamic changes. To overcome these limitations, we propose a novel approach for localizability-enhanced navigation via deep reinforcement learning in dynamic human environments. Our proposed planner automatically extracts geometric features from 2D laser data that are helpful for localization. The planner learns to assign different importance to the geometric features and encourages the robot to navigate through areas that are helpful for laser localization. To facilitate the learning of the planner, we suggest two techniques: (1) an augmented state representation that considers the dynamic changes and the confidence of the localization results, which provides more information and allows the robot to make better decisions, (2) a reward metric that is capable to offer both sparse and dense feedback on behaviors that affect localization accuracy. Our method exhibits significant improvements in lost rate and arrival rate when tested in previously unseen environments.",
      "authors": [
        "Chen, Yuan",
        "Qiu, Quecheng",
        "Liu, Xiangyu",
        "Chen, Guangda",
        "Yao, Shunyi",
        "Peng, Jie",
        "Ji, Jianmin",
        "Zhang, Yanyong"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-22",
      "selected": null,
      "title": "Deep Reinforcement Learning for Localizability-Enhanced Navigation in Dynamic Human Environments",
      "urls": [
        "http://arxiv.org/pdf/2303.12354.pdf",
        "http://arxiv.org/pdf/2303.12354v1",
        "http://arxiv.org/abs/2303.12354v1"
      ]
    },
    {
      "abstract": "<p>Cognition and brain structure undergo significant maturation from adolescence into adulthood. Model-based (MB) control is known to increase across development, which is mediated by cognitive abilities. Here, we asked two questions unaddressed in previous developmental studies. First, what are the brain structural correlates of age-related increases in MB control? Second, how are age-related increases in MB control from adolescence to adulthood influenced by motivational context? A human developmental sample (<i>n</i> = 103; age, 12\u201350, male/female, 55:48) completed structural MRI and an established task to capture MB control. The task was modified with respect to outcome valence by including (1) reward and punishment blocks to manipulate the motivational context and (2) an additional choice test to assess learning from positive versus negative feedback. After replicating that an age-dependent increase in MB control is mediated by cognitive abilities, we demonstrate first-time evidence that gray matter density (GMD) in the parietal cortex mediates the increase of MB control with age. Although motivational context did not relate to age-related changes in MB control, learning from positive feedback improved with age. Meanwhile, negative feedback learning showed no age effects. We present a first report that an age-related increase in positive feedback learning was mediated by reduced GMD in the parietal, medial, and dorsolateral prefrontal cortex. Our findings indicate that brain maturation, putatively reflected in lower GMD, in distinct and partially overlapping brain regions could lead to a more efficient brain organization and might thus be a key developmental step toward age-related increases in planning and value-based choice.</p><p><b>SIGNIFICANCE STATEMENT</b> Changes in model-based decision-making are paralleled by extensive maturation in cognition and brain structure across development. Still, to date the neuroanatomical underpinnings of these changes remain unclear. Here, we demonstrate for the first time that parietal GMD mediates age-dependent increases in model-based control. Age-related increases in positive feedback learning were mediated by reduced GMD in the parietal, medial, and dorsolateral prefrontal cortex. A manipulation of motivational context did not have an impact on age-related changes in model-based control. These findings highlight that brain maturation in distinct and overlapping cortical regions constitutes a key developmental step toward improved value-based choices.</p>",
      "authors": [
        "Vanessa Scholz",
        "Maria Waltmann",
        "Nadine Herzog",
        "Andrea Reiter",
        "Annette Horstmann",
        "Lorenz Deserno"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1418-22.2023",
      "keywords": [
        "adolescence",
        "reward learning",
        "development",
        "reinforcement learning",
        "punishment learning",
        "model-based control"
      ],
      "number_of_pages": 12,
      "pages": "2178-2189",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2023-03-22",
      "selected": null,
      "title": "Cortical Grey Matter Mediates Increases in Model-Based Control and Learning from Positive Feedback from Adolescence to Adulthood",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151043893&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) has shown promise for decision-making tasks in real-world applications. One practical framework involves training parameterized policy models from an offline dataset and subsequently deploying them in an online environment. However, this approach can be risky since the offline training may not be perfect, leading to poor performance of the RL models that may take dangerous actions. To address this issue, we propose an alternative framework that involves a human supervising the RL models and providing additional feedback in the online deployment phase. We formalize this online deployment problem and develop two approaches. The first approach uses model selection and the upper confidence bound algorithm to adaptively select a model to deploy from a candidate set of trained offline RL models. The second approach involves fine-tuning the model in the online deployment phase when a supervision signal arrives. We demonstrate the effectiveness of these approaches for robot locomotion control and traffic light control tasks through empirical validation.",
      "authors": [
        "Li, Ziniu",
        "Xu, Ke",
        "Liu, Liu",
        "Li, Lanqing",
        "Ye, Deheng",
        "Zhao, Peilin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-13",
      "selected": null,
      "title": "Deploying Offline Reinforcement Learning with Human Feedback",
      "urls": [
        "http://arxiv.org/abs/2303.07046v1",
        "http://arxiv.org/pdf/2303.07046v1",
        "http://arxiv.org/pdf/2303.07046.pdf"
      ]
    },
    {
      "abstract": "Objective:\n    Major depressive disorder (MDD) is associated with impaired reward processing and reward learning. The literature is inconclusive regarding whether these impairments persist after remission. The current study examined reward processing during a probabilistic learning task in individuals in remission from MDD (n = 19) and never depressed healthy controls (n = 31) matched for age and sex. The outcome measures were pupil dilation (an indirect index of noradrenergic activity and arousal) and computational modeling parameters. Method:\n    Participants completed two versions (facial/nonfacial feedback) of probabilistic reward learning task with changing contingencies. Pupil dilation was measured with a corneal reflection eye tracker. The hypotheses and analysis plan were preregistered. Result:\n    Healthy controls had larger pupil dilation following losses than gains (p &lt;.001), whereas no significant difference between outcomes was found in individuals with a history of MDD, resulting in an interaction between group and outcome (\u03b2 = 0.81, SE = 0.34, t = 2.37, p = .018). The rMDD group also achieved lower mean score at the last trial (t[46.77] = 2.12, p = .040) as well as a smaller proportion of correct choices (t[46.70] = 2.09, p = .041) compared with healthy controls. Conclusion:\n    Impaired reward processing may persist after remission from MDD and could constitute a latent risk factor for relapse. Measuring pupil dilation in a reward learning task is a promising method for identifying reward processing abnormalities linked to MDD. The task is simple and noninvasive, which makes it feasible for clinical research.",
      "authors": [
        "Mona Guath",
        "Charlotte Willfors",
        "Hanna Bj\u00f6rlin Avdic",
        "Ann Nordgren",
        "Johan Lundin Kleberg"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1017/S1355617722000224",
      "keywords": [
        "biomarker",
        "reinforcement learning",
        "computational psychiatry",
        "major depressive disorder (MDD) in remission",
        "pupil dilation",
        "probabilistic reward learning"
      ],
      "number_of_pages": 10,
      "pages": "306-315",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13556177",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of the International Neuropsychological Society"
      },
      "publication_date": "2023-03-12",
      "selected": null,
      "title": "Pupillary response in reward processing in adults with major depressive disorder in remission",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148113839&origin=inward"
      ]
    },
    {
      "abstract": "Large language models (LLMs) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing. This intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values. Different people may legitimately disagree on their preferences for language and conversational norms, as well as on values or ideologies which guide their communication. Personalising LLMs through micro-level preference learning processes may result in models that are better aligned with each user. However, there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. In this paper, we ask how, and in what ways, LLMs should be personalised. First, we review literature on current paradigms for aligning LLMs with human feedback, and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in who we are really aligning to. Second, we present a taxonomy of benefits and risks associated with personalised LLMs, for individuals and society at large. Finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable LLM-behaviours within (supra-)national and organisational bounds.",
      "authors": [
        "Kirk, Hannah Rose",
        "Vidgen, Bertie",
        "R\u00f6ttger, Paul",
        "Hale, Scott A."
      ],
      "categories": null,
      "citations": null,
      "comments": "19 pages, 1 table",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-09",
      "selected": null,
      "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
      "urls": [
        "http://arxiv.org/pdf/2303.05453.pdf",
        "http://arxiv.org/pdf/2303.05453v1",
        "http://arxiv.org/abs/2303.05453v1"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) presents numerous benefits compared to rule-based\napproaches in various applications. Privacy concerns have grown with the\nwidespread use of RL trained with privacy-sensitive data in IoT devices,\nespecially for human-in-the-loop systems. On the one hand, RL methods enhance\nthe user experience by trying to adapt to the highly dynamic nature of humans.\nOn the other hand, trained policies can leak the user's private information.\nRecent attention has been drawn to designing privacy-aware RL algorithms while\nmaintaining an acceptable system utility. A central challenge in designing\nprivacy-aware RL, especially for human-in-the-loop systems, is that humans have\nintrinsic variability and their preferences and behavior evolve. The effect of\none privacy leak mitigation can be different for the same human or across\ndifferent humans over time. Hence, we can not design one fixed model for\nprivacy-aware RL that fits all. To that end, we propose adaPARL, an adaptive\napproach for privacy-aware RL, especially for human-in-the-loop IoT systems.\nadaPARL provides a personalized privacy-utility trade-off depending on human\nbehavior and preference. We validate the proposed adaPARL on two IoT\napplications, namely (i) Human-in-the-Loop Smart Home and (ii)\nHuman-in-the-Loop Virtual Reality (VR) Smart Classroom. Results obtained on\nthese two applications validate the generality of adaPARL and its ability to\nprovide a personalized privacy-utility trade-off. On average, for the first\napplication, adaPARL improves the utility by $57\\%$ over the baseline and by\n$43\\%$ over randomization. adaPARL also reduces the privacy leak by $23\\%$ on\naverage. For the second application, adaPARL decreases the privacy leak to\n$44\\%$ before the utility drops by $15\\%$.",
      "authors": [
        "Mojtaba Taherisadr",
        "Stelios Andrew Stavroulakis",
        "Salma Elmalaki"
      ],
      "categories": null,
      "citations": 1,
      "comments": "This paper is accepted at CPS-IoT week (IoTDI'23)",
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1145/3576842.3582325",
      "keywords": [
        "Human-in-the-Loop",
        "Privacy",
        "Reinforcement Learning",
        "IoT"
      ],
      "number_of_pages": 13,
      "pages": "262-274",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798400700378",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation"
      },
      "publication_date": "2023-03-07",
      "selected": null,
      "title": "adaPARL: Adaptive Privacy-Aware Reinforcement Learning for Sequential-Decision Making Human-in-the-Loop Systems",
      "urls": [
        "http://arxiv.org/pdf/2303.04257v1",
        "http://dx.doi.org/10.1145/3576842.3582325",
        "https://dl.acm.org/doi/10.1145/3576842.3582325",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150605259&origin=inward",
        "http://arxiv.org/abs/2303.04257v1"
      ]
    },
    {
      "abstract": "In this paper, we focus on a novel optimization problem in which the objective function is a black-box and can only be evaluated through a ranking oracle. This problem is common in real-world applications, particularly in cases where the function is assessed by human judges. Reinforcement Learning with Human Feedback (RLHF) is a prominent example of such an application, which is adopted by the recent works \\cite{ouyang2022training,liu2023languages,chatgpt,bai2022training} to improve the quality of Large Language Models (LLMs) with human guidance. We propose ZO-RankSGD, a first-of-its-kind zeroth-order optimization algorithm, to solve this optimization problem with a theoretical guarantee. Specifically, our algorithm employs a new rank-based random estimator for the descent direction and is proven to converge to a stationary point. ZO-RankSGD can also be directly applied to the policy search problem in reinforcement learning when only a ranking oracle of the episode reward is available. This makes ZO-RankSGD a promising alternative to existing RLHF methods, as it optimizes in an online fashion and thus can work without any pre-collected data. Furthermore, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers an effective approach for aligning human and machine intentions in a wide range of domains. Our code is released here \\url{https://github.com/TZW1998/Taming-Stable-Diffusion-with-Human-Ranking-Feedback}.",
      "authors": [
        "Tang, Zhiwei",
        "Rybin, Dmitry",
        "Chang, Tsung-Hui"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-07",
      "selected": null,
      "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
      "urls": [
        "http://arxiv.org/abs/2303.03751v1",
        "http://arxiv.org/pdf/2303.03751v1",
        "http://arxiv.org/pdf/2303.03751.pdf"
      ]
    },
    {
      "abstract": "Autonomously learning diverse behaviors without an extrinsic reward signal has been a problem of interest in reinforcement learning. However, the nature of learning in such mechanisms is unconstrained, often resulting in the accumulation of several unusable, unsafe or misaligned skills. In order to avoid such issues and ensure the discovery of safe and human-aligned skills, it is necessary to incorporate humans into the unsupervised training process, which remains a largely unexplored research area. In this work, we propose Controlled Diversity with Preference (CDP), a novel, collaborative human-guided mechanism for an agent to learn a set of skills that is diverse as well as desirable. The key principle is to restrict the discovery of skills to those regions that are deemed to be desirable as per a preference model trained using human preference labels on trajectory pairs. We evaluate our approach on 2D navigation and Mujoco environments and demonstrate the ability to discover diverse, yet desirable skills.",
      "authors": [
        "Hussonnois, Maxence",
        "Karimpanal, Thommen George",
        "Rana, Santu"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to AAMAS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Artificial Intelligence",
          "Learning"
        ],
        "title": "AAMAS 2023"
      },
      "publication_date": "2023-03-07",
      "selected": null,
      "title": "Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills",
      "urls": [
        "http://arxiv.org/pdf/2303.04592v1",
        "http://arxiv.org/pdf/2303.04592.pdf",
        "http://arxiv.org/abs/2303.04592v1"
      ]
    },
    {
      "abstract": "In the field of robot reinforcement learning (RL), the reality gap has always been a problem that restricts the robustness and generalization of algorithms. We propose Simulation Twin (SimTwin) : a deep RL framework that can help directly transfer the model from simulation to reality without any real-world training. SimTwin consists of a RL module and an adaptive correct module. We train the policy using the soft actor-critic algorithm only in a simulator with demonstration and domain randomization. In the adaptive correct module, we design and train a neural network to simulate the human error correction process using force feedback. Subsequently, we combine the above two modules through digital twin to control real-world robots, correct simulator parameters by comparing the difference between simulator and reality automatically, and then generalize the correct action through the trained policy network without additional training. We demonstrate the proposed method in an open cabinet task; the experiments show that our framework can reduce the reality gap without any real-world training.",
      "authors": [
        "Yuanpei Chen",
        "Chao Zeng",
        "Zhiping Wang",
        "Peng Lu",
        "Chenguang Yang"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1017/S0263574722001230",
      "keywords": [
        "sim-to-real transfer",
        "reinforcement learning",
        "digital twin"
      ],
      "number_of_pages": 10,
      "pages": "1015-1024",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02635747",
        "publisher": "Cambridge University Press",
        "sjr": 0.704,
        "snip": 1.331,
        "subject_areas": [
          "Mechanical Engineering",
          "Artificial Intelligence",
          "Software",
          "Modeling and Simulation",
          "Computer Science Applications",
          "Mathematics (all)",
          "Control and Systems Engineering",
          "Control and Optimization",
          "Computational Mechanics",
          "Computer Vision and Pattern Recognition",
          "Rehabilitation"
        ],
        "title": "Robotica"
      },
      "publication_date": "2023-03-07",
      "selected": null,
      "title": "Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148014080&origin=inward"
      ]
    },
    {
      "abstract": "Is it possible for machines to think like humans? And if it is, how should we go about teaching them to do so? As early as 1950, Alan Turing stated that we ought to teach machines in the way of teaching a child. Reinforcement learning with human feedback (RLHF) has emerged as a strong candidate toward allowing agents to learn from human feedback in a naturalistic manner. RLHF is distinct from traditional reinforcement learning as it provides feedback from a human teacher in addition to a reward signal. It has been catapulted into public view by multiple high-profile AI applications, including OpenAI's ChatGPT, DeepMind's Sparrow, and Anthropic's Claude. These highly capable chatbots are already overturning our understanding of how AI interacts with humanity. The wide applicability and burgeoning success of RLHF strongly motivate the need to evaluate its social impacts. In light of recent developments, this paper considers an important question: can RLHF be developed and used without negatively affecting human societies? Our objectives are threefold: to provide a systematic study of the social effects of RLHF; to identify key social and ethical issues of RLHF; and to discuss social impacts for stakeholders. Although text-based applications of RLHF have received much attention, it is crucial to consider when evaluating its social implications the diverse range of areas to which it may be deployed. We describe seven primary ways in which RLHF-based technologies will affect society by positively transforming human experiences with AI. This paper ultimately proposes that RLHF has potential to net positively impact areas of misinformation, AI value-alignment, bias, AI access, cross-cultural dialogue, industry, and workforce. As RLHF raises concerns that echo those of existing AI technologies, it will be important for all to be aware and intentional in the adoption of RLHF.",
      "authors": [
        "Liu, Gabrielle Kaili-May"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-06",
      "selected": null,
      "title": "Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback",
      "urls": [
        "http://arxiv.org/abs/2303.02891v1",
        "http://arxiv.org/pdf/2303.02891.pdf",
        "http://arxiv.org/pdf/2303.02891v1"
      ]
    },
    {
      "abstract": "Exploring the unknown environment is a very crucial task where human life is at risks like search and rescue operations, abandoned nuclear plants, covert operations and more. Autonomous robots could serve this task efficiently. The existing methods use uncertainty models for localization and map building to explore the unknown areas requiring high onboard computation and time. We propose to use Deep Reinforcement Learning (DRL) for the autonomous exploration of unknown environments. In DRL, the agent interacts with the environment and learns based on experiences (feedback/reward). We propose extrinsic and curiosity-driven reward functions to explore the environment. The curiosity-based reward function motivates the agent to explore unseen areas by predicting future states, while the extrinsic reward function avoids collisions. We train the differential drive robot in one environment and evaluate its performance in another unknown environment. We observe curiosity-driven reward function outperformed the extrinsic reward by exploring more areas in the unknown environment. The test results show the generalization capability to explore unknown environments with the proposed methods.",
      "authors": [
        "Asad Ali",
        "Sarah Gul",
        "Tallat Mahmood",
        "Anayat Ullah"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRAI57502.2023.10089589",
      "keywords": [
        "Navigation",
        "Double Deep Q-Network",
        "Mobile Robots",
        "Exploration",
        "Reinforcement Learning"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-6473-4",
        "issn": "2831-3291",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 International Conference on Robotics and Automation in Industry, ICRAI 2023"
      },
      "publication_date": "2023-03-03",
      "selected": null,
      "title": "Exploration of Unknown Environment using Deep Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10089589",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153573870&origin=inward"
      ]
    },
    {
      "abstract": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
      "authors": [
        "Kim, Changyeon",
        "Park, Jongjin",
        "Shin, Jinwoo",
        "Lee, Honglak",
        "Abbeel, Pieter",
        "Lee, Kimin"
      ],
      "categories": null,
      "citations": null,
      "comments": "Project website:\n  https://sites.google.com/view/preference-transformer. Accepted to ICLR 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-03-02",
      "selected": null,
      "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
      "urls": [
        "http://arxiv.org/pdf/2303.00957.pdf",
        "http://arxiv.org/pdf/2303.00957v1",
        "http://arxiv.org/abs/2303.00957v1"
      ]
    },
    {
      "abstract": "Using a reinforcement-learning algorithm, we model an agent-based simulation of a public goods game with endogenous punishment institutions. We propose an outcome-based model of social preferences that determines the agent\u2019s utility, contribution, and voting behavior during the learning procedure. Comparing our simulation to experimental evidence, we find that the model can replicate human behavior and we can explain the underlying motives of this behavior. We argue that our approach can be generalized to more complex simulations of human behavior.",
      "authors": [
        "Christoph B\u00fchren",
        "Jan Haarde",
        "Christian Hirschmann",
        "Janis Kesten-K\u00fchne"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0282112",
      "keywords": [
        "Economic agents",
        "Machine learning",
        "Public goods game",
        "Agent-based modeling",
        "Game theory",
        "Learning",
        "Human learning",
        "Experimental economics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2023-03-01",
      "selected": null,
      "title": "Social preferences in the public goods game\u2013An Agent-Based simulation with EconSim",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0282112&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150235517&origin=inward"
      ]
    },
    {
      "abstract": "[Objective] This paper analyzes the basic technical principles of ChatGPT, and discusses its influence on the development of large language model and the development of multi-modal pretrained model. [Methods] By analyzing the development process and technical principles of ChatGPT, this paper discusses the influence of model building methods such as instruct fine-tuning, data acquisition and annotation, and reinforcement learning based on human feedback on the large language model. At the same time, this paper analyzes several key scientific problems encountered in the construction of multi-modal model, and discusses the future development of multi-modal pretrained model by referring to ChatGPT\u00e2\u0080\u0099s technical scheme. [Conclusions] The success of ChatGPT provides a good reference technology path for the development of pretrained fundamental model to downstream tasks. In the future construction of multi-modal large model and the realization of downstream tasks, we can make full use of high-quality instruction fine-tuning and other technologies to significantly improve the performance of downstream tasks. \u00c2\u00a9 2022 Chinese Medical Journals Publishing House Co.Ltd. All rights reserved.",
      "authors": [
        "Chaoyang, Z.",
        "Guibo, Z.",
        "Jinqiao, W."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.11925/infotech.2096-3467.2023.0216",
      "keywords": [
        "Pretrained Foundation Model",
        "Large Language Model",
        "Multi-modal Pretrained Model ChatGPT"
      ],
      "number_of_pages": 10,
      "pages": "26-35",
      "publication": {
        "category": "Journal",
        "cite_score": 0.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "20963467",
        "publisher": "Chinese Academy of Sciences",
        "sjr": 0.181,
        "snip": 0.369,
        "subject_areas": [
          "Computer Science Applications",
          "Information Systems",
          "Library and Information Sciences",
          "Artificial Intelligence"
        ],
        "title": "Data Analysis and Knowledge Discovery"
      },
      "publication_date": "2023-03-01",
      "selected": null,
      "title": "The Inspiration Brought by ChatGPT to LLM and the New Development Ideas of Multi-modal Large Model",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168082350&origin=inward"
      ]
    },
    {
      "abstract": "Previous neurofeedback research has shown training-related frontal theta increases and performance improvements on some executive tasks in real feedback versus sham control groups. However, typical sham control groups receive false or non-contingent feedback, making it difficult to know whether observed differences between groups are associated with accurate contingent feedback or other cognitive mechanisms (motivation, control strategies, attentional engagement, fatigue, etc.). To address this question, we investigated differences between two frontal theta training groups, each receiving accurate contingent feedback, but with different top-down goals: (1) increase and (2) alternate increase/decrease. We hypothesized that the increase group would exhibit greater increases in frontal theta compared to the alternate group, which would exhibit lower frontal theta during down- versus up-modulation blocks over sessions. We also hypothesized that the alternate group would exhibit greater performance improvements on a Go-NoGo shooting task requiring alterations in behavioral activation and inhibition, as the alternate group would be trained with greater task specificity, suggesting that receiving accurate contingent feedback may be the more salient learning mechanism underlying frontal theta neurofeedback training gains. Thirty young healthy volunteers were randomly assigned to increase or alternate groups. Training consisted of an orientation session, five neurofeedback training sessions (six blocks of six 30-s trials of FCz theta modulation (4\u20137 Hz) separated by 10-s rest intervals), and six Go-NoGo testing sessions (four blocks of 90 trials in both Low and High time-stress conditions). Multilevel modeling revealed greater frontal theta increases in the alternate group over training sessions. Further, Go-NoGo task performance increased at a greater rate in the increase group (accuracy and reaction time, but not commission errors). Overall, these results reject our hypotheses and suggest that changes in frontal theta and performance outcomes were not explained by reinforcement learning afforded by accurate contingent feedback. We discuss our findings in terms of alternative conceptual and methodological considerations, as well as limitations of this research.",
      "authors": [
        "Scott E. Kerick",
        "Justin Asbee",
        "Derek P. Spangler",
        "Justin B. Brooks",
        "Javier O. Garcia",
        "Thomas D. Parsons",
        "Nilanjan Bannerjee",
        "Ryan Robucci"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0283418",
      "keywords": [
        "Electroencephalography",
        "Learning",
        "Attention",
        "Behavior",
        "Reaction time",
        "Sensory perception",
        "Neural networks",
        "Working memory"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2023-03-01",
      "selected": null,
      "title": "Neural and behavioral adaptations to frontal theta neurofeedback training: A proof of concept study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150681266&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0283418&type=printable"
      ]
    },
    {
      "abstract": "Shared autonomy is an operational concept in which a user and an autonomous agent collaboratively control a robotic system. It provides a number of advantages over the extremes of full-teleoperation and full-autonomy in many settings. Traditional approaches to shared autonomy rely on knowledge of the environment dynamics, a discrete space of user goals that is known a priori, or knowledge of the user's policy -- assumptions that are unrealistic in many domains. Recent works relax some of these assumptions by formulating shared autonomy with model-free deep reinforcement learning (RL). In particular, they no longer need knowledge of the goal space (e.g., that the goals are discrete or constrained) or environment dynamics. However, they need knowledge of a task-specific reward function to train the policy. Unfortunately, such reward specification can be a difficult and brittle process. On top of that, the formulations inherently rely on human-in-the-loop training, and that necessitates them to prepare a policy that mimics users' behavior. In this paper, we present a new approach to shared autonomy that employs a modulation of the forward and reverse diffusion process of diffusion models. Our approach does not assume known environment dynamics or the space of user goals, and in contrast to previous work, it does not require any reward feedback, nor does it require access to the user's policy during training. Instead, our framework learns a distribution over a space of desired behaviors. It then employs a diffusion model to translate the user's actions to a sample from this distribution. Crucially, we show that it is possible to carry out this process in a manner that preserves the user's control authority. We evaluate our framework on a series of challenging continuous control tasks, and analyze its ability to effectively correct user actions while maintaining their autonomy.",
      "authors": [
        "Yoneda, Takuma",
        "Sun, Luzhe",
        "Yang, and Ge",
        "Stadie, Bradly",
        "Walter, Matthew"
      ],
      "categories": null,
      "citations": null,
      "comments": "https://diffusion-for-shared-autonomy.github.io/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-23",
      "selected": null,
      "title": "To the Noise and Back: Diffusion for Shared Autonomy",
      "urls": [
        "http://arxiv.org/pdf/2302.12244.pdf",
        "http://arxiv.org/abs/2302.12244v3",
        "http://arxiv.org/pdf/2302.12244v3"
      ]
    },
    {
      "abstract": "Recently, ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., InstructGPT. In this study, we propose BadGPT, the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated text through BadGPT.",
      "authors": [
        "Shi, Jiawen",
        "Liu, Yixin",
        "Zhou, Pan",
        "Sun, Lichao"
      ],
      "categories": null,
      "citations": null,
      "comments": "This paper is accepted as a poster in NDSS2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-21",
      "selected": null,
      "title": "BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT",
      "urls": [
        "http://arxiv.org/abs/2304.12298v1",
        "http://arxiv.org/pdf/2304.12298v1",
        "http://arxiv.org/pdf/2304.12298.pdf"
      ]
    },
    {
      "abstract": "Humans have needs motivating their behavior according to intensity and context. However, we also create preferences associated with each action's perceived pleasure, which is susceptible to changes over time. This makes decision-making more complex, requiring learning to balance needs and preferences according to the context. To understand how this process works and enable the development of robots with a motivational-based learning model, we computationally model a motivation theory proposed by Hull. In this model, the agent (an abstraction of a mobile robot) is motivated to keep itself in a state of homeostasis. We added hedonic dimensions to see how preferences affect decision-making, and we employed reinforcement learning to train our motivated-based agents. We run three agents with energy decay rates representing different metabolisms in two different environments to see the impact on their strategy, movement, and behavior. The results show that the agent learned better strategies in the environment that enables choices more adequate according to its metabolism. The use of pleasure in the motivational mechanism significantly impacted behavior learning, mainly for slow metabolism agents. When survival is at risk, the agent ignores pleasure and equilibrium, hinting at how to behave in harsh scenarios.",
      "authors": [
        "Berto, Let\u00edcia",
        "Costa, Paula",
        "Sim\u00f5es, Alexandre",
        "Gudwin, Ricardo",
        "Colombini, Esther"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-20",
      "selected": null,
      "title": "Learning Goal-based Movement via Motivational-based Models in Cognitive Mobile Robots",
      "urls": [
        "http://arxiv.org/pdf/2302.09759.pdf",
        "http://arxiv.org/abs/2302.09759v1",
        "http://arxiv.org/pdf/2302.09759v1"
      ]
    },
    {
      "abstract": "Known-item video search is effective with human-in-the-loop to interactively\ninvestigate the search result and refine the initial query. Nevertheless, when\nthe first few pages of results are swamped with visually similar items, or the\nsearch target is hidden deep in the ranked list, finding the know-item target\nusually requires a long duration of browsing and result inspection. This paper\ntackles the problem by reinforcement learning, aiming to reach a search target\nwithin a few rounds of interaction by long-term learning from user feedbacks.\nSpecifically, the system interactively plans for navigation path based on\nfeedback and recommends a potential target that maximizes the long-term reward\nfor user comment. We conduct experiments for the challenging task of video\ncorpus moment retrieval (VCMR) to localize moments from a large video corpus.\nThe experimental results on TVR and DiDeMo datasets verify that our proposed\nwork is effective in retrieving the moments that are hidden deep inside the\nranked lists of CONQUER and HERO, which are the state-of-the-art auto-search\nengines for VCMR.",
      "authors": [
        "Zhixin Ma",
        "Chong-Wah Ngo"
      ],
      "categories": null,
      "citations": 0,
      "comments": "Accepted by ACM Multimedia 2022",
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1145/3503161.3548277",
      "keywords": [
        "interactive search",
        "reinforcement learning",
        "video corpus moment retrieval",
        "user simulation"
      ],
      "number_of_pages": 11,
      "pages": "296-306",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450392037",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Computer Vision and Pattern Recognition"
        ],
        "title": "Proceedings of the 30th ACM International Conference on Multimedia\n  (2022) 296-306"
      },
      "publication_date": "2023-02-19",
      "selected": null,
      "title": "Interactive Video Corpus Moment Retrieval using Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2302.09522v1",
        "https://dl.acm.org/doi/10.1145/3503161.3548277",
        "http://dx.doi.org/10.1145/3503161.3548277",
        "http://arxiv.org/pdf/2302.09522v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151141299&origin=inward"
      ]
    },
    {
      "abstract": "This paper reviews the state-of-the-art of language models architectures and strategies for \"complex\" question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as a baseline. We discuss some challenges associated with complex QA, including domain adaptation, decomposition and efficient multi-step QA, long form and non-factoid QA, safety and multi-sensitivity data protection, multimodal search, hallucinations, explainability and truthfulness, temporal reasoning. We analyze current solutions and promising research trends, using elements such as: hybrid LLM architectural patterns, training and prompting strategies, active human reinforcement learning supervised with AI, neuro-symbolic and structured knowledge grounding, program synthesis, iterated decomposition and others.",
      "authors": [
        "Daull, Xavier",
        "Bellot, Patrice",
        "Bruno, Emmanuel",
        "Martin, Vincent",
        "Murisasco, Elisabeth"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-17",
      "selected": null,
      "title": "Complex QA and language models hybrid architectures, Survey",
      "urls": [
        "http://arxiv.org/abs/2302.09051v4",
        "http://arxiv.org/pdf/2302.09051v4",
        "http://arxiv.org/pdf/2302.09051.pdf"
      ]
    },
    {
      "abstract": "Preference Based Reinforcement Learning has shown much promise for utilizing human binary feedback on queried trajectory pairs to recover the underlying reward model of the Human in the Loop (HiL). While works have attempted to better utilize the queries made to the human, in this work we make two observations about the unlabeled trajectories collected by the agent and propose two corresponding loss functions that ensure participation of unlabeled trajectories in the reward learning process, and structure the embedding space of the reward model such that it reflects the structure of state space with respect to action distances. We validate the proposed method on one locomotion domain and one robotic manipulation task and compare with the state-of-the-art baseline PEBBLE. We further present an ablation of the proposed loss components across both the domains and find that not only each of the loss components perform better than the baseline, but the synergic combination of the two has much better reward recovery and human feedback sample efficiency.",
      "authors": [
        "Verma, Mudit",
        "Bhambri, Siddhant",
        "Kambhampati, Subbarao"
      ],
      "categories": null,
      "citations": null,
      "comments": "R2HCAI, AAAI 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-17",
      "selected": null,
      "title": "Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2302.08738v1",
        "http://arxiv.org/pdf/2302.08738v1",
        "http://arxiv.org/pdf/2302.08738.pdf"
      ]
    },
    {
      "abstract": "Reinforcement Learning has suffered from poor reward specification, and issues for reward hacking even in simple enough domains. Preference Based Reinforcement Learning attempts to solve the issue by utilizing binary feedbacks on queried trajectory pairs by a human in the loop indicating their preferences about the agent's behavior to learn a reward model. In this work, we present a state augmentation technique that allows the agent's reward model to be robust and follow an invariance consistency that significantly improved performance, i.e. the reward recovery and subsequent return computed using the learned policy over our baseline PEBBLE. We validate our method on three domains, Mountain Car, a locomotion task of Quadruped-Walk, and a robotic manipulation task of Sweep-Into, and find that using the proposed augmentation the agent not only benefits in the overall performance but does so, quite early in the agent's training phase.",
      "authors": [
        "Verma, Mudit",
        "Kambhampati, Subbarao"
      ],
      "categories": null,
      "citations": null,
      "comments": "R2HCAI, AAAI 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-17",
      "selected": null,
      "title": "A State Augmentation based approach to Reinforcement Learning from Human Preferences",
      "urls": [
        "http://arxiv.org/abs/2302.08734v1",
        "http://arxiv.org/pdf/2302.08734.pdf",
        "http://arxiv.org/pdf/2302.08734v1"
      ]
    },
    {
      "abstract": "Preference-based Reinforcement Learning (PbRL) methods utilize binary feedback from the human in the loop (HiL) over queried trajectory pairs to learn a reward model in an attempt to approximate the human's underlying reward function capturing their preferences. In this work, we investigate the issue of a high degree of variability in the initialized reward models which are sensitive to random seeds of the experiment. This further compounds the issue of degenerate reward functions PbRL methods already suffer from. We propose a data-driven reward initialization method that does not add any additional cost to the human in the loop and negligible cost to the PbRL agent and show that doing so ensures that the predicted rewards of the initialized reward model are uniform in the state space and this reduces the variability in the performance of the method across multiple runs and is shown to improve the overall performance compared to other initialization methods.",
      "authors": [
        "Verma, Mudit",
        "Kambhampati, Subbarao"
      ],
      "categories": null,
      "citations": null,
      "comments": "R2HCAI, AAAI 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-17",
      "selected": null,
      "title": "Data Driven Reward Initialization for Preference based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2302.08733.pdf",
        "http://arxiv.org/pdf/2302.08733v1",
        "http://arxiv.org/abs/2302.08733v1"
      ]
    },
    {
      "abstract": "Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally optimal objective but that different divergences present different alignment and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good balance between these objectives, and frequently outperforms forward KL divergence by a wide margin, leading to significant improvements over prior work. These distinguishing characteristics between divergences persist as the model size increases, highlighting the importance of selecting appropriate divergence objectives.",
      "authors": [
        "Go, Dongyoung",
        "Korbak, Tomasz",
        "Kruszewski, Germ\u00e1n",
        "Rozen, Jos",
        "Ryu, Nahyeon",
        "Dymetman, Marc"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 38,
      "pages": "11546-11583",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-02-16",
      "selected": null,
      "title": "Aligning Language Models with Preferences through f-divergence Minimization",
      "urls": [
        "http://arxiv.org/abs/2302.08215v2",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174398630&origin=inward",
        "http://arxiv.org/pdf/2302.08215.pdf",
        "http://arxiv.org/pdf/2302.08215v2"
      ]
    },
    {
      "abstract": "We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",
      "authors": [
        "Ganguli, Deep",
        "Askell, Amanda",
        "Schiefer, Nicholas",
        "Liao, Thomas I.",
        "Luko\u0161i\u016bt\u0117, Kamil\u0117",
        "Chen, Anna",
        "Goldie, Anna",
        "Mirhoseini, Azalia",
        "Olsson, Catherine",
        "Hernandez, Danny",
        "Drain, Dawn",
        "Li, Dustin",
        "Tran-Johnson, Eli",
        "Perez, Ethan",
        "Kernion, Jackson",
        "Kerr, Jamie",
        "Mueller, Jared",
        "Landau, Joshua",
        "Ndousse, Kamal",
        "Nguyen, Karina",
        "Lovitt, Liane",
        "Sellitto, Michael",
        "Elhage, Nelson",
        "Mercado, Noemi",
        "DasSarma, Nova",
        "Rausch, Oliver",
        "Lasenby, Robert",
        "Larson, Robin",
        "Ringer, Sam",
        "Kundu, Sandipan",
        "Kadavath, Saurav",
        "Johnston, Scott",
        "Kravec, Shauna",
        "Showk, Sheer El",
        "Lanham, Tamera",
        "Telleen-Lawton, Timothy",
        "Henighan, Tom",
        "Hume, Tristan",
        "Bai, Yuntao",
        "Hatfield-Dodds, Zac",
        "Mann, Ben",
        "Amodei, Dario",
        "Joseph, Nicholas",
        "McCandlish, Sam",
        "Brown, Tom",
        "Olah, Christopher",
        "Clark, Jack",
        "Bowman, Samuel R.",
        "Kaplan, Jared"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-15",
      "selected": null,
      "title": "The Capacity for Moral Self-Correction in Large Language Models",
      "urls": [
        "http://arxiv.org/abs/2302.07459v2",
        "http://arxiv.org/pdf/2302.07459.pdf",
        "http://arxiv.org/pdf/2302.07459v2"
      ]
    },
    {
      "abstract": "Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.",
      "authors": [
        "Zhang, Tianjun",
        "Liu, Fangchen",
        "Wong, Justin",
        "Abbeel, Pieter",
        "Gonzalez, Joseph E."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 15,
      "pages": "41414-41428",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-02-10",
      "selected": null,
      "title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers",
      "urls": [
        "http://arxiv.org/pdf/2302.05206v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174392218&origin=inward",
        "http://arxiv.org/abs/2302.05206v1",
        "http://arxiv.org/pdf/2302.05206.pdf"
      ]
    },
    {
      "abstract": "Imitation learning (IL) has recently shown impressive performance in training a reinforcement learning agent with human demonstrations, eliminating the difficulty of designing elaborate reward functions in complex environments. However, most IL methods work under the assumption of the optimality of the demonstrations and thus cannot learn policies to surpass the demonstrators. Some methods have been investigated to obtain better-than-demonstration (BD) performance with inner human feedback or preference labels. In this paper, we propose a method to learn rewards from suboptimal demonstrations via a weighted preference learning technique (LERP). Specifically, we first formulate the suboptimality of demonstrations as the inaccurate estimation of rewards. The inaccuracy is modeled with a reward noise random variable following the Gumbel distribution. Moreover, we derive an upper bound of the expected return with different noise coefficients and propose a theorem to surpass the demonstrations. Unlike existing literature, our analysis does not depend on the linear reward constraint. Consequently, we develop a BD model with a weighted preference learning technique. Experimental results on continuous control and high-dimensional discrete control tasks show the superiority of our LERP method over other state-of-the-art BD  methods.",
      "authors": [
        "Liangyu Huo",
        "Zulin Wang",
        "Mai Xu"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1609/aaai.v37i7.25962",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "7953-7961",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-880-0",
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2023-02-07",
      "selected": null,
      "title": "Learning Noise-Induced Reward Functions for Surpassing Demonstrations in Imitation Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1609/aaai.v37i7.25962"
      ]
    },
    {
      "abstract": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.",
      "authors": [
        "Liu, Hao",
        "Sferrazza, Carmelo",
        "Abbeel, Pieter"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-06",
      "selected": null,
      "title": "Chain of Hindsight Aligns Language Models with Feedback",
      "urls": [
        "http://arxiv.org/abs/2302.02676v8",
        "http://arxiv.org/pdf/2302.02676v8",
        "http://arxiv.org/pdf/2302.02676.pdf"
      ]
    },
    {
      "abstract": "There is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.",
      "authors": [
        "Yu, Chao",
        "Gao, Jiaxuan",
        "Liu, Weilin",
        "Xu, Botian",
        "Tang, Hao",
        "Yang, Jiaqi",
        "Wang, Yu",
        "Wu, Yi"
      ],
      "categories": null,
      "citations": null,
      "comments": "The first two authors share equal contributions. This paper is\n  accepted by ICLR 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-03",
      "selected": null,
      "title": "Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased",
      "urls": [
        "http://arxiv.org/abs/2302.01605v1",
        "http://arxiv.org/pdf/2302.01605.pdf",
        "http://arxiv.org/pdf/2302.01605v1"
      ]
    },
    {
      "abstract": "Cutting planes (cuts) are important for solving mixed-integer linear programs (MILPs), which formulate a wide range of important real-world applications. Cut selection -- which aims to select a proper subset of the candidate cuts to improve the efficiency of solving MILPs -- heavily depends on (P1) which cuts should be preferred, and (P2) how many cuts should be selected. Although many modern MILP solvers tackle (P1)-(P2) by manually designed heuristics, machine learning offers a promising approach to learn more effective heuristics from MILPs collected from specific applications. However, many existing learning-based methods focus on learning which cuts should be preferred, neglecting the importance of learning the number of cuts that should be selected. Moreover, we observe from extensive empirical results that (P3) what order of selected cuts should be preferred has a significant impact on the efficiency of solving MILPs as well. To address this challenge, we propose a novel hierarchical sequence model (HEM) to learn cut selection policies via reinforcement learning. Specifically, HEM consists of a two-level model: (1) a higher-level model to learn the number of cuts that should be selected, (2) and a lower-level model -- that formulates the cut selection task as a sequence to sequence learning problem -- to learn policies selecting an ordered subset with the size determined by the higher-level model. To the best of our knowledge, HEM is the first method that can tackle (P1)-(P3) in cut selection simultaneously from a data-driven perspective. Experiments show that HEM significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs, including MIPLIB 2017. Moreover, experiments demonstrate that HEM well generalizes to MILPs that are significantly larger than those seen during training.",
      "authors": [
        "Wang, Zhihai",
        "Li, Xijun",
        "Wang, Jie",
        "Kuang, Yufei",
        "Yuan, Mingxuan",
        "Zeng, Jia",
        "Zhang, Yongdong",
        "Wu, Feng"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to ICLR2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-02-01",
      "selected": null,
      "title": "Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model",
      "urls": [
        "http://arxiv.org/pdf/2302.00244.pdf",
        "http://arxiv.org/abs/2302.00244v1",
        "http://arxiv.org/pdf/2302.00244v1"
      ]
    },
    {
      "abstract": "A 10-Hz repetitive transcranial magnetic stimulation to the left dorsal lateral prefrontal cortex has been shown to increase dopaminergic activity in the dorsal striatum, a region strongly implicated in reinforcement learning. However, the behavioural influence of this effect remains largely unknown. We tested the causal effects of 10-Hz stimulation on behavioural and computational characteristics of reinforcement learning. A total of 40 healthy individuals were randomized into active and sham (placebo) stimulation groups. Each participant underwent one stimulation session (1500 pulses) in which stimulation was applied over the left dorsal lateral prefrontal cortex using a robotic arm. Participants then completed a reinforcement learning task sensitive to striatal dopamine functioning. Participants' choices were modelled using a reinforcement learning model (Q-learning) that calculates separate learning rates associated with positive and negative reward prediction errors. Subjects receiving active stimulation exhibited increased reward rate (number of correct responses per second of task activity) compared with those in sham. Computationally, although no group differences were observed, the active group displayed a higher learning rate for correct trials (\u00ce\u00b1G) compared with incorrect trials (\u00ce\u00b1L). Finally, when tested with novel pairs of stimuli, the active group displayed extremely fast reaction times, and a trend towards a higher reward rate. This study provided specific behavioural and computational accounts of altered striatal-mediated behaviour, particularly response vigour, induced by a proposed increase of dopamine activity by 10-Hz stimulation to the left dorsal lateral prefrontal cortex. Together, these findings bolster the use of repetitive transcranial magnetic stimulation to target neurocognitive disturbances attributed to the dysregulation of dopaminergic-striatal circuits. \u00c2\u00a9 2022 Federation of European Neuroscience Societies and John Wiley & Sons Ltd.",
      "authors": [
        "Biernacki, K.",
        "Myers, C.E.",
        "Cole, S.",
        "Cavanagh, J.F.",
        "Baker, T.E."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/ejn.15905",
      "keywords": [
        "probabilistic selection task",
        "reward prediction errors",
        "Q-learning",
        "reinforcement learning",
        "dopamine",
        "TMS"
      ],
      "number_of_pages": 12,
      "pages": "680-691",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0953816X",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.044,
        "snip": 0.891,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "European Journal of Neuroscience"
      },
      "publication_date": "2023-02-01",
      "selected": null,
      "title": "Prefrontal transcranial magnetic stimulation boosts response vigour during reinforcement learning in healthy adults",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145362004&origin=inward"
      ]
    },
    {
      "abstract": "Little is known about how self-schemas are formed, fluctuate, and are reinforced. In this study, we used a revised mnemic neglect paradigm to examine how self-schema fluctuates following episodic events (feedback) and its self-concordance. Participants exhibiting various depressive symptoms (BDI-II ranging from 1 to 36; M\u2009=\u200911.90) underwent psychological testing, followed by bogus feedback regarding their personality, future, and behavioural traits, where they rated their state self-schemas and feedback self-concordance trial-by-trial. Linear mixed models showed that feedback self-concordance was determined by the interaction between self-schema and the emotional valence of the feedback, and the self-schema fluctuated with the interaction between prediction error (the difference between the emotional valence of the feedback and current self-schema) and feedback self-concordance. Cognitive reactivity, the ease of responding to negative moods, was associated with higher parameters regressed onto self-schema and self-concordance regardless of the feedback valence, indicating that it enhances the likelihood of self-schema fluctuation positively and negatively. The simulation of self-schema development shows that some individuals developed a negative self-schema even after experiencing many positive events; these parameters were characteristic of individuals with high levels of cognitive reactivity. These results have significant implications for self-schema development and depression.",
      "authors": [
        "Matsumoto, Noboru",
        "Katahira, Kentaro",
        "Kawaguchi, Jun"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10608-022-10332-x",
      "keywords": [
        "Cognitive reactivity",
        "Mnemic neglect",
        "Self-schema",
        "Reinforcement learning",
        "Depression"
      ],
      "number_of_pages": 14,
      "pages": "38-51",
      "publication": {
        "category": "Journal",
        "cite_score": 4.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01475916",
        "publisher": "Springer New York",
        "sjr": 1.19,
        "snip": 1.237,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Clinical Psychology"
        ],
        "title": "Cognitive Therapy and Research"
      },
      "publication_date": "2023-02-01",
      "selected": null,
      "title": "Cognitive Reactivity Amplifies the Activation and Development of Negative Self-schema: A Revised Mnemic Neglect Paradigm and Computational Modelling",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139672927&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10608-022-10332-x.pdf"
      ]
    },
    {
      "abstract": "Grid collages (GClg) of small image collections are popular and useful in many applications, such as personal album management, online photo posting, and graphic design. In this article, we focus on how visual effects influence individual preferences through various arrangements of multiple images under such scenarios. A novel balance-aware metric is proposed to bridge the gap between multi-image joint presentation and visual pleasure. The metric merges psychological achievements into the field of grid collage. To capture user preference, a bonus mechanism related to a user-specified special location in the grid and uniqueness values of the subimages is integrated into the metric. An end-to-end reinforcement learning mechanism empowers the model without tedious manual annotations. Experiments demonstrate that our metric can evaluate the GClg visual balance in line with human subjective perception, and the model can generate visually pleasant GClg results, which is comparable to manual designs. \u00c2\u00a9 1995-2012 IEEE.",
      "authors": [
        "Yu Song",
        "Fan Tang",
        "Weiming Dong",
        "Feiyue Huang",
        "Tong-Yee Lee",
        "Changsheng Xu"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TVCG.2021.3113031",
      "keywords": [
        "visual balance",
        "Grid collage",
        "reinforcement learning"
      ],
      "number_of_pages": 15,
      "pages": "1330-1344",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2160-9306",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Visualization and Computer Graphics"
      },
      "publication_date": "2023-02-01",
      "selected": null,
      "title": "Balance-Aware Grid Collage for Small Image Collections",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9540348",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115199369&origin=inward"
      ]
    },
    {
      "abstract": "Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the existing PbRL methods. Notably, on high-dimensional control tasks, our algorithm surpasses offline RL methods that learn with ground-truth reward information. Finally, we show that our algorithm can be successfully applied to fine-tune large language models.",
      "authors": [
        "An, Gaon",
        "Lee, Junhyeok",
        "Zuo, Xingdong",
        "Kosaka, Norio",
        "Kim, Kyung-Min",
        "Song, Hyun Oh"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-01-30",
      "selected": null,
      "title": "Direct Preference-based Policy Optimization without Reward Modeling",
      "urls": [
        "http://arxiv.org/pdf/2301.12842.pdf",
        "http://arxiv.org/abs/2301.12842v3",
        "http://arxiv.org/pdf/2301.12842v3"
      ]
    },
    {
      "abstract": "The complexity of designing reward functions has been a major obstacle to the wide application of deep reinforcement learning (RL) techniques. Describing an agent's desired behaviors and properties can be difficult, even for experts. A new paradigm called reinforcement learning from human preferences (or preference-based RL) has emerged as a promising solution, in which reward functions are learned from human preference labels among behavior trajectories. However, existing methods for preference-based RL are limited by the need for accurate oracle preference labels. This paper addresses this limitation by developing a method for crowd-sourcing preference labels and learning from diverse human preferences. The key idea is to stabilize reward learning through regularization and correction in a latent space. To ensure temporal consistency, a strong constraint is imposed on the reward model that forces its latent space to be close to the prior distribution. Additionally, a confidence-based reward model ensembling method is designed to generate more stable and reliable predictions. The proposed method is tested on a variety of tasks in DMcontrol and Meta-world and has shown consistent and significant improvements over existing preference-based RL algorithms when learning from diverse feedback, paving the way for real-world applications of RL methods.",
      "authors": [
        "Xue, Wanqi",
        "An, Bo",
        "Yan, Shuicheng",
        "Xu, Zhongwen"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-01-27",
      "selected": null,
      "title": "Reinforcement Learning from Diverse Human Preferences",
      "urls": [
        "http://arxiv.org/pdf/2301.11774v2",
        "http://arxiv.org/abs/2301.11774v2",
        "http://arxiv.org/pdf/2301.11774.pdf"
      ]
    },
    {
      "abstract": "We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound for max-entropy IRL.",
      "authors": [
        "Zhu, Banghua",
        "Jiao, Jiantao",
        "Jordan, Michael I."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-01-26",
      "selected": null,
      "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons",
      "urls": [
        "http://arxiv.org/abs/2301.11270v5",
        "http://arxiv.org/pdf/2301.11270v5",
        "http://arxiv.org/pdf/2301.11270.pdf"
      ]
    },
    {
      "abstract": "<p>Threat-related information attracts attention and disrupts ongoing behavior, and particularly so for more anxious individuals. Yet, it is unknown how and to what extent threat-related information leave lingering influences on behavior (e.g., by impeding ongoing learning processes). Here, human male and female participants (<i>N</i> = 47) performed probabilistic reinforcement learning tasks where irrelevant distracting faces (neutral, happy, or fearful) were presented together with relevant monetary feedback. Behavioral modeling was combined with fMRI data (<i>N</i> = 27) to explore the neurocomputational bases of learning relevant and irrelevant information. In two separate studies, individuals with high trait anxiety showed increased avoidance of objects previously paired with the combination of neutral monetary feedback and fearful faces (but not neutral or happy faces). Behavioral modeling revealed that high anxiety increased the integration of fearful faces during feedback learning, and fMRI results (regarded as provisional, because of a relatively small sample size) further showed that variance in the prediction error signal, uniquely accounted for by fearful faces, correlated more strongly with activity in the right DLPFC for more anxious individuals. Behavioral and neuronal dissociations indicated that the threat-related distractors did not simply disrupt learning processes. By showing that irrelevant threats exert long-lasting influences on behavior, our results extend previous research that separately showed that anxiety increases learning from aversive feedbacks and distractibility by threat-related information. Our behavioral results, combined with the proposed neurocomputational mechanism, may help explain how increased exposure to irrelevant affective information contributes to the acquisition of maladaptive behaviors in more anxious individuals.</p><p><b>SIGNIFICANCE STATEMENT</b> In modern-day society, people are increasingly exposed to various types of irrelevant information (e.g., intruding social media announcements). Yet, the neurocomputational mechanisms influenced by irrelevant information during learning, and their interactions with increasingly distracted personality types are largely unknown. Using a reinforcement learning task, where relevant feedback is presented together with irrelevant distractors (emotional faces), we reveal an interaction between irrelevant threat-related information (fearful faces) and interindividual anxiety levels. fMRI shows provisional evidence for an interaction between anxiety levels and the coupling between activity in the DLPFC and learning signals specifically elicited by fearful faces. Our study reveals how irrelevant threat-related information may become entrenched in the anxious psyche and contribute to long-lasting abnormal behaviors.</p>",
      "authors": [
        "Kristoffer C. Aberg",
        "Ido Toren",
        "Rony Paz"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1186-22.2022",
      "keywords": [
        "dorsolateral PFC",
        "reinforcement learning",
        "maladaptive",
        "anxiety",
        "distractors",
        "prediction error"
      ],
      "number_of_pages": 16,
      "pages": "656-671",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2023-01-25",
      "selected": null,
      "title": "Irrelevant Threats Linger and Affect Behavior in High Anxiety",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147047233&origin=inward"
      ]
    },
    {
      "abstract": "The cold supply chain in the pharmaceutical industry is a technique designed to transport and store temperature-sensitive vaccines to sustain their effectiveness and to uphold safety measures as they are directly involved with human life. The reinforcement learning algorithm is a feedback mechanism using machine learning where the agent, environment, action, state and rewards are correlated. This paper is to sustain a cold supply chain for vaccines like Lupride and Degapride in Sun pharmaceuticals; a reinforcement learning algorithm has been devised to measure, monitor and correct the temperature of the cold storage unit. Temperature-sensitive vaccines need to maintain their chemical and biological traits during their transit from packaging to delivery.",
      "authors": [
        "Vijay Ramasamy",
        "Padmapriya Pravinkumar"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICCCI56745.2023.10128458",
      "keywords": [
        "cold supply chain",
        "component; Reinforcement learning",
        "Pharmaceuticals",
        "Temperature"
      ],
      "number_of_pages": 3,
      "pages": "1-3",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-4822-4",
        "issn": "2329-7190",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 International Conference on Computer Communication and Informatics, ICCCI 2023"
      },
      "publication_date": "2023-01-23",
      "selected": null,
      "title": "Reinforcement Learning Algorithm to Sustain Temperature in Pharma Supply Chain Management",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163108236&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10128458"
      ]
    },
    {
      "abstract": "Quadruped robots are currently used in industrial robotics as mechanical aid to automate several routine tasks. However, presently, the usage of such a robot in a domestic setting is still very much a part of the research. This paper discusses the understanding and virtual simulation of such a robot capable of detecting and understanding human emotions, generating its gait, and responding via sounds and expression on a screen. To this end, we use a combination of reinforcement learning and software engineering concepts to simulate a quadruped robot that can understand emotions, navigate through various terrains and detect sound sources, and respond to emotions using audio-visual feedback. This paper aims to establish the framework of simulating a quadruped robot that is emotionally intelligent and can primarily respond to audio-visual stimuli using motor or audio response. The emotion detection from the speech was not as performant as ERANNs or Zeta Policy learning, still managing an accuracy of 63.5%. The video emotion detection system produced results that are almost at par with the state of the art, with an accuracy of 99.66%. Due to its \"on-policy\" learning process, the PPO algorithm was extremely rapid to learn, allowing the simulated dog to demonstrate a remarkably seamless gait across the different cadences and variations. This enabled the quadruped robot to respond to generated stimuli, allowing us to conclude that it functions as predicted and satisfies the aim of this work.",
      "authors": [
        "Chakravarty, Abhiruph",
        "Tripathy, Jatin Karthik",
        "S, Sibi Chakkaravarthy",
        "Cherukuri, Aswani Kumar",
        "Anitha, S.",
        "Kamalov, Firuz",
        "Jonnalagadda, Annapurna"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2023-01-03",
      "selected": null,
      "title": "e-Inu: Simulating A Quadruped Robot With Emotional Sentience",
      "urls": [
        "http://arxiv.org/pdf/2301.00964.pdf",
        "http://arxiv.org/abs/2301.00964v1",
        "http://arxiv.org/pdf/2301.00964v1"
      ]
    },
    {
      "abstract": "The concept of a digital twin of Earth envisages the convergence of Big Earth Data with physics-based models in an interactive computational framework that enables monitoring and prediction of environmental and social perturbations for use in sustainable governance. Although computational advances are rapidly progressing, digital twins of Earth have not yet been produced. In this Review, we summarize the methodological and cyberinfrastructure advances in Big Data that have advanced the progress towards a digital Earth twin. Data assimilation provides the framework for incorporation of high-resolution observations into Earth system models but lacks the decision-making interface and learning ability needed for the digital twin. Machine learning (and particularly deep learning) in Earth system science is now more capable of reaching the high dimensionality, complexity and nonlinearity of real-life Earth systems and is expanding the learning ability from Big Data. Progress in causal inference and reinforcement learning are, respectively, increasing the interpretability of Big Data and the ability of simulations to solve sequential decision-making problems. Social sensing data could provide inputs for multiagent deep reinforcement learning via feedback loops between agents and the environment, enabling large-scale applications in human system modelling. Future research must focus on finding the optimal way to integrate these individual methodologies to achieve digital twins. A digital twin of Earth would fully integrate Big Data observations within an Earth\u2013human system model, to assess the interactions between these subsystems. This Review explores the current progress in Big Data manipulation in Earth sciences providing pathways toward digital twins of Earth.",
      "authors": [
        "Li, Xin",
        "Feng, Min",
        "Ran, Youhua",
        "Su, Yang",
        "Liu, Feng",
        "Huang, Chunlin",
        "Shen, Huanfeng",
        "Xiao, Qing",
        "Su, Jianbin",
        "Yuan, Shiwei",
        "Guo, Huadong"
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s43017-023-00409-w",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 27.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2662-138X",
        "publisher": "Springer International Publishing",
        "sjr": 6.193,
        "snip": 6.954,
        "subject_areas": [
          "Atmospheric Science",
          "Pollution",
          "Earth-Surface Processes",
          "Nature and Landscape Conservation"
        ],
        "title": "Nature Reviews Earth and Environment"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Big Data in Earth system science and progress towards a digital twin",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85156153784&origin=inward",
        "https://www.nature.com/articles/s43017-023-00409-w.pdf"
      ]
    },
    {
      "abstract": "Machine learning is a branch of artificial intelligence in which computers use data to teach themselves and improve their problem-solving abilities. In this case, learning is the process by which computers use data and algorithms to build models that improve performance, and it can be divided into supervised learning, unsupervised learning, and reinforcement learning. Among them, reinforcement learning is a learning method in which AI interacts with the environment and finds the optimal strategy through actions, and it means that AI takes certain actions and learns based on the feedback it receives from the environment. In other words, reinforcement learning is a learning algorithm that allows AI to learn by itself and determine the optimal action for the situation by learning to find patterns hidden in a large amount of data collected through trial and error. In this study, we introduce the main reinforcement learning algorithms: value-based algorithms, policy gradient-based reinforcement learning, reinforcement learning with intrinsic rewards, and deep learning-based reinforcement learning. Reinforcement learning is a technology that enables AI to develop its own problem-solving capabilities, and it has recently gained attention among AI learning methods as the usefulness of the algorithms in various industries has become more widely known. In recent years, reinforcement learning has made rapid progress and achieved remarkable results in a variety of fields. Based on these achievements, reinforcement learning has the potential to positively transform human lives. In the future, more advanced forms of reinforcement learning with enhanced interaction with the environment need to be developed.",
      "authors": [
        "Haewon Byeon"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.14569/IJACSA.2023.0140838",
      "keywords": [
        "reinforcement learning with intrinsic rewards",
        "policy gradient-based reinforcement learning",
        "Reinforcement learning",
        "deep learning-based reinforcement learning",
        "value-based algorithms"
      ],
      "number_of_pages": 7,
      "pages": "348-354",
      "publication": {
        "category": "Journal",
        "cite_score": 2.1,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2158107X",
        "publisher": "Science and Information Organization",
        "sjr": 0.258,
        "snip": 0.512,
        "subject_areas": [
          "Computer Science (all)"
        ],
        "title": "International Journal of Advanced Computer Science and Applications (IJACSA)"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Advances in Value-based, Policy-based, and Deep Learning-based Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170637582&origin=inward",
        "http://thesai.org/Downloads/Volume14No8/Paper_38-Advances_in_Value_based_Policy_based_and_Deep_Learning.pdf"
      ]
    },
    {
      "abstract": "Robots can incorporate data from human teachers when learning new tasks. However, this data can often be noisy, which can cause robots to learn slowly or not at all. One method for learning from human teachers is Human-in-the-loop Reinforcement Learning (HRL), which can combine information from both an environmental reward and external feedback from human teachers. However, many HRL methods assume near-perfect information from teachers or must know the skill level of each teacher before starting the learning process. Our algorithm, Classification for Learning Erroneous Assessments using Rewards (CLEAR), is a feedback filter for Reinforcement Learning (RL) algorithms, enabling learning agents to learn from imperfect teachers without prior modeling. CLEAR is able to determine whether human feedback is correct based on observations of the RL learning curve. Our results suggest that CLEAR improves the quality of human feedback - from 57.5% to 65% correct in a human study - and performs more reliably than baselines by matching or outperforming RL without human teachers in all tested cases. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Taylor A. Kessler Faulkner",
        "Andrea L. Thomaz"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRA48891.2023.10161105",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "9414-9420",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-2366-5",
        "issn": "10504729",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Using Learning Curve Predictions to Learn from Incorrect Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168697254&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161105"
      ]
    },
    {
      "abstract": "Thanks to the rapid growth in wearable technologies and advancements in machine learning, monitoring complex human contexts becomes feasible, paving the way to develop human-in-the-loop IoT systems that naturally evolve to adapt to the human and environment state autonomously. Nevertheless, a central challenge in designing many of these IoT systems arises from the requirement to infer the human mental state, such as intention, stress, cognition load, or learning ability. While different human contexts can be inferred from the fusion of different sensor modalities that can correlate to a particular mental state, the human brain provides a richer sensor modality that gives us more insights into the required human context. This paper proposes ERUDITE, a human-in-the-loop IoT system for the learning environment that exploits recent wearable neurotechnology to decode brain signals. Through insights from concept learning theory, ERUDITE can infer the human state of learning and understand when human learning increases or declines. By quantifying human learning as an input sensory signal, ERUDITE can provide adequate personalized feedback to humans in a learning environment to enhance their learning experience. ERUDITE is evaluated across 15 participants and showed that by using the brain signals as a sensor modality to infer the human learning state and providing personalized adaptation to the learning environment, the participants\u2019 learning performance increased on average by 26%. Furthermore, to evaluate ERUDITE practicality and scalability, we showed that ERUDITE can be deployed on an edge-based prototype consuming 75 mW power on average with 100 MB memory footprint. Authors",
      "authors": [
        "Taherisadr, M.",
        "Faruque, M.A.A.",
        "Elmalaki, S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/JIOT.2023.3343462",
      "keywords": [
        "Electroencephalography",
        "Physiology",
        "Augmented reality",
        "Q-learning",
        "Wisconsin card sorting",
        "Rule-based learning",
        "Functional magnetic resonance imaging",
        "Real-time systems",
        "Concept learning",
        "Internet of Things",
        "Reinforcement learning",
        "Human in the loop",
        "Virtual reality",
        "Biomedical monitoring"
      ],
      "number_of_pages": 1,
      "pages": "1-1",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Internet of Things Journal"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "ERUDITE: Human-in-the-Loop IoT for an Adaptive Personalized Learning System",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180352364&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 1.0,
        "is_potentially_predatory": false,
        "isbn": "9789819927883",
        "issn": "18650929",
        "publisher": "Springer Science and Business Media Deutschland GmbH",
        "sjr": 0.194,
        "snip": 0.241,
        "subject_areas": [
          "Computer Science (all)",
          "Mathematics (all)"
        ],
        "title": "International Conference on Neural Information Processing"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "1st International Conference on Cognitive Computation and Systems, ICCCS 2022",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163283340&origin=inward"
      ]
    },
    {
      "abstract": "The technologies used in smart homes have recently improved to learn the user preferences from feedback in order to enhance the user convenience and quality of experience. Most smart homes learn a uniform model to represent the thermal preferences of users, which generally fails when the pool of occupants includes people with different sensitivities to temperature, for instance, due to age and physiological factors. Thus, a smart home with a single optimal policy may fail to provide comfort when a new user with a different preference is integrated into the home. In this article, we propose a Bayesian reinforcement learning framework that can approximate the current occupant state in a partially observable smart home environment using its thermal preference and, then, identify the occupant as a new user or someone who is already known to the system. Our proposed framework can be used to identify users based on the temperature and humidity preferences of the occupant when performing different activities to enable personalization and improve comfort. We then compare the proposed framework with a baseline long short-term memory learner that learns the thermal preference of the user from the sequence of actions that it takes. We perform these experiments with up to five simulated human models each based on hierarchical reinforcement learning. The results show that our framework can approximate the belief state of the current user just by its temperature and humidity preferences across different activities with a high degree of accuracy. \u00c2\u00a9 2020 IEEE.",
      "authors": [
        "Shashi Suman",
        "Francois Rivest",
        "Ali Etemad"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TAI.2022.3178065",
      "keywords": [
        "Bayesian reinforcement learning",
        "hierarchical reinforcement learning (HRL)",
        "user personalization",
        "smart home",
        "Artificial intelligence\u2013human interaction"
      ],
      "number_of_pages": 13,
      "pages": "549-561",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2691-4581",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Artificial Intelligence"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Toward Personalization of User Preferences in Partially Observable Smart Home Environments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145561439&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9782516"
      ]
    },
    {
      "abstract": "Preference-based reinforcement learning (PbRL) develops agents using human preferences. Due to its empirical success, it has prospect of benefiting human-centered applications. Meanwhile, previous work on PbRL overlooks interpretability, which is an indispensable element of ethical artificial intelligence (AI). While prior art for explainable AI offers some machinery, there lacks an approach to select samples to construct explanations. This becomes an issue for PbRL, as transitions relevant to task solving are often outnumbered by irrelevant ones. Thus, ad-hoc sample selection undermines the credibility of explanations. The present study proposes a framework for learning reward functions and state importance from preferences simultaneously. It offers a systematic approach for selecting samples when constructing explanations. Moreover, the present study proposes a perturbation analysis to evaluate the learned state importance quantitatively. Through experiments on discrete and continuous control tasks, the present study demonstrates the proposed framework\u2019s efficacy for providing interpretability without sacrificing task performance.",
      "authors": [
        "Zhang, Guoxi",
        "Kashima, Hisashi"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10994-022-06295-5",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 8.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08856125",
        "publisher": "Springer Netherlands",
        "sjr": 1.679,
        "snip": 2.74,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Machine Learning"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Learning state importance for preference-based reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145919377&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10994-022-06295-5.pdf"
      ]
    },
    {
      "abstract": "Diversifying agents' skills has proven to be critical for adapting to a wide range of tasks. However, continuously promoting diversity can have catastrophic effects, such as accumulating unsafe, ineffective or misaligned skills. To avoid such outcomes, providing agents with the ability to modulate diversity in skill discovery remains a largely unexplored research area. In my research, I aim to design agents that can control and adapt their diversity to fit any context. Integrating context into skill discovery was my initial approach to controlling diversity. This was done by allowing the agent to use human preferences to identify regions of the environment where diversity is most likely to be desired. However, to modulate skill diversity, an agent has to be able to not only identify where to demonstrate diversity, but also comprehend how it affects the environment and others around it to decide when to be more (or less) diverse. To achieve this, an agent needs more tools, such as observing its own diversity and methods of adjusting it. The incorporation of controlled diversity will, I believe, make agents with multiple behaviors more flexible, reliable, and robustly applicable in a wide variety of contexts. \u00c2\u00a9 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
      "authors": [
        "Hussonnois, M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Human Preferences",
        "KEYWORDS Skill Diversity",
        "Reinforcement Learning"
      ],
      "number_of_pages": 3,
      "pages": "2976-2978",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Toolkit for Encouraging Safe Diversity in Skill Discovery",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171267893&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Subramani M."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/ADVAN.00036.2023",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "270-271",
      "publication": {
        "category": "Journal",
        "cite_score": 3.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10434046",
        "publisher": "American Physiological Society",
        "sjr": 0.518,
        "snip": 0.895,
        "subject_areas": [
          "Education",
          "Physiology"
        ],
        "title": "Advances in Physiology Education"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Evaluating the performance of ChatGPT in medical physiology university examination of phase I MBBS",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150966687&origin=inward"
      ]
    },
    {
      "abstract": "The proceedings contain 198 papers. The topics discussed include: does peer pressure have the same effect on intelligent decision support system as it does on human decision systems? case study: fetal ultrasound movies; the negative effect of team performance on sponsorship brand performance: the mediating role of social identity threat; sustaining business performance management: an operational framework; measurement of collaboration with agile practices in a virtual learning environment; multirobot allocation in a flexible manufacturing system, using reinforcement learning for decision-making, case of study; the guidance of opinion leader on followers\u00e2\u0080\u0099 opinions\u00e2\u0080\u0093based on opinion similarity and closeness perspective; financial fraud detection based on the part-of-speech features of textual risk disclosures in financial reports; reviving stagnated debates in group decision making environments with high number of alternatives; research on the construction of scientific and technological achievements management index system; intelligent proportional controller tuned by virtual reference feedback tuning and fictitious reference iterative tuning; and comprehensive evaluation of grid green development with ecosystem from the perspective of energy industry chain.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Procedia Computer Science"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "10th International Conference on Information Technology and Quantitative Management, ITQM 2023",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171737017&origin=inward"
      ]
    },
    {
      "abstract": "Long-term memory retention is enhanced after testing compared to restudying (testing effect). Notably, memory retrieval further improves when correct-answer feedback is provided after the retrieval attempt (test-potentiated encoding \u2013 TPE). To evaluate whether explicit positive or negative feedback further enhances memory performance beyond the effect of TPE, in two experiments additional explicit positive or negative performance-contingent feedback was presented before providing correct-answer feedback. After an initial exposure to the full material, 40 participants learned 210 weakly associated cue-target word pairs by either restudying or testing in Experiment 1. Depending on the accuracy of the retrieval attempt, the tested word pairs were followed by positive or negative performance feedback (50%) or no feedback (50%). Irrespective of the type of repetition, trials were followed by a restudy opportunity. Participants returned to perform a final cued-recall test (Day 2). Final test results replicated the testing effect (better memory performance for tested compared to restudied items) and explicit performance feedback in addition to correct-answer feedback increased retrieval performance, but only on Day 2. This pattern of results was replicated in Experiment 2 in an independent sample of 25 participants. To assess the specific effects of learning history, we examined retrieval accuracy and reaction times during repetition cycles: Explicit feedback improved retrieval for material successfully encoded in the initial study phase (consistent positive feedback) as well as material learned during the repetition phase (mixed positive and negative feedback). Hence, performance feedback improves learning beyond the effects of retrieval practice and correct-answer feedback, suggesting that it strengthens memory representations and promotes re-encoding of the material.",
      "authors": [
        "Ludowicy, Petra",
        "Paz-Alonso, Pedro M.",
        "Lachmann, Thomas",
        "Czernochowski, Daniela"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2023.1100497",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Performance feedback enhances test-potentiated encoding",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158070888&origin=inward"
      ]
    },
    {
      "abstract": "The proceedings contain 663 papers. The topics discussed include: consensus robustness under pi protocols with undirected graphs; imitating swarm behaviors by learning agent-level controllers; chance-constrained optimization in contact-rich systems; safe human-robot collaborative transportation via trust-driven role adaptation; NAPVIG: local generalized Voronoi approximation for reactive navigation in unknown and dynamic environments; dynamics learning-based fault isolation for a soft trunk robot; robot control for simultaneous impact tasks through time-invariant reference spreading; robust control barrier functions for safety using a hybrid neuroprosthesis; subspace method for generalized controller synthesis: an artificial potential field motion planning approach; data-driven deep learning based feedback linearization of systems with unknown dynamics; and decentralized multi-agent reinforcement learning for continuous-space stochastic games.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350328066",
        "issn": "07431619",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the American Control Conference"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "2023 American Control Conference, ACC 2023",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167805483&origin=inward"
      ]
    },
    {
      "abstract": "The paper proposes a novel approach to enhance the resilience of mutual collaborative activity between humans and robots in industrial assembly tasks. The approach exploits Adversarial Reinforcement Learning (ARL) to enable a robot to learn an assembly policy that is...",
      "authors": [
        "Antonelli, Dario",
        "Aliev, Khurshid"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-031-42622-3_22",
      "keywords": [
        "Cobots",
        "Adversarial Reinforcement Learning",
        "Human-Robot Collaboration",
        "Machine Learning"
      ],
      "number_of_pages": 11,
      "pages": "317-327",
      "publication": {
        "category": "Book",
        "cite_score": 1.4,
        "is_potentially_predatory": false,
        "isbn": "9783031426216",
        "issn": "18684238",
        "publisher": "Springer New York",
        "sjr": 0.255,
        "snip": 0.364,
        "subject_areas": [
          "Information Systems and Management",
          "Computer Networks and Communications",
          "Information Systems"
        ],
        "title": "IFIP Advances in Information and Communication Technology"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Using Adversarial Reinforcement Learning to Improve the Resilience of Human-Robot Collaboration in Industrial Assembly",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-031-42622-3_22.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174439063&origin=inward"
      ]
    },
    {
      "abstract": "One goal of artificial intelligence (AI) research is to teach machines how to learn from humans, such that they can perform a certain task in a natural human-like way. In this article, an online adaptive inverse reinforcement learning (IRL) approach to human behavior modeling is proposed to enhance machine intelligence for a class of linear human-in-the-loop (HiTL) systems using the state data only, where the human behavior is described by a linear quadratic optimal control model with an unknown weighting matrix for the quadratic cost function. First, an integral concurrent adaptive law is developed to learn the human feedback gain matrix online using the demonstrated state data only, which removes the persistent excitation (PE) conditions required by traditional adaptive estimation approaches and thus is more in line with real applications. Then, with the learned feedback gain matrix, the IRL problem is formulated as a linear matrix inequality (LMI) optimization problem, which can be efficiently solved to retrieve the weighting matrix of the human cost function. Finally, a simulation example is provided to illustrate the effectiveness of the proposed approach.",
      "authors": [
        "Huai-Ning Wu",
        "Mi Wang"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2023.3259581",
      "keywords": [
        "human behavior modeling",
        "Adaptive estimation",
        "human-in-the-loop (HiTL)",
        "inverse reinforcement learning (IRL)",
        "concurrent learning (CL)",
        "linear quadratic regulator (LQR)"
      ],
      "number_of_pages": 12,
      "pages": "1-12",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162-2388",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Human-in-the-Loop Behavior Modeling via an Integral Concurrent Adaptive Inverse Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151495813&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10083106"
      ]
    },
    {
      "abstract": "Rise of unmanned vehicles and autonomous robots has been accompanied by study of path planning, navigation, and decision-making algorithms. Current state-of-the-art employs deep neural nets to extract the required features. This technology, though successful in most cases, fails where the training has not been done for unseen conditions. For such unlabeled training data, transfer learning approaches have been proposed. A major drawback of using transfer learning approaches is that the actions and/or state spaces are reactive only to present circumstances. A truly intelligent autonomous operation has to consider a subordinate-to-a-human approach for its mission risks that vary with topography, path planning as well as mission goals. To address these complex combinatorial problems, DARPA has initiative a novel Explainable AI (XAI) technology in the past few years. In XAI, machine learning is paired with human intervention to make decisions by generating textual explanations of all the available relevant information / decision. In this paper, we propose to use available information along with human intelligence in a feedback loop for helping the unlabeled data to be trained and generate cost functions which were previously not programmed. We study this context/situation awareness to generate list of decision available from explanations on combinatorial tasks. Moreover, we employ this approach to a quadruped robot to learn its environment and the AI model starts in its infancy to mimic human cognitive architecture. We show that the learning process can be improved in a way that suits a particular mission in mind.",
      "authors": [
        "Sanket Lokhande",
        "Joseph Dailey",
        "Yuqing Liu",
        "Samantha Connolly",
        "Hao Xu"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1117/12.2664016",
      "keywords": [
        "Explainable AI (XAI)",
        "reinforcement learning",
        "path planning",
        "combinatorial tasks"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781510662087",
        "issn": "0277786X",
        "publisher": "SPIE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications IV"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A novel explainable AI-based situation recognition for autonomous robots with partial unlabeled data",
      "urls": [
        "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12546/1254606/A-novel-explainable-AI-based-situation-recognition-for-autonomous-robots/10.1117/12.2664016.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173806087&origin=inward"
      ]
    },
    {
      "abstract": "Following the definition in Sect. \n              1.2.3.2\n              \n            , phishing can be a typical class of reactive attention attacks that exploit inattention to evade detection. This chapter proposes ADVERT, a human-technical solution that generates...",
      "authors": [
        "Huang, Linan",
        "Zhu, Quanyan"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-031-30709-6_5",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "67-83",
      "publication": {
        "category": "Book",
        "cite_score": 0.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21915768",
        "publisher": "Springer International Publishing",
        "sjr": 0.131,
        "snip": 0.0,
        "subject_areas": [
          "Computer Science (all)"
        ],
        "title": "SpringerBriefs in Computer Science"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "ADVERT: Defending against Reactive Attention Attacks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161805194&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-031-30709-6_5.pdf"
      ]
    },
    {
      "abstract": "Title: Generating Diverse Optimal Road Management Plans in Post-Disaster by Applying Envelope Multi-Objective Deep Reinforcement Learning | Keywords: western Japan flooding, road restoration, relative importance, multi-objective reinforcement learning | Author: Soo-Hyun Joo, Yoshiki Ogawa, and Yoshihide Sekimoto",
      "authors": [
        "Soo-Hyun Joo",
        "Yoshiki Ogawa",
        "Yoshihide Sekimoto"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.20965/jdr.2023.p0884",
      "keywords": [
        "road restoration",
        "relative importance",
        "multi-objective reinforcement learning",
        "western Japan flooding"
      ],
      "number_of_pages": 11,
      "pages": "884-894",
      "publication": {
        "category": "Journal",
        "cite_score": 1.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18812473",
        "publisher": "Fuji Technology Press",
        "sjr": 0.301,
        "snip": 0.613,
        "subject_areas": [
          "Engineering (miscellaneous)",
          "Safety, Risk, Reliability and Quality"
        ],
        "title": "Journal of Disaster Research"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Generating Diverse Optimal Road Management Plans in Post-Disaster by Applying Envelope Multi-Objective Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85178660394&origin=inward"
      ]
    },
    {
      "abstract": "Collective dynamics play a key role in everyday decision-making. Whether social influence promotes the spread of accurate information and ultimately results in adaptive behavior or leads to false information cascades and maladaptive social contagion strongly depends on the cognitive mechanisms underlying social interactions. Here we argue that cognitive modeling, in tandem with experiments that allow collective dynamics to emerge, can mechanistically link cognitive processes at the individual and collective levels. We illustrate the strength of this cognitive computational approach with two highly successful cognitive models that have been applied to interactive group experiments: evidence-accumulation and reinforcement-learning models. We show how these approaches make it possible to simultaneously study (a) how individual cognition drives social systems, (b) how social systems drive individual cognition, and (c) the dynamic feedback processes between the two layers. \u00c2\u00a9 The Author(s) 2023.",
      "authors": [
        "Tump, A.N.",
        "Deffner, D.",
        "Pleskac, T.J.",
        "Romanczuk, P.",
        "M. Kurvers, R.H.J."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/17456916231186964",
      "keywords": [
        "groups",
        "reinforcement learning",
        "collective dynamics",
        "cognitive modeling",
        "drift-diffusion model"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 18.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17456916",
        "publisher": "SAGE Publications Ltd",
        "sjr": 5.529,
        "snip": 5.112,
        "subject_areas": [
          "Psychology (all)"
        ],
        "title": "Perspectives on Psychological Science"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Cognitive Computational Approach to Social and Collective Decision-Making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170523689&origin=inward"
      ]
    },
    {
      "abstract": "Contact-rich robotic manipulation tasks such as assembly are widely studied due to their close relevance with social and manufacturing industries. Although the task is highly related to vision and force, current methods lack a unified mechanism to effectively fuse the two sensors. We consider coordinating multimodality from perception to control and propose a vision-force curriculum policy learning scheme to effectively fuse the features and generate policy. Experiments in simulations indicate the priorities of our method, which could insert pegs with 0.1 mm clearance. Furthermore, the system is generalizable to various initial configurations and unseen shapes, and it can be robustly transferred from simulation to reality without fine-tuning, showing the effectiveness and generalization of our proposed method. The experiment videos and code will be available at https://sites.google.com/view/vf-assembly.",
      "authors": [
        "Jin, Piaopiao",
        "Lin, Yinjie",
        "Song, Yaoxian",
        "Li, Tiefeng",
        "Yang, Wei"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2023.1280773",
      "keywords": [
        "curriculum learning",
        "sensor fusion",
        "contact-rich manipulation",
        "robotic assembly task",
        "multimodal perception"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Vision-force-fused curriculum learning for robotic contact-rich assembly tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174801195&origin=inward"
      ]
    },
    {
      "abstract": "Large-scale, geographically distributed, and long-term risks arise from diverse underlying causes ranging from pandemics to poverty to underinvestment in protecting against natural hazards or failures of sociotechnical, economic, and financial systems. Protecting...",
      "authors": [
        "Cox Jr., Louis Anthony"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-031-32013-2_8",
      "keywords": [],
      "number_of_pages": 22,
      "pages": "251-272",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08848289",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Series in Operations Research and Management Science"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Muddling-Through and Deep Learning for Bureaucratic Decision-Making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164682230&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-031-32013-2_8.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Momennejad I."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1098/rstb.2021.0446",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 12.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09628436",
        "publisher": "The Royal Society",
        "sjr": 2.084,
        "snip": 1.629,
        "subject_areas": [
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Agricultural and Biological Sciences (all)"
        ],
        "title": "Philosophical Transactions of the Royal Society B: Biological Sciences"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A rubric for human-like agents and NeuroAI",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143993435&origin=inward"
      ]
    },
    {
      "abstract": "To realize robot skill learning in the real world, reinforcement learning algorithms need to be applied in continuous problems with high sample efficiency. Hybrid intelligence is regarded as an available solution for this problem, due to the ability to speed up the learning process with human knowledge and experience. Therefore, we propose Episode-Fuzzy-COACH (COrrective Advice Communicated by Humans), to imitate human fuzzy logic and involve human intelligence in the learning process. In this framework, human knowledge and experience are involved in the learning process, which are provided by human feedback and fuzzy rules designed by human users. Moreover, it is combined with Path Integrals Policy Improvement (<inline-formula><tex-math notation=\"LaTeX\">$PI^{2}$</tex-math></inline-formula>), to realize hybrid intelligence, which is used to realize fast robot skill learning. Throwing Movement Primitives proposed in this article is used to represent the policy of ball-throwing skill. According to the simulation results, the learning efficiency of our method is increased by 72% and 42.86%, respectively, compared with pure <inline-formula><tex-math notation=\"LaTeX\">$PI^{2}$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$PI^{2}+$</tex-math></inline-formula>COACH. Our method validated in experiments is 46.67% more effective than <inline-formula><tex-math notation=\"LaTeX\">$PI^{2}+$</tex-math></inline-formula>COACH. The results also show that the performance of our method is not affected by users' knowledge level of the related field. It is proven that <inline-formula><tex-math notation=\"LaTeX\">$PI^{2}+$</tex-math></inline-formula>Episode-Fuzzy-COACH is available for fast robot skill learning. IEEE",
      "authors": [
        "Li, B.",
        "Liu, X.",
        "Liu, Z.",
        "Huang, P."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TIE.2023.3294600",
      "keywords": [
        "Costs",
        "robot skill learning",
        "Task analysis",
        "Reinforcement learning",
        "Hybrid intelligence",
        "Service robots",
        "Heuristic algorithms",
        "interactive reinforcement learning",
        "Machine intelligence",
        "Human intelligence"
      ],
      "number_of_pages": 10,
      "pages": "1-10",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02780046",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Industrial Electronics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Episode-Fuzzy-COACH Method for Fast Robot Skill Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165411043&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Alili A."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNSRE.2023.3236217",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "895-903",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15344320",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.26,
        "snip": 1.675,
        "subject_areas": [
          "Neuroscience (all)",
          "Biomedical Engineering",
          "Internal Medicine",
          "Rehabilitation"
        ],
        "title": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Novel Framework to Facilitate User Preferred Tuning for a Robotic Knee Prosthesis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147231065&origin=inward"
      ]
    },
    {
      "abstract": "In cognitive buildings (CBs), intelligent IoT edge devices do more than gather data. They also aggregate, analyze, and stream data at the edge of the network, where cognitive controllers based on machine learning algorithms enable new levels of control and security...",
      "authors": [
        "Greco, Emilio",
        "Spezzano, Giandomenico"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-031-15160-6_13",
      "keywords": [],
      "number_of_pages": 19,
      "pages": "285-303",
      "publication": {
        "category": "Book",
        "cite_score": 2.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21991073",
        "publisher": "Springer International Publishing AG",
        "sjr": 0.131,
        "snip": 0.0,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Artificial Intelligence",
          "Signal Processing",
          "Instrumentation",
          "Computer Science Applications",
          "Computer Networks and Communications"
        ],
        "title": "Internet of Things"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Human-Centered Reinforcement Learning for Lighting and Blind Control in Cognitive Buildings",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144050739&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-031-15160-6_13.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350333817",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 International Conference on Advances in Intelligent Computing and Applications, AICAPS 2023"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "2023 International Conference on Advances in Intelligent Computing and Applications, AICAPS 2023",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152199864&origin=inward"
      ]
    },
    {
      "abstract": "This article investigates the problem of enhancing underwater visual observations for the purpose of accurate underwater object detection. Most existing underwater visual enhancement algorithms tend to follow human vision preference but do not necessarily favor the effectiveness of an object detection algorithm. We observe that it should not be the human vision preference but the object detection algorithm that knows what underwater visual enhancement configuration is most beneficial to the detection tasks. In light of this observation, we propose a reinforcement learning paradigm of configuring visual enhancement for object detection in underwater scenes. Specifically, we use underwater image features as states and object detection score increments as rewards. We set up a collection of extensible actions that consist of multiple visual enhancement algorithms. The optimal policy is learned in the form of an action sequence, which characterizes a stepwise process of visual enhancement. Experimental results validate that the sequence of visual enhancement algorithms configured with respect to the object detection algorithm is in favor of improving the detection results. \u00c2\u00a9 1976-2012 IEEE.",
      "authors": [
        "Hao Wang",
        "Shixin Sun",
        "Xiao Bai",
        "Jian Wang",
        "Peng Ren"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/JOE.2022.3226202",
      "keywords": [
        "underwater scenes",
        "reinforcement learning",
        "Object detection",
        "visual enhancement"
      ],
      "number_of_pages": 19,
      "pages": "443-461",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2373-7786",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Journal of Oceanic Engineering"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Reinforcement Learning Paradigm of Configuring Visual Enhancement for Object Detection in Underwater Scenes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149469905&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10058092"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bennett A."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 30,
      "pages": "10871-10900",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Provable Safe Reinforcement Learning with Binary Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165203278&origin=inward"
      ]
    },
    {
      "abstract": "Due to the complex coupling of greenhouse environments, a number of challenges have been encountered in the research of automatic control in Venlo greenhouses. Most algorithms are only concerned with accuracy, yet energy-saving control is of great importance for improving economic benefits. Reinforcement learning, as an unsupervised machine learning method with a framework similar to that of feedback control, is a powerful tool for autonomous decision making in complex environments. However, the loss of benefits and increased time cost in the exploration process make it difficult to apply it to practical scenarios. This work proposes an energy-saving control algorithm for Venlo greenhouse skylights and wet curtain fan based on Reinforcement Learning with Soft Action Mask (SAM), which establishes a trainable SAM network with artificial rules to achieve sub-optimal policy initiation, safe exploration, and efficient optimization. Experiments in a simulated Venlo greenhouse model show that the approach, which is a feasible solution encoding human knowledge to improve the reinforcement learning process, can start with a safe, sub-optimal level and effectively and efficiently achieve reductions in the energy consumption, providing a suitable environment for crops and preventing frequent operation of the facility during the control process.",
      "authors": [
        "Chen, Lihan",
        "Xu, Lihong",
        "Wei, Ruihua"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/agriculture13010141",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2077-0472",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.561,
        "snip": 1.162,
        "subject_areas": [
          "Plant Science",
          "Food Science",
          "Agronomy and Crop Science"
        ],
        "title": "Agriculture (Switzerland)"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Energy-Saving Control Algorithm of Venlo Greenhouse Skylight and Wet Curtain Fan Based on Reinforcement Learning with Soft Action Mask",
      "urls": [
        "https://www.mdpi.com/2077-0472/13/1/141/pdf?version=1672912875",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146711880&origin=inward"
      ]
    },
    {
      "abstract": "Editorial: Bringing together data- and knowledge-driven solutions for a better understanding and effective diagnostics of neurological disorders",
      "authors": [
        "Kaplun, Dmitrii",
        "Bogachev, Mikhail",
        "Singh, Pawan Kumar",
        "Sarkar, Ram"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fninf.2023.1229945",
      "keywords": [
        "artificial intelligence",
        "knowledge-driven models",
        "interpretable functional models",
        "data-driven models",
        "neurological disorders"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5196",
        "publisher": "Frontiers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Neuroinformatics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Editorial: Bringing together data- and knowledge-driven solutions for a better understanding and effective diagnostics of neurological disorders",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166526653&origin=inward"
      ]
    },
    {
      "abstract": "How do people teach robots tasks? Here, we focus on main methods and models enabling humans to teach embodied social agents such as social robots, using natural interaction. Humans guide the learning process of such agents by providing various teaching signals, which...",
      "authors": [
        "Chetouani, Mohamed"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-031-24349-3_9",
      "keywords": [],
      "number_of_pages": 33,
      "pages": "140-172",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Interactive Robot Learning: An Overview",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-031-24349-3_9.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152548893&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hejna J."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 12,
      "pages": "2014-2025",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Few-Shot Preference Learning for Human-in-the-Loop RL",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164964560&origin=inward"
      ]
    },
    {
      "abstract": "The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients' trust to make them share their problems comfortably. Thus, it is essential for the counselor to adequately comprehend the client's emotions and ensure client's welfare, i.e. s/he should adapt and deal with the clients politely and empathetically to provide a pleasant, cordial and personalized experience. Motivated by this, in this work, we attempt to build a novel Polite and empAthetic counseLing conversational agent PAL. To have client's emotion-based polite and empathetic responses, two counseling datasets laying down the counseling support to substance addicts and crime victims are annotated. These annotated datasets are used to build PAL in a reinforcement learning framework. A novel reward function is formulated to ensure correct politeness and empathy preferences as per client's emotions with naturalness and non-repetitiveness in responses. Thorough automatic and human evaluation showcases the usefulness and strength of the designed novel reward function. Our proposed system is scalable and can be easily modified with different modules of preference models as per need. \u00c2\u00a9 2023 Association for Computational Linguistics.",
      "authors": [
        "Mishra, K.",
        "Priya, P.",
        "Ekbal, A."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 18,
      "pages": "12254-12271",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174405813&origin=inward"
      ]
    },
    {
      "abstract": "In biological neural systems, different neurons are capable of self-organizing to form different neural circuits for achieving a variety of cognitive functions. However, the current design paradigm of spiking neural networks is based on structures derived from deep learning. Such structures are dominated by feedforward connections without taking into account different types of neurons, which significantly prevent spiking neural networks from realizing their potential on complex tasks. It remains an open challenge to apply the rich dynamical properties of biological neural circuits to model the structure of current spiking neural networks. This paper provides a more biologically plausible evolutionary space by combining feedforward and feedback connections with excitatory and inhibitory neurons. We exploit the local spiking behavior of neurons to adaptively evolve neural circuits such as forward excitation, forward inhibition, feedback inhibition, and lateral inhibition by the local law of spike-timing-dependent plasticity and update the synaptic weights in combination with the global error signals. By using the evolved neural circuits, we construct spiking neural networks for image classification and reinforcement learning tasks. Using the brain-inspired Neural circuit Evolution strategy (NeuEvo) with rich neural circuit types, the evolved spiking neural network greatly enhances capability on perception and reinforcement learning tasks. NeuEvo achieves state-of-the-art performance on CIFAR10, DVS-CIFAR10, DVS-Gesture, and N-Caltech101 datasets and achieves advanced performance on ImageNet. Combined with on-policy and off-policy deep reinforcement learning algorithms, it achieves comparable performance with artificial neural networks. The evolved spiking neural circuits lay the foundation for the evolution of complex networks with functions. Copyright \u00c2\u00a9 2023 the Author(s).",
      "authors": [
        "Shen, G.",
        "Zhao, D.",
        "Dong, Y.",
        "Zeng, Y."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.2218173120",
      "keywords": [
        "brain-inspired",
        "spiking neural networks",
        "neural circuit evolution"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Brain-inspired neural circuit evolution for spiking neural networks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171810310&origin=inward"
      ]
    },
    {
      "abstract": "The proceedings contain 36 papers. The special focus in this conference is on Industrial, Engineering, and other Applications of Applied Intelligent Systems. The topics include: Clothing Detection and\u00c2 Classification with\u00c2 Fine-Tuned YOLO-Based Models; a Novel Parallel Spatiotemporal Image Fusion Method for\u00c2 Predicting High-Resolution Satellite Images; a Transfer Learning-Based Approach for\u00c2 Rice Plant Disease Detection; unsupervised Defogging for\u00c2 Rotary Kilns Image; a Robust Document Localization Solution with\u00c2 Segmentation and\u00c2 Clustering; a Group Decision-Making Method Based on\u00c2 Reciprocal Preference Relations Created from\u00c2 Sentiment Analysis; automatically Generated Weight Methods for Human and Machine Decision-Making; reconciling Inconsistent Preference Information in Group Multicriteria Decision Support with Reference Sets; a Design Method for\u00c2 an\u00c2 Intelligent Tutoring System with\u00c2 Algorithms Visualization; feature Selection on Imbalanced Domains: A Stability-Based Analysis; course Recommendation Based on\u00c2 Graph Convolutional Neural Network; collaborative Filtering Based on Non-Negative Matrix Factorization for Programming Problem Recommendation; MAF: Multimodal Auto Attention Fusion for Video Classification; engineering Drawing Text Detection via\u00c2 Better Feature Fusion; a Transformer Based Multimodal Fine-Fusion Model for False Information Detection; domain-Specific Knowledge Graph Adaption with\u00c2 Industrial Text Data; towards Knowledge Graph Creation from\u00c2 Greek Governmental Documents; information Retrieval from\u00c2 Legal Documents with\u00c2 Ontology and\u00c2 Graph Embeddings Approach; recommendations Based on Reinforcement Learning and Knowledge Graph; link-Aware Link Prediction over\u00c2 Temporal Graph by\u00c2 Pattern Recognition; principal Components Analysis Based Imputation for\u00c2 Logistic Regression; unsupervised Disentanglement Learning via\u00c2 Dirichlet Variational Autoencoder; spiking Generative Networks in\u00c2 Lifelong Learning Environment; a Contrastive Method for\u00c2 Continual Generalized Zero-Shot Learning; active Learning Based Labeling Method for\u00c2 Fault Disposal Pre-plans; Global Spatial Representation: EEG Correcting for\u00c2 Subject-Independent Emotion Recognition; micro-expression Recognition Based on Local Optical Flow Capsule Network; emotion Prediction Based on\u00c2 Conversational Context and\u00c2 Commonsense Knowledge Graphs.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Proceedings of the 36th International Conference on Industrial, Engineering, and Other Applications of Applied Intelligent Systems, IEA/AIE 2023",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172443143&origin=inward"
      ]
    },
    {
      "abstract": "Various interpretations of the literature detailing the neural basis of learning have in part led to disagreements concerning how consciousness arises. Further, artificial learning model design has suffered in replicating intelligence as it occurs in the human brain. Here, we present a novel learning model, which we term the \u201cRecommendation Architecture (RA) Model\u201d from Coward (2004a), using a dual-learning approach featuring both consequence feedback and non-consequence feedback. The RA model is tested on a categorical learning task where no two inputs are the same throughout training and/or testing. We compare this to three consequence feedback only models based on backpropagation and reinforcement learning. Results indicate that the RA model learns novelty more efficiently and can accurately return to prior learning after new learning with less computational resources expenditure. The final results of the study show that consequence feedback as interpretation, not creation, of cortical activity creates a learning style more similar to human learning in terms of resource efficiency. Stable information meanings underlie conscious experiences. The work provided here attempts to link the neural basis of nonconscious and conscious learning while providing early results for a learning protocol more similar to human brains than is currently available.",
      "authors": [
        "St. Clair, Rachel",
        "Coward, L. Andrew",
        "Schneider, Susan"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2023.1090126",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Leveraging conscious and nonconscious learning for efficient AI",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151970286&origin=inward"
      ]
    },
    {
      "abstract": "The task of summarisation is notoriously difficult to evaluate, with agreement even between expert raters unlikely to be perfect. One technique for summary evaluation relies on collecting comparison data by presenting annotators with generated summaries and tasking them with selecting the best one. This paradigm is currently being exploited in reinforcement learning using human feedback, whereby a reward function is trained using pairwise choice data. Comparisons are an easier way to elicit human feedback for summarisation, however, such decisions can be bottle necked by the usability of the annotator interface. In this paper, we present the results of a pilot study exploring how the user interface impacts annotator agreement when judging summary quality. \u00c2\u00a9 2023 Association for Computational Linguistics.",
      "authors": [
        "Gooding, S.",
        "Werner, L.",
        "Ca\u00cb\u0087rbune, V."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "179-187",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Pilot Study on Annotation Interfaces for Summary Comparisons",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174821492&origin=inward"
      ]
    },
    {
      "abstract": "Temporal difference (TD) prediction error signal models are instrumental in simulating brain function during reinforcement learning (RL). Recent evidence suggests a significant role of TD prediction error signals in the action-selection and action-execution brain networks. We introduce a novel neuro-computational model that addresses the effects of temporal difference error signal variations on reinforcement learning for action-selection and action-execution networks. These networks represent the basal ganglia and prefrontal cortex brain regions, while the TD prediction error signal represents the dopamine neurotransmitter. The model incorporates dopamine genetic parameters in the two networks (COMT gene for action-selection; DAT1 gene for action-execution) to generate four different parameter combinations. The model simulation showed that TD signaling in both networks plays a significant role in RL under optimal conditions of medium, not high, TD signals. Moreover, each parameter combination showed a unique pattern of RL, corresponding with experimental data obtained using a computer-based RL task. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Natsheh, A.Y.",
        "Natsheh, J.Y.",
        "Mousa, A.H.",
        "Al-Saheb, M.H.",
        "Moustafa, A.A.",
        "Herzallah, M.M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICIT58056.2023.10226147",
      "keywords": [
        "feedback-based learning",
        "computational modeling",
        "reinforcement learning",
        "dopamine"
      ],
      "number_of_pages": 8,
      "pages": "236-243",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350320060",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 International Conference on Information Technology: Cybersecurity Challenges for Sustainable Cities, ICIT 2023 - Proceeding"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Reinforcement Learning Model of Temporal Difference Variations for Action-Selection and Action-Execution in the Human Brain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171741452&origin=inward"
      ]
    },
    {
      "abstract": "Artificial Intelligence systems have a significant impact on human lives. Machine Ethics tries to align these systems with human values, by integrating 'ethical considerations'. However, most approaches consider a single objective, and thus cannot accommodate different, contextual human preferences. Multi-Objective Reinforcement Learning algorithms account for various preferences, but they often are not intelligible nor contextual (e.g., weighted preferences). Our novel approach identifies dilemmas, presents them to users, and learns to settle them, based on intelligible and contextualized preferences over actions. We intend to maximize understandability and opportunities for user-system co-construction by showing dilemmas, and triggering interactions, thus empowering users. The block-based architecture enables leveraging simple mechanisms that can be updated and improved. Validation on a Smart Grid use-case shows that our algorithm finds actions for various trade-offs, and quickly learns to settle dilemmas, reducing the cognitive load on users. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Chaput, R.",
        "Matignon, L.",
        "Guillermin, M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICTAI59109.2023.00075",
      "keywords": [
        "Human Preferences",
        "Machine Ethics",
        "Moral Dilemmas",
        "Multi-Objective Reinforcement Learning"
      ],
      "number_of_pages": 6,
      "pages": "474-479",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350342734",
        "issn": "10823409",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Learning to identify and settle dilemmas through contextual user preferences",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85182400235&origin=inward"
      ]
    },
    {
      "abstract": "Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free, textual-entailment rewards to optimize for factual consistency and explore the ensuing tradeoffs, as improved consistency may come at the cost of less informative or more extractive summaries. Our results, according to both automatic metrics and human evaluation, show that our method considerably improves the faithfulness, salience and conciseness of the generated summaries. \u00c2\u00a9 2023 Association for Computational Linguistics.",
      "authors": [
        "Roit, P.",
        "Ferret, J.",
        "Shani, L.",
        "Aharoni, R.",
        "Cideron, G.",
        "Dadashi, R.",
        "Geist, M.",
        "Girgin, S.",
        "Hussenot, L.",
        "Keller, O.",
        "Momchev, N.",
        "Ramos, S.",
        "Stanczyk, P.",
        "Vieillard, N.",
        "Bachem, O.",
        "Elidan, G.",
        "Hassidim, A.",
        "Pietquin, O.",
        "Szpektor, I."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 21,
      "pages": "6252-6272",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174388494&origin=inward"
      ]
    },
    {
      "abstract": "Locomotion control of legged robots is a challenging problem. Recently, reinforcement learning has been applied to legged locomotion and made a great success. However, the reward signal design remains a challenging problem to produce a humanlike motion such as...",
      "authors": [
        "Ye, Linqi",
        "Wang, Xueqian",
        "Liang, Bin"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-981-99-6492-5_38",
      "keywords": [
        "Reinforcement Learning",
        "Locomotion Control",
        "Biped Robot"
      ],
      "number_of_pages": 13,
      "pages": "439-451",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Realizing Human-like Walking and Running with Feedforward Enhanced Reinforcement Learning",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-981-99-6492-5_38.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175861373&origin=inward"
      ]
    },
    {
      "abstract": "The game industry is challenged to cope with increasing growth in demand and game complexity while maintaining acceptable quality standards for released games. Classic approaches solely depending on human efforts for quality assurance and game testing do not scale effectively in terms of time and cost. Game-testing AI agents that learn by interaction with the environment have the potential to mitigate these challenges with good scalability properties on time and costs. However, most recent work in this direction depends on game state information for the agent's state representation, which limits generalization across different game scenarios. Moreover, game test engineers usually prefer exploring a game in a specific style, such as exploring the golden path, yet, current game testing AI agents do not provide an explicit way to satisfy such a preference. This paper addresses these limitations by proposing an agent design that mainly depends on pixel-based state observations while exploring the environment conditioned on a user's preference specified by demonstration trajectories. In addition, we propose an imitation learning method that couples self-supervised and supervised learning objectives to enhance the quality of imitation behaviors. Our agent significantly outperforms state-of-the-art pixel-based game testing agents over exploration coverage and test execution quality when evaluated on a complex open-world environment resembling many aspects of real AAA games. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Abdelfattah, S.",
        "Brown, A.",
        "Zhang, P."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/CoG57401.2023.10333200",
      "keywords": [
        "game",
        "learning",
        "preference",
        "agent",
        "bug",
        "testing",
        "reinforcement"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350322774",
        "issn": "23254270",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Conference on Computatonal Intelligence and Games, CIG"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Preference-conditioned Pixel-based AI Agent For Game Testing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180554842&origin=inward"
      ]
    },
    {
      "abstract": "The lower-limb exoskeleton for human performance augmentation (LEHPA) in sensitivity amplification control (SAC) is vulnerable to model parameter uncertainties and unmodeled dynamics due to its large sensitivity to external disturbances resulting from the positive feedback by the inverse dynamic model of the exoskeleton. This paper firstly proposes to combine SAC with deep reinforcement learning (DRL) to reduce the dependence on the model accuracy and tackle the ever-changing human-exoskeleton interaction (HEI) dynamics. The sensitivity adjustment is interpreted as finding the optimal policy for a Markov Decision Process (MDP) and solved using deep reinforcement learning algorithms. To train the policy safely and efficiently, a multibody simulation environment is created to implement the training process, accompanied by a novel hybrid inverse-forward dynamics simulation method to carry out the simulation. For comparison purposes, the SAC controller is introduced as a benchmark. A novel performance evaluation method based on the HEI forces at the back, thighs, and shanks is proposed to evaluate the control effect of the trained SADRL controller quantitatively. The SADRL controller is compared with the SAC controller at five specified walking speeds, resulting in a lumped HEI force ratio as low as 0.54. The total decrease of HEI forces demonstrates the superior control effect of the SADRL strategy.",
      "authors": [
        "Ranran Zheng",
        "Zhiyuan Yu",
        "Hongwei Liu",
        "Zhe Zhao",
        "Jing Chen",
        "Longfei Jia"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2023.3265895",
      "keywords": [
        "sensitivity amplification control",
        "sensitivity adaptation",
        "deep reinforcement learning",
        "Lower-limb exoskeleton for human performance augmentation"
      ],
      "number_of_pages": 12,
      "pages": "36029-36040",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Sensitivity Adaptation of Lower-Limb Exoskeleton for Human Performance Augmentation Based on Deep Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10097746",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153329259&origin=inward"
      ]
    },
    {
      "abstract": "To facilitate research in the direction of fine-tuning foundation models from human feedback, we held the MineRL BASALT Competition on Fine-Tuning from Human Feedback at NeurIPS 2022. The BASALT challenge asks teams to compete to develop algorithms to solve tasks with hard-to-specify reward functions in Minecraft. Through this competition, we aimed to promote the development of algorithms that use human feedback as channels to learn the desired behavior. We describe the competition and provide an overview of the top solutions. We conclude by discussing the impact of the competition and future directions for improvement. \u00c2\u00a9 2023 S. Milani et al.",
      "authors": [
        "Milani, S.",
        "Kanervisto, A.",
        "Ramanauskas, K.",
        "Schulhoff, S.",
        "Houghton, B.",
        "Mohanty, S.",
        "Galbraith, B.",
        "Chen, K.",
        "Song, Y.",
        "Zhou, T.",
        "Yu, B.",
        "Liu, H.",
        "Guan, K.",
        "Hu, Y.",
        "Lv, T.",
        "Malato, F.",
        "Leopold, F.",
        "Raut, A.",
        "Hautam\u00c3\u00a4ki, V.",
        "Melnik, A.",
        "Ishida, S.",
        "Henriques, J.F.",
        "Klassert, R.",
        "Laurito, W.",
        "Cazzonelli, L.",
        "Kulbach, C.",
        "Popovic, N.",
        "Schweizer, M.",
        "Novoseller, E.",
        "Goecks, V.G.",
        "Waytowich, N.",
        "Watkins, D.",
        "Miller, J.",
        "Shah, R."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "imitation learning",
        "reward modeling",
        "fine-tuning",
        "preference learning",
        "reinforcement learning from human feedback",
        "Learning from humans"
      ],
      "number_of_pages": 18,
      "pages": "171-188",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the MineRL BASALT 2022 Competition",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179129024&origin=inward"
      ]
    },
    {
      "abstract": "Abstractive dialogue summarization has received increasing attention recently. Despite the fact that most of the current dialogue summarization systems are trained to maximize the likelihood of human-written summaries and have achieved significant results, there is still a huge gap in generating high-quality summaries as determined by humans, such as coherence and faithfulness, partly due to the misalignment in maximizing a single human-written summary. To this end, we propose to incorporate different levels of human feedback into the training process. This will enable us to guide the models to capture the behaviors humans care about for summaries. Specifically, we ask humans to highlight the salient information to be included in summaries to provide the local feedback, and to make overall comparisons among summaries in terms of coherence, accuracy, coverage, concise and overall quality, as the global feedback. We then combine both local and global feedback to fine-tune the dialog summarization policy with Reinforcement Learning. Experiments conducted on multiple datasets demonstrate the effectiveness and generalization of our methods over the state-of-the-art supervised baselines, especially in terms of human judgments. \u00c2\u00a9 2023 Association for Computational Linguistics.",
      "authors": [
        "Chen, J.",
        "Dodda, M.",
        "Yang, D."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 15,
      "pages": "9176-9190",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Human-in-the-loop Abstractive Dialogue Summarization",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175444009&origin=inward"
      ]
    },
    {
      "abstract": "In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \u00e2\u0080\u009cgold-standard\u00e2\u0080\u009d reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment. \u00c2\u00a9 2023 Proceedings of Machine Learning Research. All rights reserved.",
      "authors": [
        "Gao, L.",
        "Schulman, J.",
        "Hilton, J."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 32,
      "pages": "10835-10866",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Scaling Laws for Reward Model Overoptimization",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171869810&origin=inward"
      ]
    },
    {
      "abstract": "Brain-Machine Interfaces (BMIs) have the potential to allow subjects to brain control (BC) external devices, where their brain signals could be translated to the action of the neuro-prosthesis by reinforcement learning (RL) based decoder. During the BC task, feedback cues are provided to guide subject's learning. Subjects will adapt the neural signals according to the feedback cues. Concurrently, the RL decoding parameters are adjusted when the subject explores the BC task through trial and error, leading to a co-adaptive process between the subject and the decoder. However, when subjects receive the feedback cues and enhance their learning, the decoder does not actively utilize the feedback cues. If the RL decoder could integrate both neural signals and feedback cues, the training efficiency of the BC task would increase. A major challenge is the different temporal scales of neural signals and feedback cues, making it difficult to integrate them into a single decoder. In this paper, we propose a novel kernel RL decoding method as the first attempt to combine two signals with different temporal scales for RL decoding. The neural signals and the feedback cues comprise the decoding input, which is then projected into individual Reproducing Kernel Hilbert Spaces (RKHSs) respectively. These two RKHSs form a joint feature space, where the action of the neuro-prosthesis could be decoded linearly. We evaluate the proposed method on a simulated brain control cursor-reaching task. Our proposed method is compared with the kernel RL that only uses neural signals as the input. The proposed method has a faster learning speed and better decoding accuracy. The results demonstrate that our proposed method has successfully integrated the information of the feedback cue and facilitates the training procedure for the BC task.Clinical Relevance - This paper provides an integrated reinforcement learning decoding framework, which combines the neural signals and the feedback cues to increase the learning speed and the accuracy of the brain control task. Subjects could learn the task more easily with this decoder. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Zhang, X.",
        "Wang, Y."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/EMBC40787.2023.10340203",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350324471",
        "issn": "1557170X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Kernel Reinforcement Learning Decoding Framework Integrating Neural and Feedback Signals for Brain Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179645136&origin=inward"
      ]
    },
    {
      "abstract": "Neuromorphic hardware paired with braininspired learning strategies have enormous potential for robot control. Explicitly, these advantages include low energy consumption, low latency, and adaptability. Therefore, developing and improving learning strategies, algorithms, and neuromorphic hardware integration in simulation is a key to moving the state-of-the-art forward. In this work we use the Neurorobotics Platform (NRP) simulation framework to implement spiking reinforcement learning control for a robotic arm. We implement a force-torque feedback based classic object insertion task (\"peg-inhole\") and control the robot for the first time with neuromorphic hardware in the loop. We therefore provide a solution for training the system in uncertain environmental domains by using randomized simulation parameters. This leads to policies that are robust to realworld parameter variations in the target domain, filling the sim-to-real gap. To our knowledge it is the first neuromorphic implementation of the peg-in-hole task in simulation with the neuromorphic Loihi chip in the loop, and with scripted accelerated interactive training in the Neurorobotics Platform, including randomized domains.",
      "authors": [
        "Amaya, Camilo",
        "von Arnim, Axel"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2023.1239581",
      "keywords": [
        "domain randomization",
        "neuromorphic computing",
        "reinforcement learning",
        "spiking neural networks",
        "robot control",
        "neurorobotics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Neurorobotic reinforcement learning for domains with parametrical uncertainty",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176253801&origin=inward"
      ]
    },
    {
      "abstract": "Constructive studies on symbol emergence systems seek to investigate computational models that can better explain human language evolution, the creation of symbol systems, and the construction of internal representations. Specifically, emergent communication aims to formulate a computational model that enables agents to build efficient sign systems and internal representations. This study provides a new model for emergent communication, which is based on a probabilistic generative model (PGM) instead of a discriminative model based on deep reinforcement learning. We define the Metropolis-Hastings (MH) naming game by generalizing previously proposed models. It is not a referential game with explicit feedback, as assumed by many emergent communication studies. Instead, it is a game based on joint attention without explicit feedback. Mathematically, the MH naming game is proved to be a type of MH algorithm for an integrative PGM that combines two agents that play the naming game. From this viewpoint, symbol emergence is regarded as decentralized Bayesian inference, and semiotic communication is regarded as inter-personal cross-modal inference. This notion leads to the collective predictive coding hypothesis regarding language evolution and, in general, the emergence of symbols. We also propose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE), a deep generative model for emergent communication based on the MH naming game. In this model, two agents create internal representations and categories and share signs (i.e. names of objects) from raw visual images observed from different viewpoints. The model has been validated on MNIST and Fruits 360 datasets. Experimental findings demonstrate that categories are formed from real images observed by agents, and signs are correctly shared across agents by successfully utilizing both of the observations of agents via the MH naming game. Furthermore, scholars verified that visual images were recalled from signs uttered by agents. Notably, emergent communication without supervision and reward feedback improved the performance of the unsupervised representation learning of agents. \u00c2\u00a9 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.",
      "authors": [
        "Taniguchi, T.",
        "Yoshida, Y.",
        "Matsui, Y.",
        "Le Hoang, N.",
        "Taniguchi, A.",
        "Hagiwara, Y."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/01691864.2023.2260856",
      "keywords": [
        "representation learning",
        "Symbol emergence",
        "deep generative models",
        "emergent communication"
      ],
      "number_of_pages": 17,
      "pages": "1266-1282",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01691864",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advanced Robotics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Emergent communication through Metropolis-Hastings naming game with deep generative models",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173914839&origin=inward"
      ]
    },
    {
      "abstract": "Conversational query production is an emerging fundamental task for the dialogue system, where search queries are generated to explore the vast and continually updating knowledge from a search engine. To accelerate this line of research, previous studies have released several datasets with human-annotated search queries. However, the limited annotations still can not cover conversations of various domains. To solve this challenge, we propose a novel domain adaptation framework. It is inspired by a weakly supervised learning algorithm from previous work (Wang et al., 2023b) that guides a model using reinforcement learning with BM25 scores as feedback. Though effective, it is fragile facing noisy content on webpages from a commercial search engine and variance in conversations because of ignoring deep semantic information of dialogue contexts. Thus, we improve the algorithm by taking the advance of retrieval-augmented generation (RAG) and exploring several practical techniques such as knowledge distillation for stable training. We conduct experiments in multiple settings across different languages. Guided by the RAG model feedback, our model is more robust and performs significantly better especially in a more challenging setting over strong baselines. \u00c2\u00a9 2023 Association for Computational Linguistics.",
      "authors": [
        "Wang, A.",
        "Song, L.",
        "Xu, G.",
        "Su, J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 13,
      "pages": "9129-9141",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798891760615",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Findings of the Association for Computational Linguistics: EMNLP 2023"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Domain Adaptation for Conversational Query Production with the RAG Model Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180826579&origin=inward"
      ]
    },
    {
      "abstract": "Integration of human feedback plays a key role in improving the learning capabilities of intelligent systems. This comparative study delves into the performance, robustness, and limitations of imitation learning compared to traditional reinforcement learning methods within these systems. Recognizing the value of human-in-the-loop feedback, we investigate the influence of expert guidance and suboptimal demonstrations on the learning process. Through extensive experimentation and evaluations conducted in a pre-existing simulation environment using the Unity platform, we meticulously analyze the effectiveness and limitations of these learning approaches. The insights gained from this study contribute to the advancement of human-centered artificial intelligence by highlighting the benefits and challenges associated with the incorporation of human feedback into the learning process. Ultimately, this research promotes the development of models that can effectively address complex real-world problems. \u00c2\u00a9 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",
      "authors": [
        "Gomaa, A.",
        "Mahdy, B."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Learning From Demonstrations",
        "Personalization",
        "Imitation Learning",
        "Human-in-the-loop Learning",
        "Reinforcement Learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "16130073",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CEUR Workshop Proceedings"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Unveiling the Role of Expert Guidance: A Comparative Analysis of User-centered Imitation Learning and Traditional Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177042098&origin=inward"
      ]
    },
    {
      "abstract": "Generative adversarial imitation learning (GAIL) - a general model-free imitation learning method, allows robots to directly learn policies from expert trajectories in large environments. However, GAIL shares the limitation of other imitation learning methods that they can seldom surpass the performance of demonstrations. In this paper, to address the limit of GAIL, we propose GAN-based interactive reinforcement learning (GAIRL) from demonstrations and human evaluative feedback, by combining the advantages of GAIL and interactive reinforcement learning. We test GAIRL in six physics-based control tasks, ranging from simple low-dimensional control tasks - Cart Pole, Mountain Car and Lunar Lander, to difficult high-dimensional tasks - Inverted Double Pendulum, Hopper and HalfCheetah. Our results suggest that, the GAIRL agent can generally surpass the performance of demonstrations in both low-dimensional and high-dimensional tasks and get an optimal or close to optimal policy. \u00c2\u00a9 2023 IEEE.",
      "authors": [
        "Jie Huang",
        "Jiangshan Hao",
        "Rongshun Juan",
        "Randy Gomez",
        "Keisuke Nakamura",
        "Guangliang Li"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRA48891.2023.10160939",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "4991-4998",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-2366-5",
        "issn": "10504729",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "GAN-Based Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85168653057&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160939"
      ]
    },
    {
      "abstract": "It aims to explore ChatGPT's (GPT-3.5 version) functionalities, including reinforcement learning, diverse applications, and limitations. ChatGPT is an AI chatbot powered by OpenAI's Generative Pre-trained Transformer (GPT) model. The chatbot's applications span education, programming, content generation, and more, demonstrating its versatility. ChatGPT can enhance education by creating assignments and offering personalized feedback, as shown by its notable performance in medical exams and the USMLE. However, concerns include plagiarism, reliability, and educational disparities. It aids in various research tasks, from design to writing, and has shown proficiency in summarizing and suggesting titles. Its use in scientific writing and language translation is promising, but professional oversight is needed for accuracy and originality. It assists in programming tasks like writing code, debugging, and guiding installation and updates. It offers diverse applications, from cheering up individuals to generating creative content like essays, news articles, and business plans. ChatGPT, unlike search engines, provides interactive, generative responses and understands context, making it more akin to human conversation. These characteristics are contrasted with conventional search engines' keyword-based, non-interactive nature. ChatGPT has limitations, such as potential bias, dependence on outdated data, and revenue generation challenges. Despite these issues, ChatGPT is seen as a transformative AI tool poised to redefine the future of generative technology. In conclusion, advancements in AI, like ChatGPT, are altering how knowledge is acquired and applied, marking a shift from search engines to creativity engines. This transformation highlights the increasing importance of AI literacy and the ability to utilize AI in various life aspects effectively.",
      "authors": [
        "Tae Won Kim"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3352/jeehp.2023.20.38",
      "keywords": [
        "Writing",
        "Reproducibility of results",
        "Artificial intelligence",
        "Search engine",
        "Literacy"
      ],
      "number_of_pages": 1,
      "pages": "38",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1975-5937",
        "publisher": "Korea Health Personnel Licensing Examination Institute",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of educational evaluation for health professions"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Application of artificial intelligence chatbots, including ChatGPT, in education, scholarly work, programming, and content generation and its prospects: a narrative review",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180819924&origin=inward"
      ]
    },
    {
      "abstract": "As we progress deeper into the digital age, the robust development and application of advanced artificial intelligence (AI) technology, specifically generative language models like ChatGPT (OpenAI), have potential implications in all sectors including medicine. This viewpoint article aims to present the authors\u2019 perspective on the integration of AI models such as ChatGPT in clinical medicine and medical education. The unprecedented capacity of ChatGPT to generate human-like responses, refined through Reinforcement Learning with Human Feedback, could significantly reshape the pedagogical methodologies within medical education. Through a comprehensive review and the authors\u2019 personal experiences, this viewpoint article elucidates the pros, cons, and ethical considerations of using ChatGPT within clinical medicine and notably, its implications for medical education. This exploration is crucial in a transformative era where AI could potentially augment human capability in the process of knowledge creation and dissemination, potentially revolutionizing medical education and clinical practice. The importance of maintaining academic integrity and professional standards is highlighted. The relevance of establishing clear guidelines for the responsible and ethical use of AI technologies in clinical medicine and medical education is also emphasized.",
      "authors": [
        "Wong, R.S.-Y.",
        "Ming, L.C.",
        "Ali, R.A.R."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.2196/47274",
      "keywords": [
        "artificial intelligence",
        "clinical research",
        "AI",
        "large language model",
        "ethical considerations",
        "OpenAI",
        "ChatGPT"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMIR Publications Inc., Toronto, Canada",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "JMIR Medical Education"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "The Intersection of ChatGPT, Clinical Medicine, and Medical Education",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179160271&origin=inward"
      ]
    },
    {
      "abstract": "We investigate an approach for enabling a reinforcement learning agent to learn about dangerous states or constraints from stop-feedback preventing the agent from taking any further, potentially dangerous, actions. Such feedback could be provided by human supervisors overseeing the RL agent's behavior while carrying out some complex tasks. To enable the RL agent to learn from the supervisor's feedback, we propose a probabilistic model for approximating how the supervisor's feedback could have been generated and consider a Bayesian approach for inferring dangerous states. We evaluated our approach using an OpenAI Safety Gym environment and demonstrated that our agent can effectively infer the imposed safety constraints. Furthermore, we conducted a user study to validate our human-inspired feedback model and to obtain insights into the human provision of stop-feedback. \u00c2\u00a9 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
      "authors": [
        "Poletti, S.",
        "Testolin, A.",
        "Tschiatschek, S."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "safety",
        "reinforcement learning",
        "constraint learning",
        "human feedback"
      ],
      "number_of_pages": 3,
      "pages": "2328-2330",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Learning Constraints From Human Stop-Feedback in Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171297786&origin=inward"
      ]
    },
    {
      "abstract": "Synthetic Aperture Radar (SAR) automatic target recognition (ATR) is a key technique for SAR image analysis in military activities. Accurate SAR ATR can promote command and decision-making. In this work, we propose a novel SAR ATR framework with human-in-the-loop. The framework consists of a Reinforcement Learning (RL) Agent, which is followed by a GNN-based classifier. The RL Agent is capable of learning from human feedback to identify the region of target (RoT) in the SAR image. The RoT is then used to construct the input graph for the GNN classifier to perform target classification. By learning from human feedback, the RL Agent can focus on the RoT and filter out irrelevant and distracting signals in the input SAR images. We evaluate the proposed framework on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset. The results show that incorporating human feedback can improve classification accuracy. By visualizing the results, we observe that the RL Agent can effectively reduce irrelevant SAR signals in the input SAR images after learning from human feedback. ",
      "authors": [
        "Bingyi Zhang",
        "Sasindu Wijeratne",
        "Rajgopal Kannan",
        "Viktor Prasanna",
        "Carl Busart"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1117/12.2663728",
      "keywords": [
        "human-in-the-loop",
        "graph neural network",
        "automatic target recognition",
        "Synthetic aperture radar"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781510662087",
        "issn": "0277786X",
        "publisher": "SPIE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications IV"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Graph neural network based SAR automatic target recognition with human-in-the-loop",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167458515&origin=inward",
        "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12520/125200J/Graph-neural-network-based-SAR-automatic-target-recognition-with-human/10.1117/12.2663728.pdf"
      ]
    },
    {
      "abstract": "We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the K-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and Max Entropy Inverse Reinforcement Learning, and provide the first sample complexity bound for both problems. \u00c2\u00a9 2023 Proceedings of Machine Learning Research. All rights reserved.",
      "authors": [
        "Zhu, B.",
        "Jordan, M.I.",
        "Jiao, J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 31,
      "pages": "43037-43067",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174396309&origin=inward"
      ]
    },
    {
      "abstract": "Autonomously learning diverse behaviors without an extrinsic reward signal has been a problem of interest in reinforcement learning. However, the nature of learning in such mechanisms is unconstrained, often resulting in the accumulation of several unusable, unsafe or misaligned skills. In order to avoid such issues and ensure the discovery of safe and human-aligned skills, it is necessary to incorporate humans into the unsupervised training process, which remains a largely unexplored research area. In this work, we propose Controlled diversity with Preference (CDP), a novel, collaborative human-guided mechanism for an agent to learn a set of skills that is diverse as well as desirable. The key principle is to restrict the discovery of skills to those regions that are deemed to be desirable as per a preference model trained using human preference labels on trajectory pairs. We evaluate our approach on 2D navigation and Mujoco environments and demonstrate the ability to discover diverse, yet desirable skills. \u00c2\u00a9 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
      "authors": [
        "Hussonnois, M.",
        "Karimpanal, T.G.",
        "Rana, S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Human Preferences",
        "Reinforcement Learning",
        "Skill Diversity"
      ],
      "number_of_pages": 9,
      "pages": "1135-1143",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Controlled Diversity with Preference: Towards Learning a Diverse Set of Desired Skills",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171279406&origin=inward"
      ]
    },
    {
      "abstract": "We present a framework for developing software agents via Machine Learning (ML) entitled Curriculum-Heavy  Accelerated Learning in a Competitive Environment (CHALICE). CHALICE is designed to train and deploy intelligent  agents capable of executing strategies for air-ground combat as embodied in AFRL\u2019s MIST turn-based wargame system.  Such agents can be used to suggest courses of action in real-time to operational planners and to provide an adversarial  opponent for evaluation of proposed courses of action. CHALICE uses state-of-the-art Deep Neural Networks (DNNs)  to represent the state of the environment and Deep Reinforcement Learning (DRL) to train each agent via repeated  feedback from outcomes of the MIST Stratagem game. Unlike recent DRL approaches for strategy games such as Go or  StarCraft [1] [2], CHALICE minimizes dependence on existing corpora of human gameplay and trains efficiently with  low computational resources and short convergence time (hours to days rather than weeks to months). Over the course of  four government-led competitions, CHALICE produced agents that continually improved their performance, resulting in  competitive play against human and automated opposing agents at relatively low training cost and time. In this paper, we  motivate the operational problem and technical challenges, provide an overview of our technical approach, elaborate on  our vision-based and graph-based DNN architecture design and agent training procedure, and present results from the  most recent Stratagem competition. We close with a discussion of future research recommendations.",
      "authors": [
        "Nicholas Pioch",
        "Lucas Sheldon",
        "Thomas Harris",
        "Matt Henry",
        "Andrew Spisak",
        "Mikayla Timm"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1117/12.2663299",
      "keywords": [
        "League play",
        "Strategy games",
        "Deep Reinforcement Learning",
        "Graph Convolutional Networks",
        "Convolutional Neural Networks"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781510662087",
        "issn": "0277786X",
        "publisher": "SPIE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications IV"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Curriculum-heavy reinforcement learning for multi-domain operations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85170645078&origin=inward",
        "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12538/1253819/Curriculum-heavy-reinforcement-learning-for-multi-domain-operations/10.1117/12.2663299.pdf"
      ]
    },
    {
      "abstract": "The proceedings contain 431 papers. The special focus in this conference is on Intelligent Robotics and Applications. The topics include: A Self-loading Suction Cup Driven by Resonant-Impact Dielectric Elastomer Artificial Muscles; model-free Adaptive Control of Dielectric Elastomer Actuator; modeling and\u00c2 Design Optimization of\u00c2 a\u00c2 Pre-stretched Rolled Dielectric Elastomer Actuator; structural Dynamics Modeling with Modal Parameters and\u00c2 Excitation\u00c2 Decoupling Method Based on Energy\u00c2 Distribution; force Sensor-Based Linear Actuator Stiffness Rendering Control; design, Modeling and\u00c2 Control of\u00c2 a\u00c2 Dielectric Elastomer Actuated Micro-positioning Stage; tension Distribution Algorithm of Flexible Cable Redundant Traction for Stable Motion of Air-Bearing Platform; Research on High-Frequency Motion Control of Voice Coil Motors Based on Fuzzy PID; Feedback Linearization with Improved ESO for Quadrotor Attitude Control; ultraviolet Curable Materials for 3D Printing Soft Robots: From Hydrogels to Elastomers and Shape Memory Polymers; Design and Analysis of a High-performance Flexible Joint Actuator Based on the Peano-HASEL Actuator; walking Stability Analysis of Biped Robot Based on Actuator Response Characteristics; design of an Actuator for Biped Robots Based on the Axial Flux Motor; omnidirectional Walking Realization of\u00c2 a\u00c2 Biped Robot; reinforcement Learning and\u00c2 Sim-to-Real Method of\u00c2 Dual-Arm Robot for\u00c2 Capturing Non-Cooperative Dynamic Targets; control of\u00c2 the\u00c2 Wheeled Bipedal Robot on\u00c2 Roads with\u00c2 Large and\u00c2 Unknown Inclination; nonsmooth Dynamic Modeling of\u00c2 a\u00c2 Humanoid Robot with\u00c2 Parallel Mechanisms; human-Like Dexterous Manipulation for\u00c2 the\u00c2 Anthropomorphic Hand-Arm Robotic System via\u00c2 Teleoperation; design of\u00c2 a\u00c2 Compact Anthropomorphic Robotic Hand with\u00c2 Hybrid Linkage and\u00c2 Direct Actuation; application of Compliant Control in Position-Based Humanoid Robot; design and Grasping Experiments of a Three-Branch Dexterous Soft Gripper; fast Walking of Position-Controlled Biped Robot Based on Whole-Body Compliance Control; whole Body Balance Control for Bipedal Robots Based on Virtual Model Control.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "16th International Conference on Intelligent Robotics and Applications, ICIRA 2023",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175819363&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175972782&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175943457&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175943768&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176018164&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176009520&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175979526&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175969896&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175949340&origin=inward"
      ]
    },
    {
      "abstract": "ChatGPT, an artificial intelligence generated content (AIGC) model developed by OpenAI, has attracted world-wide attention for its capability of dealing with challenging language understanding and generation tasks in the form of conversations. This paper briefly provides an overview on the history, status quo and potential future development of ChatGPT, helping to provide an entry point to think about ChatGPT. Specifically, from the limited open-accessed resources, we conclude the core techniques of ChatGPT, mainly including large-scale language models, in-context learning, reinforcement learning from human feedback and the key technical steps for developing Chat-GPT. We further analyze the pros and cons of ChatGPT and we rethink the duality of ChatGPT in various fields. Although it has been widely acknowledged that ChatGPT brings plenty of opportunities for various fields, mankind should still treat and use ChatGPT properly to avoid the potential threat, e.g., academic integrity and safety challenge. Finally, we discuss several open problems as the potential development of ChatGPT. \u00c2\u00a9 2014 Chinese Association of Automation.",
      "authors": [
        "Tianyu Wu",
        "Shizhu He",
        "Jingping Liu",
        "Siqi Sun",
        "Kang Liu",
        "Qing-Long Han",
        "Yang Tang"
      ],
      "categories": null,
      "citations": 69,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/JAS.2023.123618",
      "keywords": [
        "GPT-3",
        "GPT-4",
        "AIGC",
        "human feedback",
        "large language models",
        "ChatGPT"
      ],
      "number_of_pages": 15,
      "pages": "1122-1136",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2329-9274",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE/CAA Journal of Automatica Sinica"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10113601",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85158874443&origin=inward"
      ]
    },
    {
      "abstract": "Data-driven approaches like reinforcement learning (RL) allow a model-free, self-adaptive controller design that enables a fast and largely automatic controller development process with minimum human effort. While it was already shown in various power electronic applications that the transient control behavior for complex systems can be sufficiently handled by RL, the challenge of non-vanishing steady-state control errors remains, which arises from the usage of control policy approximations and finite training times. This is a crucial problem in power electronic applications which require steady-state control accuracy, e.g., voltage control of grid-forming inverters or accurate current control in motor drives. To overcome this issue, an integral action state augmentation for RL controllers is introduced that mimics an integrating feedback and does not require any expert knowledge, leaving the approach model free. Therefore, the RL controller learns how to suppress steady-state control deviations more effectively. The benefit of the developed method both for reference tracking and disturbance rejection is validated for two voltage source inverter control tasks targeting islanded microgrid as well as traction drive applications. In comparison to a standard RL setup, the suggested extension allows to reduce the steady-state error by up to 52% within the considered validation scenarios. \u00c2\u00a9 2013 IEEE.",
      "authors": [
        "Daniel Weber",
        "Maximilian Schenke",
        "Oliver Wallscheid"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2023.3297274",
      "keywords": [
        "disturbance rejection",
        "reinforcement learning",
        "power electronic systems",
        "Control",
        "reference tracking",
        "steady-state error"
      ],
      "number_of_pages": 13,
      "pages": "76524-76536",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Steady-State Error Compensation for Reinforcement Learning-Based Control of Power Electronic Systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165396854&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10188929"
      ]
    },
    {
      "abstract": "An optimal energy scheduling strategy for integrated energy systems (IESs) can effectively improve the energy utilization efficiency and reduce carbon emissions. Due to the large-scale state space of IES caused by uncertain factors, it would be beneficial for the model training process to formulate a reasonable state-space representation. Thus, a condition knowledge representation and feedback learning framework based on contrastive reinforcement learning is designed in this study. Considering that different state conditions would bring inconsistent daily economic costs, a dynamic optimization model based on deterministic deep policy gradient is established, so that the condition samples can be partitioned according to the preoptimized daily costs. In order to represent the overall conditions on a daily basis and constrain the uncertain states in the IES environment, the state-space representation is constructed by a contrastive network considering the time dependence of variables. A Monte-Carlo policy gradient-based learning architecture is further proposed to optimize the condition partition and improve the policy learning performance. To verify the effectiveness of the proposed method, typical load operation scenarios of an IES are used in our simulations. The human experience strategies and state-of-the-art approaches are selected for comparisons. The results validate the advantages of the proposed approach in terms of cost effectiveness and ability to adapt in uncertain environments.",
      "authors": [
        "Tianyu Wang",
        "Jun Zhao",
        "Henry Leung",
        "Wei Wang"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TCYB.2023.3234077",
      "keywords": [
        "reinforcement learning (RL)",
        "dynamic optimization",
        "integrated energy system (IES)",
        "Condition knowledge representation",
        "deep contrastive network"
      ],
      "number_of_pages": 11,
      "pages": "1-11",
      "publication": {
        "category": "Journal",
        "cite_score": 22.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2168-2275",
        "publisher": "IEEE Advancing Technology for Humanity",
        "sjr": 5.365,
        "snip": 4.286,
        "subject_areas": [
          "Software",
          "Information Systems",
          "Computer Science Applications",
          "Human-Computer Interaction",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "IEEE Transactions on Cybernetics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A Condition Knowledge Representation and Feedback Learning Framework for Dynamic Optimization of Integrated Energy Systems",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10037214",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148448988&origin=inward"
      ]
    },
    {
      "abstract": "The process of learning any new technology requires acquiring the best knowledge about the information of that technology. The better the knowledge humans get about digital technology, the more they become efficient in implementing technological development. In developing the musical rhythm and tuning, the application of programming technologies helps improve the quality. In constructing networking sites and sensing technologies, algorithmic learning processes help in effective development. This development occurs by making the systematic process of transforming a data processing language and data interpreter. Thus, it helps in performing programming effectively in the present as well as future purposes. Therefore, it reflects all the benefits of machine learning. Thus, the preference for machine learning increases technological impact. This development of the programming used in the computer makes humans learn about something easily and get the best information. The effectiveness of the technological development by the algorithm used in the data processing implements the best way to improve the technological language transformation from human language to computer operating language. There is a transnational perspective of the average beat commonness of each part of the music. \u201cReinforcement algorithms-based learning\u201d incorporated with sensor networks has proposed compelling opportunities for improving \u201cmusic improvisation\u201d and interpretation.",
      "authors": [
        "Xiaoling Hu"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.12694/scpe.v24i3.2390",
      "keywords": [
        "Music Improvisation",
        "Sensor Networks",
        "Learning-Based Algorithms",
        "Machine Learning",
        "Internet of Things"
      ],
      "number_of_pages": 12,
      "pages": "499-510",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18951767",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Scalable Computing"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Reinforcement Learning-based Algorithms for Music Improvisation and Arrangement in Sensor Networks for the Internet of Things",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172330856&origin=inward"
      ]
    },
    {
      "abstract": "Goal-oriented dialog systems aim to efficiently and accurately exchange information with people using natural language. A goal-oriented dialog policy is used for suggesting language actions for such dialog systems. Reinforcement learning (RL) has been used for computing dialog policies from the experience of language-based interaction. Learning efficiency is particularly important in dialog policy learning, due to the considerable cost of interacting with human users, and the potentially very poor user experience from low-quality conversations. In this article, we develop deep RL algorithms to improve the efficiency of dialog policy learning. Our contribution is threefold, aiming at the central goal of improving the efficiency of dialog policy learning. First, we present a novel 'hindsight' approach to make use of unsuccessful dialog instances to provide the dialog learning agent with extra positive feedback. Second, we introduce user modeling, and enable the dialog agent to learn from simulated interaction experience. Third, we have developed a metalearning algorithm that enables the dialog agent to adaptively learn from simulated users and hindsight experience at the same time. The threefold contribution altogether, for the first time, enables our dialog agent outperforming a number of state-of-the-art dialog policy learning methods, as demonstrated via our experimental results. \u00c2\u00a9 2016 IEEE.",
      "authors": [
        "Keting Lu",
        "Yan Cao",
        "Xiaoping Chen",
        "Shiqi Zhang"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TCDS.2021.3061121",
      "keywords": [
        "reinforcement learning (RL)",
        "Deep learning",
        "metalearning",
        "dialog systems",
        "intrinsically motivated learning"
      ],
      "number_of_pages": 14,
      "pages": "395-408",
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2379-8939",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.173,
        "snip": 1.584,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Cognitive and Developmental Systems"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Efficient Dialog Policy Learning With Hindsight, User Modeling, and Adaptation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101752151&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9360657"
      ]
    },
    {
      "abstract": "In this paper, we present HuatuoGPT, a Large Language Model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both distilled data from ChatGPT and real-world data from doctors in the supervised fine-tuning stage. This is not only because purely using ChatGPT-distilled data might cause 'model collapse', but also because real-world data from doctors would be complementary to ChatGPT-distilled data. The responses from ChatGPT are usually detailed, well-presented, fluent, and instruction-followed, but it cannot perform like a doctor in many aspects, e.g. for interactive diagnosis. Therefore, the extra doctors' data could tame a distilled language model to perform like doctors. To synergize the strengths of both data sources, we introduce RLMF (Reinforcement Learning from Mixed Feedback) where a reward model is trained to align the language model with the merits that both sources (ChatGPT and doctors) bring. Experimental results (in GPT-4 evaluation, human evaluation, and medical benchmark datasets) demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs. It is worth noting that by using additional real-world data and RLMF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model (i.e., ChatGPT) in most cases. \u00c2\u00a9 2023 Association for Computational Linguistics.",
      "authors": [
        "Zhang, H.",
        "Chen, J.",
        "Jiang, F.",
        "Yu, F.",
        "Chen, Z.",
        "Li, J.",
        "Chen, G.",
        "Wu, X.",
        "Zhang, Z.",
        "Xiao, Q.",
        "Wan, X.",
        "Wang, B.",
        "Li, H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 27,
      "pages": "10859-10885",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798891760615",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Findings of the Association for Computational Linguistics: EMNLP 2023"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "HuatuoGPT, Towards Taming Language Models To Be a Doctor",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85183308763&origin=inward"
      ]
    },
    {
      "abstract": "In recent times, our research has focused on training large language models and exploring their potential. With the emergence of ChatGPT, it has been demonstrated that it is possible to fine-tune language models in a task-agnostic way. The success of ChatGPT is attributed to the\r\nreinforcement learning method, which integrates human feedback into the\r\nlanguage model fine-tuning process. As a part of our research, we initially\r\nadapted the method of reinforcement learning for a specific task, which is machine translation, respectively. In this paper, we propose a novel approach to\r\nenhance machine translation with reinforcement learning and quality estimation methods. Our proposed approach uses reinforcement learning to learn\r\nto adjust the machine translation output based on quality estimation feedback, with the goal of improving the overall translation quality. We evaluated\r\nour approach on the WMT09 dataset for English-Hungarian language pair.\r\nWe conducted an analysis to show how our approach improves the quality of\r\nmachine translation output. Our approach offers a promising avenue for enhancing the quality of machine translation and demonstrates the potential of\r\nutilizing reinforcement learning to improve other natural language processing\r\ntasks.",
      "authors": [
        "Yang, Z.G.",
        "Laki, L.J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.33039/ami.2023.08.008",
      "keywords": [
        "machine translation",
        "reinforcement learning",
        "mT5",
        "quality estimation"
      ],
      "number_of_pages": 9,
      "pages": "182-190",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17875021",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Annales Mathematicae et Informaticae"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Enhancing machine translation with quality estimation and reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176798382&origin=inward"
      ]
    },
    {
      "abstract": "Playtesting is an essential step in the game design process. Game designers use the feedback from playtests to refine their designs. Game designers may employ procedural personas to automate the playtesting process. In this article, we present two approaches to improve automated playtesting. First, we propose developing persona, which allows a persona to progress to different goals. In contrast, the procedural persona is fixed to a single goal. Second, a human playtester knows which paths she has tested before, and during the consequent tests, she may test different paths. However, reinforcement learning (RL) agents disregard these previous paths. We propose a novel methodology that we refer to as alternative path finder (APF). We train APF with previous paths and employ APF during the training of an RL agent. APF modulates the reward structure of the environment, while preserving the agent's goal. When evaluated, the agent generates a different trajectory that achieves the same goal. We use the general video game artificial intelligence and VizDoom frameworks to test our proposed methodologies. We use proximal policy optimization RL agent during experiments. First, we compare the playtest data generated by developing and procedural persona. Our experiments show that developing persona provides better insight into the game and how different players would play. Second, we present the alternative paths found using APF and argue why traditional RL agents cannot learn those paths. \u00c2\u00a9 2018 IEEE.",
      "authors": [
        "Sinan Ariyurek",
        "Elif Surer",
        "Aysu Betin-Can"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TG.2022.3165882",
      "keywords": [
        "reinforcement learning (RL)",
        "player modeling",
        "play persona",
        "Automated playtesting"
      ],
      "number_of_pages": 12,
      "pages": "348-359",
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2475-1510",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 0.598,
        "snip": 1.444,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "IEEE Transactions on Games"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Playtesting: What is Beyond Personas",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754697",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128683205&origin=inward"
      ]
    },
    {
      "abstract": "[No abstract available]",
      "authors": [
        "Wang, Rubin",
        "Su, Jianzhong"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyt.2023.1230587",
      "keywords": [
        "Neuroscience",
        "Computational models",
        "Cognitive Function",
        "Neurodynamics",
        "mental disorder"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-0640",
        "publisher": "Frontiers Media SA",
        "sjr": 1.222,
        "snip": 1.265,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "Frontiers in Psychiatry"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Editorial: Computational models of brain in cognitive function and mental disorder",
      "urls": [
        "https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2023.1230587/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85179308547&origin=inward"
      ]
    },
    {
      "abstract": "Deployment of Reinforcement Learning (RL) algorithms for robotics applications in the real world requires ensuring the safety of the robot and its environment. Safe Robot RL (SRRL) is a crucial step towards achieving human-robot coexistence. In this paper, we envision a human-centered SRRL framework consisting of three stages: safe exploration, safety value alignment, and safe collaboration. We examine the research gaps in these areas and propose to leverage interactive behaviors for SRRL. Interactive behaviors enable bi-directional information transfer between humans and robots, such as conversational robot ChatGPT [61]. We argue that interactive behaviors need further attention from the SRRL community. We discuss four open challenges related to the robustness, efficiency, transparency, and adaptability of SRRL with interactive behaviors.",
      "authors": [
        "Gu, Shangding",
        "Kshirsagar, Alap",
        "Du, Yali",
        "Chen, Guang",
        "Peters, Jan",
        "Knoll, Alois"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2023.1280341",
      "keywords": [
        "safe exploration",
        "bi-direction information",
        "safe collaboration",
        "interactive behaviors",
        "value alignment"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "A human-centered safe robot reinforcement learning framework with interactive behaviors",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177694230&origin=inward"
      ]
    },
    {
      "abstract": "<p>The cognitive impact of psychological trauma can manifest as a range of post-traumatic stress symptoms that are often attributed to impairments in learning from positive and negative outcomes, aka reinforcement learning. Research on the impact of trauma on reinforcement learning has mainly been inconclusive. This study aimed to circumscribe the impact of psychological trauma on reinforcement learning in the context of neural response in time and frequency domains. Two groups of participants were tested - those who had experienced psychological trauma and a control group who had not - while they performed a probabilistic classification task that dissociates learning from positive and negative feedback during a magnetoencephalography (MEG) examination. While the exposure to trauma did not exhibit any effects on learning accuracy or response time for positive or negative feedback, MEG cortical activity was modulated in response to positive feedback. In particular, the medial and lateral orbitofrontal cortices (mOFC and lOFC) exhibited increased activity, while the insular and supramarginal cortices showed decreased activity during positive feedback presentation. Furthermore, when receiving negative feedback, the trauma group displayed higher activity in the medial portion of the superior frontal cortex. The timing of these activity changes occurred between 160 and 600 ms post feedback presentation. Analysis of the time-frequency domain revealed heightened activity in theta and alpha frequency bands (4\u201310\u2009Hz) in the lOFC in the trauma group. Moreover, dividing the two groups according to their learning performance, the activity for the non-learner subgroup was found to be lower in lOFC and higher in the supramarginal cortex. These differences were found in the trauma group only. The results highlight the localization and neural dynamics of feedback processing that could be affected by exposure to psychological trauma. This approach and associated findings provide a novel framework for understanding the cognitive correlates of psychological trauma in relation to neural dynamics in the space, time, and frequency domains. Subsequent work will focus on the stratification of cognitive and neural correlates as a function of various symptoms of psychological trauma. Clinically, the study findings and approach open the possibility for neuromodulation interventions that synchronize cognitive and psychological constructs for individualized treatment.</p>",
      "authors": [
        "Sawalma, Abdulrahman S.",
        "Kiefer, Christian M.",
        "Boers, Frank",
        "Shah, N. Jon",
        "Khudeish, Nibal",
        "Neuner, Irene",
        "Herzallah, Mohammad M.",
        "Dammers, J\u00fcrgen"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2023.1172549",
      "keywords": [
        "Magnetoencephalography (MEG)",
        "PTSD",
        "feedback-based learning",
        "Spatio-temporal cluster permutation test",
        "Trauma"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "The effects of trauma on feedback processing: an MEG study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85177190910&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1172549/pdf"
      ]
    },
    {
      "abstract": "<sec><title>Introduction</title><p>Pavlovian bias is an innate motivational tendency to approach rewards and remain passive in the face of punishment. The relative reliance on Pavlovian valuation has been found to increase when the perceived control over environmental reinforcers is compromised, leading to behavior resembling learned helplessness (LH).</p></sec><sec><title>Methods</title><p>Sixty healthy young adults underwent a Go-NoGo reinforcement learning task and received anodal high-definition transcranial direct current stimulation (HD-tDCS) over the medial prefrontal/dorsal anterior cingulate cortex in our randomized, double-blind, sham- controlled study. Furthermore, we evaluated changes in cue-locked mid-frontal theta power derived from simultaneous electroencephalography (EEG). We hypothesized that active stimulation would reduce Pavlovian bias during manipulation of outcome controllability, and the effect would be accompanied by stronger mid-frontal theta activity, representing arbitration between choice strategies in favor of instrumental relative to Pavlovian valuation.</p></sec><sec><title>Results</title><p>We found a progressive decrease in Pavlovian bias during and after loss of control over feedback. Active HD-tDCS counteracted this effect while not affecting the mid-frontal theta signal.</p></sec><sec><title>Discussion</title><p>The results were at odds with our hypotheses but also with previous findings reporting LH-like patterns during and after loss of control without brain stimulation. The discrepancy may be related to different protocols for the controllability manipulation. We argue that the subjective evaluation of task controllability is crucial in mediating the balance between Pavlovian and instrumental valuation during reinforcement learning and that the medial prefrontal/dorsal anterior cingulate cortex is a key region in this respect. These findings have implications for understanding the behavioral and neural underpinnings of LH in humans.</p></sec>",
      "authors": [
        "Sedlinsk\u00e1, Terezie",
        "Bolte, Lara",
        "Mels\u00e6ter, Eirik",
        "Mittner, Matthias",
        "Csifcs\u00e1k, G\u00e1bor"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyt.2023.1164208",
      "keywords": [
        "controllability",
        "Medial prefrontal cortex",
        "learned helplessness",
        "Pavlovian bias",
        "mid-frontal theta power",
        "tDCS"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-0640",
        "publisher": "Frontiers Media SA",
        "sjr": 1.222,
        "snip": 1.265,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "Frontiers in Psychiatry"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Transcranial direct-current stimulation enhances Pavlovian tendencies during intermittent loss of control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159776776&origin=inward",
        "https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2023.1164208/pdf"
      ]
    },
    {
      "abstract": "ASD may be caused by a combination of genetic and environmental factors, including gene mutations and exposure to toxins. People with ASD may also have trouble forming social relationships, have difficulty with communication and language, and struggle with sensory sensitivity. These difficulties can range from mild to severe and can affect a person's ability to interact with the world around them. Autism spectrum disorder (ASD) is a developmental disorder that affects people in different ways. But early detection of ASD in a child is a good option for parents to start corrective therapies and treatment. They can take action to reduce the ASD symptoms in their child. The proposed work is the detection of ASD in a child using a parent\u00e2\u0080\u0099s dialog. The most popular Bert model and recent ChatGPT have been utilized to analyze the sentiment of each statement from parents for the detection of symptoms of ASD. The Bert model has been developed by the transformers which are the most popular in the natural language processing field whereas the ChatGPT model is a large language model (LLM). It is based on Reinforcement learning from human feedback (RLHF) that can able to generate the sentiment of the sentence, computer language codes, text paragraphs, etc. The sentiment analysis has been done on parents\u00e2\u0080\u0099 dialog using the Bert model and ChatGPT model. The data has been prepared from various Autism groups on social sites and other resources on the internet. The data has been cleaned and prepared to train the Bert model and ChatGPT model. The Bert model is able to detect the sentiment of each sentence from parents. Any positive sentiment detection means parents should be aware of their children. The proposed model has given 83 percent accuracy according to the prepared data.",
      "authors": [
        "Prasenjit Mukherjee",
        "Gokul R. S",
        "Sourav Sadhukhan",
        "Manish Godse",
        "Baisakhi Chakraborty"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.14569/IJACSA.2023.0141041",
      "keywords": [
        "ChatGPT model",
        "autism",
        "machine learning",
        "autism detection",
        "BERT model",
        "generative AI"
      ],
      "number_of_pages": 15,
      "pages": "382-396",
      "publication": {
        "category": "Journal",
        "cite_score": 2.1,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2158107X",
        "publisher": "Science and Information Organization",
        "sjr": 0.258,
        "snip": 0.512,
        "subject_areas": [
          "Computer Science (all)"
        ],
        "title": "International Journal of Advanced Computer Science and Applications (IJACSA)"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Detection of Autism Spectrum Disorder (ASD) from Natural Language Text using BERT and ChatGPT Models",
      "urls": [
        "http://thesai.org/Downloads/Volume14No10/Paper_41-Detection_of_Autism_Spectrum_Disorder.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85175352568&origin=inward"
      ]
    },
    {
      "abstract": "Designing reward functions is a difficult task in AI and robotics. The complex task of directly specifying all the desirable behaviors a robot needs to optimize often proves challenging for humans. A popular solution is to learn reward functions using expert demonstrations. This approach, however, is fraught with many challenges. Some methods require heavily structured models, for example, reward functions that are linear in some predefined set of features, while others adopt less structured reward functions that may necessitate tremendous amounts of data. Moreover, it is difficult for humans to provide demonstrations on robots with high degrees of freedom, or even quantifying reward values for given trajectories. To address these challenges, we present a preference-based learning approach, where human feedback is in the form of comparisons between trajectories. We do not assume highly constrained structures on the reward function. Instead, we employ a Gaussian process to model the reward function and propose a mathematical formulation to actively fit the model using only human preferences. Our approach enables us to tackle both inflexibility and data-inefficiency problems within a preference-based learning framework. We further analyze our algorithm in comparison to several baselines on reward optimization, where the goal is to find the optimal robot trajectory in a data-efficient way instead of learning the reward function for every possible trajectory. Our results in three different simulation experiments and a user study show our approach can efficiently learn expressive reward functions for robotic tasks, and outperform the baselines in both reward learning and reward optimization. \u00c2\u00a9 The Author(s) 2023.",
      "authors": [
        "B\u00c4\u00b1y\u00c4\u00b1k, E.",
        "Huynh, N.",
        "Kochenderfer, M.J.",
        "Sadigh, D."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/02783649231208729",
      "keywords": [
        "Gaussian processes",
        "human-robot interaction",
        "inverse reinforcement learning",
        "Reward learning",
        "preference-based learning",
        "active learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02783649",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Robotics Research"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Active preference-based Gaussian process regression for reward learning and optimization",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176268526&origin=inward"
      ]
    },
    {
      "abstract": "In order to solve the problems of telemedicine and human health monitoring, wireless body area network (WBAN) came into being. WBAN is a communication network based on human body, which is composed of network elements related to human body to monitor and maintain...",
      "authors": [
        "Chen, Jiaxuan",
        "Mu, Jiasong"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-981-99-2653-4_2",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "9-17",
      "publication": {
        "category": "Book",
        "cite_score": 0.6,
        "is_potentially_predatory": false,
        "isbn": "9789819926527",
        "issn": "18761100",
        "publisher": "Springer Verlag",
        "sjr": 0.147,
        "snip": 0.158,
        "subject_areas": [
          "Industrial and Manufacturing Engineering"
        ],
        "title": "International Conference in Communications, Signal Processing, and Systems "
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Routing Protocol Based on Q-Learning in WBAN",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161196795&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-981-99-2653-4_2.pdf"
      ]
    },
    {
      "abstract": "Recently, federated learning has become a powerful technique for medical image classification due to its ability to utilize datasets from multiple clinical clients while satisfying privacy constraints. However, there are still some obstacles in federated learning. Firstly, most existing methods directly average the model parameters collected by medical clients on the server, ignoring the specificities of the local models. Secondly, class imbalance is a common issue in medical datasets. In this paper, to handle these two challenges, we propose a novel specificity-aware federated learning framework that benefits from an Adaptive Aggregation Mechanism (AdapAM) and a Dynamic Feature Fusion Strategy (DFFS). Considering the specificity of each local model, we set the AdapAM on the server. The AdapAM utilizes reinforcement learning to adaptively weight and aggregate the parameters of local models based on their data distribution and performance feedback for obtaining the global model parameters. For the class imbalance in local datasets, we propose the DFFS to dynamically fuse the features of majority classes based on the imbalance ratio in the min-batch and collaborate the rest of features. We conduct extensive experiments on a dermoscopic dataset and a fundus image dataset. Experimental results show that our method can achieve state-of-the-art results in these two real-world medical applications. IEEE",
      "authors": [
        "Yue, G.",
        "Wei, P.",
        "Zhou, T.",
        "Song, Y.",
        "Zhao, C.",
        "Wang, T.",
        "Lei, B."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/JBHI.2023.3319516",
      "keywords": [
        "class imbalance",
        "Servers",
        "Federated learning",
        "medical image classification",
        "Adaptation models",
        "Data models",
        "Training",
        "Biomedical imaging",
        "Biological system modeling",
        "Image classification",
        "adaptive aggregation"
      ],
      "number_of_pages": 11,
      "pages": "1-11",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21682194",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Journal of Biomedical and Health Informatics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Specificity-aware Federated Learning with Dynamic Feature Fusion Network for Imbalanced Medical Image Classification",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85173373303&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 1.8,
        "is_potentially_predatory": false,
        "isbn": "9783031210891",
        "issn": "25111256",
        "publisher": "Springer, Cham",
        "sjr": 1.679,
        "snip": 2.74,
        "subject_areas": [
          "Mechanical Engineering",
          "Artificial Intelligence",
          "Computer Science Applications",
          "Engineering (miscellaneous)",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering",
          "Applied Mathematics"
        ],
        "title": "International Symposium on Experimental Robotics"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "15th Workshop on the Algorithmic Foundations of Robotics, WAFR 2022",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145200535&origin=inward"
      ]
    },
    {
      "abstract": "Brain-Machine Interfaces (BMIs) assist paralyzed people to brain control (BC) the neuro-prosthesis continuously moving in space. During the BC process, the subject imagines the movement of the real limb and adapts the brain activity according to the sensory feedback. The neural adaptation in the closed-loop control results in complex and changing brain signals. Simultaneously, the decoder interprets the time-varying functional mapping between neural activity and continuous trajectory. It is crucial and challenging to accurately and adaptively track the mapping to help the subject accomplish the BC task with a stable performance. Existing Kalman Filter (KF) based decoders achieve continuous trajectory control by linearly interpreting neural firing observations into self-evolving prosthetic states. However, the linear neural-state mapping might not accurately reflect the movement intention of the subject. In this paper, we propose a novel method that allows subjects to achieve continuous brain control efficiently and stably. The proposed method incorporates a kernel reinforcement learning method into a state-observation model to decode the nonlinearly neural observation into a continuous trajectory state. The state transition function ensures the continuity of the prosthetic state. The kernel reinforcement learning allows the quick adaptation of the nonlinear neural-movement mapping during the BC process. The proposed method is tested in an online brain control reaching task for rats. Compared with KF, our method achieved more successful trials, faster response time, shorter inter-trial time, and remained stable over days. These results demonstrate that the proposed method is an efficient tool to assist subjects in brain control tasks. \u00c2\u00a9 2001-2011 IEEE.",
      "authors": [
        "Zhang, X.",
        "Chen, S.",
        "Wang, Y."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNSRE.2023.3321756",
      "keywords": [
        "Brain-machine interface",
        "continuous brain control",
        "kernel reinforcement learning",
        "state-observation model",
        "stability over days"
      ],
      "number_of_pages": 10,
      "pages": "4125-4134",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15344320",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.26,
        "snip": 1.675,
        "subject_areas": [
          "Neuroscience (all)",
          "Biomedical Engineering",
          "Internal Medicine",
          "Rehabilitation"
        ],
        "title": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Kernel Reinforcement Learning-Assisted Adaptive Decoder Facilitates Stable and Continuous Brain Control Tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174860969&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mahmud S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "16130073",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CEUR Workshop Proceedings"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "REVEALE: Reward Verification and Learning Using Explanations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159373760&origin=inward"
      ]
    },
    {
      "abstract": "Successful Artificial Intelligence systems often require numerous labeled data to extract information from document images. In this paper, we investigate the problem of improving the performance of Artificial Intelligence systems in understanding document images,...",
      "authors": [
        "Nguyen, Bao-Sinh",
        "Le, Dung Tien",
        "Vu, Hieu M.",
        "Nguyen, Tuan-Anh D.",
        "Nguyen, Minh-Tien",
        "Le, Hung"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-981-99-1648-1_5",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "51-63",
      "publication": {
        "category": "Book",
        "cite_score": 1.0,
        "is_potentially_predatory": false,
        "isbn": "9789819927883",
        "issn": "18650929",
        "publisher": "Springer Science and Business Media Deutschland GmbH",
        "sjr": 0.194,
        "snip": 0.241,
        "subject_areas": [
          "Computer Science (all)",
          "Mathematics (all)"
        ],
        "title": "International Conference on Neural Information Processing"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Improving Document Image Understanding with\u00a0Reinforcement Finetuning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161697508&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-981-99-1648-1_5.pdf"
      ]
    },
    {
      "abstract": "Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting, and bi-gathering. To learn effective policies, we introduce appropriate reward functions for these tasks and propose a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a tactile sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world. \u00c2\u00a9 2016 IEEE.",
      "authors": [
        "Yijiong Lin",
        "Alex Church",
        "Max Yang",
        "Haoran Li",
        "John Lloyd",
        "Dandan Zhang",
        "Nathan F. Lepora"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/LRA.2023.3295991",
      "keywords": [
        "Force and tactile sensing",
        "reinforcement learning"
      ],
      "number_of_pages": 8,
      "pages": "5472-5479",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2377-3774",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Bi-Touch: Bimanual Tactile Manipulation With Sim-to-Real Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85165235717&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10184426"
      ]
    },
    {
      "abstract": "End-to-end driving based on reinforcement learning (RL) has emerged as a promising approach in autonomous driving, with the potential to surpass human drivers. However, the exploration and exploitation dilemma of RL leads to low sample efficiency and high computational costs to train an optimal driving model, and the environment feedback-oriented learning mode limits the model's ability to handle unseen traffic scenarios, especially when dealing with the highly varied tasks of urban driving. In this paper, we proposed a context-aware meta-RL framework with two-stage constrained adaptation for challenging urban driving. Firstly, the context-aware state representation enhanced by historical behavior is construct to improve the learning efficiency and robustness, where the most efficient context encoder is selected among four different forms. Next, the end-to-end meta-driving model with high generalization ability is built through the parallel rollouts of multiple tasks and simplified meta-training procedure. Then, a two-stage constrained adaptation strategy is designed to quickly transfer the meta model to new tasks while maintaining good performance, where the meta-training data are reused through the context-based propensity estimation to constrain the optimization objective of new tasks. Extensive experiments are performed in CARLA simulator with various urban scenarios, and the results validate the superiority of our proposed models in both learning efficiency and generalization compared to state-of-the-art algorithms. IEEE",
      "authors": [
        "Qi Deng",
        "Ruyang Li",
        "Qifu Hu",
        "Yaqian Zhao",
        "Rengang Li"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TVT.2023.3312495",
      "keywords": [
        "Task analysis",
        "Transformers",
        "Context modeling",
        "Adaptation models",
        "Context-aware state representation",
        "Behavioral sciences",
        "Constrained adaptation",
        "Robustness",
        "Autonomous vehicles",
        "Meta-reinforcement learning",
        "Autonomous driving"
      ],
      "number_of_pages": 15,
      "pages": "1-15",
      "publication": {
        "category": "Journal",
        "cite_score": 13.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1939-9359",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 2.802,
        "snip": 2.086,
        "subject_areas": [
          "Aerospace Engineering",
          "Applied Mathematics",
          "Electrical and Electronic Engineering",
          "Automotive Engineering"
        ],
        "title": "IEEE Transactions on Vehicular Technology"
      },
      "publication_date": "2023-01-01",
      "selected": null,
      "title": "Context-Aware Meta-RL with Two-Stage Constrained Adaptation for Urban Driving",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10242012",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171592302&origin=inward"
      ]
    },
    {
      "abstract": "Deep reinforcement learning has been successfully applied to the generation of goal-directed behavior in artificial agents. However, existing algorithms are often not designed to reproduce human-like behavior, which may be desired in many environments, such as human\u2013robot collaborations, social robotics and autonomous vehicles. Here we introduce a model-free and easy-to-implement deep reinforcement learning approach to mimic the stochastic behavior of a human expert by learning distributions of task variables from examples. As tractable use-cases, we study static and dynamic obstacle avoidance tasks for an autonomous vehicle on a highway road in simulation (Unity). Our control algorithm receives a feedback signal from two sources: a deterministic (handcrafted) part encoding basic task goals and a stochastic (data-driven) part that incorporates human expert knowledge. Gaussian processes are used to model human state distributions and to assess the similarity between machine and human behavior. Using this generic approach, we demonstrate that the learning agent acquires human-like driving skills and can generalize to new roads and obstacle distributions unseen during training.",
      "authors": [
        "Emuna, Ran",
        "Duffney, Rotem",
        "Borowsky, Avinoam",
        "Biess, Armin"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s00521-022-07947-2",
      "keywords": [
        "Gaussian processes",
        "Human driving policies",
        "Imitation learning",
        "Deep reinforcement learning"
      ],
      "number_of_pages": 14,
      "pages": "16791-16804",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0941-0643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2022-12-23",
      "selected": null,
      "title": "Example-guided learning of stochastic human driving policies using deep reinforcement learning",
      "urls": [
        "https://dl.acm.org/doi/10.1007/s00521-022-07947-2",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144651088&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s00521-022-07947-2.pdf"
      ]
    },
    {
      "abstract": "KAiPP prototype provides a reference for industrial implementation of the approach.",
      "authors": [
        "Chao Zhang",
        "Guanghui Zhou",
        "Jingjing Li",
        "Tianyu Qin",
        "Kai Ding",
        "Fengtian Chang"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.knosys.2022.110009",
      "keywords": [
        "Knowledge reuse",
        "Interaction recommendation",
        "Intelligent process planning",
        "Reinforcement learning",
        "Industry 4.0"
      ],
      "number_of_pages": 13,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 12.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0950-7051",
        "publisher": "Elsevier B.V.",
        "sjr": 2.065,
        "snip": 2.578,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Management Information Systems",
          "Information Systems and Management"
        ],
        "title": "Knowledge-Based Systems"
      },
      "publication_date": "2022-12-22",
      "selected": null,
      "title": "KAiPP: An interaction recommendation approach for knowledge aided intelligent process planning with reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144214280&origin=inward",
        "https://dl.acm.org/doi/10.1016/j.knosys.2022.110009"
      ]
    },
    {
      "abstract": "Reinforcement learning depends upon the integrity of emotional circuitry to establish associations between environmental cues, decisions, and positive or negative outcomes in order to guide behavior through experience. The emotional dysregulation characteristic of Major Depressive Disorder (MDD) may alter activity in frontal and limbic structures that are key to learning. Although reward and decision-making have been examined in MDD, the effects of depression on associative learning is less studied. We investigated whether emotional dysregulation observed with MDD would be related to abnormalities in learning-related brain activity as measured by fMRI. Also, we explored whether melancholic and atypical features were associated with altered brain activity. We conducted MRI scans on a 4T Varian MRI system in 10 individuals with major depressive disorder and 10 healthy subjects. We examined event-related brain activation during feedback-based learning task using AFNI for image processing and statistical analysis. We observed that MDD patients exhibited reduced activation in visual cortex but increased activation in cingulate and insular regions compared to healthy participants. Also, in relation to features of depressive subtypes, we observed that levels of activation in striatal, thalamic, and precuneus regions were negatively correlated with atypical characteristics. These results suggest that the effects of MDD, including emotional dysregulation, change the neural circuitry underlying associative learning, and these effects may depend upon subtype features of MDD.",
      "authors": [
        "Kustubayeva, Almira M.",
        "Nelson, Erik B.",
        "Smith, Michael L.",
        "Allendorfer, Jane B.",
        "Eliassen, James C."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fninf.2022.1028121",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5196",
        "publisher": "Frontiers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Neuroinformatics"
      },
      "publication_date": "2022-12-20",
      "selected": null,
      "title": "Functional MRI study of feedback-based reinforcement learning in depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145454103&origin=inward"
      ]
    },
    {
      "abstract": "Abstractive dialogue summarization has received increasing attention recently. Despite the fact that most of the current dialogue summarization systems are trained to maximize the likelihood of human-written summaries and have achieved significant results, there is still a huge gap in generating high-quality summaries as determined by humans, such as coherence and faithfulness, partly due to the misalignment in maximizing a single human-written summary. To this end, we propose to incorporate different levels of human feedback into the training process. This will enable us to guide the models to capture the behaviors humans care about for summaries. Specifically, we ask humans to highlight the salient information to be included in summaries to provide the local feedback , and to make overall comparisons among summaries in terms of coherence, accuracy, coverage, concise and overall quality, as the global feedback. We then combine both local and global feedback to fine-tune the dialog summarization policy with Reinforcement Learning. Experiments conducted on multiple datasets demonstrate the effectiveness and generalization of our methods over the state-of-the-art supervised baselines, especially in terms of human judgments.",
      "authors": [
        "Chen, Jiaao",
        "Dodda, Mohan",
        "Yang, Diyi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-12-19",
      "selected": null,
      "title": "Human-in-the-loop Abstractive Dialogue Summarization",
      "urls": [
        "http://arxiv.org/pdf/2212.09750.pdf",
        "http://arxiv.org/abs/2212.09750v1",
        "http://arxiv.org/pdf/2212.09750v1"
      ]
    },
    {
      "abstract": "Well-designed prompts can guide text-to-image models to generate amazing images. However, the performant prompts are often model-specific and misaligned with user input. Instead of laborious human engineering, we propose prompt adaptation, a general framework that automatically adapts original user input to model-preferred prompts. Specifically, we first perform supervised fine-tuning with a pretrained language model on a small collection of manually engineered prompts. Then we use reinforcement learning to explore better prompts. We define a reward function that encourages the policy to generate more aesthetically pleasing images while preserving the original user intentions. Experimental results on Stable Diffusion show that our method outperforms manual prompt engineering in terms of both automatic metrics and human preference ratings. Moreover, reinforcement learning further boosts performance, especially on out-of-domain prompts. The pretrained checkpoints are available at https://aka.ms/promptist. The demo can be found at https://aka.ms/promptist-demo.",
      "authors": [
        "Hao, Yaru",
        "Chi, Zewen",
        "Dong, Li",
        "Wei, Furu"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted by NeurIPS-23",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-12-19",
      "selected": null,
      "title": "Optimizing Prompts for Text-to-Image Generation",
      "urls": [
        "http://arxiv.org/pdf/2212.09611.pdf",
        "http://arxiv.org/pdf/2212.09611v2",
        "http://arxiv.org/abs/2212.09611v2"
      ]
    },
    {
      "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",
      "authors": [
        "Bai, Yuntao",
        "Kadavath, Saurav",
        "Kundu, Sandipan",
        "Askell, Amanda",
        "Kernion, Jackson",
        "Jones, Andy",
        "Chen, Anna",
        "Goldie, Anna",
        "Mirhoseini, Azalia",
        "McKinnon, Cameron",
        "Chen, Carol",
        "Olsson, Catherine",
        "Olah, Christopher",
        "Hernandez, Danny",
        "Drain, Dawn",
        "Ganguli, Deep",
        "Li, Dustin",
        "Tran-Johnson, Eli",
        "Perez, Ethan",
        "Kerr, Jamie",
        "Mueller, Jared",
        "Ladish, Jeffrey",
        "Landau, Joshua",
        "Ndousse, Kamal",
        "Lukosuite, Kamile",
        "Lovitt, Liane",
        "Sellitto, Michael",
        "Elhage, Nelson",
        "Schiefer, Nicholas",
        "Mercado, Noemi",
        "DasSarma, Nova",
        "Lasenby, Robert",
        "Larson, Robin",
        "Ringer, Sam",
        "Johnston, Scott",
        "Kravec, Shauna",
        "Showk, Sheer El",
        "Fort, Stanislav",
        "Lanham, Tamera",
        "Telleen-Lawton, Timothy",
        "Conerly, Tom",
        "Henighan, Tom",
        "Hume, Tristan",
        "Bowman, Samuel R.",
        "Hatfield-Dodds, Zac",
        "Mann, Ben",
        "Amodei, Dario",
        "Joseph, Nicholas",
        "McCandlish, Sam",
        "Brown, Tom",
        "Kaplan, Jared"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-12-15",
      "selected": null,
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "urls": [
        "http://arxiv.org/pdf/2212.08073.pdf",
        "http://arxiv.org/abs/2212.08073v1",
        "http://arxiv.org/pdf/2212.08073v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Yang R."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3598438.3598467",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "173-178",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450396882",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM International Conference Proceeding Series"
      },
      "publication_date": "2022-12-09",
      "selected": null,
      "title": "A study of emotional interaction decision making in human-computer interaction based on the concept of emotional cognitive evaluation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85166308140&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our contribution to the learning process is through designing the reward function. Like programmers, we have a behavior in mind and have to translate it into a formal specification, namely rewards. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we propose an environment-independent tiered reward structure and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we empirically evaluate tiered reward functions on several environments and show they induce desired behavior and lead to fast learning.",
      "authors": [
        "Zhou, Zhiyuan",
        "Sowerby, Henry",
        "Littman, Michael L."
      ],
      "categories": null,
      "citations": null,
      "comments": "For code, see https://github.com/zhouzypaul/tiered-reward",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-12-07",
      "selected": null,
      "title": "Specifying Behavior Preference with Tiered Reward Functions",
      "urls": [
        "http://arxiv.org/abs/2212.03733v1",
        "http://arxiv.org/pdf/2212.03733.pdf",
        "http://arxiv.org/pdf/2212.03733v1"
      ]
    },
    {
      "abstract": "While reinforcement learning (RL) has become a more popular approach for robotics, designing sufficiently informative reward functions for complex tasks has proven to be extremely difficult due their inability to capture human intent and policy exploitation. Preference based RL algorithms seek to overcome these challenges by directly learning reward functions from human feedback. Unfortunately, prior work either requires an unreasonable number of queries implausible for any human to answer or overly restricts the class of reward functions to guarantee the elicitation of the most informative queries, resulting in models that are insufficiently expressive for realistic robotics tasks. Contrary to most works that focus on query selection to \\emph{minimize} the amount of data required for learning reward functions, we take an opposite approach: \\emph{expanding} the pool of available data by viewing human-in-the-loop RL through the more flexible lens of multi-task learning. Motivated by the success of meta-learning, we pre-train preference models on prior task data and quickly adapt them for new tasks using only a handful of queries. Empirically, we reduce the amount of online feedback needed to train manipulation policies in Meta-World by 20$\\times$, and demonstrate the effectiveness of our method on a real Franka Panda Robot. Moreover, this reduction in query-complexity allows us to train robot policies from actual human users. Videos of our results and code can be found at https://sites.google.com/view/few-shot-preference-rl/home.",
      "authors": [
        "Hejna, Joey",
        "Sadigh, Dorsa"
      ],
      "categories": null,
      "citations": null,
      "comments": "6th Annual Conference on Robot Learning (CoRL) 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-12-06",
      "selected": null,
      "title": "Few-Shot Preference Learning for Human-in-the-Loop RL",
      "urls": [
        "http://arxiv.org/abs/2212.03363v1",
        "http://arxiv.org/pdf/2212.03363v1",
        "http://arxiv.org/pdf/2212.03363.pdf"
      ]
    },
    {
      "abstract": "The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\\pi$. To do this, we need a model of how $\\pi$ relates to $R$. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecification in IRL, together with formal tools that can be used to easily derive the misspecification robustness of new IRL models.",
      "authors": [
        "Skalse, Joar",
        "Abate, Alessandro"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Learning"
        ],
        "title": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  2023"
      },
      "publication_date": "2022-12-06",
      "selected": null,
      "title": "Misspecification in Inverse Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2212.03201.pdf",
        "http://arxiv.org/pdf/2212.03201v2",
        "http://arxiv.org/abs/2212.03201v2"
      ]
    },
    {
      "abstract": "Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they do not require any expert knowledge. With PrefRec, we can fully exploit the advantages of RL in optimizing long-term goals, while avoiding complex reward engineering. PrefRec uses the preferences to automatically train a reward function in an end-to-end manner. The reward function is then used to generate learning signals to train the recommendation policy. Furthermore, we design an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance. We conduct experiments on a variety of long-term user engagement optimization tasks. The results show that PrefRec significantly outperforms previous state-of-the-art methods in all the tasks.",
      "authors": [
        "Xue, Wanqi",
        "Cai, Qingpeng",
        "Xue, Zhenghai",
        "Sun, Shuo",
        "Liu, Shuchang",
        "Zheng, Dong",
        "Jiang, Peng",
        "Gai, Kun",
        "An, Bo"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-12-06",
      "selected": null,
      "title": "PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement",
      "urls": [
        "http://arxiv.org/abs/2212.02779v2",
        "http://arxiv.org/pdf/2212.02779v2",
        "http://arxiv.org/pdf/2212.02779.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450394727",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - SIGGRAPH Asia 2022 Emerging Technologies, SA 2022"
      },
      "publication_date": "2022-12-06",
      "selected": null,
      "title": "Proceedings - SIGGRAPH Asia 2022 Emerging Technologies, SA 2022",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144957846&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Reddy G."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.2215352119",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2022-12-06",
      "selected": null,
      "title": "A reinforcement-based mechanism for discontinuous learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142862834&origin=inward"
      ]
    },
    {
      "abstract": "The ability of mobile robots to navigate in an unfamiliar environment in human terms is decisive for their applicability to practical activities. Bearing this view in mind, we propose a novel framework for navigation in settings where the environment is a priori unknown and can only be partially observed by the robot with onboard sensors. The proposed hierarchical navigation solution combines deep reinforcement learning-based perception with model-based control. Specifically, a deep reinforcement learning (DRL) network based on Soft Actor-Critic (SAC) algorithm and Long Short-Term Memory (LSTM) is trained to map the robot's states, 2D lidar inputs and goal position to a series of local waypoints which are optimal in the sense of collision avoidance. The waypoints are then employed by a dynamic window approach (DWA) based planner to generate a smooth and dynamically feasible trajectory that is tracked by using feedback control. The experiments performed on an actual wheeled robot demonstrate that the proposed scheme enables the robot to reach goal locations more reliably and efficiently in unstructured environments in comparison with purely learning based approach.",
      "authors": [
        "Jinzhou Wang",
        "Ran Huang"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ROBIO55434.2022.10011923",
      "keywords": [
        "mapless navigation",
        "obstacle avoidance",
        "deep reinforcement learning"
      ],
      "number_of_pages": 6,
      "pages": "1781-1786",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8110-6",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 IEEE International Conference on Robotics and Biomimetics, ROBIO 2022"
      },
      "publication_date": "2022-12-05",
      "selected": null,
      "title": "A Mapless Navigation Method Based on Deep Reinforcement Learning and Path Planning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147323770&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10011923"
      ]
    },
    {
      "abstract": "Content personalization is one of the foundations of today\u2019s digital marketing. Often the same image needs to be adapted for different design schemes for content that is created for different occasions, geographic locations or other aspects of the target population. We present a novel reinforcement learning (RL) based method for automatically stylizing images to complement the design scheme of media, e.g., interactive websites, apps, or posters. Our approach considers attributes related to the design of the media and adapts the style of the input image to match the context. We do so using a preferential reward system in the RL framework that learns a reward function using human feedback. We conducted several user studies to evaluate our approach and demonstrate that we are able to effectively adapt image styles to different design schemes. In user studies, images stylized through our approach were the most preferred variation across a majority of our experiments. Additionally, we also release a dataset consisting of perceptual associations of web context with the associated image style.",
      "authors": [
        "Pooja Guhan",
        "Saayan Mitra",
        "Somdeb Sarkhel",
        "Stefano Petrangeli",
        "Ritwik Sinha",
        "Viswanathan Swaminathan",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ISM55400.2022.00037",
      "keywords": [
        "image modification",
        "context",
        "reinforcement learning",
        "content variant generation",
        "image enhancement"
      ],
      "number_of_pages": 4,
      "pages": "169-172",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-7173-2",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2022 IEEE International Symposium on Multimedia, ISM 2022"
      },
      "publication_date": "2022-12-05",
      "selected": null,
      "title": "Contextualized Styling of Images for Web Interfaces using Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10019633",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147543759&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mukherjee S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1115/1.4055680",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of biomechanical engineering"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Generating Human Arm Kinematics Using Reinforcement Learning to Train Active Muscle Behavior in Automotive Research",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139379180&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Li Y."
      ],
      "categories": null,
      "citations": 91,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2021.3087796",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "7791-7805",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Observer-Based Adaptive Optimized Control for Stochastic Nonlinear Systems With Input and State Constraints",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112418638&origin=inward"
      ]
    },
    {
      "abstract": "The dopaminergic midbrain is associated with reinforcement learning, motivation and decision-making \u2013 functions often disturbed in neuropsychiatric disorders. Previous research has shown that dopaminergic midbrain activity can be endogenously modulated via neurofeedback. However, the robustness of endogenous modulation, a requirement for clinical translation, is unclear. Here, we examine whether the activation of particular brain regions associates with successful regulation transfer when feedback is no longer available. Moreover, to elucidate mechanisms underlying effective self-regulation, we study the relation of successful transfer with learning (temporal difference coding) outside the midbrain during neurofeedback training and with individual reward sensitivity in a monetary incentive delay (MID) task. Fifty-nine participants underwent neurofeedback training either in standard (Study 1\u2009N\u2009=\u200915, Study 2\u2009N\u2009=\u200928) or control feedback group (Study 1, N\u2009=\u200916). We find that successful self-regulation is associated with prefrontal reward sensitivity in the MID task (N\u2009=\u200925), with a decreasing relation between prefrontal activity and midbrain learning signals during neurofeedback training and with increased activity within cognitive control areas during transfer. The association between midbrain self-regulation and prefrontal temporal difference and reward sensitivity suggests that reinforcement learning contributes to successful self-regulation. Our findings provide insights in the control of midbrain activity and may facilitate individually tailoring neurofeedback training. Analysis of real-time fMRI data from 59 participants undergoing neurofeedback training suggests that reinforcement learning contributes to successful self-regulation in the dopaminergic midbrain.",
      "authors": [
        "Hellrung, Lydia",
        "Kirschner, Matthias",
        "Sulzer, James",
        "Sladky, Ronald",
        "Scharnowski, Frank",
        "Herdener, Marcus",
        "Tobler, Philippe N."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s42003-022-03756-4",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2399-3642",
        "publisher": "Springer Nature",
        "sjr": 2.251,
        "snip": 1.359,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Agricultural and Biological Sciences (all)",
          "Biochemistry, Genetics and Molecular Biology (all)"
        ],
        "title": "Communications Biology"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Analysis of individual differences in neurofeedback training illuminates successful self-regulation of the dopaminergic midbrain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136675587&origin=inward",
        "https://www.nature.com/articles/s42003-022-03756-4.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhao M.Y."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.13328/j.cnki.jos.006521",
      "keywords": [],
      "number_of_pages": 28,
      "pages": "4616-4643",
      "publication": {
        "category": "Journal",
        "cite_score": 2.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10009825",
        "publisher": "Chinese Academy of Sciences",
        "sjr": 0.305,
        "snip": 0.832,
        "subject_areas": [
          "Software"
        ],
        "title": "Ruan Jian Xue Bao/Journal of Software"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Survey on Conversational Recommendation Algorithms",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143631469&origin=inward"
      ]
    },
    {
      "abstract": "Curiosity for machine agents has been a focus of lively research activity. The study of human and animal curiosity, particularly specific curiosity, has unearthed several properties that would offer important benefits for machine learners, but that have not yet been well-explored in machine intelligence. In this work, we conduct a comprehensive, multidisciplinary survey of the field of animal and machine curiosity. As a principal contribution of this work, we use this survey as a foundation to introduce and define what we consider to be five of the most important properties of specific curiosity: 1) directedness towards inostensible referents, 2) cessation when satisfied, 3) voluntary exposure, 4) transience, and 5) coherent long-term learning. As a second main contribution of this work, we show how these properties may be implemented together in a proof-of-concept reinforcement learning agent: we demonstrate how the properties manifest in the behaviour of this agent in a simple non-episodic grid-world environment that includes curiosity-inducing locations and induced targets of curiosity. As we would hope, our example of a computational specific curiosity agent exhibits short-term directed behaviour while updating long-term preferences to adaptively seek out curiosity-inducing situations. This work, therefore, presents a landmark synthesis and translation of specific curiosity to the domain of machine learning and reinforcement learning and provides a novel view into how specific curiosity operates and in the future might be integrated into the behaviour of goal-seeking, decision-making computational agents in complex environments.",
      "authors": [
        "Ady, Nadia M.",
        "Shariff, Roshan",
        "G\u00fcnther, Johannes",
        "Pilarski, Patrick M."
      ],
      "categories": null,
      "citations": null,
      "comments": "Submitted to the Journal of Artificial Intelligence Research (JAIR)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Five Properties of Specific Curiosity You Didn't Know Curious Machines Should Have",
      "urls": [
        "http://arxiv.org/pdf/2212.00187v1",
        "http://arxiv.org/pdf/2212.00187.pdf",
        "http://arxiv.org/abs/2212.00187v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Xiaofei Wang",
        "Hsiang-Ting Chen",
        "Chin-Teng Lin"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/1741-2552/aca4fb",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17412560",
        "publisher": "IOP Publishing Ltd.",
        "sjr": 1.135,
        "snip": 1.25,
        "subject_areas": [
          "Biomedical Engineering",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Journal of Neural Engineering"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Error-related potential-based shared autonomy via deep recurrent reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143915141&origin=inward",
        "https://iopscience.iop.org/article/10.1088/1741-2552/aca4fb/pdf"
      ]
    },
    {
      "abstract": "A de novo molecular design workflow can be used together with technologies such as reinforcement learning to navigate the chemical space. A bottleneck in the workflow that remains to be solved is how to integrate human feedback in the exploration of the chemical space to optimize molecules. A human drug designer still needs to design the goal, expressed as a scoring function for the molecules that captures the designer\u2019s implicit knowledge about the optimization task. Little support for this task exists and, consequently, a chemist usually resorts to iteratively building the objective function of multi-parameter optimization (MPO) in de novo design. We propose a principled approach to use human-in-the-loop machine learning to help the chemist to adapt the MPO scoring function to better match their goal. An advantage is that the method can learn the scoring function directly from the user\u2019s feedback while they browse the output of the molecule generator, instead of the current manual tuning of the scoring function with trial and error. The proposed method uses a probabilistic model that captures the user\u2019s idea and uncertainty about the scoring function, and it uses active learning to interact with the user. We present two case studies for this: In the first use-case, the parameters of an MPO are learned, and in the second use-case a non-parametric component of the scoring function to capture human domain knowledge is developed. The results show the effectiveness of the methods in two simulated example cases with an oracle, achieving significant improvement in less than 200 feedback queries, for the goals of a high QED score and identifying potent molecules for the DRD2 receptor, respectively. We further demonstrate the performance gains with a medicinal chemist interacting with the system. \n                  \n                    \n                      \n                    \n                  \n                ",
      "authors": [
        "Sundin, Iiris",
        "Voronov, Alexey",
        "Xiao, Haoping",
        "Papadopoulos, Kostas",
        "Bjerrum, Esben Jannik",
        "Heinonen, Markus",
        "Patronov, Atanas",
        "Kaski, Samuel",
        "Engkvist, Ola"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1186/s13321-022-00667-8",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 12.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1758-2946",
        "publisher": "Chemistry Central",
        "sjr": 1.63,
        "snip": 1.836,
        "subject_areas": [
          "Computer Science Applications",
          "Library and Information Sciences",
          "Physical and Theoretical Chemistry",
          "Computer Graphics and Computer-Aided Design"
        ],
        "title": "Journal of Cheminformatics"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Human-in-the-loop assisted de novo molecular design",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145417532&origin=inward",
        "https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-022-00667-8"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Pires G."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/1741-2552/aca798",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17412560",
        "publisher": "IOP Publishing Ltd.",
        "sjr": 1.135,
        "snip": 1.25,
        "subject_areas": [
          "Biomedical Engineering",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Journal of Neural Engineering"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "A new error-monitoring brain-computer interface based on reinforcement learning for people with autism spectrum disorders",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144294059&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Yin M."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.chb.2022.107441",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 17.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "07475632",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.464,
        "snip": 3.223,
        "subject_areas": [
          "Human-Computer Interaction",
          "Arts and Humanities (miscellaneous)",
          "Psychology (all)"
        ],
        "title": "Computers in Human Behavior"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Effects of reward and punishment in prosocial video games on attentional bias and prosocial behaviors",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136023124&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dai Q."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.trc.2022.103916",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 15.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0968090X",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.882,
        "snip": 2.959,
        "subject_areas": [
          "Management Science and Operations Research",
          "Civil and Structural Engineering",
          "Transportation",
          "Automotive Engineering"
        ],
        "title": "Transportation Research Part C: Emerging Technologies"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Calibration of human driving behavior and preference using vehicle trajectory data",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140324713&origin=inward"
      ]
    },
    {
      "abstract": "Personalizing prosthesis control is often structured as human-in-the-loop optimization. However, gait performance is influenced by both human control and intelligent prosthesis control. Hence, we need to consider both human and prosthesis control, and their cooperation, to achieve desired gait patterns. In this study, we developed a novel paradigm that engages human gait control via user-fed visual feedback (FB) of stance time to cooperate with automatic prosthesis control tuning. Three initial questions were studied: (1) does user control of gait timing (via visual FB) help the prosthesis tuning algorithm to converge faster? (2) in turn, does the prosthesis control influence the user\u2019s ability to reach and maintain the target stance time defined by the feedback? and (3) does the prosthesis control parameters tuned with extended stance time on prosthesis side allow the user to maintain this potentially beneficial behavior even after feedback is removed (short- and long-term retention)? A reinforcement learning algorithm was used to achieve prosthesis control to meet normative knee kinematics in walking. A visual FB system cued the user to control prosthesis-side stance time to facilitate the prosthesis tuning goal. Seven individuals without amputation (AB) and four individuals with transfemoral amputation (TFA) walked with a powered knee prosthesis on a treadmill. Participants completed prosthesis auto-tuning with three visual feedback conditions: no FB, self-selected stance time FB (SS FB), and increased stance time FB (Inc FB). The retention of FB effects was studied by comparing the gait performance across three different prosthesis controls, tuned with different visual FB. (1) Human control of gait timing reduced the tuning duration in individuals without amputation, but not for individuals with TFA. (2) The change of prosthesis control did not influence users\u2019 ability to reach and maintain the visual FB goal. (3) All participants increased their prosthesis-side stance time with the feedback and maintain it right after feedback was removed. However, in the post-test, the prosthesis control parameters tuned with visual FB only supported a few participants with longer stance time and better stance time symmetry. The study provides novel insights on human-prosthesis interaction when cooperating in walking, which may guide the future successful adoption of this paradigm in prosthesis control personalization or human-in-the-loop optimization to improve the prosthesis user\u2019s gait performance.",
      "authors": [
        "Fylstra, Bretta L.",
        "Lee, I-Chieh",
        "Li, Minhan",
        "Lewek, Michael D.",
        "Huang, He"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1186/s12984-022-01118-z",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1743-0003",
        "publisher": "BioMed Central Ltd.",
        "sjr": 1.134,
        "snip": 1.924,
        "subject_areas": [
          "Rehabilitation",
          "Health Informatics"
        ],
        "title": "Journal of NeuroEngineering and Rehabilitation"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Human-prosthesis cooperation: combining adaptive prosthesis control with visual feedback guided gait",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144315570&origin=inward",
        "https://jneuroengrehab.biomedcentral.com/counter/pdf/10.1186/s12984-022-01118-z"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Su Z."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.smhl.2022.100346",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Smart Health"
      },
      "publication_date": "2022-12-01",
      "selected": null,
      "title": "Adaptation of a robotic dialog system for medication reminder in elderly care",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140469979&origin=inward"
      ]
    },
    {
      "abstract": "The increasing complexity of gameplay mechanisms in modern video games is leading to the emergence of a wider range of ways to play games. The variety of possible play-styles needs to be anticipated and taken into account by designers, through automated tests. Reinforcement Learning (RL) is a promising answer to the need of automating video game testing. To that effect one needs to train an agent to play the game, while ensuring this agent will generate the same play-styles as the players in order to give meaningful feedback to the designers.\nWe present CARMI : a Configurable Agent with Relative Metrics as Input. An agent able to emulate the players play-styles, even on previously unseen levels. Unlike current methods it does not rely on having full trajectories, but only summary data. Moreover it only requires little human data, thus compatible with the constraints of modern video game production. This novel agent could be used to investigate behaviors and balancing during the production of a video game with a realistic amount of training time.",
      "authors": [
        "Pierre Le Pelletier de Woillemont",
        "R\u00e9mi Labory",
        "Vincent Corruble"
      ],
      "categories": null,
      "citations": 1,
      "comments": "Proceedings of the AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment, 18(1)",
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1609/aiide.v18i1.21958",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "146-154",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-877-0",
        "issn": "2326909X",
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Artificial Intelligence",
          "Learning"
        ],
        "title": "Vol. 18 No. 1 (2022): Eighteenth AAAI Conference on Artificial\n  Intelligence and Interactive Digital Entertainment"
      },
      "publication_date": "2022-11-29",
      "selected": null,
      "title": "Automated Play-Testing through RL Based Human-Like Play-Styles Generation",
      "urls": [
        "https://dl.acm.org/doi/10.1609/aiide.v18i1.21958",
        "http://dx.doi.org/10.1609/aiide.v18i1.21958",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172122909&origin=inward",
        "http://arxiv.org/pdf/2211.17188v1",
        "http://arxiv.org/abs/2211.17188v1"
      ]
    },
    {
      "abstract": "It is expected that autonomous vehicles(AVs) and heterogeneous human-driven vehicles(HVs) will coexist on the same road. The safety and reliability of AVs will depend on their social awareness and their ability to engage in complex social interactions in a socially accepted manner. However, AVs are still inefficient in terms of cooperating with HVs and struggle to understand and adapt to human behavior, which is particularly challenging in mixed autonomy. In a road shared by AVs and HVs, the social preferences or individual traits of HVs are unknown to the AVs and different from AVs, which are expected to follow a policy, HVs are particularly difficult to forecast since they do not necessarily follow a stationary policy. To address these challenges, we frame the mixed-autonomy problem as a multi-agent reinforcement learning (MARL) problem and propose an approach that allows AVs to learn the decision-making of HVs implicitly from experience, account for all vehicles' interests, and safely adapt to other traffic situations. In contrast with existing works, we quantify AVs' social preferences and propose a distributed reward structure that introduces altruism into their decision-making process, allowing the altruistic AVs to learn to establish coalitions and influence the behavior of HVs.",
      "authors": [
        "Valiente, Rodolfo",
        "Toghi, Behrad",
        "Razzaghpour, Mahdi",
        "Pedarsani, Ramtin",
        "Fallah, Yaser P."
      ],
      "categories": null,
      "citations": null,
      "comments": "arXiv admin note: substantial text overlap with arXiv:2202.00881",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-11-22",
      "selected": null,
      "title": "Learning-based social coordination to improve safety and robustness of cooperative autonomous vehicles in mixed traffic",
      "urls": [
        "http://arxiv.org/abs/2211.11963v1",
        "http://arxiv.org/pdf/2211.11963v1",
        "http://arxiv.org/pdf/2211.11963.pdf"
      ]
    },
    {
      "abstract": "Interactive Imitation Learning in Robotics: A Survey",
      "authors": [
        "Carlos Celemin",
        "Rodrigo P\u00e9rez-Dattari",
        "Eugenio Chisari",
        "Giovanni Franzese",
        "Leandro de Souza Rosa",
        "Ravi Prakash",
        "Zlatan Ajanovi\u0107",
        "Marta Ferraz",
        "Abhinav Valada",
        "Jens Kober"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1561/2300000072",
      "keywords": [],
      "number_of_pages": 212,
      "pages": "1-197",
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1935-8253",
        "publisher": "Now Publishers Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Found. Trends Robot"
      },
      "publication_date": "2022-11-22",
      "selected": null,
      "title": "Interactive Imitation Learning in Robotics: A Survey",
      "urls": [
        "https://dl.acm.org/doi/10.1561/2300000072"
      ]
    },
    {
      "abstract": "An important goal in artificial intelligence is to create agents that can both interact naturally with humans and learn from their feedback. Here we demonstrate how to use reinforcement learning from human feedback (RLHF) to improve upon simulated, embodied agents trained to a base level of competency with imitation learning. First, we collected data of humans interacting with agents in a simulated 3D world. We then asked annotators to record moments where they believed that agents either progressed toward or regressed from their human-instructed goal. Using this annotation data we leveraged a novel method - which we call \"Inter-temporal Bradley-Terry\" (IBT) modelling - to build a reward model that captures human judgments. Agents trained to optimise rewards delivered from IBT reward models improved with respect to all of our metrics, including subsequent human judgment during live interactions with agents. Altogether our results demonstrate how one can successfully leverage human judgments to improve agent behaviour, allowing us to use reinforcement learning in complex, embodied domains without programmatic reward functions. Videos of agent behaviour may be found at https://youtu.be/v_Z9F2_eKk4.",
      "authors": [
        "Abramson, Josh",
        "Ahuja, Arun",
        "Carnevale, Federico",
        "Georgiev, Petko",
        "Goldin, Alex",
        "Hung, Alden",
        "Landon, Jessica",
        "Lhotka, Jirka",
        "Lillicrap, Timothy",
        "Muldal, Alistair",
        "Powell, George",
        "Santoro, Adam",
        "Scully, Guy",
        "Srivastava, Sanjana",
        "von Glehn, Tamara",
        "Wayne, Greg",
        "Wong, Nathaniel",
        "Yan, Chen",
        "Zhu, Rui"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-11-21",
      "selected": null,
      "title": "Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2211.11602.pdf",
        "http://arxiv.org/abs/2211.11602v1",
        "http://arxiv.org/pdf/2211.11602v1"
      ]
    },
    {
      "abstract": "Learning new task-specific skills from a few trials is a fundamental challenge for artificial intelligence. Meta reinforcement learning (meta-RL) tackles this problem by learning transferable policies that support few-shot adaptation to unseen tasks. Despite recent advances in meta-RL, most existing methods require the access to the environmental reward function of new tasks to infer the task objective, which is not realistic in many practical applications. To bridge this gap, we study the problem of few-shot adaptation in the context of human-in-the-loop reinforcement learning. We develop a meta-RL algorithm that enables fast policy adaptation with preference-based feedback. The agent can adapt to new tasks by querying human's preference between behavior trajectories instead of using per-step numeric rewards. By extending techniques from information theory, our approach can design query sequences to maximize the information gain from human interactions while tolerating the inherent error of non-expert human oracle. In experiments, we extensively evaluate our method, Adaptation with Noisy OracLE (ANOLE), on a variety of meta-RL benchmark tasks and demonstrate substantial improvement over baseline algorithms in terms of both feedback efficiency and error tolerance.",
      "authors": [
        "Ren, Zhizhou",
        "Liu, Anji",
        "Liang, Yitao",
        "Peng, Jian",
        "Ma, Jianzhu"
      ],
      "categories": null,
      "citations": 2,
      "comments": "Thirty-sixth Conference on Neural Information Processing Systems\n  (NeurIPS 2022)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-11-20",
      "selected": null,
      "title": "Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85153893726&origin=inward",
        "http://arxiv.org/abs/2211.10861v1",
        "http://arxiv.org/pdf/2211.10861v1",
        "http://arxiv.org/pdf/2211.10861.pdf"
      ]
    },
    {
      "abstract": "The basic objective of the study is to establish the reinforcement learning technique in the decoding of imagined speech neural signals. The purpose of imagined speech neural computational studies is to give people who are unable to communicate due to physical or neurological limitations of speech generation alternative natural communication pathways. The advanced human-computer interface based on imagined speech decoding based on measurable neural activity could enable natural interactions and significantly improve quality of life, especially for people with few communication alternatives. Recent advances in signal processing and reinforcement learning based on deep learning algorithms have enabled high-quality imagined speech decoding from noninvasively recorded neural activity. Most of the prior research focused on the supervised classification of collected signals, with no naturalistic feedback-based training of imagined speech models for brain-computer interfaces. We employ deep reinforcement learning in this study to create an imagined speech decoder artificial agent based on the deep Q-network (DQN), so that the artificial agent could indeed learn effective policies directly from multidimensional neural electroencephalography (EEG) signal inputs adopting end-to-end reinforcement learning. We show that the artificial agent, supplied only with neural signals and rewards as inputs, was able to decode the imagined speech neural signals efficiently with 81.6947% overall accuracy.",
      "authors": [
        "Nrushingh Charan Mahapatra",
        "Prachet Bhuyan"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ASSIC55218.2022.10088387",
      "keywords": [
        "deep reinforcement learning (DRL)",
        "brain-computer interface (BCI)",
        "electroencephalography (EEG)",
        "imagined speech",
        "neural signal processing"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-6110-8",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ASSIC 2022 - Proceedings: International Conference on Advancements in Smart, Secure and Intelligent Computing"
      },
      "publication_date": "2022-11-19",
      "selected": null,
      "title": "Decoding of Imagined Speech Neural EEG Signals Using Deep Reinforcement Learning Technique",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85154545586&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10088387"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Palminteri S."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/bne0000541",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "78-88",
      "publication": {
        "category": "Journal",
        "cite_score": 3.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "07357044",
        "publisher": "American Psychological Association",
        "sjr": 0.855,
        "snip": 0.641,
        "subject_areas": [
          "Behavioral Neuroscience"
        ],
        "title": "Behavioral Neuroscience"
      },
      "publication_date": "2022-11-17",
      "selected": null,
      "title": "Choice-Confirmation Bias and Gradual Perseveration in Human Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145827905&origin=inward"
      ]
    },
    {
      "abstract": "Developing interactive software, such as websites or games, is a particularly engaging way to learn computer science. However, teaching and giving feedback on such software is time-consuming -- standard approaches require instructors to manually grade student-implemented interactive programs. As a result, online platforms that serve millions, like Code.org, are unable to provide any feedback on assignments for implementing interactive programs, which critically hinders students' ability to learn. One approach toward automatic grading is to learn an agent that interacts with a student's program and explores states indicative of errors via reinforcement learning. However, existing work on this approach only provides binary feedback of whether a program is correct or not, while students require finer-grained feedback on the specific errors in their programs to understand their mistakes. In this work, we show that exploring to discover errors can be cast as a meta-exploration problem. This enables us to construct a principled objective for discovering errors and an algorithm for optimizing this objective, which provides fine-grained feedback. We evaluate our approach on a set of over 700K real anonymized student programs from a Code.org interactive assignment. Our approach provides feedback with 94.3% accuracy, improving over existing approaches by 17.7% and coming within 1.5% of human-level accuracy. Project web page: https://ezliu.github.io/dreamgrader.",
      "authors": [
        "Liu, Evan Zheran",
        "Stephan, Moritz",
        "Nie, Allen",
        "Piech, Chris",
        "Brunskill, Emma",
        "Finn, Chelsea"
      ],
      "categories": null,
      "citations": 1,
      "comments": "Advances in Neural Information Processing Systems (NeurIPS 2022).\n  Selected as Oral",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-11-16",
      "selected": null,
      "title": "Giving Feedback on Interactive Student Programs with Meta-Exploration",
      "urls": [
        "http://arxiv.org/pdf/2211.08802.pdf",
        "http://arxiv.org/abs/2211.08802v1",
        "http://arxiv.org/pdf/2211.08802v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146892091&origin=inward"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning can effectively facilitate the agent training via human feedback. However, such methods often require the human teacher to know what is the correct action that the agent should take. In other words, if the human teacher is not...",
      "authors": [
        "Guo, Zhaori",
        "Norman, Timothy J.",
        "Gerding, Enrico H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-3-031-21203-1_14",
      "keywords": [
        "Human-in-the-loop reinforcement learning",
        "Interactive reinforcement learning",
        "Multiple people decision"
      ],
      "number_of_pages": 16,
      "pages": "227-242",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-3-031-21202-4",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "PRIMA 2022: Principles and Practice of Multi-Agent Systems: 24th International Conference, Valencia, Spain, November 16\u201318, 2022, Proceedings"
      },
      "publication_date": "2022-11-16",
      "selected": null,
      "title": "MTIRL: Multi-trainer Interactive Reinforcement Learning System",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142747460&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-031-21203-1_14.pdf",
        "https://dl.acm.org/doi/10.1007/978-3-031-21203-1_14"
      ]
    },
    {
      "abstract": "Global explanations of a reinforcement learning (RL) agent's expected behavior can make it safer to deploy. However, such explanations are often difficult to understand because of the complicated nature of many RL policies. Effective human explanations are often contrastive, referencing a known contrast (policy) to reduce redundancy. At the same time, these explanations also require the additional effort of referencing that contrast when evaluating an explanation. We conduct a user study to understand whether and when contrastive explanations might be preferable to complete explanations that do not require referencing a contrast. We find that complete explanations are generally more effective when they are the same size or smaller than a contrastive explanation of the same policy, and no worse when they are larger. This suggests that contrastive explanations are not sufficient to solve the problem of effectively explaining reinforcement learning policies, and require additional careful study for use in this context.",
      "authors": [
        "Narayanan, Sanjana",
        "Lage, Isaac",
        "Doshi-Velez, Finale"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to NeurIPS 2022 workshop on Human in the Loop Learning",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-11-14",
      "selected": null,
      "title": "(When) Are Contrastive Explanations of Reinforcement Learning Helpful?",
      "urls": [
        "http://arxiv.org/pdf/2211.07719v1",
        "http://arxiv.org/abs/2211.07719v1",
        "http://arxiv.org/pdf/2211.07719.pdf"
      ]
    },
    {
      "abstract": "This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable extractive TLS method on two of the three benchmark datasets, and participants prefer our method's summaries to those of both the extractive TLS method and the pretrained abstractive model. The method does not require expensive reference summaries and needs only a small number of preferences to align the generated summaries with human preferences.",
      "authors": [
        "Ye, Yuxuan",
        "Simpson, Edwin"
      ],
      "categories": null,
      "citations": null,
      "comments": "ECAI 2023",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-11-14",
      "selected": null,
      "title": "Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2211.07596v2",
        "http://arxiv.org/pdf/2211.07596.pdf",
        "http://arxiv.org/abs/2211.07596v2"
      ]
    },
    {
      "abstract": "Preference-based reinforcement learning (RL) algorithms help avoid the pitfalls of hand-crafted reward functions by distilling them from human preference feedback, but they remain impractical due to the burdensome number of labels required from the human, even for relatively simple tasks. In this work, we demonstrate that encoding environment dynamics in the reward function (REED) dramatically reduces the number of preference labels required in state-of-the-art preference-based RL frameworks. We hypothesize that REED-based methods better partition the state-action space and facilitate generalization to state-action pairs not included in the preference dataset. REED iterates between encoding environment dynamics in a state-action representation via a self-supervised temporal consistency task, and bootstrapping the preference-based reward function from the state-action representation. Whereas prior approaches train only on the preference-labelled trajectory pairs, REED exposes the state-action representation to all transitions experienced during policy training. We explore the benefits of REED within the PrefPPO [1] and PEBBLE [2] preference learning frameworks and demonstrate improvements across experimental conditions to both the speed of policy learning and the final policy performance. For example, on quadruped-walk and walker-walk with 50 preference labels, REED-based reward functions recover 83% and 66% of ground truth reward policy performance and without REED only 38\\% and 21\\% are recovered. For some domains, REED-based reward functions result in policies that outperform policies trained on the ground truth reward.",
      "authors": [
        "Metcalf, Katherine",
        "Sarabia, Miguel",
        "Theobald, Barry-John"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-11-12",
      "selected": null,
      "title": "Rewards Encoding Environment Dynamics Improves Preference-based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2211.06527v1",
        "http://arxiv.org/pdf/2211.06527.pdf",
        "http://arxiv.org/pdf/2211.06527v1"
      ]
    },
    {
      "abstract": "Reinforcement learning from human feedback (RLHF) is a powerful technique for training agents to perform difficult-to-specify tasks. However, human feedback can be noisy, particularly when human teachers lack relevant knowledge or experience. Levels of expertise vary across teachers, and a given teacher may have differing levels of expertise for different components of a task. RLHF algorithms that learn from multiple teachers therefore face an expertise problem: the reliability of a given piece of feedback depends both on the teacher that it comes from and how specialized that teacher is on relevant components of the task. Existing state-of-the-art RLHF algorithms assume that all evaluations come from the same distribution, obscuring this inter- and intra-human variance, and preventing them from accounting for or taking advantage of variations in expertise. We formalize this problem, implement it as an extension of an existing RLHF benchmark, evaluate the performance of a state-of-the-art RLHF algorithm, and explore techniques to improve query and teacher selection. Our key contribution is to demonstrate and characterize the expertise problem, and to provide an open-source implementation for testing future solutions.",
      "authors": [
        "Daniels-Koch, Oliver",
        "Freedman, Rachel"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to the ML Safety Workshop, NeurIPS 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-11-12",
      "selected": null,
      "title": "The Expertise Problem: Learning from Specialized Feedback",
      "urls": [
        "http://arxiv.org/pdf/2211.06519v1",
        "http://arxiv.org/pdf/2211.06519.pdf",
        "http://arxiv.org/abs/2211.06519v1"
      ]
    },
    {
      "abstract": "Information Extraction (IE) aims to extract structured information from heterogeneous sources. IE from natural language texts include sub-tasks such as Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction (EE). Most IE systems require comprehensive understandings of sentence structure, implied semantics, and domain knowledge to perform well; thus, IE tasks always need adequate external resources and annotations. However, it takes time and effort to obtain more human annotations. Low-Resource Information Extraction (LRIE) strives to use unsupervised data, reducing the required resources and human annotation. In practice, existing systems either utilize self-training schemes to generate pseudo labels that will cause the gradual drift problem, or leverage consistency regularization methods which inevitably possess confirmation bias. To alleviate confirmation bias due to the lack of feedback loops in existing LRIE learning paradigms, we develop a Gradient Imitation Reinforcement Learning (GIRL) method to encourage pseudo-labeled data to imitate the gradient descent direction on labeled data, which can force pseudo-labeled data to achieve better optimization capabilities similar to labeled data. Based on how well the pseudo-labeled data imitates the instructive gradient descent direction obtained from labeled data, we design a reward to quantify the imitation process and bootstrap the optimization capability of pseudo-labeled data through trial and error. In addition to learning paradigms, GIRL is not limited to specific sub-tasks, and we leverage GIRL to solve all IE sub-tasks (named entity recognition, relation extraction, and event extraction) in low-resource settings (semi-supervised IE and few-shot IE).",
      "authors": [
        "Hu, Xuming",
        "Meng, Shiao",
        "Zhang, Chenwei",
        "Yang, Xiangli",
        "Wen, Lijie",
        "King, Irwin",
        "Yu, Philip S."
      ],
      "categories": null,
      "citations": null,
      "comments": "This work has been submitted to the IEEE for possible publication.\n  This work is a substantially extended version of arXiv:2109.06415, with the\n  summary of difference provided in the appendix",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-11-11",
      "selected": null,
      "title": "Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction",
      "urls": [
        "http://arxiv.org/abs/2211.06014v2",
        "http://arxiv.org/pdf/2211.06014v2",
        "http://arxiv.org/pdf/2211.06014.pdf"
      ]
    },
    {
      "abstract": "Outcomes and feedbacks on performance may influence behavior beyond the context in which it was received, yet it remains unclear what neurobehavioral mechanisms may account for such lingering influences on behavior. The average reward rate (ARR) has been suggested to regulate motivated behavior, and was found to interact with dopamine-sensitive cognitive processes, such as vigilance and associative memory encoding. The ARR could therefore provide a bridge between independent tasks when these are performed in temporal proximity, such that the reward rate obtained in one task could influence performance in a second subsequent task. Reinforcement learning depends on the coding of prediction error signals by dopamine neurons and their downstream targets, in particular the nucleus accumbens. Because these brain regions also respond to changes in ARR, reinforcement learning may be vulnerable to changes in ARR. To test this hypothesis, we designed a novel paradigm in which participants (n=245) performed two probabilistic reinforcement learning tasks presented in interleaved trials. The ARR was controlled by an \u2018induction\u2019 task which provided feedback with a low (p=0.58), a medium (p=0.75), or a high probability of reward (p=0.92), while the impact of ARR on reinforcement learning was tested by a second \u2018reference\u2019 task with a constant reward probability (p=0.75). We find that performance was significantly lower in the reference task when the induction task provided low reward probabilities (i.e. during low levels of ARR), as compared to the medium and high ARR conditions. Behavioral modeling further revealed that the influence of ARR is best described by models which accumulates average rewards (rather than average prediction errors), and where the ARR directly modulates the prediction error signal (rather than affecting learning rates or exploration). Our results demonstrate how affective information in one domain may transfer and affect motivated behavior in other domains. These findings are particularly relevant for understanding mood disorders, but may also inform abnormal behaviors attributed to dopamine dysfunction.",
      "authors": [
        "Aberg, Kristoffer C.",
        "Paz, Rony"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2022.1041566",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2022-11-09",
      "selected": null,
      "title": "Average reward rates enable motivational transfer across independent reinforcement learning tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142420371&origin=inward"
      ]
    },
    {
      "abstract": "<p>Every normally developing human infant solves the difficult problem of mapping their native-language phonology, but the neural mechanisms underpinning this behavior remain poorly understood. Here, motor constellation theory, an integrative neurophonological model, is presented, with the goal of explicating this issue. It is assumed that infants\u00e2\u0080\u0099 motor-auditory phonological mapping takes place through infants\u00e2\u0080\u0099 orosensory \u00e2\u0080\u009creaching\u00e2\u0080\u009d for phonological elements observed in the language-specific ambient phonology, <italic>via</italic> reference to kinesthetic feedback from motor systems (e.g., articulators), and auditory feedback from resulting speech and speech-like sounds. Attempts are regulated by basal ganglion\u00e2\u0080\u0093cerebellar speech neural circuitry, and successful attempts at reproduction are enforced through dopaminergic signaling. Early in life, the pace of anatomical development constrains mapping such that complete language-specific phonological mapping is prohibited by infants\u00e2\u0080\u0099 undeveloped supralaryngeal vocal tract and undescended larynx; constraints gradually dissolve with age, enabling adult phonology. Where appropriate, reference is made to findings from animal and clinical models. Some implications for future modeling and simulation efforts, as well as clinical settings, are also discussed.</p>",
      "authors": [
        "Ekstr\u00c3\u00b6m, Axel G."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyg.2022.996894",
      "keywords": [
        "phonological development",
        "Child Development",
        "biology of speech",
        "reinforcement learning",
        "speech acquisition",
        "neurolinguistics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-1078",
        "publisher": "Frontiers Media SA",
        "sjr": 0.891,
        "snip": 1.422,
        "subject_areas": [
          "Psychology (all)"
        ],
        "title": "Frontiers in Psychology"
      },
      "publication_date": "2022-11-03",
      "selected": null,
      "title": "Motor constellation theory: A model of infants\u00e2\u0080\u0099 phonological development",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142138368&origin=inward",
        "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.996894/pdf"
      ]
    },
    {
      "abstract": "Realistic and complex decision tasks often allow for many possible solutions. How do we find the correct one? Introspection suggests a process of trying out solutions one after the other until success. However, such methodical serial testing may be too slow, especially in environments with noisy feedback. Alternatively, the underlying learning process may involve implicit reinforcement learning that learns about many possibilities in parallel. Here we designed a multi-dimensional probabilistic active-learning task tailored to study how people learn to solve such complex problems. Participants configured three-dimensional stimuli by selecting features for each dimension and received probabilistic reward feedback. We manipulated task complexity by changing how many feature dimensions were relevant to maximizing reward, as well as whether this information was provided to the participants. To investigate how participants learn the task, we examined models of serial hypothesis testing, feature-based reinforcement learning, and combinations of the two strategies. Model comparison revealed evidence for hypothesis testing that relies on reinforcement-learning when selecting what hypothesis to test. The extent to which participants engaged in hypothesis testing depended on the instructed task complexity: people tended to serially test hypotheses when instructed that there were fewer relevant dimensions, and relied more on gradual and parallel learning of feature values when the task was more complex. This demonstrates a strategic use of task information to balance the costs and benefits of the two methods of learning.",
      "authors": [
        "Mingyu Song",
        "Persis A. Baah",
        "Ming Bo Cai",
        "Yael Niv"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1010699",
      "keywords": [
        "Learning",
        "Human performance",
        "Human learning",
        "Decision making",
        "Games",
        "Simulation and modeling",
        "Reaction time",
        "Learning curves"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2022-11-01",
      "selected": null,
      "title": "Humans combine value learning and hypothesis testing strategically in multi-dimensional probabilistic reward learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142939196&origin=inward",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1010699&type=printable"
      ]
    },
    {
      "abstract": "Conversational Question Answering (CQA) aims to answer questions contained within dialogues, which are not easily interpretable without context. Developing a model to rewrite conversational questions into self-contained ones is an emerging solution in industry settings as it allows using existing single-turn QA systems to avoid training a CQA model from scratch. Previous work trains rewriting models using human rewrites as supervision. However, such objectives are disconnected with QA models and therefore more human-like rewrites do not guarantee better QA performance. In this paper we propose using QA feedback to supervise the rewriting model with reinforcement learning. Experiments show that our approach can effectively improve QA performance over baselines for both extractive and retrieval QA. Furthermore, human evaluation shows that our method can generate more accurate and detailed rewrites when compared to human annotations.",
      "authors": [
        "Chen, Zhiyu",
        "Zhao, Jie",
        "Fang, Anjie",
        "Fetahu, Besnik",
        "Rokhlenko, Oleg",
        "Malmasi, Shervin"
      ],
      "categories": null,
      "citations": 4,
      "comments": "A cleaned version of our paper Accepted by EMNLP 2022 (Industry\n  Track)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 14,
      "pages": "367-380",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781952148255",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "EMNLP 2022 - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track"
      },
      "publication_date": "2022-10-27",
      "selected": null,
      "title": "Reinforced Question Rewriting for Conversational Question Answering",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152916233&origin=inward",
        "http://arxiv.org/pdf/2210.15777.pdf",
        "http://arxiv.org/abs/2210.15777v2",
        "http://arxiv.org/pdf/2210.15777v2"
      ]
    },
    {
      "abstract": "Safety is a crucial necessity in many applications of reinforcement learning (RL), whether robotic, automotive, or medical. Many existing approaches to safe RL rely on receiving numeric safety feedback, but in many cases this feedback can only take binary values; that is, whether an action in a given state is safe or unsafe. This is particularly true when feedback comes from human experts. We therefore consider the problem of provable safe RL when given access to an offline oracle providing binary feedback on the safety of state, action pairs. We provide a novel meta algorithm, SABRE, which can be applied to any MDP setting given access to a blackbox PAC RL algorithm for that setting. SABRE applies concepts from active learning to reinforcement learning to provably control the number of queries to the safety oracle. SABRE works by iteratively exploring the state space to find regions where the agent is currently uncertain about safety. Our main theoretical results shows that, under appropriate technical assumptions, SABRE never takes unsafe actions during training, and is guaranteed to return a near-optimal safe policy with high probability. We provide a discussion of how our meta-algorithm may be applied to various settings studied in both theoretical and empirical frameworks.",
      "authors": [
        "Bennett, Andrew",
        "Misra, Dipendra",
        "Kallus, Nathan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-26",
      "selected": null,
      "title": "Provable Safe Reinforcement Learning with Binary Feedback",
      "urls": [
        "http://arxiv.org/pdf/2210.14492v1",
        "http://arxiv.org/abs/2210.14492v1",
        "http://arxiv.org/pdf/2210.14492.pdf"
      ]
    },
    {
      "abstract": "Intelligent robots designed to interact with hu-mans in the real world need to adapt to the preferences of different individuals. Preference-based reinforcement learning (RL) has shown great potential for teaching robots to learn personalized behaviors from interacting with humans with-out a meticulous, hand-crafted reward function, replaced by learning reward based on a human's preferences between two robot trajectories. However, poor feedback efficiency and poor exploration in the state and reward spaces make current preference-based RL algorithms perform poorly in complex interactive tasks. To improve the performance of preference-based RL, we incorporate prior knowledge of the task into preference-based RL. Specifically, we decouple the task from preference in human-robot interaction. We utilize a sketchy task reward derived from task priori to instruct robots to conduct more effective task exploration. Then a learned reward from preference-based RL is used to optimize the robot's policy to align with human preferences. In addition, these two parts are combined organically via reward shaping. The experimental results show that our method is a practical and effective solution for personalized human-robot interaction. Code is available at https://github.com/Wenminggong/PbRL_for_PHRI.",
      "authors": [
        "Mingjiang Liu",
        "Chunlin Chen"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/IROS47612.2022.9981076",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "848-855",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2022-10-23",
      "selected": null,
      "title": "Task Decoupling in Preference-based Reinforcement Learning for Personalized Human-Robot Interaction",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9981076"
      ]
    },
    {
      "abstract": "Socially aware robot navigation, where a robot is required to optimize its trajectory to maintain comfortable and compliant spatial interactions with humans in addition to reaching its goal without collisions, is a fundamental yet challenging task in the context of human-robot interaction. While existing learning-based methods have achieved better performance than the preceding model-based ones, they still have drawbacks: reinforcement learning depends on the handcrafted reward that is unlikely to effectively quantify broad social compliance, and can lead to reward exploitation problems; meanwhile, inverse rein-forcement learning suffers from the need for expensive human demonstrations. In this paper, we propose a feedback-efficient active preference learning approach, FAPL, that distills human comfort and expectation into a reward model to guide the robot agent to explore latent aspects of social compliance. We further introduce hybrid experience learning to improve the efficiency of human feedback and samples, and evaluate benefits of robot behaviors learned from FAPL through extensive simulation experiments and a user study (N=10) employing a physical robot to navigate with human subjects in real-world scenarios. Source code and experiment videos for this work are available at: https://sites.google.com/view/san-fapl.",
      "authors": [
        "Ruiqi Wang",
        "Weizheng Wang",
        "Byung-Cheol Min"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS47612.2022.9981616",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "11336-11343",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2022-10-23",
      "selected": null,
      "title": "Feedback-efficient Active Preference Learning for Socially Aware Robot Navigation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146347754&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9981616"
      ]
    },
    {
      "abstract": "Interactive Reinforcement Learning (IntRL) allows human teachers to accelerate the learning process of Reinforcement Learning (RL) robots. However, IntRL has largely been limited to tasks with discrete-action spaces in which actions are relatively slow. This limits IntRL's application to more complicated and challenging robotic tasks, the very tasks that modern RL is particularly well-suited for. We seek to bridge this gap by presenting Continuous Action-space Interactive Reinforcement learning (CAIR): the first continuous action-space IntRL algorithm that is capable of using teacher feedback to out-perform state-of-the-art RL algorithms in those tasks. CAIR combines policies learned from the environment and the teacher into a single policy that proportionally weights the two policies based on their agreement. This allows a CAIR agent to learn a relatively stable policy despite potentially noisy or coarse teacher feedback. We validate our approach in two simulated robotics tasks with easy-to-design and - understand heuristic oracle teachers. Furthermore, we validate our approach in a human subjects study through Amazon Mechanical Turk and show CAIR out-performs the prior state-of-the-art in Interactive RL.",
      "authors": [
        "Isaac Sheidlower",
        "Allison Moore",
        "Elaine Short"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS47612.2022.9982282",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "863-870",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2022-10-23",
      "selected": null,
      "title": "Keeping Humans in the Loop: Teaching via Feedback in Continuous Action Space Environments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146329852&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9982282"
      ]
    },
    {
      "abstract": "We propose a human-in-the-loop reinforcement learning mechanism to help robots learn emotional behavior. Unlike the previous methods of providing explicit feedback via pressing keyboard buttons or mouse clicks, we provide a more natural way for ordinary people to train social robots how to perform social tasks according to their preferences - facial expressions. The whole experiment is carried out on the desktop robot Haru, which is mainly used for the research of emotion and empathy participation. Our experimental results show that through learning from implicit feedback of facial features, Haru can quickly understand and dynamically adapt to individual preferences, and obtain a similar performance to learning from explicit feedback. In addition, we observe that the recognition error of human feedback will cause a \u201ctemporary regress\u201d of the robot's learning performance, which is more obvious at the beginning of the training process. This phenomenon is shown to be correlated with the accuracy of recognizing negative implicit feedback.",
      "authors": [
        "Hui Wang",
        "Jinying Lin",
        "Zhen Ma",
        "Yurii Vasylkiv",
        "Heike Brock",
        "Keisuke Nakamura",
        "Randy Gomez",
        "Bo He",
        "Guangliang Li"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS47612.2022.9981752",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "3881-3888",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2022-10-23",
      "selected": null,
      "title": "Affective Behavior Learning for Social Robot Haru with Implicit Evaluative Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9981752",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146334332&origin=inward"
      ]
    },
    {
      "abstract": "Deep reinforcement learning (deep RL) has emerged as an effective tool for developing controllers for legged robots. However, vanilla deep RL often requires a tremendous amount of training samples and is not feasible for achieving robust behaviors. Instead, researchers have investigated a novel policy architecture by incorporating human experts' knowledge, such as Policies Modulating Trajectory Generators (PMTG). This architecture builds a recurrent control loop by combining a parametric trajectory generator (TG) and a feedback policy network to achieve more robust behaviors. In this work, we propose Policies Modulating Finite State Machine (PM-FSM) by replacing TGs with contact-aware finite state machines (FSM), which offers more flexible control of each leg. This invention offers an explicit notion of contact events to the policy to negotiate unexpected perturbations. We demonstrated that the proposed architecture could achieve more robust behaviors in various scenarios, such as challenging terrains or external perturbations, on both simulated and real robots.",
      "authors": [
        "Ren Liu",
        "Nitish Sontakke",
        "Sehoon Ha"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IROS47612.2022.9982259",
      "keywords": [
        "Finite State Machine",
        "Quadrupedal Locomotion",
        "Reinforcement Learning"
      ],
      "number_of_pages": 7,
      "pages": "4063-4069",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2022-10-23",
      "selected": null,
      "title": "PM-FSM: Policies Modulating Finite State Machine for Robust Quadrupedal Locomotion",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9982259",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146321231&origin=inward"
      ]
    },
    {
      "abstract": "<p>With the development of artificial intelligence and robotic technology in recent years, robots are gradually integrated into human daily life. Most of the human-robot interaction technologies currently applied to home service robots are programmed by the manufacturer first, and then instruct the user to trigger the implementation through voice commands or gesture commands. Although these methods are simple and effective, they lack some flexibility, especially when the programming program is contrary to user habits, which will lead to a significant decline in user experience satisfaction. To make that robots can better serve human beings, adaptable, simple, and flexible human-robot interaction technology is essential. Based on the neural mechanism of reinforcement learning, we propose a brain-inspired intention prediction model to enable the robot to perform actions according to the user's intention. With the spike-timing-dependent plasticity (STDP) mechanisms and the simple feedback of right or wrong, the humanoid robot NAO could successfully predict the user's intentions in Human Intention Prediction Experiment and Trajectory Tracking Experiment. Compared with the traditional Q-learning method, the training times required by the proposed model are reduced by (<italic>N</italic><sup>2</sup> \u2212 <italic>N</italic>)/4, where N is the number of intentions.</p>",
      "authors": [
        "Zhao, Yuxuan",
        "Zeng, Yi"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2022.1009237",
      "keywords": [
        "human-robot interaction",
        "Brain-inspired model",
        "Intention prediction",
        "spiking neural networks",
        "humanoid robot"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2022-10-21",
      "selected": null,
      "title": "A brain-inspired intention prediction model and its applications to humanoid robot",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141221301&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.1009237/pdf"
      ]
    },
    {
      "abstract": "<sec><title>Background</title><p>Alcohol use disorder is characterized by perseverative alcohol use despite negative consequences. This hallmark feature of addiction potentially relates to impairments in behavioral flexibility, which can be measured by probabilistic reversal learning (PRL) paradigms. We here aimed to examine the cognitive mechanisms underlying impaired PRL task performance in patients with alcohol use disorder (AUDP) using computational models of reinforcement learning.</p></sec><sec><title>Methods</title><p>Twenty-eight early abstinent AUDP and 27 healthy controls (HC) performed an extensive PRL paradigm. We compared conventional behavioral variables of choices (perseveration; correct responses) between groups. Moreover, we fitted Bayesian computational models to the task data to compare differences in latent cognitive variables including reward and punishment learning and choice consistency between groups.</p></sec><sec><title>Results</title><p>AUDP and HC did not significantly differ with regard to direct perseveration rates after reversals. However, AUDP made overall less correct responses and specifically showed decreased win\u2013stay behavior compared to HC. Interestingly, AUDP showed premature switching after no or little negative feedback but elevated proneness to stay when accumulation of negative feedback would make switching a more optimal option. Computational modeling revealed that AUDP compared to HC showed enhanced learning from punishment, a tendency to learn less from positive feedback and lower choice consistency.</p></sec><sec><title>Conclusion</title><p>Our data do not support the assumption that AUDP are characterized by increased perseveration behavior. Instead our findings provide evidence that enhanced negative reinforcement and decreased non-drug-related reward learning as well as diminished choice consistency underlie dysfunctional choice behavior in AUDP.</p></sec>",
      "authors": [
        "Ba\u011fci, Ba\u015fak",
        "Zorlu, Nabi",
        "Heinz, Andreas",
        "Schad, Daniel J.",
        "Sebold, Miriam"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyt.2022.960238",
      "keywords": [
        "Computational Psychiatry",
        "reinforcement learning",
        "cognitive flexibility",
        "Alcohol use disorder (AUD)",
        "Reversal Learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-0640",
        "publisher": "Frontiers Media SA",
        "sjr": 1.222,
        "snip": 1.265,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "Frontiers in Psychiatry"
      },
      "publication_date": "2022-10-19",
      "selected": null,
      "title": "Computational analysis of probabilistic reversal learning deficits in male subjects with alcohol use disorder",
      "urls": [
        "https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2022.960238/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141194016&origin=inward"
      ]
    },
    {
      "abstract": "When robots enter everyday human environments, they need to understand their tasks and how they should perform those tasks. To encode these, reward functions, which specify the objective of a robot, are employed. However, designing reward functions can be extremely challenging for complex tasks and environments. A promising approach is to learn reward functions from humans. Recently, several robot learning works embrace this approach and leverage human demonstrations to learn the reward functions. Known as inverse reinforcement learning, this approach relies on a fundamental assumption: humans can provide near-optimal demonstrations to the robot. Unfortunately, this is rarely the case: human demonstrations to the robot are often suboptimal due to various reasons, e.g., difficulty of teleoperation, robot having high degrees of freedom, or humans' cognitive limitations. This thesis is an attempt towards learning reward functions from human users by using other, more reliable data modalities. Specifically, we study how reward functions can be learned using comparative feedback, in which the human user compares multiple robot trajectories instead of (or in addition to) providing demonstrations. To this end, we first propose various forms of comparative feedback, e.g., pairwise comparisons, best-of-many choices, rankings, scaled comparisons; and describe how a robot can use these various forms of human feedback to infer a reward function, which may be parametric or non-parametric. Next, we propose active learning techniques to enable the robot to ask for comparison feedback that optimizes for the expected information that will be gained from that user feedback. Finally, we demonstrate the applicability of our methods in a wide variety of domains, ranging from autonomous driving simulations to home robotics, from standard reinforcement learning benchmarks to lower-body exoskeletons.",
      "authors": [
        "B\u0131y\u0131k, Erdem"
      ],
      "categories": null,
      "citations": null,
      "comments": "Ph.D. Thesis (Stanford University), 198 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-19",
      "selected": null,
      "title": "Learning Preferences for Interactive Autonomy",
      "urls": [
        "http://arxiv.org/pdf/2210.10899.pdf",
        "http://arxiv.org/pdf/2210.10899v1",
        "http://arxiv.org/abs/2210.10899v1"
      ]
    },
    {
      "abstract": "In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.",
      "authors": [
        "Gao, Leo",
        "Schulman, John",
        "Hilton, Jacob"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-19",
      "selected": null,
      "title": "Scaling Laws for Reward Model Overoptimization",
      "urls": [
        "http://arxiv.org/pdf/2210.10760.pdf",
        "http://arxiv.org/pdf/2210.10760v1",
        "http://arxiv.org/abs/2210.10760v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rosell&#243;-Mar&#237;n, Eric",
        "Lopez-Sanchez, Maite",
        "Rodr&#237;guez, Inmaculada",
        "Rodr&#237;guez-Soto, Manel",
        "Rodr&#237;guez-Aguilar, Juan A."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/FAIA220356",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "335-344",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781643684369",
        "issn": "09226389",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Artificial Intelligence and Applications"
      },
      "publication_date": "2022-10-17",
      "selected": null,
      "title": "An Ethical Conversational Agent to Respectfully Conduct In-Game Surveys",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141697734&origin=inward",
        "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA220356"
      ]
    },
    {
      "abstract": "We propose a novel concept of augmented reality (AR) human-drone interaction driven by RL-based swarm behavior to achieve intuitive and immersive control of a swarm formation of unmanned aerial vehicles. The DroneARchery system developed by us allows the user to quickly deploy a swarm of drones, generating flight paths simulating archery. The haptic interface LinkGlide delivers a tactile stimulus of the bowstring tension to the forearm to increase the precision of aiming. The swarm of released drones dynamically avoids collisions between each other, the drone following the user, and external obstacles with behavior control based on deep reinforcement learning. The developed concept was tested in the scenario with a human, where the user shoots from a virtual bow with a real drone to hit the target. The human operator observes the ballistic trajectory of the drone in an AR and achieves a realistic and highly recognizable experience of the bowstring tension through the haptic display. The experimental results revealed that the system improves trajectory prediction accuracy by 63.3% through applying AR technology and conveying haptic feedback of pulling force. DroneARchery users highlighted the naturalness (4.3 out of 5 point Likert scale) and increased confidence (4.7 out of 5) when controlling the drone. We have designed the tactile patterns to present four sliding distances (tension) and three applied force levels (stiffness) of the haptic display. Users demonstrated the ability to distinguish tactile patterns produced by the haptic display representing varying bowstring tension(average recognition rate is of 72.8%) and stiffness (average recognition rate is of 94.2%). The novelty of the research is the development of an AR-based approach for drone control that does not require special skills and training from the operator. In the future, the proposed interaction can be applied in various fields, for example, for fast swarm deployment in search and rescue missions, crop monitoring, inspection and maintenance.",
      "authors": [
        "Ekaterina Dorzhieva",
        "Ahmed Baza",
        "Ayush Gupta",
        "Aleksey Fedoseev",
        "Miguel Altamirano Cabrera",
        "Ekaterina Karmanova",
        "Dzmitry Tsetserukou"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ISMAR55827.2022.00042",
      "keywords": [
        "Human computer interaction (HCI)",
        "Haptic devices",
        "Interaction paradigms",
        "Interaction devices",
        "Mixed / augmented reality",
        "Human-centered computing"
      ],
      "number_of_pages": 8,
      "pages": "270-277",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-5326-4",
        "issn": "1554-7868",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2022 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2022"
      },
      "publication_date": "2022-10-17",
      "selected": null,
      "title": "DroneARchery: Human-Drone Interaction through Augmented Reality with Haptic Feedback and Multi-UAV Collision Avoidance Driven by Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146430913&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9995060"
      ]
    },
    {
      "abstract": "<p>Cognitive flexibility, the ability to adapt to unexpected changes, is critical for healthy environmental and social interactions, and thus to everyday functioning. In neuropsychiatric diseases, cognitive flexibility is often impaired and treatment options are lacking. Probabilistic reversal learning (PRL) is commonly used to measure cognitive flexibility in rodents and humans. In PRL tasks, subjects must sample choice options and, from probabilistic feedback, find the current best choice which then changes without warning. However, in rodents, pharmacological models of human cognitive impairment tend to disrupt only the first (or few) of several contingency reversals, making quantitative assessment of behavioral effects difficult. To address this limitation, we developed a novel rat PRL where reversals occur at relatively long intervals in time that demonstrates increased sensitivity to the non-competitive NMDA receptor antagonist MK-801. Here, we quantitively compare behavior in time-based PRL with a widely used task where reversals occur based on choice behavior. In time-based PRL, MK-801 induced sustained reversal learning deficits both in time and across reversal blocks but, at the same dose, only transient weak effects in performance-based PRL. Moreover, time-based PRL yielded better estimates of behavior and reinforcement learning model parameters, which opens meaningful pharmacological windows to efficiently test and develop novel drugs preclinically with the goal of improving cognitive impairment in human patients.</p>",
      "authors": [
        "Latuske, Patrick",
        "von Heimendahl, Moritz",
        "Deiana, Serena",
        "Wotjak, Carsten T.",
        "du Hoffmann, Johann"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fphar.2022.898548",
      "keywords": [
        "behavioral task",
        "Schizophrenia",
        "cognitive flexibility",
        "dizocilpine (MK-801)",
        "Drug Discovery",
        "Reversal Learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1663-9812",
        "publisher": "Frontiers Media SA",
        "sjr": 1.064,
        "snip": 1.199,
        "subject_areas": [
          "Pharmacology (medical)",
          "Pharmacology"
        ],
        "title": "Frontiers in Pharmacology"
      },
      "publication_date": "2022-10-14",
      "selected": null,
      "title": "Sustained MK-801 induced deficit in a novel probabilistic reversal learning task",
      "urls": [
        "https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2022.898548/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140993991&origin=inward"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning can effectively facilitate the agent training via human feedback. However, such methods often require the human teacher to know what is the correct action that the agent should take. In other words, if the human teacher is not always reliable, then it will not be consistently able to guide the agent through its training. In this paper, we propose a more effective interactive reinforcement learning system by introducing multiple trainers, namely Multi-Trainer Interactive Reinforcement Learning (MTIRL), which could aggregate the binary feedback from multiple non-perfect trainers into a more reliable reward for an agent training in a reward-sparse environment. In particular, our trainer feedback aggregation experiments show that our aggregation method has the best accuracy when compared with the majority voting, the weighted voting, and the Bayesian method. Finally, we conduct a grid-world experiment to show that the policy trained by the MTIRL with the review model is closer to the optimal policy than that without a review model.",
      "authors": [
        "Guo, Zhaori",
        "Norman, Timothy J.",
        "Gerding, Enrico H."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-14",
      "selected": null,
      "title": "Multi-trainer Interactive Reinforcement Learning System",
      "urls": [
        "http://arxiv.org/pdf/2210.08050v1",
        "http://arxiv.org/abs/2210.08050v1",
        "http://arxiv.org/pdf/2210.08050.pdf"
      ]
    },
    {
      "abstract": "Controlled automated story generation seeks to generate natural language stories satisfying constraints from natural language critiques or preferences. Existing methods to control for story preference utilize prompt engineering which is labor intensive and often inconsistent. They may also use logit-manipulation methods which require annotated datasets to exist for the desired attributes. To address these issues, we first train a contrastive bi-encoder model to align stories with corresponding human critiques, named CARP, building a general purpose preference model. This is subsequently used as a reward function to fine-tune a generative language model via reinforcement learning. However, simply fine-tuning a generative language model with a contrastive reward model does not always reliably result in a story generation system capable of generating stories that meet user preferences. To increase story generation robustness we further fine-tune the contrastive reward model using a prompt-learning technique. A human participant study is then conducted comparing generations from our full system, ablations, and two baselines. We show that the full fine-tuning pipeline results in a story generator preferred over a LLM 20x as large as well as logit-based methods. This motivates the use of contrastive learning for general purpose human preference modeling.",
      "authors": [
        "Castricato, Louis",
        "Havrilla, Alexander",
        "Matiana, Shahbuland",
        "Pieler, Michael",
        "Ye, Anbang",
        "Yang, Ian",
        "Frazier, Spencer",
        "Riedl, Mark"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-14",
      "selected": null,
      "title": "Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2210.07792.pdf",
        "http://arxiv.org/pdf/2210.07792v2",
        "http://arxiv.org/abs/2210.07792v2"
      ]
    },
    {
      "abstract": "We propose a novel concept of augmented reality (AR) human-drone interaction driven by RL-based swarm behavior to achieve intuitive and immersive control of a swarm formation of unmanned aerial vehicles. The DroneARchery system developed by us allows the user to quickly deploy a swarm of drones, generating flight paths simulating archery. The haptic interface LinkGlide delivers a tactile stimulus of the bowstring tension to the forearm to increase the precision of aiming. The swarm of released drones dynamically avoids collisions between each other, the drone following the user, and external obstacles with behavior control based on deep reinforcement learning. The developed concept was tested in the scenario with a human, where the user shoots from a virtual bow with a real drone to hit the target. The human operator observes the ballistic trajectory of the drone in an AR and achieves a realistic and highly recognizable experience of the bowstring tension through the haptic display. The experimental results revealed that the system improves trajectory prediction accuracy by 63.3% through applying AR technology and conveying haptic feedback of pulling force. DroneARchery users highlighted the naturalness (4.3 out of 5 point Likert scale) and increased confidence (4.7 out of 5) when controlling the drone. We have designed the tactile patterns to present four sliding distances (tension) and three applied force levels (stiffness) of the haptic display. Users demonstrated the ability to distinguish tactile patterns produced by the haptic display representing varying bowstring tension(average recognition rate is of 72.8%) and stiffness (average recognition rate is of 94.2%). The novelty of the research is the development of an AR-based approach for drone control that does not require special skills and training from the operator.",
      "authors": [
        "Dorzhieva, Ekaterina",
        "Baza, Ahmed",
        "Gupta, Ayush",
        "Fedoseev, Aleksey",
        "Cabrera, Miguel Altamirano",
        "Karmanova, Ekaterina",
        "Tsetserukou, Dzmitry"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to the IEEE Int. Symp. on Mixed and Augmented Reality (ISMAR\n  2022). Copyright 20XX IEEE. Personal use of this material is permitted",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-14",
      "selected": null,
      "title": "DroneARchery: Human-Drone Interaction through Augmented Reality with Haptic Feedback and Multi-UAV Collision Avoidance Driven by Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2210.07730v1",
        "http://arxiv.org/pdf/2210.07730.pdf",
        "http://arxiv.org/abs/2210.07730v1"
      ]
    },
    {
      "abstract": "Offline reinforcement learning (ORL) holds great promise for robot learning due to its ability to learn from arbitrary pre-generated experience. However, current ORL benchmarks are almost entirely in simulation and utilize contrived datasets like replay buffers of online RL agents or sub-optimal trajectories, and thus hold limited relevance for real-world robotics. In this work (Real-ORL), we posit that data collected from safe operations of closely related tasks are more practical data sources for real-world robot learning. Under these settings, we perform an extensive (6500+ trajectories collected over 800+ robot hours and 270+ human labor hour) empirical study evaluating generalization and transfer capabilities of representative ORL methods on four real-world tabletop manipulation tasks. Our study finds that ORL and imitation learning prefer different action spaces, and that ORL algorithms can generalize from leveraging offline heterogeneous data sources and outperform imitation learning. We release our dataset and implementations at URL: https://sites.google.com/view/real-orl",
      "authors": [
        "Zhou, Gaoyue",
        "Ke, Liyiming",
        "Srinivasa, Siddhartha",
        "Gupta, Abhinav",
        "Rajeswaran, Aravind",
        "Kumar, Vikash"
      ],
      "categories": null,
      "citations": null,
      "comments": "Project website: https://sites.google.com/view/real-orl",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-12",
      "selected": null,
      "title": "Real World Offline Reinforcement Learning with Realistic Data Source",
      "urls": [
        "http://arxiv.org/pdf/2210.06479.pdf",
        "http://arxiv.org/pdf/2210.06479v1",
        "http://arxiv.org/abs/2210.06479v1"
      ]
    },
    {
      "abstract": "Design time uncertainty poses an important challenge when developing a self-adaptive system. As an example, defining how the system should adapt when facing a new environment state, requires understanding the precise effect of an adaptation, which may not be known at design time. Online reinforcement learning, i.e., employing reinforcement learning (RL) at runtime, is an emerging approach to realizing self-adaptive systems in the presence of design time uncertainty. By using Online RL, the self-adaptive system can learn from actual operational data and leverage feedback only available at runtime. Recently, Deep RL is gaining interest. Deep RL represents learned knowledge as a neural network whereby it can generalize over unseen inputs, as well as handle continuous environment states and adaptation actions. A fundamental problem of Deep RL is that learned knowledge is not explicitly represented. For a human, it is practically impossible to relate the parametrization of the neural network to concrete RL decisions and thus Deep RL essentially appears as a black box. Yet, understanding the decisions made by Deep RL is key to (1) increasing trust, and (2) facilitating debugging. Such debugging is especially relevant for self-adaptive systems, because the reward function, which quantifies the feedback to the RL algorithm, must be defined by developers. The reward function must be explicitly defined by developers, thus introducing a potential for human error. To explain Deep RL for self-adaptive systems, we enhance and combine two existing explainable RL techniques from the machine learning literature. The combined technique, XRL-DINE, overcomes the respective limitations of the individual techniques. We present a proof-of-concept implementation of XRL-DINE, as well as qualitative and quantitative results of applying XRL-DINE to a self-adaptive system exemplar.",
      "authors": [
        "Feit, Felix",
        "Metzger, Andreas",
        "Pohl, Klaus"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published at 3rd Intl Conference on Autonomic Computing and\n  Self-Organizing Systems (ACSOS 2022)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-12",
      "selected": null,
      "title": "Explaining Online Reinforcement Learning Decisions of Self-Adaptive Systems",
      "urls": [
        "http://arxiv.org/abs/2210.05931v1",
        "http://arxiv.org/pdf/2210.05931.pdf",
        "http://arxiv.org/pdf/2210.05931v1"
      ]
    },
    {
      "abstract": "Players can sometimes engage with parts of a video game that they do not enjoy if the game does not try to adapt the experience to the player\u2019s preference. AI directors have been used in the past to tailor player experience to different people. In industry, AI directors are relatively uncommon and are typically domain-specific and rules-based. In this paper, we present a reinforcement learning-based AI director developed for the industry game Nightingale with the help of Inflexion Games. We ran an experiment to evaluate the effectiveness of the AI director in creating a desired player experience, but found inconclusive evidence. In line with this year\u2019s theme, we present our negative results and their implications for future AI directors, along with general discussion from working closely with an industry partner.",
      "authors": [
        "Kristen K. Yu",
        "Matthew Guzdial",
        "Nathan R. Sturtevant",
        "Morgan Cselinacz",
        "Chris Corfe",
        "Izzy Hubert Lyall",
        "Chris Smith"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1609/aiide.v18i1.21949",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "70-77",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "157735883X",
        "issn": "2326909X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - AAAI Artificial Intelligence and Interactive Digital Entertainment Conference, AIIDE"
      },
      "publication_date": "2022-10-11",
      "selected": null,
      "title": "Adventures of AI Directors Early in the Development of Nightingale",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85172093235&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement Learning (RL) algorithms suffer from the dependency on accurately engineered reward functions to properly guide the learning agents to do the required tasks. Preference-based reinforcement learning (PbRL) addresses that by utilizing human preferences as feedback from the experts instead of numeric rewards. Due to its promising advantage over traditional RL, PbRL has gained more focus in recent years with many significant advances. In this survey, we present a unified PbRL framework to include the newly emerging approaches that improve the scalability and efficiency of PbRL. In addition, we give a detailed overview of the theoretical guarantees and benchmarking work done in the field, while presenting its recent applications in complex real-world tasks. Lastly, we go over the limitations of the current approaches and the proposed future research directions.",
      "authors": [
        "Youssef Abdelkareem",
        "Shady Shehata",
        "Fakhri Karray"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/SMC53654.2022.9945333",
      "keywords": [
        "Benchmarking.",
        "Preference-based Reinforcement Learning",
        "Theoretical Guarantees"
      ],
      "number_of_pages": 6,
      "pages": "2527-2532",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3703-7",
        "issn": "1062-922X",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics"
      },
      "publication_date": "2022-10-09",
      "selected": null,
      "title": "Advances in Preference-based Reinforcement Learning: A Review",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142696795&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9945333"
      ]
    },
    {
      "abstract": "To achieve desirable performance, current AI systems often require huge amounts of training data. This is especially problematic in domains where collecting data is both expensive and time-consuming, e.g., where AI systems require having numerous interactions with humans, collecting feedback from them. In this work, we substantiate the idea of $\\textit{cognitive models as simulators}$, which is to have AI systems interact with, and collect feedback from, cognitive models instead of humans, thereby making their training process both less costly and faster. Here, we leverage this idea in the context of moral decision-making, by having reinforcement learning (RL) agents learn about fairness through interacting with a cognitive model of the Ultimatum Game (UG), a canonical task in behavioral and brain sciences for studying fairness. Interestingly, these RL agents learn to rationally adapt their behavior depending on the emotional state of their simulated UG responder. Our work suggests that using cognitive models as simulators of humans is an effective approach for training AI systems, presenting an important way for computational cognitive science to make contributions to AI.",
      "authors": [
        "Nobandegani, Ardavan S.",
        "Shultz, Thomas R.",
        "Rish, Irina"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-08",
      "selected": null,
      "title": "Cognitive Models as Simulators: The Case of Moral Decision-Making",
      "urls": [
        "http://arxiv.org/pdf/2210.04121.pdf",
        "http://arxiv.org/abs/2210.04121v1",
        "http://arxiv.org/pdf/2210.04121v1"
      ]
    },
    {
      "abstract": "Human-in-the-loop (HiL) reinforcement learning is gaining traction in domains with large action and state spaces, and sparse rewards by allowing the agent to take advice from HiL. Beyond advice accommodation, a sequential decision-making agent must be able to express the extent to which it was able to utilize the human advice. Subsequently, the agent should provide a means for the HiL to inspect parts of advice that it had to reject in favor of the overall environment objective. We introduce the problem of Advice-Conformance Verification which requires reinforcement learning (RL) agents to provide assurances to the human in the loop regarding how much of their advice is being conformed to. We then propose a Tree-based lingua-franca to support this communication, called a Preference Tree. We study two cases of good and bad advice scenarios in MuJoCo's Humanoid environment. Through our experiments, we show that our method can provide an interpretable means of solving the Advice-Conformance Verification problem by conveying whether or not the agent is using the human's advice. Finally, we present a human-user study with 20 participants that validates our method.",
      "authors": [
        "Verma, Mudit",
        "Kharkwal, Ayush",
        "Kambhampati, Subbarao"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted at IROS-RLCONFORM 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-07",
      "selected": null,
      "title": "Advice Conformance Verification by Reinforcement Learning agents for Human-in-the-Loop",
      "urls": [
        "http://arxiv.org/pdf/2210.03455v1",
        "http://arxiv.org/abs/2210.03455v1",
        "http://arxiv.org/pdf/2210.03455.pdf"
      ]
    },
    {
      "abstract": "Credit Defaulters in Banking is a term used for account holders unable to repay financial loans in time. Till recently, credit and loan systems were human dependent and had limitations in identifying defaulters due to weak data monitoring. Today, in the era of Data Science and Machine Learning (ML), data is closely monitored with inferences and timely feedback. However, the data available for credit defaulters is usually heavily imbalanced. The objective of this paper is to use a technique named \u2018Reinforcement Learning\u2019 (RL) to mitigate this bias using the concept of reward feedback wherein current decisions influence future decisions. We aim to do a comparative study of the Double Deep Q-Network (DDQN) algorithm under reinforcement learning with the existing supervised learning algorithms and find out if RL is suitable for the task. We find that RL gives a performance at par with supervised learning, in dataset specific cases.",
      "authors": [
        "Meryl Jacob",
        "G S Harshavardhan Reddy",
        "Christeena Rappai",
        "Pranay Kapoor",
        "Megha Kolhekar"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/GCAT55367.2022.9972092",
      "keywords": [
        "DDQN",
        "Deep Learning",
        "imbalanced classification",
        "Reinforcement Learning",
        "Credit defaulters"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-6856-5",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 IEEE 3rd Global Conference for Advancement in Technology, GCAT 2022"
      },
      "publication_date": "2022-10-07",
      "selected": null,
      "title": "A Comparative Study of Supervised and Reinforcement Learning Techniques for the Application of Credit Defaulters",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85145432913&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9972092"
      ]
    },
    {
      "abstract": "Digital human recommendation system has been developed to help customers find their favorite products and is playing an active role in various recommendation contexts. How to timely catch and learn the dynamics of the preferences of the customers, while meeting their exact requirements, becomes crucial in the digital human recommendation domain. We design a novel practical digital human interactive recommendation agent framework based on Reinforcement Learning(RL) to improve the efficiency of the interactive recommendation decision-making by leveraging both the digital human features and the superior flexibility of RL. Our proposed framework learns through real-time interactions between the digital human and customers dynamically through the state-of-art RL algorithms, combined with multimodal embedding and graph embedding, to improve the accuracy of personalization and thus enable the digital human agent to timely catch the attention of the customer. Experiments on real business data demonstrate that our framework can provide better personalized customer engagement and better customer experiences.",
      "authors": [
        "Junwu, Xiong",
        "Feng, Xiaoyun",
        "Shi, YunZhou",
        "Zhang, James",
        "Zhao, Zhongzhou",
        "Zhou, Wei"
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages, 1 figure, 1 table, the paper has been accepted and this is\n  the final camera-ready for NeurIPS 2022 Workshop on Human in the Loop\n  Learning, https://neurips-hill.github.io/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-06",
      "selected": null,
      "title": "Digital Human Interactive Recommendation Decision-Making Based on Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2210.10638.pdf",
        "http://arxiv.org/abs/2210.10638v4",
        "http://arxiv.org/pdf/2210.10638v4"
      ]
    },
    {
      "abstract": "How do humans and animals perform trial-and-error learning when the space of possibilities is infinite? In a previous study (Wang et al., 2020), we used an interval timing production task and discovered an updating strategy in which the agent adjusted the behavioral and neuronal noise for exploration. In the experiment, human subjects proactively generated a series of timed motor outputs. Positive or negative feedback was provided after each response based on the timing accuracy. We found that the sequential motor timing varied at two temporal scales: long-term correlation around the target interval due to memory drifts and short-term adjustments of timing variability according to feedback. We have previously described these two key features of timing variability with an augmented Gaussian process, termed reward-sensitive Gaussian process (RSGP). In a nutshell, the temporal covariance of the timing variable was updated based on the feedback history to recreate the two behavioral characteristics mentioned above. However, the RSGP was mainly descriptive and lacked a neurobiological basis of how the reward feedback can be used by a neural circuit to adjust motor variability. Here we provide a mechanistic model and simulate the process by borrowing the architecture of recurrent neural networks. While recurrent connection provided the long-term serial correlation in motor timing, to facilitate reward-driven short-term variations, we introduced reward-dependent variability in the network connectivity, inspired by the stochastic nature of synaptic transmission in the brain. Our model was able to recursively generate an output sequence incorporating internal variability and external reinforcement in a Bayesian framework. We show that the model can generate the temporal structure of the motor variability as a basis for exploration and exploitation trade-off. Unlike other neural network models that search for unique network connectivity for the best match between the model prediction and observation, this model can estimate the uncertainty associated with each outcome and thus did a better job in teasing apart adjustable task-relevant variability from unexplained variability. The proposed artificial neural network model parallels the mechanisms of information processing in neural systems and can extend the framework of brain-inspired reinforcement learning in continuous state control.",
      "authors": [
        "Wang, Jing",
        "El-Jayyousi, Yousuf",
        "Ozden, Ilker"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2022.918031",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2022-10-05",
      "selected": null,
      "title": "A neural network model for timing control with reinforcement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140112607&origin=inward"
      ]
    },
    {
      "abstract": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.",
      "authors": [
        "Ramamurthy, Rajkumar",
        "Ammanabrolu, Prithviraj",
        "Brantley, Kiant\u00e9",
        "Hessel, Jack",
        "Sifa, Rafet",
        "Bauckhage, Christian",
        "Hajishirzi, Hannaneh",
        "Choi, Yejin"
      ],
      "categories": null,
      "citations": null,
      "comments": "In Proceedings of ICLR 2023. Code found at\n  https://github.com/allenai/rl4lms and Project website at\n  https://rl4lms.apps.allenai.org/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-10-03",
      "selected": null,
      "title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
      "urls": [
        "http://arxiv.org/pdf/2210.01241v3",
        "http://arxiv.org/pdf/2210.01241.pdf",
        "http://arxiv.org/abs/2210.01241v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chen C."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jpsychires.2022.08.003",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "307-314",
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223956",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.554,
        "snip": 1.364,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "Journal of Psychiatric Research"
      },
      "publication_date": "2022-10-01",
      "selected": null,
      "title": "Computational markers of experience- but not description-based decision-making are associated with future depressive symptoms in young adults",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136758606&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning algorithms have a long-standing success story in explaining the dynamics of instrumental conditioning in humans and other species. While normative reinforcement learning models are critically dependent on external feedback, recent findings in the field of perceptual learning point to a crucial role of internally generated reinforcement signals based on subjective confidence, when external feedback is not available. Here, we investigated the existence of such confidence-based learning signals in a key domain of reinforcement-based learning: instrumental conditioning. We conducted a value-based decision making experiment which included phases with and without external feedback and in which participants reported their confidence in addition to choices. Behaviorally, we found signatures of self-reinforcement in phases without feedback, reflected in an increase of subjective confidence and choice consistency. To clarify the mechanistic role of confidence in value-based learning, we compared a family of confidence-based learning models with more standard models predicting either no change in value estimates or a devaluation over time when no external reward is provided. We found that confidence-based models indeed outperformed these reference models, whereby the learning signal of the winning model was based on the prediction error between current confidence and a stimulus-unspecific average of previous confidence levels. Interestingly, individuals with more volatile reward-based value updates in the presence of feedback also showed more volatile confidence-based value updates when feedback was not available. Together, our results provide evidence that confidence-based learning signals affect instrumentally learned subjective values in the absence of external feedback.",
      "authors": [
        "Lena Esther Ptasczynski",
        "Isa Steinecker",
        "Philipp Sterzer",
        "Matthias Guggenmos"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1010580",
      "keywords": [
        "Experimental design",
        "Linear regression analysis",
        "Learning",
        "Human learning",
        "Decision making",
        "Signal processing",
        "Operant conditioning",
        "Perceptual learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2022-10-01",
      "selected": null,
      "title": "The value of confidence: Confidence prediction errors drive value-based learning in the absence of external feedback",
      "urls": [
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1010580&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139762953&origin=inward"
      ]
    },
    {
      "abstract": "A key aspect of social human-robot interaction is natural non-verbal communication. In this work, we train an agent with batch reinforcement learning to generate nods and smiles as backchannels in order to increase the naturalness of the interaction and to engage humans. We introduce the Sequential Random Deep Q-Network (SRDQN) method to learn a policy for backchannel generation, that explicitly maximizes user engagement. The proposed SRDQN method outperforms the existing vanilla Q-learning methods when evaluated using off-policy policy evaluation techniques. Furthermore, to verify the effectiveness of SRDQN, a human-robot experiment has been designed and conducted with an expressive 3d robot head. The experiment is based on a story-shaping game designed to create an interactive social activity with the robot. The engagement of the participants during the interaction is computed from user's social signals like backchannels, mutual gaze and adjacency pair. The subjective feedback from participants and the engagement values strongly indicate that our framework is a step forward towards the autonomous learning of a socially acceptable backchanneling behavior.",
      "authors": [
        "Nusrah Hussain",
        "Engin Erzin",
        "T. Metin Sezgin",
        "Y\u00fccel Yemez"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TAFFC.2022.3190233",
      "keywords": [
        "batch reinforcement learning",
        "Human-robot interaction",
        "user engagement",
        "backchannels"
      ],
      "number_of_pages": 14,
      "pages": "1840-1853",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2371-9850",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Affective Computing"
      },
      "publication_date": "2022-10-01",
      "selected": null,
      "title": "Training Socially Engaging Robots: Modeling Backchannel Behaviors with Batch Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134249481&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9829283"
      ]
    },
    {
      "abstract": "The question of how people change their opinions through social interactions has been on the agenda of social scientific research for many decades. Now that the Internet has led to an ever greater interconnectedness and new forms of exchange that seem to go hand in hand with increasing political polarization, it is once again gaining in relevance. Most recently, the field of opinion dynamics has been complemented by social feedback theory, which explains opinion polarization phenomena by means of a reinforcement learning mechanism. According to the assumptions, individuals not only evaluate the opinion alternatives available to them based on the social feedback received as a result of expressing an opinion within a certain social environment. Rather, they also internalize the expected and thus rewarded opinion to the point where it becomes their actual private opinion. In order to put the implications of social feedback theory to a test, we conducted a randomized controlled laboratory experiment. The study combined preceding and follow-up opinion measurements via online surveys with a laboratory treatment. Social feedback was found to have longer-term effects on private opinions, even when received in an anonymous and sanction free setting. Interestingly and contrary to our expectations, however, it was the mixture of supportive and rejective social feedback that resulted in the strongest influence. In addition, we observed a high degree of opinion volatility, highlighting the need for further research to help identify additional internal and external factors that might influence whether and how social feedback affects private opinions.",
      "authors": [
        "Marcel Sark\u00f6zi",
        "Stephanie J\u00fctersonke",
        "Sven Banisch",
        "Stephan Poppe",
        "Roger Berger"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0274903",
      "keywords": [
        "Control theory",
        "Social influence",
        "Experimental design",
        "Social communication",
        "Social theory",
        "Personality traits",
        "Surveys",
        "Animal sociality"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2022-10-01",
      "selected": null,
      "title": "The effects of social feedback on private opinions. Empirical evidence from the laboratory",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0274903&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139321302&origin=inward"
      ]
    },
    {
      "abstract": "Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user's intent of producing natural language at inference time, however only one word would minimize the task's loss function at training time. We provide a simple yet plausible causal mechanism describing the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B) pre-training objectives: from masked & autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement learning from human feedback (RLHF). Code and open-source demos available at https: //github.com/2dot71mily/sib_paper.",
      "authors": [
        "McMilin, Emily"
      ],
      "categories": null,
      "citations": null,
      "comments": "25 pages, 34 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-09-30",
      "selected": null,
      "title": "Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution",
      "urls": [
        "http://arxiv.org/pdf/2210.00131v3",
        "http://arxiv.org/pdf/2210.00131.pdf",
        "http://arxiv.org/abs/2210.00131v3"
      ]
    },
    {
      "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.",
      "authors": [
        "Glaese, Amelia",
        "McAleese, Nat",
        "Tr\u0119bacz, Maja",
        "Aslanides, John",
        "Firoiu, Vlad",
        "Ewalds, Timo",
        "Rauh, Maribeth",
        "Weidinger, Laura",
        "Chadwick, Martin",
        "Thacker, Phoebe",
        "Campbell-Gillingham, Lucy",
        "Uesato, Jonathan",
        "Huang, Po-Sen",
        "Comanescu, Ramona",
        "Yang, Fan",
        "See, Abigail",
        "Dathathri, Sumanth",
        "Greig, Rory",
        "Chen, Charlie",
        "Fritz, Doug",
        "Elias, Jaume Sanchez",
        "Green, Richard",
        "Mokr\u00e1, So\u0148a",
        "Fernando, Nicholas",
        "Wu, Boxi",
        "Foley, Rachel",
        "Young, Susannah",
        "Gabriel, Iason",
        "Isaac, William",
        "Mellor, John",
        "Hassabis, Demis",
        "Kavukcuoglu, Koray",
        "Hendricks, Lisa Anne",
        "Irving, Geoffrey"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-09-28",
      "selected": null,
      "title": "Improving alignment of dialogue agents via targeted human judgements",
      "urls": [
        "http://arxiv.org/pdf/2209.14375.pdf",
        "http://arxiv.org/abs/2209.14375v1",
        "http://arxiv.org/pdf/2209.14375v1"
      ]
    },
    {
      "abstract": "We define a novel neuro-symbolic framework, argumentative reward learning, which combines preference-based argumentation with existing approaches to reinforcement learning from human feedback. Our method improves prior work by generalising human preferences, reducing the burden on the user and increasing the robustness of the reward model. We demonstrate this with a number of experiments.",
      "authors": [
        "Ward, Francis Rhys",
        "Belardinelli, Francesco",
        "Toni, Francesca"
      ],
      "categories": null,
      "citations": null,
      "comments": "4 pages, ICML HMCaT workshop",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-09-28",
      "selected": null,
      "title": "Argumentative Reward Learning: Reasoning About Human Preferences",
      "urls": [
        "http://arxiv.org/abs/2209.14010v1",
        "http://arxiv.org/pdf/2209.14010.pdf",
        "http://arxiv.org/pdf/2209.14010v1"
      ]
    },
    {
      "abstract": "Commonly in reinforcement learning (RL), rewards are discounted over time using an exponential function to model time preference, thereby bounding the expected long-term reward. In contrast, in economics and psychology, it has been shown that humans often adopt a hyperbolic discounting scheme, which is optimal when a specific task termination time distribution is assumed. In this work, we propose a theory for continuous-time model-based reinforcement learning generalized to arbitrary discount functions. This formulation covers the case in which there is a non-exponential random termination time. We derive a Hamilton-Jacobi-Bellman (HJB) equation characterizing the optimal policy and describe how it can be solved using a collocation method, which uses deep learning for function approximation. Further, we show how the inverse RL problem can be approached, in which one tries to recover properties of the discount function given decision data. We validate the applicability of our proposed approach on two simulated problems. Our approach opens the way for the analysis of human discounting in sequential decision-making tasks.",
      "authors": [
        "Schultheis, Matthias",
        "Rothkopf, Constantin A.",
        "Koeppl, Heinz"
      ],
      "categories": null,
      "citations": 0,
      "comments": "22 pages, 3 figures, published at 36th Conference on Neural\n  Information Processing Systems (NeurIPS 2022)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-09-27",
      "selected": null,
      "title": "Reinforcement Learning with Non-Exponential Discounting",
      "urls": [
        "http://arxiv.org/pdf/2209.13413.pdf",
        "http://arxiv.org/abs/2209.13413v2",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151837416&origin=inward",
        "http://arxiv.org/pdf/2209.13413v2"
      ]
    },
    {
      "abstract": "Most intelligent transportation systems, such as driverless cars and drones, try to reduce people\u2019s participation and realize intelligent transportation. However, the lack of human participation will lead to the weak adaptive ability of intelligent transportation systems, which can't cope with emergencies and other problems. In response to this situation, this paper introduces the adaptive decision-making method of human-in-the-loop. Firstly, the agent is trained by introducing human feedback into the traditional reinforcement learning algorithm. Through the interaction between agents with the environment and humans, the process of \"exploration-learning-decision making\" is repeated constantly, accumulating experience and optimizing strategies in the process of interacting with the environment and humans. The agent continuously updates the decision-making process through human feedback and can avoid static obstacles and dynamic obstacles in the environment, finally reach the target point. Secondly, the human-in-the-loop algorithm of DQN-TAMER is put forward, and experiments are carried out through three groups of human-in-the-loop algorithms. The experimental results show that the self-adaptive decision-making method of human-in-the-loop traffic can obviously improve the decision-making ability of intelligent transportation systems and the adaptability of the whole system, and the learning efficiency of agents is significantly improved by DQN-TAMER's human-in-the-loop algorithm. Finally, the medical material transportation simulation system is used as a prototype.",
      "authors": [
        "Peng Zhang",
        "Wei Liu",
        "Junjie Shao"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRCV55858.2022.9953216",
      "keywords": [
        "intelligent transportation",
        "adaptive system",
        "human-in-the-loop"
      ],
      "number_of_pages": 5,
      "pages": "272-276",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8171-7",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 4th International Conference on Robotics and Computer Vision, ICRCV 2022"
      },
      "publication_date": "2022-09-25",
      "selected": null,
      "title": "Research on Human-in-the-loop Traffic Adaptive Decision Making Method",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9953216",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143612528&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hui J."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7527/S1000-6893.2021.25960",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 2.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10006893",
        "publisher": "AAAS Press of Chinese Society of Aeronautics and Astronautics",
        "sjr": 0.322,
        "snip": 0.967,
        "subject_areas": [
          "Modeling and Simulation",
          "Aerospace Engineering",
          "Applied Mathematics",
          "Space and Planetary Science"
        ],
        "title": "Hangkong Xuebao/Acta Aeronautica et Astronautica Sinica"
      },
      "publication_date": "2022-09-25",
      "selected": null,
      "title": "Generating new quality flight corridor for reentry aircraft based on reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141926284&origin=inward"
      ]
    },
    {
      "abstract": "Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task preferences), efficiency (i.e., the robot achieves sample-efficient adaptation), and scalability (i.e., the model grows sublinearly with the number of demonstrations while maintaining high performance). FLAIR surpasses benchmarks across three control tasks with an average 57% improvement in policy returns and an average 78% fewer episodes required for demonstration modeling using policy mixtures. Finally, we demonstrate the success of FLAIR in a table tennis task and find users rate FLAIR as having higher task (p<.05) and personalization (p<.05) performance.",
      "authors": [
        "Chen, Letian",
        "Jayanthi, Sravan",
        "Paleja, Rohan",
        "Martin, Daniel",
        "Zakharov, Viacheslav",
        "Gombolay, Matthew"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Learning",
          "Robotics"
        ],
        "title": "Proceedings of Conference on Robot Learning (CoRL) 2022"
      },
      "publication_date": "2022-09-24",
      "selected": null,
      "title": "Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations",
      "urls": [
        "http://arxiv.org/abs/2209.11908v7",
        "http://arxiv.org/pdf/2209.11908v7",
        "http://arxiv.org/pdf/2209.11908.pdf"
      ]
    },
    {
      "abstract": "Table-based fact verification aims to check whether a statement is entailed by the content of relevant table. Existing works mainly either parse a statement with logical form or design a table-aware neural network to represent the statement-table pair. However, they fail to directly exploit guidance signals to capture enough evidence from the table, which lead to performance degradation. Thus, to investigate how to select potential key words from the table for fact verification, we propose a Reinforced Evidence Reasoning framework with Graph neural network (RERG), which simulates human inference process of focusing on some words at each step. Specifically, we employ a Transformer-based graph neural network to represent multi-granularity features. Then, we design a monitor node and connect it with some potential key nodes by reinforcement learning on each graph layer, according to the feedback of the reward. In this way, the monitor node can be used to predict the label, which has aggregated various key information through multiple graph layers. Besides, we add secondary updating after the attention mechanism to enhance information aggregation of each graph layer. Experimental results on two benchmark datasets TABFACT and INFOTABS show performance improvements over state-of-the-art baselines and the feasibility of selecting some meaningful evidences during graph reasoning.",
      "authors": [
        "Zhao, Guangzhen",
        "Yang, Peng",
        "Yao, Yu"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10489-022-04130-x",
      "keywords": [
        "Natural language processing",
        "Table-based fact verification",
        "Reinforcement learning",
        "Graph neural network"
      ],
      "number_of_pages": 16,
      "pages": "12308-12323",
      "publication": {
        "category": "Journal",
        "cite_score": 6.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0924-669X",
        "publisher": "Springer Netherlands",
        "sjr": 1.145,
        "snip": 1.78,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Applied Intelligence"
      },
      "publication_date": "2022-09-24",
      "selected": null,
      "title": "RERG: Reinforced evidence reasoning with graph neural network for table-based fact verification",
      "urls": [
        "https://dl.acm.org/doi/10.1007/s10489-022-04130-x",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138700341&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10489-022-04130-x.pdf"
      ]
    },
    {
      "abstract": "Addressing such diverse ends as safety alignment with human preferences, and the efficiency of learning, a growing line of reinforcement learning research focuses on risk functionals that depend on the entire distribution of returns. Recent work on \\emph{off-policy risk assessment} (OPRA) for contextual bandits introduced consistent estimators for the target policy's CDF of returns along with finite sample guarantees that extend to (and hold simultaneously over) all risk. In this paper, we lift OPRA to Markov decision processes (MDPs), where importance sampling (IS) CDF estimators suffer high variance on longer trajectories due to small effective sample size. To mitigate these problems, we incorporate model-based estimation to develop the first doubly robust (DR) estimator for the CDF of returns in MDPs. This estimator enjoys significantly less variance and, when the model is well specified, achieves the Cramer-Rao variance lower bound. Moreover, for many risk functionals, the downstream estimates enjoy both lower bias and lower variance. Additionally, we derive the first minimax lower bounds for off-policy CDF and risk estimation, which match our error bounds up to a constant factor. Finally, we demonstrate the precision of our DR CDF estimates experimentally on several different environments.",
      "authors": [
        "Huang, Audrey",
        "Leqi, Liu",
        "Lipton, Zachary Chase",
        "Azizzadenesheli, Kamyar"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-09-21",
      "selected": null,
      "title": "Off-Policy Risk Assessment in Markov Decision Processes",
      "urls": [
        "http://arxiv.org/pdf/2209.10444v1",
        "http://arxiv.org/abs/2209.10444v1",
        "http://arxiv.org/pdf/2209.10444.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Verma, Mehul",
        "Acar, Erman"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/FAIA220189",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "46-59",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781643684369",
        "issn": "09226389",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Artificial Intelligence and Applications"
      },
      "publication_date": "2022-09-19",
      "selected": null,
      "title": "Learning to Cooperate with Human Evaluative Feedback and Demonstrations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142166795&origin=inward",
        "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA220189"
      ]
    },
    {
      "abstract": "Design time uncertainty poses an important challenge when developing a self-adaptive system. As an example, defining how the system should adapt when facing a new environment state, requires understanding the precise effect of an adaptation, which may not be known at design time. Online reinforcement learning, i.e., employing reinforcement learning (RL) at runtime, is an emerging approach to realizing self-adaptive systems in the presence of design time uncertainty. By using Online RL, the self-adaptive system can learn from actual operational data and leverage feedback only available at runtime. Recently, Deep RL is gaining interest. Deep RL represents learned knowledge as a neural network whereby it can generalize over unseen inputs, as well as handle continuous environment states and adaptation actions. A fundamental problem of Deep RL is that learned knowledge is not explicitly represented. For a human, it is practically impossible to relate the parametrization of the neural network to concrete RL decisions and thus Deep RL essentially appears as a black box. Yet, understanding the decisions made by Deep RL is key to (1) increasing trust, and (2) facilitating debugging. Such debugging is especially relevant for self-adaptive systems, because the reward function, which quantifies the feedback to the RL algorithm, must be defined by developers. The reward function must be explicitly defined by developers, thus introducing a potential for human error. To explain Deep RL for self-adaptive systems, we enhance and combine two existing explainable RL techniques from the machine learning literature. The combined technique, XRL-DINE, overcomes the respective limitations of the individual techniques. We present a proof-of-concept implementation of XRL-DINE, as well as qualitative and quantitative results of applying XRL-DINE to a self-adaptive system exemplar.",
      "authors": [
        "Felix Feit",
        "Andreas Metzger",
        "Klaus Pohl"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACSOS55765.2022.00023",
      "keywords": [
        "Explainable AI",
        "Reinforcement Learning",
        "Adaptive Systems"
      ],
      "number_of_pages": 10,
      "pages": "51-60",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-7138-1",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2022 IEEE International Conference on Autonomic Computing and Self-Organizing Systems, ACSOS 2022"
      },
      "publication_date": "2022-09-19",
      "selected": null,
      "title": "Explaining Online Reinforcement Learning Decisions of Self-Adaptive Systems",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9935001",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142381896&origin=inward"
      ]
    },
    {
      "abstract": "A trustworthy reinforcement learning algorithm should be competent in solving challenging real-world problems, including {robustly} handling uncertainties, satisfying {safety} constraints to avoid catastrophic failures, and {generalizing} to unseen scenarios during deployments. This study aims to overview these main perspectives of trustworthy reinforcement learning considering its intrinsic vulnerabilities on robustness, safety, and generalizability. In particular, we give rigorous formulations, categorize corresponding methodologies, and discuss benchmarks for each perspective. Moreover, we provide an outlook section to spur promising future directions with a brief discussion on extrinsic vulnerabilities considering human feedback. We hope this survey could bring together separate threads of studies together in a unified framework and promote the trustworthiness of reinforcement learning.",
      "authors": [
        "Xu, Mengdi",
        "Liu, Zuxin",
        "Huang, Peide",
        "Ding, Wenhao",
        "Cen, Zhepeng",
        "Li, Bo",
        "Zhao, Ding"
      ],
      "categories": null,
      "citations": null,
      "comments": "36 pages, 5 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-09-16",
      "selected": null,
      "title": "Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities: Robustness, Safety, and Generalizability",
      "urls": [
        "http://arxiv.org/pdf/2209.08025v1",
        "http://arxiv.org/pdf/2209.08025.pdf",
        "http://arxiv.org/abs/2209.08025v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Cao, Yushi",
        "Teo, Yon Shin",
        "Zheng, Yan",
        "Toh, Yuxuan",
        "Lin, Shang-Wei"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/FAIA220259",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "286-297",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781643684369",
        "issn": "09226389",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Artificial Intelligence and Applications"
      },
      "publication_date": "2022-09-14",
      "selected": null,
      "title": "A Holistic Automated Software Structure Exploration Framework for Testing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139791555&origin=inward",
        "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA220259"
      ]
    },
    {
      "abstract": " Today, most recommender systems employ Machine Learning to recommend posts, products, and other items, usually produced by the users. Although the impressive progress in Deep Learning and Reinforcement Learning, we observe that recommendations made by such systems still do not correlate with actual human preferences. In our tutorial, we will bridge the gap between crowdsourcing and recommender systems communities by showing how one can incorporate human-in-the-loop into their recommender system to gather the real human feedback on the ranked recommendations. We will discuss the ranking data lifecycle and run through it step-by-step. A significant portion of tutorial time is devoted to a hands-on practice, when the attendees will, under our guidance, sample recommendations and build the ground truth dataset using crowdsourced data, and compute the offline evaluation scores.",
      "authors": [
        "Dmitry Ustalov",
        "Natalia Fedorova",
        "Nikita Pavlichenko"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3523227.3547373",
      "keywords": [
        "offline evaluation",
        "recommender systems",
        "human-in-the-loop",
        "crowdsourcing"
      ],
      "number_of_pages": 2,
      "pages": "708-709",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450392785",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "RecSys 2022 - Proceedings of the 16th ACM Conference on Recommender Systems"
      },
      "publication_date": "2022-09-13",
      "selected": null,
      "title": "Improving Recommender Systems with Human-in-the-Loop",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3523227.3547373",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139548273&origin=inward"
      ]
    },
    {
      "abstract": "The capability to interactively learn from human feedback would enable agents in new settings. For example, even novice users could train service robots in new tasks naturally and interactively. Human-in-the-loop Reinforcement Learning (HRL) combines human feedback and Reinforcement Learning (RL) techniques. State-of-the-art interactive learning techniques suffer from slow learning speed, thus leading to a frustrating experience for the human. We approach this problem by extending the HRL framework TAMER for evaluative feedback with the possibility to enhance human feedback with two different types of counterfactual explanations (action and state based). We experimentally show that our extensions improve the speed of learning.",
      "authors": [
        "Jakob Karalus",
        "Felix Lindner"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICDL53763.2022.9962222",
      "keywords": [
        "Counterfactuals",
        "Explainability",
        "Human-in-the-loop Reinforcement Learning"
      ],
      "number_of_pages": 7,
      "pages": "362-368",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-1312-1",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 IEEE International Conference on Development and Learning, ICDL 2022"
      },
      "publication_date": "2022-09-12",
      "selected": null,
      "title": "Accelerating the Learning of TAMER with Counterfactual Explanations",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9962222",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143988368&origin=inward"
      ]
    },
    {
      "abstract": "Social robots assisting in cognitive stimulation therapies, physical rehabilitation, or entertainment sessions have gained visibility in the last years. In these activities, users may present different features and needs, so personalization is essential. This manuscript presents a Preference Learning System for social robots to personalize Human-Robot Interaction during entertainment activities. Our system is integrated into Mini, a social robot dedicated to research with a wide repertoire of entertainment activities like games, displaying multimedia content, or storytelling. The learning model we propose consists of four stages. First, the robot creates a unique profile of its users by obtaining their defining features using interaction. Secondly, a Preference Learning algorithm predicts the users\u2019 favorite entertainment activities using their features and a database with the features and preferences of other users. Third, the prediction is adapted using Reinforcement Learning while entertainment sessions occur. Finally, the robot personalizes Human-Robot Interaction by autonomously selecting the users\u2019 favorite activities. Thus, the robot aims at promoting longer-lasting interactions and sustaining engagement.",
      "authors": [
        "Marcos Maroto-G\u00f3mez",
        "Sara Marqu\u00e9s Villarroya",
        "Mar\u00eda Malfaz",
        "\u00c1lvaro Castro-Gonz\u00e1lez",
        "Jos\u00e8 Carlos Castillo",
        "Miguel \u00c1ngel Salichs"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICDL53763.2022.9962204",
      "keywords": [
        "Social robots",
        "User Profiling",
        "Preference Learning",
        "Robot personalization",
        "Autonomous Decision-making",
        "Reinforcement Learning"
      ],
      "number_of_pages": 6,
      "pages": "343-348",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-1312-1",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 IEEE International Conference on Development and Learning, ICDL 2022"
      },
      "publication_date": "2022-09-12",
      "selected": null,
      "title": "A Preference Learning System for the Autonomous Selection and Personalization of Entertainment Activities during Human-Robot Interaction",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9962204",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143984864&origin=inward"
      ]
    },
    {
      "abstract": "Bio-inspired tendon-driven manipulators have the potential to achieve human-level dexterity. However, their control is more complex than prevailing robotic hands because the relation between actuation and hand motion (Jacobian) is hard to obtain. On the other hand, humans maneuver their complex hands skillfully and conduct adaptive object grasping and manipulation. We conjecture that the foundation of this ability is a visual posing of hands (i.e., a skill to make arbitrary hand poses with visual and proprioceptive feedback). Children develop this skill before or in parallel with learning grasping and manipulation. Inspired by this developmental process, this study explored a method to equip compliant tendon-driven manipulators with the visual posing. To overcome the complexity of the system, we used a learning-based approach. Specifically, we adopted PlaNet, model-based reinforcement learning that leverages a dynamics model on a compact latent representation. To further accelerate learning, we restricted the control space using the idea of muscle synergy found in the human body control. We validated the effectiveness of the proposed method in a simulation. We also demonstrated that the posing skill acquired using our method is useful for object grasping. This study will contribute to achieving human-level dexterity in manipulations.",
      "authors": [
        "Matthew Ishige",
        "Tadahiro Taniguchi",
        "Yoshihiro Kawahara"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICDL53763.2022.9962220",
      "keywords": [
        "world model",
        "pose control",
        "tendon driven manipulator",
        "reinforcement learning",
        "visual feedback",
        "muscle synergy"
      ],
      "number_of_pages": 7,
      "pages": "250-256",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-1312-1",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 IEEE International Conference on Development and Learning, ICDL 2022"
      },
      "publication_date": "2022-09-12",
      "selected": null,
      "title": "Dream to Pose in a Tendon-Driven Manipulator with Muscle Synergy",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143989133&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9962220"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "McGregor J.N."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7554/eLife.75691",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "eLife"
      },
      "publication_date": "2022-09-01",
      "selected": null,
      "title": "Shared mechanisms of auditory and non-auditory vocal learning in the songbird brain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139376641&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Piette J.D."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1001/jamainternmed.2022.3178",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "975-983",
      "publication": {
        "category": "Journal",
        "cite_score": 43.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21686106",
        "publisher": "American Medical Association",
        "sjr": 4.922,
        "snip": 5.779,
        "subject_areas": [
          "Internal Medicine"
        ],
        "title": "JAMA Internal Medicine"
      },
      "publication_date": "2022-09-01",
      "selected": null,
      "title": "Patient-Centered Pain Care Using Artificial Intelligence and Mobile Health Tools: A Randomized Comparative Effectiveness Trial",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136184821&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bastani H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1287/inte.2022.1128",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "398-411",
      "publication": {
        "category": "Journal",
        "cite_score": 2.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00922102",
        "publisher": "INFORMS Institute for Operations Research and the Management Sciences",
        "sjr": 0.77,
        "snip": 0.771,
        "subject_areas": [
          "Strategy and Management",
          "Management of Technology and Innovation",
          "Management Science and Operations Research"
        ],
        "title": "Interfaces"
      },
      "publication_date": "2022-09-01",
      "selected": null,
      "title": "Interpretable Operations Research for High-Stakes Decisions: Designing the Greek COVID-19 Testing System",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146976288&origin=inward"
      ]
    },
    {
      "abstract": "Urban-safety perception is crucial for urban planning and pedestrian street preference studies. With the development of deep learning and the availability of high-resolution street images, the use of artificial intelligence methods to deal with urban-safety perception has been considered adequate by many researchers. However, most current methods are based on the feature-extraction capability of convolutional neural networks (CNNs) with large-scale annotated data for training, mainly aimed at providing a regression or classification model. There remains a lack of interpretable and complete evaluation systems for urban-safety perception. To improve the interpretability of evaluation models and achieve human-like safety perception, we proposed a complete decision-making framework based on reinforcement learning (RL). We developed a novel feature-extraction module, a scalable visual computational model based on visual semantic and functional features that could fully exploit the knowledge of domain experts. Furthermore, we designed the RL module\u2014comprising a combination of a Markov decision process (MDP)-based street-view observation environment and an intelligent agent trained using a deep reinforcement-learning (DRL) algorithm\u2014to achieve human-level perception abilities. Experimental results using our crowdsourced dataset showed that the framework achieved satisfactory prediction performance and excellent visual interpretability.",
      "authors": [
        "Wang, Yaxuan",
        "Zeng, Zhixin",
        "Li, Qiushan",
        "Deng, Yingrui"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/ijgi11090465",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2220-9964",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.738,
        "snip": 1.194,
        "subject_areas": [
          "Earth and Planetary Sciences (miscellaneous)",
          "Computers in Earth Sciences",
          "Geography, Planning and Development"
        ],
        "title": "ISPRS International Journal of Geo-Information"
      },
      "publication_date": "2022-09-01",
      "selected": null,
      "title": "A Complete Reinforcement-Learning-Based Framework for Urban-Safety Perception",
      "urls": [
        "https://www.mdpi.com/2220-9964/11/9/465/pdf?version=1662516781",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138506447&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hung F."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jhydrol.2022.128015",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 10.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00221694",
        "publisher": "Elsevier B.V.",
        "sjr": 1.67,
        "snip": 1.731,
        "subject_areas": [
          "Water Science and Technology"
        ],
        "title": "Journal of Hydrology"
      },
      "publication_date": "2022-09-01",
      "selected": null,
      "title": "Investigating uncertainties in human adaptation and their impacts on water scarcity in the Colorado river Basin, United States",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132356264&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sakai Y."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.celrep.2022.111275",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Cell Reports"
      },
      "publication_date": "2022-08-30",
      "selected": null,
      "title": "Memory trace imbalance in reinforcement and punishment systems can reinforce implicit choices leading to obsessive-compulsive behavior",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137051647&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Jin Y."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.2202789119",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2022-08-30",
      "selected": null,
      "title": "Superstitious learning of abstract order from random reinforcement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136421316&origin=inward"
      ]
    },
    {
      "abstract": "Social robots that are able to express emotions can potentially improve human&#x2019;s well-being. Whether and how they can learn from interactions between them and human being in a natural way will be key to their success and acceptance by ordinary people. In this paper, we proposed to shape social robot Haru affective behaviors with predicted continuous rewards based on received implicit facial feedback via human-centered reinforcement learning. The implicit facial feedback was estimated with the valence and arousal of received implicit facial feedback using Russell&#x2019;s circumplex model, which can provide a more accurate estimation of the subtle psychological changes of human user, resulting in more effective robot behavior learning. The whole experiment is conducted on the desktop robot Haru, which is primarily used to study emotional interactions with human in different scenarios. Our experimental results show that with our proposed method, Haru can obtain a similar performance to learning from explicit feedback, eliminating the need for human users to get familiar with training interface in advance and resulting in an unobtrusive learning process.",
      "authors": [
        "Hui Wang",
        "Guodong Chen",
        "Randy Gomez",
        "Keisuke Nakamura",
        "Bo He",
        "Guangliang Li"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN53752.2022.9900540",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "769-776",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "RO-MAN 2022 - 31st IEEE International Conference on Robot and Human Interactive Communication: Social, Asocial, and Antisocial Robots"
      },
      "publication_date": "2022-08-29",
      "selected": null,
      "title": "Shaping Haru&amp;#x2019;s Affective Behavior with Valence and Arousal Based Implicit Facial Feedback",
      "urls": [
        "https://dl.acm.org/doi/10.1109/RO-MAN53752.2022.9900540",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140795222&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9900540"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Pennartz C.M.A."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bbr.2022.113969",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01664328",
        "publisher": "Elsevier B.V.",
        "sjr": 0.881,
        "snip": 0.802,
        "subject_areas": [
          "Behavioral Neuroscience"
        ],
        "title": "Behavioural Brain Research"
      },
      "publication_date": "2022-08-26",
      "selected": null,
      "title": "What is neurorepresentationalism? From neural activity and predictive processing to multi-level representations and consciousness",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133229057&origin=inward"
      ]
    },
    {
      "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.",
      "authors": [
        "Ganguli, Deep",
        "Lovitt, Liane",
        "Kernion, Jackson",
        "Askell, Amanda",
        "Bai, Yuntao",
        "Kadavath, Saurav",
        "Mann, Ben",
        "Perez, Ethan",
        "Schiefer, Nicholas",
        "Ndousse, Kamal",
        "Jones, Andy",
        "Bowman, Sam",
        "Chen, Anna",
        "Conerly, Tom",
        "DasSarma, Nova",
        "Drain, Dawn",
        "Elhage, Nelson",
        "El-Showk, Sheer",
        "Fort, Stanislav",
        "Hatfield-Dodds, Zac",
        "Henighan, Tom",
        "Hernandez, Danny",
        "Hume, Tristan",
        "Jacobson, Josh",
        "Johnston, Scott",
        "Kravec, Shauna",
        "Olsson, Catherine",
        "Ringer, Sam",
        "Tran-Johnson, Eli",
        "Amodei, Dario",
        "Brown, Tom",
        "Joseph, Nicholas",
        "McCandlish, Sam",
        "Olah, Chris",
        "Kaplan, Jared",
        "Clark, Jack"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-08-23",
      "selected": null,
      "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
      "urls": [
        "http://arxiv.org/pdf/2209.07858.pdf",
        "http://arxiv.org/pdf/2209.07858v2",
        "http://arxiv.org/abs/2209.07858v2"
      ]
    },
    {
      "abstract": "This paper addresses the problem of inverse reinforcement learning (IRL) -- inferring the reward function of an agent from observing its behavior. IRL can provide a generalizable and compact representation for apprenticeship learning, and enable accurately inferring the preferences of a human in order to assist them. %and provide for more accurate prediction. However, effective IRL is challenging, because many reward functions can be compatible with an observed behavior. We focus on how prior reinforcement learning (RL) experience can be leveraged to make learning these preferences faster and more efficient. We propose the IRL algorithm BASIS (Behavior Acquisition through Successor-feature Intention inference from Samples), which leverages multi-task RL pre-training and successor features to allow an agent to build a strong basis for intentions that spans the space of possible goals in a given domain. When exposed to just a few expert demonstrations optimizing a novel goal, the agent uses its basis to quickly and effectively infer the reward function. Our experiments reveal that our method is highly effective at inferring and optimizing demonstrated reward functions, accurately inferring reward functions from less than 100 trajectories.",
      "authors": [
        "Abdulhai, Marwa",
        "Jaques, Natasha",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-08-09",
      "selected": null,
      "title": "Basis for Intentions: Efficient Inverse Reinforcement Learning using Past Experience",
      "urls": [
        "http://arxiv.org/pdf/2208.04919v1",
        "http://arxiv.org/pdf/2208.04919.pdf",
        "http://arxiv.org/abs/2208.04919v1"
      ]
    },
    {
      "abstract": "In recent years, image generation has made great strides in improving the quality of images, producing high-fidelity ones. Also, quite recently, there are architecture designs, which enable GAN to unsupervisedly learn the semantic attributes represented in different layers. However, there is still a lack of research on generating face images more consistent with human aesthetics. Based on EigenGAN [He et al., ICCV 2021], we build the techniques of reinforcement learning into the generator of EigenGAN. The agent tries to figure out how to alter the semantic attributes of the generated human faces towards more preferable ones. To accomplish this, we trained an aesthetics scoring model that can conduct facial beauty prediction. We also can utilize this scoring model to analyze the correlation between face attributes and aesthetics scores. Empirically, using off-the-shelf techniques from reinforcement learning would not work well. So instead, we present a new variant incorporating the ingredients emerging in the reinforcement learning communities in recent years. Compared to the original generated images, the adjusted ones show clear distinctions concerning various attributes. Experimental results using the MindSpore, show the effectiveness of the proposed method. Altered facial images are commonly more attractive, with significantly improved aesthetic levels.",
      "authors": [
        "Jin, Xin",
        "Zhao, Shu",
        "Zhang, Le",
        "Zhao, Xin",
        "Deng, Qiang",
        "Xiao, Chaoen"
      ],
      "categories": null,
      "citations": null,
      "comments": "13 pages, 5 figures. ACM Multimedia 2022 Technical Demos and Videos\n  Program",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-08-09",
      "selected": null,
      "title": "Attribute Controllable Beautiful Caucasian Face Generation by Aesthetics Driven Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2208.04517v1",
        "http://arxiv.org/pdf/2208.04517.pdf",
        "http://arxiv.org/abs/2208.04517v1"
      ]
    },
    {
      "abstract": "Human-centered AI considers human experiences with AI performance. While abundant research has been helping AI achieve superhuman performance either by fully automatic or weak supervision learning, fewer endeavors are experimenting with how AI can tailor to humans' preferred skill level given fine-grained input. In this work, we guide the curriculum reinforcement learning results towards a preferred performance level that is neither too hard nor too easy via learning from the human decision process. To achieve this, we developed a portable, interactive platform that enables the user to interact with agents online via manipulating the task difficulty, observing performance, and providing curriculum feedback. Our system is highly parallelizable, making it possible for a human to train large-scale reinforcement learning applications that require millions of samples without a server. The result demonstrates the effectiveness of an interactive curriculum for reinforcement learning involving human-in-the-loop. It shows reinforcement learning performance can successfully adjust in sync with the human desired difficulty level. We believe this research will open new doors for achieving flow and personalized adaptive difficulties.",
      "authors": [
        "Zeng, Yilei",
        "Duan, Jiali",
        "Li, Yang",
        "Ferrara, Emilio",
        "Pinto, Lerrel",
        "Kuo, C. -C. Jay",
        "Nikolaidis, Stefanos"
      ],
      "categories": null,
      "citations": null,
      "comments": "6 pages, 7 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-08-04",
      "selected": null,
      "title": "Human Decision Makings on Curriculum Reinforcement Learning with Difficulty Adjustment",
      "urls": [
        "http://arxiv.org/pdf/2208.02932.pdf",
        "http://arxiv.org/abs/2208.02932v1",
        "http://arxiv.org/pdf/2208.02932v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Xue S."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2022.04.013",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "212-223",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "Event-triggered integral reinforcement learning for nonzero-sum games with asymmetric input saturation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129754824&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chen Z."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2022.05.002",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "407-418",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "LAP: Latency-aware automated pruning with dynamic-based filter selection",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132454958&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lubianiker N."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.tins.2022.03.008",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "579-593",
      "publication": {
        "category": "Journal",
        "cite_score": 25.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01662236",
        "publisher": "Elsevier Ltd.",
        "sjr": 4.784,
        "snip": 3.343,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Trends in Neurosciences"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "Neurofeedback through the lens of reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130010323&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "van Diepen M."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1115/1.4054522",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10500472",
        "publisher": "The American Society of Mechanical Engineers(ASME)",
        "sjr": 0.971,
        "snip": 1.485,
        "subject_areas": [
          "Computer Science Applications",
          "Mechanical Engineering",
          "Mechanics of Materials",
          "Computer Graphics and Computer-Aided Design"
        ],
        "title": "Journal of Mechanical Design"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "Co-Design of the Morphology and Actuation of Soft Robots for Locomotion",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144173403&origin=inward"
      ]
    },
    {
      "abstract": "Planning the optimal trajectory of emergency avoidance maneuvers for highly automated vehicles is a complex task with many challenges. The algorithm needs to decrease accident risk by reducing the severity and keeping the car in a controllable state. Optimal trajectory generation considering all aspects of vehicle and environment dynamics is numerically complex, especially if the object to be avoided is moving. This paper presents a hierarchical method for the avoidance of moving objects in an autonomous vehicle, where a reinforcement learning agent is responsible for local planning, while longitudinal and lateral control is performed by the low-level model-predictive controller and Stanley controllers. In the developed architecture, the agent is responsible for the optimization. It is trained in various scenarios to provide the necessary parameters for a polynomial-based path and a velocity profile in a neural network output. The vehicle performs only the first step of the trajectory, which is redesigned repeatedly by the planner based on the new state. In the training phase, the vehicle executes the entire trajectory via low-level controllers to determine the reward value, which realizes a prediction for the future. The agent receives feedback and can further improve its performance. Finally, the proposed framework was tested in a simulation environment and was also compared to human drivers\u2019 abilities.",
      "authors": [
        "Feh\u00e9r, \u00c1rp\u00e1d",
        "Aradi, Szil\u00e1rd",
        "B\u00e9csi, Tam\u00e1s"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/electronics11152346",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2079-9292",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.628,
        "snip": 1.045,
        "subject_areas": [
          "Signal Processing",
          "Hardware and Architecture",
          "Computer Networks and Communications",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Electronics (Switzerland)"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "Online Trajectory Planning with Reinforcement Learning for Pedestrian Avoidance",
      "urls": [
        "https://www.mdpi.com/2079-9292/11/15/2346/pdf?version=1659664244",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136804545&origin=inward"
      ]
    },
    {
      "abstract": "Adaptive dynamic range optimization (ADRO) is a hearing aid fitting rationale which involves adjusting the gains in a number of frequency bands by using a series of rules. The rules reflect the comparison of the estimated percentile occurrences of the sound levels with the audibility and comfort hearing levels of a person suffering from hearing loss. In the study reported in this paper, a previously developed machine learning method was utilized to personalize the ADRO fitting in order to provide an improved hearing experience as compared to the standard ADRO hearing aid fitting. The personalization was carried out based on the user preference model within the framework of maximum likelihood inverse reinforcement learning. The testing of ten subjects with hearing loss was conducted, which indicated that the personalized ADRO was preferred over the standard ADRO on average by about 10 times. Furthermore, a word recognition experiment was conducted, which showed that the personalized ADRO had no adverse impact on speech understanding as compared to the standard ADRO.",
      "authors": [
        "Ni, Aoxin",
        "Akbarzadeh, Sara",
        "Lobarinas, Edward",
        "Kehtarnavaz, Nasser"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/s22166033",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14248220",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.764,
        "snip": 1.317,
        "subject_areas": [
          "Atomic and Molecular Physics, and Optics",
          "Information Systems",
          "Instrumentation",
          "Biochemistry",
          "Analytical Chemistry",
          "Electrical and Electronic Engineering"
        ],
        "title": "Sensors (Switzerland)"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "Personalization of Hearing Aid Fitting Based on Adaptive Dynamic Range Optimization",
      "urls": [
        "https://www.mdpi.com/1424-8220/22/16/6033/pdf?version=1660303530",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136540750&origin=inward"
      ]
    },
    {
      "abstract": "Deep reinforcement learning (Deep RL) algorithms are defined with fully continuous or discrete action spaces. Among DRL algorithms, soft actor\u2013critic (SAC) is a powerful method capable of handling complex and continuous state\u2013action spaces. However, a long training time and data efficiency are the main drawbacks of this algorithm, even though SAC is robust for complex and dynamic environments. One of the proposed solutions to overcome this issue is to utilize human feedback. In this paper, we investigate different forms of human feedback: head direction vs. steering and discrete vs. continuous feedback. To this end, a real-time human demonstration from steer and human head direction with discrete or continuous actions were employed as human feedback in an autonomous driving task in the CARLA simulator. We used alternating actions from a human expert and SAC to have a real-time human demonstration. Furthermore, to test the method without potential individual differences in human performance, we tested the discrete vs. continuous feedback in an inverted pendulum task, with an ideal controller to stand in for the human expert. The results for both the CARLA and the inverted pendulum tasks showed a significant reduction in the training time and a significant increase in gained rewards with discrete feedback, as opposed to continuous feedback, while the action space remained continuous. It was also shown that head direction feedback can be almost as good as steering feedback. We expect our findings to provide a simple yet efficient training method for Deep RL for autonomous driving, utilizing multiple sources of human feedback.",
      "authors": [
        "Savari, Maryam",
        "Choe, Yoonsuck"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/machines10080609",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 2.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2075-1702",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.449,
        "snip": 1.102,
        "subject_areas": [
          "Mechanical Engineering",
          "Computer Science (miscellaneous)",
          "Control and Optimization",
          "Industrial and Manufacturing Engineering",
          "Control and Systems Engineering",
          "Electrical and Electronic Engineering"
        ],
        "title": "Machines"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "Utilizing Human Feedback in Autonomous Driving: Discrete vs. Continuous",
      "urls": [
        "https://www.mdpi.com/2075-1702/10/8/609/pdf?version=1658828791",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137596633&origin=inward"
      ]
    },
    {
      "abstract": "While current autonomous navigation systems allow robots to successfully drive themselves from one point to another in specific environments, they typically require extensive manual parameter re-tuning by human robotics experts in order to function in new environments. Furthermore, even for just one complex environment, a single set of fine-tuned parameters may not work well in different regions of that environment. These problems prohibit reliable mobile robot deployment by non-expert users. As a remedy, we propose Adaptive Planner Parameter Learning (appl), a machine learning framework that can leverage non-expert human interaction via several modalities \u2013 including teleoperated demonstrations, corrective interventions, and evaluative feedback \u2013 and also unsupervised reinforcement learning to learn a parameter policy that can dynamically adjust the parameters of classical navigation systems in response to changes in the environment. appl inherits safety and explainability from classical navigation systems while also enjoying the benefits of machine learning, i.e., the ability to adapt and improve from experience. We present a suite of individual appl methods and also a unifying cycle-of-learning scheme that combines all the proposed methods in a framework that can improve navigation performance through continual, iterative human interaction and simulation training.",
      "authors": [
        "Xuesu Xiao",
        "Zizhao Wang",
        "Zifan Xu",
        "Bo Liu",
        "Garrett Warnell",
        "Gauraang Dhamankar",
        "Anirudh Nair",
        "Peter Stone"
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.robot.2022.104132",
      "keywords": [
        "Machine learning",
        "Motion planning",
        "Mobile robot navigation"
      ],
      "number_of_pages": 15,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 9.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0921-8890",
        "publisher": "Elsevier B.V.",
        "sjr": 1.245,
        "snip": 1.85,
        "subject_areas": [
          "Software",
          "Computer Science Applications",
          "Mathematics (all)",
          "Control and Systems Engineering"
        ],
        "title": "Robotics and Autonomous Systems"
      },
      "publication_date": "2022-08-01",
      "selected": null,
      "title": "APPL: Adaptive Planner Parameter Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129750079&origin=inward",
        "https://dl.acm.org/doi/10.1016/j.robot.2022.104132"
      ]
    },
    {
      "abstract": "Trip planning, which targets at planning a trip consisting of several ordered Points of Interest (POIs) under user-provided constraints, has long been treated as an important application for location-based services. The goal of trip planning is to maximize the chance that the users will follow the planned trip while it is difficult to directly quantify and optimize the chance. Conventional methods either leverage statistical analysis to rank POIs to form a trip or generate trips following pre-defined objectives based on constraint programming to bypass such a problem. However, these methods may fail to reflect the complex latent patterns hidden in the human mobility data. On the other hand, though there are a few deep learning-based trip recommendation methods, these methods still cannot handle the time budget constraint so far. To this end, we propose a TIme-aware Neural Trip Planning (TINT) framework to tackle the above challenges. First of all, we devise a novel attention-based encoder-decoder trip generator that can learn the correlations among POIs and generate trips under given constraints. Then, we propose a specially-designed reinforcement learning (RL) paradigm to directly optimize the objective to obtain an optimal trip generator. For this purpose, we introduce a discriminator, which distinguishes the generated trips from real-life trips taken by users, to provide reward signals to optimize the generator. Subsequently, to ensure the feedback from the discriminator is always instructive, we integrate an adversarial learning strategy into the RL paradigm to update the trip generator and the discriminator alternately. Moreover, we devise a novel pre-training schema to speed up the convergence for an efficient training process. Extensive experiments on four real-world datasets validate the effectiveness and efficiency of our framework, which shows that TINT could remarkably outperform the state-of-the-art baselines within short response time.",
      "authors": [
        "Linlang Jiang",
        "Jingbo Zhou",
        "Tong Xu",
        "Yanyan Li",
        "Hao Chen",
        "Dejing Dou"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IJCNN55064.2022.9892652",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8868-6",
        "issn": "2161-4393",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Neural Networks"
      },
      "publication_date": "2022-07-18",
      "selected": null,
      "title": "Time-aware Neural Trip Planning Reinforced by Human Mobility",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9892652",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140793022&origin=inward"
      ]
    },
    {
      "abstract": "Attention deficit hyperactivity disorder (ADHD) is the most common neurodevelopmental disorder in children. Although the involvement of dopamine in this disorder seems to be established, the nature of dopaminergic dysfunction remains controversial. The purpose of this study was to test whether the key response characteristics of ADHD could be simulated by a mechanistic model that combines a decrease in tonic dopaminergic activity with an increase in phasic responses in cortical-striatal loops during learning reinforcement. To this end, we combined a dynamic model of dopamine with a neurocomputational model of the basal ganglia with multiple action channels. We also included a dynamic model of tonic and phasic dopamine release and control, and a learning procedure driven by tonic and phasic dopamine levels. In the model, the dopamine imbalance is the result of impaired presynaptic regulation of dopamine at the terminal level. Using this model, virtual individuals from a dopamine imbalance group and a control group were trained to associate four stimuli with four actions with fully informative reinforcement feedback. In a second phase, they were tested without feedback. Subjects in the dopamine imbalance group showed poorer performance with more variable reaction times due to the presence of fast and very slow responses, difficulty in choosing between stimuli even when they were of high intensity, and greater sensitivity to noise. Learning history was also significantly more variable in the dopamine imbalance group, explaining 75\\% of the variability in reaction time using quadratic regression. The response profile of the virtual subjects varied as a function of the learning history variability index to produce increasingly severe impairment, beginning with an increase in response variability alone, then accumulating a decrease in performance and finally a learning deficit. Although ADHD is certainly a heterogeneous disorder, these results suggest that typical features of ADHD can be explained by a phasic/tonic imbalance in dopaminergic activity alone.",
      "authors": [
        "V\u00e9ronneau-Veilleux, Florence",
        "Robaey, Philippe",
        "Ursino, Mauro",
        "Nekka, Fahima"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2022.849323",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2022-07-18",
      "selected": null,
      "title": "A mechanistic model of ADHD as resulting from dopamine phasic/tonic imbalance during reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135279882&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mishra K."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neucom.2022.04.029",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "242-254",
      "publication": {
        "category": "Journal",
        "cite_score": 10.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09252312",
        "publisher": "Elsevier B.V.",
        "sjr": 1.481,
        "snip": 1.853,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neurocomputing"
      },
      "publication_date": "2022-07-14",
      "selected": null,
      "title": "Please be polite: Towards building a politeness adaptive dialogue system for goal-oriented conversations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129055304&origin=inward"
      ]
    },
    {
      "abstract": "Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop WebShop -- a simulated e-commerce website environment with $1.18$ million real-world products and $12,087$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. WebShop provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over $1,600$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of $29\\%$, which outperforms rule-based heuristics ($9.6\\%$) but is far lower than human expert performance ($59\\%$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.",
      "authors": [
        "Yao, Shunyu",
        "Chen, Howard",
        "Yang, John",
        "Narasimhan, Karthik"
      ],
      "categories": null,
      "citations": 2,
      "comments": "Project page with code, data, demos: https://webshop-pnlp.github.io.\n  v3 is NeurIPS camera ready version. v4 fixes the choice oracle result as per\n  https://github.com/princeton-nlp/WebShop/issues/15",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-07-04",
      "selected": null,
      "title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
      "urls": [
        "http://arxiv.org/abs/2207.01206v4",
        "http://arxiv.org/pdf/2207.01206.pdf",
        "http://arxiv.org/pdf/2207.01206v4",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163176803&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Maharaj S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/asmb.2684",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "572-589",
      "publication": {
        "category": "Journal",
        "cite_score": 2.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15241904",
        "publisher": "John Wiley and Sons Ltd",
        "sjr": 0.439,
        "snip": 0.892,
        "subject_areas": [
          "Modeling and Simulation",
          "Business, Management and Accounting (all)",
          "Management Science and Operations Research"
        ],
        "title": "Applied Stochastic Models in Business and Industry"
      },
      "publication_date": "2022-07-01",
      "selected": null,
      "title": "Gambits: Theory and evidence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132629897&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Patt V.M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_01873",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "1429-1446",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2022-07-01",
      "selected": null,
      "title": "Hippocampal Contribution to Probabilistic Feedback Learning: Modeling Observation-and Reinforcement-based Processes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133423156&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) commonly assumes access to well-specified reward functions, which many practical applications do not provide. Instead, recently, more work has explored learning what to do from interacting with humans. So far, most of these approaches model humans as being (nosily) rational and, in particular, giving unbiased feedback. We argue that these models are too simplistic and that RL researchers need to develop more realistic human models to design and evaluate their algorithms. In particular, we argue that human models have to be personal, contextual, and dynamic. This paper calls for research from different disciplines to address key questions about how humans provide feedback to AIs and how we can build more robust human-in-the-loop RL systems.",
      "authors": [
        "Lindner, David",
        "El-Assady, Mennatallah"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to Communication in Human-AI Interaction Workshop (CHAI) at\n  IJCAI-ECAI-22",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-06-27",
      "selected": null,
      "title": "Humans are not Boltzmann Distributions: Challenges and Opportunities for Modelling Human Feedback and Interaction in Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2206.13316.pdf",
        "http://arxiv.org/pdf/2206.13316v1",
        "http://arxiv.org/abs/2206.13316v1"
      ]
    },
    {
      "abstract": "The human brain has been an object of extensive investigation in different fields. While several studies have focused on understanding the neural correlates of error processing, advances in brain-machine interface systems using non-invasive techniques further enabled the use of the measured signals in different applications. The possibility of detecting these error-related potentials (ErrPs) under different experimental setups on a single-trial basis has further increased interest in their integration in closed-loop settings to improve system performance, for example, by performing error correction. Fewer works have, however, aimed at reducing future mistakes or learning. We present a review focused on the current literature using non-invasive systems that have combined the ErrPs information specifically in a reinforcement learning framework to go beyond error correction and have used these signals for learning.",
      "authors": [
        "Xavier Fid\u00eancio, Aline",
        "Klaes, Christian",
        "Iossifidis, Ioannis"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2022.806517",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2022-06-24",
      "selected": null,
      "title": "Error-Related Potentials in Reinforcement Learning-Based Brain-Machine Interfaces",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134039823&origin=inward"
      ]
    },
    {
      "abstract": "From the earliest years of our lives, humans use language to express our beliefs and desires. Being able to talk to artificial agents about our preferences would thus fulfill a central goal of value alignment. Yet today, we lack computational models explaining such language use. To address this challenge, we formalize learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviors. We study two distinct types of language: $\\textit{instructions}$, which provide information about the desired policy, and $\\textit{descriptions}$, which provide information about the reward function. We show that the agent's degree of autonomy determines which form of language is optimal: instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker's reward function by reasoning about $\\textit{how}$ the speaker expresses themselves. We validate our models with a behavioral experiment, demonstrating that (1) our speaker model predicts human behavior, and (2) our pragmatic listener successfully recovers humans' reward functions. Finally, we show that this form of social learning can integrate with and reduce regret in traditional reinforcement learning. We hope these insights facilitate a shift from developing agents that $\\textit{obey}$ language to agents that $\\textit{learn}$ from it.",
      "authors": [
        "Sumers, Theodore R",
        "Hawkins, Robert D",
        "Ho, Mark K",
        "Griffiths, Thomas L",
        "Hadfield-Menell, Dylan"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages, 5 figures. Published as a conference paper at NeurIPS 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-06-16",
      "selected": null,
      "title": "How to talk so AI will learn: Instructions, descriptions, and autonomy",
      "urls": [
        "http://arxiv.org/pdf/2206.07870v3",
        "http://arxiv.org/abs/2206.07870v3",
        "http://arxiv.org/pdf/2206.07870.pdf"
      ]
    },
    {
      "abstract": "We develop a novel framework for deep reinforcement learning (RL) algorithms in task constrained path generation problems of robotic manipulators from the human demonstrated trajectories. The main contribution of this paper is to design a reward function that can be used with generic RL algorithms by utilizing the Koopman operator theory to build a human intent model from the human demonstrated trajectories. In order to ensure the developed reward function produces the correct reward, the demonstrated trajectories are further used to create a trust domain within which Koopman operator-based human intent prediction is considered. Otherwise, the proposed algorithm asks for human feedback to receive rewards from the human expert. The designed reward function is incorporated inside the deep Q-learning (DQN) framework, which results in a modified DQN algorithm. The effectiveness of the proposed learning algorithm is demonstrated using a simulated robotic arm to learn paths for constrained end-effector motion and considering the safety of the human in the surroundings of the robot.",
      "authors": [
        "Sinha, Anirban",
        "Wang, Yue"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/frobt.2022.779194",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2296-9144",
        "publisher": "Frontiers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Robotics and AI"
      },
      "publication_date": "2022-06-16",
      "selected": null,
      "title": "Koopman Operator\u2013Based Knowledge-Guided Reinforcement Learning for Safe Human\u2013Robot Interaction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133530759&origin=inward"
      ]
    },
    {
      "abstract": "Motor performance and learning have distinct behavioral and neural signatures and can be uniquely modulated by various informational and motivational factors. Contemporary frameworks describe four different motor learning mechanisms mapped onto specific neural regions which are key for motor skill acquisition: error-based learning (cerebellum), reinforcement learning (basal ganglia), cognitive strategies (prefrontal cortex), and use-dependent learning (motor cortex). However, little is known about the neural circuits engaged during skill acquisition that are modulated specifically by practice-based performance improvement and those that predict recall performance. Based on previous work, we hypothesize that brain activity during practice in primary motor cortex and basal ganglia 1) is associated with trial-by-trial practice performance and 2) is predictive of immediate recall performance. Leveraging the contemporary framework, we use a well-known task paradigm that primarily relies upon cognitive strategy, reinforcement, and use-based learning mechanisms to test our hypotheses. Forty neurotypical young adults were asked to practice a pinch force tracking task. Participants received performance feedback after each trial during practice. We used whole brain analysis of functional magnetic resonance imaging (fMRI) and behavioral performance measures (i.e., time-on-target and self-efficacy) during the practice phase to determine which brain activation patterns are 1) associated with trial-by-trial tracking performance and 2) predictive of immediate no-feedback retention performance. We observed brain activations in the frontal orbital cortex, putamen, amygdala, and insula correlated with tracking performance improvement during practice. In contrast, a different set of performance-related activated regions were observed that were associated with immediate retention performance that included the primary motor cortex, superior frontal gyrus, somatosensory cortex, angular gyrus, and parietal gyrus. Our findings demonstrate that improved practice performance and recall of a sensorimotor skill are correlated with distinct neural activity patterns during acquisition, drawing on different motor learning mechanisms during encoding. While motor performance improvements depend on both cortical and subcortical regions, motor skill recall depends primarily on prefrontal and motor cortices. We discuss possible interpretations for why our hypothesis regarding basal ganglia activity and retention performance was not supported. Understanding the different neural mechanisms engaged in motor performance and learning may inform novel interventions to enhance motor skill learning. ",
      "authors": [
        "Beroukhim-Kay, Dorsa",
        "Kim, Bokkyu",
        "Monterosso, John",
        "Lewthwaite, Rebecca",
        "Winstein, Carolee"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2022.900405",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2022-06-13",
      "selected": null,
      "title": "Different Patterns of Neural Activity Characterize Motor Skill Performance During Acquisition and Retention",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133532540&origin=inward"
      ]
    },
    {
      "abstract": "Many database optimization problems, e.g., slow SQL diagnosis, database testing, optimizer tuning, require a large volume of SQL queries. Due to privacy issues, it is hard to obtain real SQL queries, and thus SQL generation is a very important task in database optimization. Existing SQL generation methods either randomly generate SQL queries or rely on human-crafted SQL templates to generate SQL queries, but they cannot meet various user specific requirements, e.g., slow SQL queries, SQL queries with large result sizes. To address this problem, this paper studies the problem of constraint-aware SQL generation, which, given a constraint (e.g., cardinality within [1k,2k]), generates SQL queries satisfying the constraint. This problem is rather challenging, because it is rather hard to capture the relationship from query constraint (e.g., cardinality and cost) to SQL queries and thus it is hard to guide a generation method to explore the SQL generation direction towards meeting the constraint. To address this challenge, we propose a reinforcement learning (RL) based framework LearnedSQLGen, for generating queries satisfying the constraint. LearnedSQLGen adopts an exploration-exploitation strategy that exploits the generation direction following the query constraint, which is learned from query execution feedback. We judiciously design the reward function in RL to guide the generation process accurately. We integrate a finite-state machine to generate valid SQL queries. Experimental results on three benchmarks showed that LearnedSQLGen significantly outperformed the baselines in terms of both accuracy (30% better) and efficiency (10-35 times).",
      "authors": [
        "Lixi Zhang",
        "Chengliang Chai",
        "Xuanhe Zhou",
        "Guoliang Li"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3514221.3526155",
      "keywords": [
        "machine learning",
        "database",
        "SQL generation"
      ],
      "number_of_pages": 14,
      "pages": "945-958",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450392495",
        "issn": "07308078",
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the ACM SIGMOD International Conference on Management of Data"
      },
      "publication_date": "2022-06-11",
      "selected": null,
      "title": "LearnedSQLGen: Constraint-aware SQL Generation using Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3514221.3526155",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132790127&origin=inward"
      ]
    },
    {
      "abstract": "The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperforms the partial return preference model with finite training data in otherwise the same setting. Additionally, we find that our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research. We have open sourced our experimental code, the human preferences dataset we gathered, and our training and preference elicitation interfaces for gathering a such a dataset.",
      "authors": [
        "Knox, W. Bradley",
        "Hatgis-Kessell, Stephane",
        "Booth, Serena",
        "Niekum, Scott",
        "Stone, Peter",
        "Allievi, Alessandro"
      ],
      "categories": null,
      "citations": null,
      "comments": "16 pages (40 pages with references and appendix), 23 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-06-05",
      "selected": null,
      "title": "Models of human preference for learning reward functions",
      "urls": [
        "http://arxiv.org/abs/2206.02231v3",
        "http://arxiv.org/pdf/2206.02231.pdf",
        "http://arxiv.org/pdf/2206.02231v3"
      ]
    },
    {
      "abstract": "Automotive is becoming more and more sensor-equipped. Collision avoidance, lane departure warning, and self-parking are examples of applications becoming possible with the adoption of more sensors in the automotive industry. Moreover, the driver is now equipped with sensory systems like wearables and mobile phones. This rich sensory environment and the real-time streaming of contextual data from the vehicle make the human factor integral in the loop of computation. By integrating the human&#x2019;s behavior and reaction into the advanced driver-assistance systems (ADAS), the vehicles become a more context-aware entity. Hence, we propose MAConAuto, a framework that helps design human-in-the-loop automotive systems by providing a common platform to engage the rich sensory systems in wearables and mobile to have context-aware applications. By personalizing the context adaptation in automotive applications, MAConAuto learns the behavior and reactions of the human to adapt to the personalized preference where interventions are continuously tuned using Reinforcement Learning. Our general framework satisfies three main design properties, adaptability, generalizability, and conflict resolution. We show how MAConAuto can be used as a framework to design two applications as human-centric applications, forward collision warning, and vehicle HVAC system with negligible time overhead to the average human response time.",
      "authors": [
        "Salma Elmalaki"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/IV51971.2022.9827415",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "740-749",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8822-8",
        "issn": null,
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Intelligent Vehicles Symposium, Proceedings"
      },
      "publication_date": "2022-06-04",
      "selected": null,
      "title": "MAConAuto: Framework for Mobile-Assisted Human-in-the-Loop Automotive System",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9827415",
        "https://dl.acm.org/doi/10.1109/IV51971.2022.9827415",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135378104&origin=inward"
      ]
    },
    {
      "abstract": "<p>This study aimed to investigate whether instrumental reward learning is affected by the cardiac cycle. To this end, we examined the effects of the cardiac cycle (systole or diastole) on the computational processes underlying the participants\u2019 choices in the instrumental learning task. In the instrumental learning task, participants were required to select one of two discriminative stimuli (neutral visual stimuli) and immediately receive reward/punishment feedback depending on the probability assigned to the chosen stimuli. To manipulate the cardiac cycle, the presentation of discriminative stimuli was timed to coincide with either cardiac systole or diastole. We fitted the participants\u2019 choices in the task with reinforcement learning (RL) models and estimated parameters involving instrumental learning (i.e., learning rate and inverse temperature) separately in the systole and diastole trials. Model-based analysis revealed that the learning rate for positive prediction errors was higher than that for negative prediction errors in the systole trials; however, learning rates did not differ between positive and negative prediction errors in the diastole trials. These results demonstrate that the natural fluctuation of cardiac afferent signals can affect asymmetric value updating in instrumental reward learning.</p>",
      "authors": [
        "Kimura, Kenta",
        "Kanayama, Noriaki",
        "Toyama, Asako",
        "Katahira, Kentaro"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2022.889440",
      "keywords": [
        "interoception",
        "Baroreflex",
        "decision-making",
        "cardiac cycle",
        "Reinforcement Leaning",
        "Reward Learning",
        "instrumental learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2022-06-02",
      "selected": null,
      "title": "Cardiac Cycle Affects the Asymmetric Value Updating in Instrumental Reward Learning",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.889440/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132810861&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Eckstein M.K."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.dcn.2022.101106",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 8.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18789293",
        "publisher": "Elsevier B.V.",
        "sjr": 1.792,
        "snip": 1.42,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Developmental Cognitive Neuroscience"
      },
      "publication_date": "2022-06-01",
      "selected": null,
      "title": "Reinforcement learning and Bayesian inference provide complementary models for the unique advantage of adolescents in stochastic reversal",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129761509&origin=inward"
      ]
    },
    {
      "abstract": "Autonomous excavation operation is a major trend in the development of a new generation of intelligent tunnel boring machines (TBMs). However, existing technologies are limited to supervised machine learning and static optimization, which cannot outperform human operation and deal with ever changing geological conditions and the long-term performance measure. The aim of this study is to resolve the problem of dynamic optimization of the shield excavation performance, as well as to achieve autonomous optimal excavation. In this study, a novel autonomous optimal excavation approach that integrates deep reinforcement learning and optimal control is proposed for shield machines. Based on a first-principles analysis of the machine-ground interaction dynamics of the excavation process, a deep neural network model is developed using construction field data consisting of 1.1 million samples. The multi-system coupling mechanism is revealed by establishing an overall system model. Based on the overall system analysis, the autonomous optimal excavation problem is decomposed into a multi-objective dynamic optimization problem and an optimal control problem. Subsequently, a dimensionless multi-objective comprehensive excavation performance measure is proposed. A deep reinforcement learning method is used to solve for the optimal action sequence trajectory, and optimal closed-loop feedback controllers are designed to achieve accurate execution. The performance of the proposed approach is compared to that of human operation by using the construction field data. The simulation results show that the proposed approach not only has the potential to replace human operation but also can significantly improve the comprehensive excavation performance.",
      "authors": [
        "Zhang, Ya-kun",
        "Gong, Guo-fang",
        "Yang, Hua-yong",
        "Chen, Yu-xi",
        "Chen, Geng-lin"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1631/jzus.A2100325",
      "keywords": [],
      "number_of_pages": 21,
      "pages": "458-478",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1673565X",
        "publisher": "Zhejiang University Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Zhejiang University: Science A"
      },
      "publication_date": "2022-06-01",
      "selected": null,
      "title": "Towards autonomous and optimal excavation of shield machine: a deep reinforcement learning-based approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133660151&origin=inward",
        "https://link.springer.com/content/pdf/10.1631/jzus.A2100325.pdf"
      ]
    },
    {
      "abstract": "Motor adaptation can be achieved through error-based learning, driven by sensory prediction errors, or reinforcement learning, driven by reward prediction errors. Recent work on visuomotor adaptation has shown that reinforcement learning leads to more persistent adaptation when visual feedback is removed, compared to error-based learning in which continuous visual feedback of the movement is provided. However, there is evidence that error-based learning with terminal visual feedback of the movement (provided at the end of movement) may be driven by both sensory and reward prediction errors. Here we examined the influence of feedback on learning using a visuomotor adaptation task in which participants moved a cursor to a single target while the gain between hand and cursor movement displacement was gradually altered. Different groups received either continuous error feedback (EC), terminal error feedback (ET), or binary reinforcement feedback (success/fail) at the end of the movement (R). Following adaptation we tested generalization to targets located in different directions and found that generalization in the ET group was intermediate between the EC and R groups. We then examined the persistence of adaptation in the EC and ET groups when the cursor was extinguished and only binary reward feedback was provided. Whereas performance was maintained in the ET group, it quickly deteriorated in the EC group. These results suggest that terminal error feedback leads to a more robust form of learning than continuous error feedback. In addition our findings are consistent with the view that error-based learning with terminal feedback involves both error-based and reinforcement learning.",
      "authors": [
        "Tsuyoshi Ikegami",
        "J. Randall Flanagan",
        "Daniel M. Wolpert"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0269297",
      "keywords": [
        "Robotics",
        "Learning",
        "Radii",
        "Vision",
        "Hands",
        "Sensory perception",
        "Mexican people",
        "Musculoskeletal mechanics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2022-06-01",
      "selected": null,
      "title": "Reach adaption to a visuomotor gain with terminal error feedback involves reinforcement learning",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0269297&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131215916&origin=inward"
      ]
    },
    {
      "abstract": "We generalise the problem of reward modelling (RM) for reinforcement learning (RL) to handle non-Markovian rewards. Existing work assumes that human evaluators observe each step in a trajectory independently when providing feedback on agent behaviour. In this work, we remove this assumption, extending RM to capture temporal dependencies in human assessment of trajectories. We show how RM can be approached as a multiple instance learning (MIL) problem, where trajectories are treated as bags with return labels, and steps within the trajectories are instances with unseen reward labels. We go on to develop new MIL models that are able to capture the time dependencies in labelled trajectories. We demonstrate on a range of RL tasks that our novel MIL models can reconstruct reward functions to a high level of accuracy, and can be used to train high-performing agent policies.",
      "authors": [
        "Early, Joseph",
        "Bewley, Tom",
        "Evers, Christine",
        "Ramchurn, Sarvapali"
      ],
      "categories": null,
      "citations": 0,
      "comments": "27 pages (10 main content; 2 references; 1 checklist; 14 appendix).\n  14 figures (9 main content; 5 appendix). Published at NeurIPS 2022.\n  Revisions: v2) Updated to NeurIPS camera ready version (extra experiments)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-05-30",
      "selected": null,
      "title": "Non-Markovian Reward Modelling from Trajectory Labels via Interpretable Multiple Instance Learning",
      "urls": [
        "http://arxiv.org/abs/2205.15367v2",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150164765&origin=inward",
        "http://arxiv.org/pdf/2205.15367v2",
        "http://arxiv.org/pdf/2205.15367.pdf"
      ]
    },
    {
      "abstract": "Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Reinforcement Learning agent coupled with Monte Carlo Tree Search to quickly identify promising recourse plans. Our empirical evaluation on real-world datasets highlights how PEAR produces high-quality personalized recourse in only a handful of iterations.",
      "authors": [
        "De Toni, Giovanni",
        "Viappiani, Paolo",
        "Teso, Stefano",
        "Lepri, Bruno",
        "Passerini, Andrea"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published in Transactions in Machine Learning Research (TMLR),\n  January 2024. See https://openreview.net/forum?id=8sg2I9zXgO for the official\n  submission",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-27",
      "selected": null,
      "title": "Personalized Algorithmic Recourse with Preference Elicitation",
      "urls": [
        "http://arxiv.org/pdf/2205.13743v5",
        "http://arxiv.org/abs/2205.13743v5",
        "http://arxiv.org/pdf/2205.13743.pdf"
      ]
    },
    {
      "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.",
      "authors": [
        "Liang, Xinran",
        "Shu, Katherine",
        "Lee, Kimin",
        "Abbeel, Pieter"
      ],
      "categories": null,
      "citations": 7,
      "comments": "ICLR 2022. Last two authors advised equally",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ICLR 2022 - 10th International Conference on Learning Representations"
      },
      "publication_date": "2022-05-24",
      "selected": null,
      "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2205.12401v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143398485&origin=inward",
        "http://arxiv.org/pdf/2205.12401.pdf",
        "http://arxiv.org/pdf/2205.12401v1"
      ]
    },
    {
      "abstract": "Constructive studies on symbol emergence systems seek to investigate computational models that can better explain human language evolution, the creation of symbol systems, and the construction of internal representations. This study provides a new model for emergent communication, which is based on a probabilistic generative model (PGM) instead of a discriminative model based on deep reinforcement learning. We define the Metropolis-Hastings (MH) naming game by generalizing previously proposed models. It is not a referential game with explicit feedback, as assumed by many emergent communication studies. Instead, it is a game based on joint attention without explicit feedback. Mathematically, the MH naming game is proved to be a type of MH algorithm for an integrative PGM that combines two agents that play the naming game. From this viewpoint, symbol emergence is regarded as decentralized Bayesian inference, and semiotic communication is regarded as inter-personal cross-modal inference. This notion leads to the collective predictive coding hypothesis} regarding language evolution and, in general, the emergence of symbols. We also propose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE), a deep generative model for emergent communication based on the MH naming game. The model has been validated on MNIST and Fruits 360 datasets. Experimental findings demonstrate that categories are formed from real images observed by agents, and signs are correctly shared across agents by successfully utilizing both of the observations of agents via the MH naming game. Furthermore, scholars verified that visual images were recalled from signs uttered by agents. Notably, emergent communication without supervision and reward feedback improved the performance of the unsupervised representation learning of agents.",
      "authors": [
        "Taniguchi, Tadahiro",
        "Yoshida, Yuto",
        "Taniguchi, Akira",
        "Hagiwara, Yoshinobu"
      ],
      "categories": null,
      "citations": null,
      "comments": "23 pages, 12 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-24",
      "selected": null,
      "title": "Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models",
      "urls": [
        "http://arxiv.org/pdf/2205.12392.pdf",
        "http://arxiv.org/abs/2205.12392v2",
        "http://arxiv.org/pdf/2205.12392v2"
      ]
    },
    {
      "abstract": "How can we train an assistive human-machine interface (e.g., an electromyography-based limb prosthesis) to translate a user's raw command signals into the actions of a robot or computer when there is no prior mapping, we cannot ask the user for supervision in the form of action labels or reward feedback, and we do not have prior knowledge of the tasks the user is trying to accomplish? The key idea in this paper is that, regardless of the task, when an interface is more intuitive, the user's commands are less noisy. We formalize this idea as a completely unsupervised objective for optimizing interfaces: the mutual information between the user's command signals and the induced state transitions in the environment. To evaluate whether this mutual information score can distinguish between effective and ineffective interfaces, we conduct an observational study on 540K examples of users operating various keyboard and eye gaze interfaces for typing, controlling simulated robots, and playing video games. The results show that our mutual information scores are predictive of the ground-truth task completion metrics in a variety of domains, with an average Spearman's rank correlation of 0.43. In addition to offline evaluation of existing interfaces, we use our unsupervised objective to learn an interface from scratch: we randomly initialize the interface, have the user attempt to perform their desired tasks using the interface, measure the mutual information score, and update the interface to maximize mutual information through reinforcement learning. We evaluate our method through a user study with 12 participants who perform a 2D cursor control task using a perturbed mouse, and an experiment with one user playing the Lunar Lander game using hand gestures. The results show that we can learn an interface from scratch, without any user supervision or prior knowledge of tasks, in under 30 minutes.",
      "authors": [
        "Reddy, Siddharth",
        "Levine, Sergey",
        "Dragan, Anca D."
      ],
      "categories": null,
      "citations": 0,
      "comments": "Accepted to Neural Information Processing Systems (NeurIPS) 2022",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-05-24",
      "selected": null,
      "title": "First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual Information Maximization",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85159483398&origin=inward",
        "http://arxiv.org/abs/2205.12381v2",
        "http://arxiv.org/pdf/2205.12381.pdf",
        "http://arxiv.org/pdf/2205.12381v2"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution.",
      "authors": [
        "Korbak, Tomasz",
        "Perez, Ethan",
        "Buckley, Christopher L"
      ],
      "categories": null,
      "citations": null,
      "comments": "Findings of EMNLP 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-23",
      "selected": null,
      "title": "RL with KL penalties is better viewed as Bayesian inference",
      "urls": [
        "http://arxiv.org/abs/2205.11275v2",
        "http://arxiv.org/pdf/2205.11275.pdf",
        "http://arxiv.org/pdf/2205.11275v2"
      ]
    },
    {
      "abstract": "We study human-in-the-loop reinforcement learning (RL) with trajectory preferences, where instead of receiving a numeric reward at each step, the agent only receives preferences over trajectory pairs from a human overseer. The goal of the agent is to learn the optimal policy which is most preferred by the human overseer. Despite the empirical successes, the theoretical understanding of preference-based RL (PbRL) is only limited to the tabular case. In this paper, we propose the first optimistic model-based algorithm for PbRL with general function approximation, which estimates the model using value-targeted regression and calculates the exploratory policies by solving an optimistic planning problem. Our algorithm achieves the regret of $\\tilde{O} (\\operatorname{poly}(d H) \\sqrt{K} )$, where $d$ is the complexity measure of the transition and preference model depending on the Eluder dimension and log-covering numbers, $H$ is the planning horizon, $K$ is the number of episodes, and $\\tilde O(\\cdot)$ omits logarithmic terms. Our lower bound indicates that our algorithm is near-optimal when specialized to the linear setting. Furthermore, we extend the PbRL problem by formulating a novel problem called RL with $n$-wise comparisons, and provide the first sample-efficient algorithm for this new setting. To the best of our knowledge, this is the first theoretical result for PbRL with (general) function approximation.",
      "authors": [
        "Chen, Xiaoyu",
        "Zhong, Han",
        "Yang, Zhuoran",
        "Wang, Zhaoran",
        "Wang, Liwei"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 21,
      "pages": "3773-3793",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2022-05-23",
      "selected": null,
      "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation",
      "urls": [
        "http://arxiv.org/pdf/2205.11140v2",
        "http://arxiv.org/pdf/2205.11140.pdf",
        "http://arxiv.org/abs/2205.11140v2",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147349740&origin=inward"
      ]
    },
    {
      "abstract": "In the peg insertion task, human pays attention to the seam between the peg and the hole and tries to fill it continuously with visual feedback. By imitating the human&#x0027;s behavior, we design architectures with position and orientation estimators based on the seam representation for pose alignment, which proves to be general to the unseen peg geometries. By putting the estimators into the closed-loop control with reinforcement learning, we further achieve higher or comparable success rate, efficiency, and robustness compared with the baseline methods. The policy is trained totally in simulation without any manual intervention. To achieve sim-to-real, a learnable segmentation module with automatic data collecting and labeling can be easily trained to decouple the perception and the policy, which helps the model trained in simulation quickly adapting to the real world with negligible effort. Results are presented in simulation and on a physical robot. Code, videos, and supplemental material are available at https://github.com/xieliang555/SFN.git",
      "authors": [
        "Liang Xie",
        "Hongxiang Yu",
        "Yinghao Zhao",
        "Haodong Zhang",
        "Zhongxiang Zhou",
        "Minhang Wang",
        "Yue Wang",
        "Rong Xiong"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA46639.2022.9812429",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "2982-2988",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-9682-4",
        "issn": "10504729",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2022-05-23",
      "selected": null,
      "title": "Learning to Fill the Seam by Vision: Sub-millimeter Peg-in-hole on Unseen Shapes in Real World",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136324234&origin=inward",
        "https://dl.acm.org/doi/10.1109/ICRA46639.2022.9812429",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9812429"
      ]
    },
    {
      "abstract": "Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (e.g., webcam images of eye gaze) can be challenging, especially when it involves inferring the user&#x0027;s desired action in the absence of a natural &#x2018;default&#x2019; interface. Reinforcement learning from online user feedback on the system&#x0027;s performance presents a natural solution to this problem, and enables the interface to adapt to individual users. However, this approach tends to require a large amount of human-in-the-loop training data, especially when feedback is sparse. We propose a hierarchical solution that learns efficiently from sparse user feedback: we use offline pre-training to acquire a latent embedding space of useful, high-level robot behaviors, which, in turn, enables the system to focus on using online user feedback to learn a mapping from user inputs to desired high-level behaviors. The key insight is that access to a pre-trained policy enables the system to learn more from sparse rewards than a na&#x00EF;ve RL algorithm: using the pre-trained policy, the system can make use of successful task executions to relabel, in hindsight, what the user actually meant to do during unsuccessful executions. We evaluate our method primarily through a user study with 12 participants who perform tasks in three simulated robotic manipulation domains using a webcam and their eye gaze: flipping light switches, opening a shelf door to reach objects inside, and rotating a valve. The results show that our method successfully learns to map 128-dimensional gaze features to 7-dimensional joint torques from sparse rewards in under 10 minutes of online training, and seamlessly helps users who employ different gaze strategies, while adapting to distributional shift in webcam inputs, tasks, and environments",
      "authors": [
        "Sean Chen",
        "Jensen Gao",
        "Siddharth Reddy",
        "Glen Berseth",
        "Anca D. Dragan",
        "Sergey Levine"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA46639.2022.9812442",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7505-7512",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-9682-4",
        "issn": "10504729",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2022-05-23",
      "selected": null,
      "title": "ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9812442",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136334505&origin=inward",
        "https://dl.acm.org/doi/10.1109/ICRA46639.2022.9812442"
      ]
    },
    {
      "abstract": "Driving automation is gradually replacing human driving maneuvers in different applications such as adaptive cruise control and lane keeping. However, contemporary driving automation applications based on expert systems or prede-fined control strategies are not in line with individual human driver&#x0027;s preference. To overcome this problem, we propose a Personalized Adaptive Cruise Control (P-ACC) system that can learn the driver&#x0027;s car-following preferences from historical data using model-based maximum entropy Inverse Reinforcement Learning (IRL). Once activated in real-time, the P-ACC system first classifies the driver type and the weather type (at that moment). The vehicle is then controlled using the pre-trained IRL model on the cloud of the associated class. The personalized IRL model on the cloud will be updated as more human driving data is collected from various scenarios. Numerical simulation with real-world naturalistic driving data shows that, the accuracy of reproducing the real-world driving profile improves up to 30.1&#x0025; in terms of speed and 36.5&#x0025; in terms of distance gap, when P-ACC is compared with the Intelligent Driver Model (IDM). Game engine-based human-in-the-loop simulation demonstrates that, the takeover frequency of the driver during the usage of P-ACC decreases up to 93.4&#x0025;, compared with that during the usage of IDM-based ACC.",
      "authors": [
        "Zhouqiao Zhao",
        "Ziran Wang",
        "Kyungtae Han",
        "Rohit Gupta",
        "Prashant Tiwari",
        "Guoyuan Wu",
        "Matthew J. Barth"
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA46639.2022.9812446",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "2891-2897",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-9682-4",
        "issn": "10504729",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2022-05-23",
      "selected": null,
      "title": "Personalized Car Following for Autonomous Driving with Inverse Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1109/ICRA46639.2022.9812446",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9812446",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136139530&origin=inward"
      ]
    },
    {
      "abstract": "In the foreseeable future, connected and auto-mated vehicles (CAVs) and human-driven vehicles will share the road networks together. In such a mixed traffic environment, CAVs need to understand and predict maneuvers of surrounding vehicles for safer and more efficient interactions, especially when human drivers bring in a wide range of uncertainties. In this paper, we propose a learning-based lane-change prediction algorithm that considers the driving behaviors of the target human driver. To provide accurate maneuver prediction, we adopt a hierarchical structure that seamlessly seals both the lane-change decision prediction and the vehicle trajectory pre-diction together. Specifically, we propose a lane-change decision prediction method based on a Long-Short Term Memory (LSTM) network, and a trajectories prediction considering driver preference and vehicular interactions based on Inverse Reinforcement Learning (IRL). To validate the performance of the proposed methodology, a case study of an on-ramp merging scenario is conducted on a uniquely built human-in-the-loop simulation platform that can provide an immersive driving environment, collect data of lane-change behaviors, and test drivers&#x0027; reactions to the prediction results in real time. It is shown in the simulation results that we can predict the lane-change decision 3 seconds before the vehicle crosses the line to another lane, and the Mean Euclidean Distance between the predicted trajectory and ground truth is 0.39 meters within a 4-second prediction window.",
      "authors": [
        "Xishun Liao",
        "Ziran Wang",
        "Xuanpeng Zhao",
        "Zhouqiao Zhao",
        "Kyungtae Han",
        "Prashant Tiwari",
        "Matthew J. Barth",
        "Guoyuan Wu"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA46639.2022.9812269",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "948-954",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-9682-4",
        "issn": "10504729",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2022-05-23",
      "selected": null,
      "title": "Online Prediction of Lane Change with a Hierarchical Learning-Based Approach",
      "urls": [
        "https://dl.acm.org/doi/10.1109/ICRA46639.2022.9812269",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9812269",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136136388&origin=inward"
      ]
    },
    {
      "abstract": "In the field of Natural Language Processing, there are many tasks that can be tackled effectively using the cross-entropy (CE) loss function. However, the task of dialog generation poses unique challenges for CE loss. This is because CE loss assumes that, for any given input, the only possible output is the one available as the ground truth in the training dataset. But, in dialog generation, there can be multiple valid responses (for a given context) that not only have different surface forms but can also be semantically different. Furthermore, CE loss computation for the dialog generation task does not take the input context into consideration and, hence, it grades the response irrespective of the context. To grade the generated response for qualities like relevance, engagingness, etc., the loss function should depend on both the context and the generated response. To address these limitations, this paper proposes CORAL, a novel loss function based on a reinforcement learning (RL) view of the dialog generation task with a reward function that estimates human preference for generated responses while considering both the context and the response. Furthermore, to overcome challenges such as high sample complexity of RL training and a large action space, we propose a mix-policy training algorithm. Notably, using CORAL we can train dialog generation models without assuming the ground-truth as the only correct response. Extensive comparisons on benchmark datasets demonstrate that CORAL based models outperform strong state-of-the-art baseline models of different sizes.",
      "authors": [
        "Santra, Bishal",
        "Ghadia, Ravi",
        "Gupta, Manish",
        "Goyal, Pawan"
      ],
      "categories": null,
      "citations": null,
      "comments": "15 pages, 3 figures. TLDR: CORAL proposes a novel loss function for\n  dialog generation, incorporating context and multiple valid responses. It\n  outperforms existing models by optimizing human preference through\n  reinforcement learning",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-21",
      "selected": null,
      "title": "CORAL: Contextual Response Retrievability Loss Function for Training Dialog Generation Models",
      "urls": [
        "http://arxiv.org/abs/2205.10558v3",
        "http://arxiv.org/pdf/2205.10558.pdf",
        "http://arxiv.org/pdf/2205.10558v3"
      ]
    },
    {
      "abstract": "Curiosity for machine agents has been a focus of intense research. The study of human and animal curiosity, particularly specific curiosity, has unearthed several properties that would offer important benefits for machine learners, but that have not yet been well-explored in machine intelligence. In this work, we introduce three of the most immediate of these properties -- directedness, cessation when satisfied, and voluntary exposure -- and show how they may be implemented together in a proof-of-concept reinforcement learning agent; further, we demonstrate how the properties manifest in the behaviour of this agent in a simple non-episodic grid-world environment that includes curiosity-inducing locations and induced targets of curiosity. As we would hope, the agent exhibits short-term directed behaviour while updating long-term preferences to adaptively seek out curiosity-inducing situations. This work therefore presents a novel view into how specific curiosity operates and in the future might be integrated into the behaviour of goal-seeking, decision-making agents in complex environments.",
      "authors": [
        "Ady, Nadia M.",
        "Shariff, Roshan",
        "G\u00fcnther, Johannes",
        "Pilarski, Patrick M."
      ],
      "categories": null,
      "citations": null,
      "comments": "5 pages, 6 figures, accepted at the 5th Multi-disciplinary Conference\n  on Reinforcement Learning and Decision Making (RLDM2022), June 8-11, 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-20",
      "selected": null,
      "title": "Prototyping three key properties of specific curiosity in computational reinforcement learning",
      "urls": [
        "http://arxiv.org/pdf/2205.10407.pdf",
        "http://arxiv.org/pdf/2205.10407v1",
        "http://arxiv.org/abs/2205.10407v1"
      ]
    },
    {
      "abstract": "Developments in reinforcement learning (RL) have allowed algorithms to achieve impressive performance in highly complex, but largely static problems. In contrast, biological learning seems to value efficiency of adaptation to a constantly-changing world. Here we build on a recently-proposed neuronal learning rule that assumes each neuron can optimize its energy balance by predicting its own future activity. That assumption leads to a neuronal learning rule that uses presynaptic input to modulate prediction error. We argue that an analogous RL rule would use action probability to modulate reward prediction error. This modulation makes the agent more sensitive to negative experiences, and more careful in forming preferences. We embed the proposed rule in both tabular and deep-Q-network RL algorithms, and find that it outperforms conventional algorithms in simple, but highly-dynamic tasks. We suggest that the new rule encapsulates a core principle of biological intelligence; an important component for allowing algorithms to adapt to change in a human-like way.",
      "authors": [
        "Chalmers, Eric",
        "Luczak, Artur"
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages, 5 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-19",
      "selected": null,
      "title": "Reinforcement Learning with Brain-Inspired Modulation can Improve Adaptation to Environmental Changes",
      "urls": [
        "http://arxiv.org/abs/2205.09729v1",
        "http://arxiv.org/pdf/2205.09729.pdf",
        "http://arxiv.org/pdf/2205.09729v1"
      ]
    },
    {
      "abstract": "Control room video surveillance is an important source of information for ensuring public safety. To facilitate the process, a Decision-Support System (DSS) designed for the security task force is vital and necessary to take decisions rapidly using a sea of information. In case of mission critical operation, Situational Awareness (SA) which consists of knowing what is going on around you at any given time plays a crucial role across a variety of industries and should be placed at the center of our DSS. In our approach, SA system will take advantage of the human factor thanks to the reinforcement signal whereas previous work on this field focus on improving knowledge level of DSS at first and then, uses the human factor only for decision-making. In this paper, we propose a situational awareness-centric decision-support system framework for mission-critical operations driven by Quality of Experience (QoE). Our idea is inspired by the reinforcement learning feedback process which updates the environment understanding of our DSS. The feedback is injected by a QoE built on user perception. Our approach will allow our DSS to evolve according to the context with an up-to-date SA.",
      "authors": [
        "Abhishek Djeachandrane",
        "Said Hoceini",
        "Serge Delmas",
        "Jean-Michel Duquerrois",
        "Abdelhamid Mellouk"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICC45855.2022.9838601",
      "keywords": [
        "surveillance videos",
        "reinforcement signal",
        "QoE",
        "decision-support system",
        "Situational awareness"
      ],
      "number_of_pages": 6,
      "pages": "335-340",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-8348-4",
        "issn": "1550-3607",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ICC 2022 - IEEE International Conference on Communications"
      },
      "publication_date": "2022-05-16",
      "selected": null,
      "title": "QoE-based Situational Awareness-Centric Decision Support for Network Video Surveillance",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137263009&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9838601"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dan O."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsych.2021.09.019",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "860-868",
      "publication": {
        "category": "Journal",
        "cite_score": 19.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00063223",
        "publisher": "Elsevier Inc.",
        "sjr": 3.768,
        "snip": 2.412,
        "subject_areas": [
          "Biological Psychiatry"
        ],
        "title": "Biological Psychiatry"
      },
      "publication_date": "2022-05-15",
      "selected": null,
      "title": "A Neuroeconomics Approach to Obesity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120413151&origin=inward"
      ]
    },
    {
      "abstract": "Advances in artificial intelligence often stem from the development of new environments that abstract real-world situations into a form where research can be done conveniently. This paper contributes such an environment based on ideas inspired by elementary Microeconomics. Agents learn to produce resources in a spatially complex world, trade them with one another, and consume those that they prefer. We show that the emergent production, consumption, and pricing behaviors respond to environmental conditions in the directions predicted by supply and demand shifts in Microeconomics. We also demonstrate settings where the agents' emergent prices for goods vary over space, reflecting the local abundance of goods. After the price disparities emerge, some agents then discover a niche of transporting goods between regions with different prevailing prices -- a profitable strategy because they can buy goods where they are cheap and sell them where they are expensive. Finally, in a series of ablation experiments, we investigate how choices in the environmental rewards, bartering actions, agent architecture, and ability to consume tradable goods can either aid or inhibit the emergence of this economic behavior. This work is part of the environment development branch of a research program that aims to build human-like artificial general intelligence through multi-agent interactions in simulated societies. By exploring which environment features are needed for the basic phenomena of elementary microeconomics to emerge automatically from learning, we arrive at an environment that differs from those studied in prior multi-agent reinforcement learning work along several dimensions. For example, the model incorporates heterogeneous tastes and physical abilities, and agents negotiate with one another as a grounded form of communication.",
      "authors": [
        "Johanson, Michael Bradley",
        "Hughes, Edward",
        "Timbers, Finbarr",
        "Leibo, Joel Z."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-13",
      "selected": null,
      "title": "Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2205.06760.pdf",
        "http://arxiv.org/abs/2205.06760v1",
        "http://arxiv.org/pdf/2205.06760v1"
      ]
    },
    {
      "abstract": "There is a clear desire to model and comprehend human behavior. Trends in research covering this topic show a clear assumption that many view human reasoning as the presupposed standard in artificial reasoning. As such, topics such as game theory, theory of mind, machine learning, etc. all integrate concepts which are assumed components of human reasoning. These serve as techniques to attempt to both replicate and understand the behaviors of humans. In addition, next generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, which allow them not only to replicate human models as a technique to \"learn\", but to to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this paper it to provide a succinct yet systematic review of the most important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques which learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without going necessarily learning via trial-and-error.",
      "authors": [
        "Fuchs, Andrew",
        "Passarella, Andrea",
        "Conti, Marco"
      ],
      "categories": null,
      "citations": null,
      "comments": "Part 1 of our review (see Modeling Human Behavior Part II - Cognitive\n  approaches and Uncertainty) relating to learning and modeling behavior. This\n  work was partially funded by the following projects. European Union's Horizon\n  2020 research and innovation programme: HumaneAI-Net (No 952026). CHIST-ERA\n  program: SAI project (grant CHIST-ERA-19-XAI-010, funded by MUR, grant number\n  not yet available)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-13",
      "selected": null,
      "title": "Modeling Human Behavior Part I -- Learning and Belief Approaches",
      "urls": [
        "http://arxiv.org/pdf/2205.06485.pdf",
        "http://arxiv.org/pdf/2205.06485v1",
        "http://arxiv.org/abs/2205.06485v1"
      ]
    },
    {
      "abstract": "<p>Context remarkably affects learning behavior by adjusting option values according to the distribution of available options. Displaying counterfactual outcomes, the outcomes of the unchosen option alongside the chosen one (i.e., providing complete feedback), would increase the contextual effect by inducing participants to compare the two outcomes during learning. However, when the context only consists of the juxtaposition of several options and there is no such explicit counterfactual factor (i.e., only partial feedback is provided), it is not clear whether and how the contextual effect emerges. In this research, we employ Partial and Complete feedback paradigms in which options are associated with different reward distributions. Our modeling analysis shows that the model that uses the outcome of the chosen option for updating the values of both chosen and unchosen options in opposing directions can better account for the behavioral data. This is also in line with the diffusive effect of dopamine on the striatum. Furthermore, our data show that the contextual effect is not limited to probabilistic rewards, but also extends to magnitude rewards. These results suggest that by extending the counterfactual concept to include the effect of the chosen outcome on the unchosen option, we can better explain why there is a contextual effect in situations in which there is no extra information about the unchosen outcome.</p>",
      "authors": [
        "Barakchian, Zahra",
        "Vahabie, Abdol-Hossein",
        "Nili Ahmadabadi, Majid"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2022.631347",
      "keywords": [
        "Partial and Complete feedback",
        "reinforcement learning",
        "Counterfactual outcome",
        "value learning",
        "contextual effect"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2022-05-10",
      "selected": null,
      "title": "Implicit Counterfactual Effect in Partial Feedback Reinforcement Learning: Behavioral and Modeling Approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130688439&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.631347/pdf"
      ]
    },
    {
      "abstract": "How do humans and animals perform trial-and-error learning when the space of possibilities is infinite? In a previous study, we used an interval timing production task and discovered an updating strategy in which the agent adjusted the behavioral and neuronal noise for exploration. In the experiment, human subjects proactively generated a series of timed motor outputs. We found that the sequential motor timing varied at two temporal scales: long-term correlation around the target interval due to memory drifts and short-term adjustments of timing variability according to feedback. We have previously described these features of timing variability with an augmented Gaussian process, termed reward sensitive Gaussian process (RSGP). Here we provide a mechanistic model and simulate the process by borrowing the architecture of recurrent neural networks. While recurrent connection provided the long-term serial correlation in motor timing, to facilitate reward-driven short-term variations, we introduced reward-dependent variability in the network connectivity, inspired by the stochastic nature of synaptic transmission in the brain. Our model was able to recursively generate an output sequence incorporating the internal variability and external reinforcement in a Bayesian framework. We show that the model can learn the key features of human behavior. Unlike other neural network models that search for unique network connectivity for the best match between the model prediction and observation, this model can estimate the uncertainty associated with each outcome and thus did a better job in teasing apart adjustable task-relevant variability from unexplained variability. The proposed artificial neural network model parallels the mechanisms of information processing in neural systems and can extend the framework of brain-inspired reinforcement learning in continuous state control.",
      "authors": [
        "Wang, Jing",
        "El-Jayyousi, Yousuf",
        "Ozden, Ilker"
      ],
      "categories": null,
      "citations": null,
      "comments": "submitted to Frontiers in Computational Neuroscience",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-05-09",
      "selected": null,
      "title": "A neural network model for timing control with reinforcement",
      "urls": [
        "http://arxiv.org/pdf/2205.04347.pdf",
        "http://arxiv.org/abs/2205.04347v1",
        "http://arxiv.org/pdf/2205.04347v1"
      ]
    },
    {
      "abstract": "The potential of reinforcement learning (RL) to deliver aligned and performant agents is partially bottlenecked by the reward engineering problem. One alternative to heuristic trial-and-error is preference-based RL (PbRL), where a reward function is inferred from sparse human feedback. However, prior PbRL methods lack interpretability of the learned reward structure, which hampers the ability to assess robustness and alignment. We propose an online, active preference learning algorithm that constructs reward functions with the intrinsically interpretable, compositional structure of a tree. Using both synthetic and human-provided feedback, we demonstrate sample-efficient learning of tree-structured reward functions in several environments, then harness the enhanced interpretability to explore and debug for alignment.",
      "authors": [
        "Tom Bewley",
        "Freddy Lecue"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3535850.3535865",
      "keywords": [
        "decision trees",
        "interpretability",
        "preference models",
        "interactive RL"
      ],
      "number_of_pages": 9,
      "pages": "118-126",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450392136",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems"
      },
      "publication_date": "2022-05-09",
      "selected": null,
      "title": "Interpretable Preference-based Reinforcement Learning with Tree-Structured Reward Functions",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3535850.3535865"
      ]
    },
    {
      "abstract": "In settings without well-defined goals, methods for reward learning allow reinforcement learning agents to infer goals from human feedback. Existing work has discussed the problem that such agents may manipulate humans, or the reward learning process, in order to gain higher reward. We introduce the neglected problem that, in multi-agent settings, agents may have incentives to manipulate one another's reward functions in order to change each other's behavioral policies. We focus on the setting with humans acting alongside assistive (artificial) agents who must learn the reward function by interacting with these humans. We propose a possible solution to manipulation of human feedback in this setting: the Shared Value Prior (SVP). The SVP equips agents with an assumption that the reward functions of all humans are similar. Given this assumption, the actions of any human provide information to an agent about its reward, and so the agent is incentivised to observe these actions rather than to manipulate them. We present an expository example in which the SVP prevents manipulation.",
      "authors": [
        "Francis Rhys Ward",
        "Francesca Toni",
        "Francesco Belardinelli"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3535850.3536101",
      "keywords": [
        "manipulation",
        "human-AI interaction",
        "multi-agent learning"
      ],
      "number_of_pages": 3,
      "pages": "1759-1761",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450392136",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems"
      },
      "publication_date": "2022-05-09",
      "selected": null,
      "title": "On Agent Incentives to Manipulate Human Feedback in Multi-Agent Reward Learning Scenarios",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3535850.3536101"
      ]
    },
    {
      "abstract": "Recent models of spiking neuronal networks have been trained to perform behaviors in static environments using a variety of learning rules, with varying degrees of biological realism. Most of these models have not been tested in dynamic visual environments where models must make predictions on future states and adjust their behavior accordingly. The models using these learning rules are often treated as black boxes, with little analysis on circuit architectures and learning mechanisms supporting optimal performance. Here we developed visual/motor spiking neuronal network models and trained them to play a virtual racket-ball game using several reinforcement learning algorithms inspired by the dopaminergic reward system. We systematically investigated how different architectures and circuit-motifs (feed-forward, recurrent, feedback) contributed to learning and performance. We also developed a new biologically-inspired learning rule that significantly enhanced performance, while reducing training time. Our models included visual areas encoding game inputs and relaying the information to motor areas, which used this information to learn to move the racket to hit the ball. Neurons in the early visual area relayed information encoding object location and motion direction across the network. Neuronal association areas encoded spatial relationships between objects in the visual scene. Motor populations received inputs from visual and association areas representing the dorsal pathway. Two populations of motor neurons generated commands to move the racket up or down. Model-generated actions updated the environment and triggered reward or punishment signals that adjusted synaptic weights so that the models could learn which actions led to reward. Here we demonstrate that our biologically-plausible learning rules were effective in training spiking neuronal network models to solve problems in dynamic environments. We used our models to dissect the circuit architectures and learning rules most effective for learning. Our model shows that learning mechanisms involving different neural circuits produce similar performance in sensory-motor tasks. In biological networks, all learning mechanisms may complement one another, accelerating the learning capabilities of animals. Furthermore, this also highlights the resilience and redundancy in biological systems.",
      "authors": [
        "Haroon Anwar",
        "Simon Caby",
        "Salvador Dura-Bernal",
        "David D\u2019Onofrio",
        "Daniel Hasegan",
        "Matt Deible",
        "Sara Grunblatt",
        "George L. Chadderdon",
        "Cliff C. Kerr",
        "Peter Lakatos",
        "William W. Lytton",
        "Hananel Hazan",
        "Samuel A. Neymotin"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0265808",
      "keywords": [
        "Neurons",
        "Learning",
        "Behavior",
        "Vision",
        "Motor neurons",
        "Neural pathways",
        "Sensory perception",
        "Neural networks"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2022-05-01",
      "selected": null,
      "title": "Training a spiking neuronal network model of visual-motor cortex to play a virtual racket-ball game using reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129945699&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0265808&type=printable"
      ]
    },
    {
      "abstract": "An important aspect of making good decisions is the ability to adapt to changes in the values of available choice options, and research suggests that we are poor at changing behavior and adapting our choices successfully. The current paper contributes to clarifying the role of memory on learning and successful adaptation to changing decision environments. We test two aspects of changing decision environments: the direction of change and the type of feedback. The direction of change refers to how options become more or less rewarding compared to other options, over time. Feedback refers to whether full or partial information about decision outcomes is received. Results from behavioral experiments revealed a robust effect of the direction of change: risk that becomes more rewarding over time is harder to detect than risk that becomes less rewarding over time; even with full feedback. We rely on three distinct computational models to interpret the role of memory on learning and adaptation. The distributions of individual model parameters were analyzed in relation to participants\u2019 ability to successfully adapt to the changing conditions of the various decision environments. Consistent across the three models and two distinct data sets, results revealed the importance of recency as an individual memory component for choice adaptation. Individuals relying more on recent experiences were more successful at adapting to change, regardless of its direction. We explain the value and limitations of these findings as well as opportunities for future research.",
      "authors": [
        "Konstantinidis, Emmanouil",
        "Harman, Jason L.",
        "Gonzalez, Cleotilde"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13421-021-01244-4",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "864-881",
      "publication": {
        "category": "Journal",
        "cite_score": 4.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0090502X",
        "publisher": "Springer New York",
        "sjr": 1.025,
        "snip": 1.262,
        "subject_areas": [
          "Arts and Humanities (miscellaneous)",
          "Experimental and Cognitive Psychology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Memory and Cognition"
      },
      "publication_date": "2022-05-01",
      "selected": null,
      "title": "Patterns of choice adaptation in dynamic risky environments",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13421-021-01244-4.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125909207&origin=inward"
      ]
    },
    {
      "abstract": "Recent success in scaling deep reinforcement algorithms (DRL) to complex problems has been driven by well-designed extrinsic rewards, which limits their applicability to many real-world tasks where rewards are naturally extremely sparse. One solution to this problem is to introduce human guidance to drive the agent\u2019s learning. Although low-level demonstrations is a promising approach, it was shown that such guidance may be difficult for experts to demonstrate since some tasks require a large amount of high-quality demonstrations. In this work, we explore human guidance in the form of high-level preferences between sub-goals, leading to drastic reductions in both human effort and cost of exploration. We design a novel hierarchical reinforcement learning method that introduces non-expert human preferences at the high-level, and curiosity to drastically speed up the convergence of subpolicies to reach any sub-goals. We further propose a strategy based on curiosity to automatically discover sub-goals. We evaluate the proposed method on 2D navigation tasks, robotic control tasks, and image-based video games (Atari 2600), which have high-dimensional observations, sparse rewards, and complex state dynamics. The experimental results show that the proposed method can learn significantly faster than traditional hierarchical RL methods and drastically reduces the amount of human effort required over standard imitation learning approaches.",
      "authors": [
        "Bougie, Nicolas",
        "Ichise, Ryutaro"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10489-021-02726-3",
      "keywords": [],
      "number_of_pages": 21,
      "pages": "7459-7479",
      "publication": {
        "category": "Journal",
        "cite_score": 6.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0924669X",
        "publisher": "Springer Netherlands",
        "sjr": 1.145,
        "snip": 1.78,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Applied Intelligence"
      },
      "publication_date": "2022-05-01",
      "selected": null,
      "title": "Hierarchical learning from human preferences and curiosity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115878216&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10489-021-02726-3.pdf"
      ]
    },
    {
      "abstract": "Although physiological feedback in machine learning faces low learning rates, the resulting interaction offers a fresh perspective on our human-home office relation.",
      "authors": [
        "Emanuel Gollob",
        "Maria Kyrou",
        "Panagiotis C. Petrantonakis",
        "Ioannis Kompatsiaris"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3491101.3516390",
      "keywords": [
        "reinforcement learning",
        "ml-agents",
        "virtual environments",
        "sensible space",
        "galvanic skin response",
        "generative aesthetics",
        "stress and affect detection"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450391566",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems"
      },
      "publication_date": "2022-04-28",
      "selected": null,
      "title": "From Sensable to Sensible Spaces: Enhancing the Sensibility of a Home Office using Stress-Aware Deep Reinforcement Learning in Virtual Environments",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3491101.3516390",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129777248&origin=inward"
      ]
    },
    {
      "abstract": "In the peg insertion task, human pays attention to the seam between the peg and the hole and tries to fill it continuously with visual feedback. By imitating the human behavior, we design architectures with position and orientation estimators based on the seam representation for pose alignment, which proves to be general to the unseen peg geometries. By putting the estimators into the closed-loop control with reinforcement learning, we further achieve a higher or comparable success rate, efficiency, and robustness compared with the baseline methods. The policy is trained totally in simulation without any manual intervention. To achieve sim-to-real, a learnable segmentation module with automatic data collecting and labeling can be easily trained to decouple the perception and the policy, which helps the model trained in simulation quickly adapt to the real world with negligible effort. Results are presented in simulation and on a physical robot. Code, videos, and supplemental material are available at https://github.com/xieliang555/SFN.git",
      "authors": [
        "Xie, Liang",
        "Yu, Hongxiang",
        "Zhao, Yinghao",
        "Zhang, Haodong",
        "Zhou, Zhongxiang",
        "Wang, Minhang",
        "Wang, Yue",
        "Xiong, Rong"
      ],
      "categories": null,
      "citations": null,
      "comments": "6 pages; accepted to IEEE International Conference on Robotics and\n  Automation 2022 (ICRA 2022)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-04-16",
      "selected": null,
      "title": "Learning to Fill the Seam by Vision: Sub-millimeter Peg-in-hole on Unseen Shapes in Real World",
      "urls": [
        "http://arxiv.org/pdf/2204.07776v2",
        "http://arxiv.org/pdf/2204.07776.pdf",
        "http://arxiv.org/abs/2204.07776v2"
      ]
    },
    {
      "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",
      "authors": [
        "Bai, Yuntao",
        "Jones, Andy",
        "Ndousse, Kamal",
        "Askell, Amanda",
        "Chen, Anna",
        "DasSarma, Nova",
        "Drain, Dawn",
        "Fort, Stanislav",
        "Ganguli, Deep",
        "Henighan, Tom",
        "Joseph, Nicholas",
        "Kadavath, Saurav",
        "Kernion, Jackson",
        "Conerly, Tom",
        "El-Showk, Sheer",
        "Elhage, Nelson",
        "Hatfield-Dodds, Zac",
        "Hernandez, Danny",
        "Hume, Tristan",
        "Johnston, Scott",
        "Kravec, Shauna",
        "Lovitt, Liane",
        "Nanda, Neel",
        "Olsson, Catherine",
        "Amodei, Dario",
        "Brown, Tom",
        "Clark, Jack",
        "McCandlish, Sam",
        "Olah, Chris",
        "Mann, Ben",
        "Kaplan, Jared"
      ],
      "categories": null,
      "citations": null,
      "comments": "Data available at https://github.com/anthropics/hh-rlhf",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-04-12",
      "selected": null,
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2204.05862v1",
        "http://arxiv.org/abs/2204.05862v1",
        "http://arxiv.org/pdf/2204.05862.pdf"
      ]
    },
    {
      "abstract": "Theory of Mind (ToM) - the ability of the human mind to attribute mental states to others - is a key component of human cognition. In order to understand other people\u2019s mental states or viewpoint and to have successful interactions with others within social and occupational environments, this form of social cognition is essential. The same capability of inferring human mental states is a prerequisite for artificial intelligence (AI) to be integrated into society, for example in healthcare and the motoring industry. Autonomous cars will need to be able to infer the mental states of human drivers and pedestrians to predict their behaviour. In the literature, there has been an increasing understanding of ToM, specifically with increasing cognitive science studies in children and in individuals with Autism Spectrum Disorder. Similarly, with neuroimaging studies there is now a better understanding of the neural mechanisms that underlie ToM. In addition, new AI algorithms for inferring human mental states have been proposed with more complex applications and better generalisability. In this review, we synthesise the existing understanding of ToM in cognitive and neurosciences and the AI computational models that have been proposed. We focus on domains that have not been previously reviewed and critically compare some of the proposed neurocognitive and computational ToM models. We also discuss the limitations of existing models and hint at potential approaches to allow ToM models to fully express the complexity of the human mind in all its aspects, including values and preferences.",
      "authors": [
        "Langley, Christelle",
        "Cirstea, Bogdan Ionut",
        "Cuzzolin, Fabio",
        "Sahakian, Barbara J."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/frai.2022.778852",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.9,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2624-8212",
        "publisher": "Frontiers Media SA",
        "sjr": 0.737,
        "snip": 1.364,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Frontiers in Artificial Intelligence"
      },
      "publication_date": "2022-04-05",
      "selected": null,
      "title": "Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI: A Review",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128846514&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Morita K."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.tins.2022.01.008",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "254-256",
      "publication": {
        "category": "Journal",
        "cite_score": 25.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01662236",
        "publisher": "Elsevier Ltd.",
        "sjr": 4.784,
        "snip": 3.343,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Trends in Neurosciences"
      },
      "publication_date": "2022-04-01",
      "selected": null,
      "title": "Dopamine ramps for accurate value learning under uncertainty",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124748896&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chen Y."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/LRA.2022.3156648",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "5365-5372",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2022-04-01",
      "selected": null,
      "title": "Interactive Multi-Modal Motion Planning with Branch Model Predictive Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126309560&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Elder J."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/09567976211045934",
      "keywords": [],
      "number_of_pages": 19,
      "pages": "629-647",
      "publication": {
        "category": "Journal",
        "cite_score": 12.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09567976",
        "publisher": "SAGE Publications Inc.",
        "sjr": 2.854,
        "snip": 3.039,
        "subject_areas": [
          "Psychology (all)"
        ],
        "title": "Psychological Science"
      },
      "publication_date": "2022-04-01",
      "selected": null,
      "title": "Learning About the Self: Motives for Coherence and Positivity Constrain Learning From Self-Relevant Social Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127327746&origin=inward"
      ]
    },
    {
      "abstract": "\u9488\u5bf9\u5f53\u524d\u7acb\u4f53\u5168\u666f\u89c6\u9891\u4f20\u8f93\u7f3a\u5c11\u6709\u6548\u7684\u6d41\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4e14\u4f20\u7edf\u5168\u666f\u89c6\u9891\u6d41\u81ea\u9002\u5e94\u7b56\u7565\u4f20\u8f93\u53cc\u76ee\u7acb\u4f53\u5168\u666f\u89c6\u9891\u4f7f\u5f97\u4f20\u8f93\u6570\u636e\u52a0\u500d\uff0c\u6240\u9700\u5e26\u5bbd\u5de8\u5927\u7684\u95ee\u9898\uff0c\u8be5\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u7acb\u4f53\u5168\u666f\u89c6\u9891\u975e\u5bf9\u79f0\u4f20\u8f93\u81ea\u9002\u5e94\u6d41\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u65f6\u5e94\u5bf9\u7f51\u7edc\u5e26\u5bbd\u6ce2\u52a8\u3002\u9996\u5148\uff0c\u6839\u636e\u4eba\u773c\u5bf9\u89c6\u9891\u663e\u8457\u6027\u533a\u57df\u7684\u504f\u7231\uff0c\u5de6\u53f3\u89c6\u70b9\u4e2d\u6bcf\u4e2a\u74e6\u7247(tile)\u5bf9\u7acb\u4f53\u89c6\u9891\u7684\u611f\u77e5\u8d28\u91cf\u7684\u8d21\u732e\u5ea6\u4e0d\u540c\uff0c\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8etiles\u7684\u5de6\u53f3\u89c6\u70b9\u89c2\u770b\u6982\u7387\u9884\u6d4b\u65b9\u6cd5\u3002\u5176\u6b21\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565-\u8bc4\u4ef7(Actor-Critic)\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5bf9\u5de6\u53f3\u89c6\u70b9\u8fdb\u884c\u8054\u5408\u7801\u7387\u63a7\u5236\u3002\u6700\u540e\uff0c\u6839\u636e\u6a21\u578b\u7ed3\u6784\u548c\u53cc\u76ee\u6291\u5236\u539f\u7406\uff0c\u8bbe\u8ba1\u5408\u7406\u7684\u5956\u52b1\u51fd\u6570\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u6d41\u81ea\u9002\u5e94\u4f20\u8f93\u7b56\u7565\u76f8\u6bd4\uff0c\u8be5\u6587\u6240\u63d0\u65b9\u6cd5\u66f4\u52a0\u9002\u7528\u4e8e\u57fa\u4e8etiles\u7684\u7acb\u4f53\u5168\u666f\u89c6\u9891\u4f20\u8f93\uff0c\u5b9e\u73b0\u5728\u6709\u9650\u5e26\u5bbd\u4e0b\u63d0\u9ad8\u7528\u6237\u7684\u4f53\u9a8c\u8d28\u91cf(QoE)\uff0c\u4e3a\u7acb\u4f53\u5168\u666f\u89c6\u9891\u8054\u5408\u7801\u7387\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u65b9\u6cd5\u548c\u601d\u8def\u3002",
      "authors": [
        "\u5170\u8bda\u680b",
        "\u9976\u8fce\u8282",
        "\u5b8b\u5f69\u971e",
        "\u9648\u5efa"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.11999/JEIT200908",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1461-1468",
      "publication": {
        "category": "Journal",
        "cite_score": 1.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10095896",
        "publisher": "Science Press",
        "sjr": 0.219,
        "snip": 0.509,
        "subject_areas": [
          "Electrical and Electronic Engineering"
        ],
        "title": "Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology"
      },
      "publication_date": "2022-04-01",
      "selected": null,
      "title": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7acb\u4f53\u5168\u666f\u89c6\u9891\u81ea\u9002\u5e94\u6d41",
      "urls": [
        "https://jeit.ac.cn/cn/article/doi/10.11999/JEIT200908",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128427845&origin=inward"
      ]
    },
    {
      "abstract": "We present a multi-attribute incentive salience (MAIS) model as a computational account of incentive salience in model-based Pavlovian learning. A model of incentive salience as a joint function of reward value and physiological state has been previously proposed by Zhang et al. (2009). In that model, the function takes additive or multiplicative forms depending on whether a preference shifts from positive to negative or vice versa. We demonstrate that arbitrarily varying this function is unnecessary to explain observed data. A multiplicative function is sufficient if one takes into account empirical data suggesting the incentive salience function for an incentive is comprised of multiple physiological signals. We compare our model to the previously proposed model on two datasets. We find the MAIS model predicts the outcomes equally well, fits empirical data describing multiple sensory representations of a single stimulus, better approximates the dual-structure appetitive-aversive nature of the reward system, is compatible with existing knowledge about incentive salience in Pavlovian learning, and better describes revaluation in Pavlovian learning. This model addresses a call (Dayan &amp; Berridge, 2014) for algorithmic and computational models of model-based Pavlovian learning that consistently and fully explain empirical observations. Because a multi-attribute model is relevant even for simple Pavlovian associations, it should be useful in a wide variety of decision-making contexts, including agent modeling and addiction research.",
      "authors": [
        "Smith, Benjamin J.",
        "Read, Stephen J."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-021-00953-2",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "244-257",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2022-04-01",
      "selected": null,
      "title": "Modeling incentive salience in Pavlovian learning more parsimoniously using a multiple attribute model",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-021-00953-2.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117467921&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Walter E.E."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/pchj.452",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "179-193",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "PsyCh Journal"
      },
      "publication_date": "2022-04-01",
      "selected": null,
      "title": "Not all stress is created equal: Acute, not ambient stress, impairs learning in high schizotypes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105145976&origin=inward"
      ]
    },
    {
      "abstract": "As it is known, it is not easy to get familiar with haptics for people that have never used it, and that if not properly transmitted, the cognitive workflow does not produce any improvements. However, the main findings of the proposed work are that haptic-driven multi-modal feedback information is a valuable means of collaboration since it allows to establish a common frame of reference between the two participants. The machine learning experiments show that even independent agents, implemented with properly designed rewards, can learn the intentions of the other participant in the same environment and collaborate to accomplish a common task.",
      "authors": [
        "Salvatore D\u2019Avella",
        "Gerardo Camacho-Gonzalez",
        "Paolo Tripicchio"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.neucom.2022.01.025",
      "keywords": [
        "Multi-Modal Feedback",
        "Cognitive Collaboration",
        "Multi-Agent Reinforcement Learning"
      ],
      "number_of_pages": 12,
      "pages": "27-38",
      "publication": {
        "category": "Journal",
        "cite_score": 10.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0925-2312",
        "publisher": "Elsevier B.V.",
        "sjr": 1.481,
        "snip": 1.853,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neurocomputing"
      },
      "publication_date": "2022-04-01",
      "selected": null,
      "title": "On Multi-Agent Cognitive Cooperation: Can virtual agents behave like humans?",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.neucom.2022.01.025",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123619981&origin=inward"
      ]
    },
    {
      "abstract": "In this paper, we develop a socially cooperative optimal control framework to address the motion planning problem for connected and automated vehicles (CAVs) in mixed traffic using social value orientation (SVO) and a potential game approach. In the proposed framework, we formulate the interaction between a CAV and a human-driven vehicle (HDV) as a simultaneous game where each vehicle minimizes a weighted sum of its egoistic objective and a cooperative objective. The SVO angles are used to quantify preferences of the vehicles toward the egoistic and cooperative objectives. Using the potential game approach, we propose a single objective function for the optimal control problem whose weighting factors are chosen based on the SVOs of the vehicles. We prove that a Nash equilibrium can be obtained by minimizing the proposed objective function. To estimate the SVO angle of the HDV, we develop a moving horizon estimation algorithm based on maximum entropy inverse reinforcement learning. The effectiveness of the proposed approach is demonstrated by numerical simulations of a vehicle merging scenario.",
      "authors": [
        "Le, Viet-Anh",
        "Malikopoulos, Andreas A."
      ],
      "categories": null,
      "citations": null,
      "comments": "final version to CDC2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-03-31",
      "selected": null,
      "title": "A Cooperative Optimal Control Framework for Connected and Automated Vehicles in Mixed Traffic Using Social Value Orientation",
      "urls": [
        "http://arxiv.org/abs/2203.17106v2",
        "http://arxiv.org/pdf/2203.17106.pdf",
        "http://arxiv.org/pdf/2203.17106v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Xiang J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.4271/2022-01-0807",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "SAE Technical Papers"
      },
      "publication_date": "2022-03-29",
      "selected": null,
      "title": "Comfort Improvement for Autonomous Vehicles Using Reinforcement Learning with In-Situ Human Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85128068623&origin=inward"
      ]
    },
    {
      "abstract": "For the most comfortable, human-aware robot navigation, subjective user\npreferences need to be taken into account. This paper presents a novel\nreinforcement learning framework to train a personalized navigation controller\nalong with an intuitive virtual reality demonstration interface. The conducted\nuser study provides evidence that our personalized approach significantly\noutperforms classical approaches with more comfortable human-robot experiences.\nWe achieve these results using only a few demonstration trajectories from\nnon-expert users, who predominantly appreciate the intuitive demonstration\nsetup. As we show in the experiments, the learned controller generalizes well\nto states not covered in the demonstration data, while still reflecting user\npreferences during navigation. Finally, we transfer the navigation controller\nwithout loss in performance to a real robot.",
      "authors": [
        "Jorge de Heuvel",
        "Nathan Corral",
        "Lilli Bruckschen",
        "Maren Bennewitz"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": "10.1109/RO-MAN53752.2022.9900554",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Artificial Intelligence",
          "Robotics"
        ],
        "title": "2022 31st IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)"
      },
      "publication_date": "2022-03-28",
      "selected": null,
      "title": "Learning Personalized Human-Aware Robot Navigation Using Virtual Reality Demonstrations from a User Study",
      "urls": [
        "http://arxiv.org/pdf/2203.14741v2",
        "http://arxiv.org/abs/2203.14741v2",
        "http://dx.doi.org/10.1109/RO-MAN53752.2022.9900554"
      ]
    },
    {
      "abstract": "Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train \"open-book\" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.",
      "authors": [
        "Menick, Jacob",
        "Trebacz, Maja",
        "Mikulik, Vladimir",
        "Aslanides, John",
        "Song, Francis",
        "Chadwick, Martin",
        "Glaese, Mia",
        "Young, Susannah",
        "Campbell-Gillingham, Lucy",
        "Irving, Geoffrey",
        "McAleese, Nat"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-03-21",
      "selected": null,
      "title": "Teaching language models to support answers with verified quotes",
      "urls": [
        "http://arxiv.org/pdf/2203.11147.pdf",
        "http://arxiv.org/abs/2203.11147v1",
        "http://arxiv.org/pdf/2203.11147v1"
      ]
    },
    {
      "abstract": "Training automated agents to complete complex tasks in interactive environments is challenging: reinforcement learning requires careful hand-engineering of reward functions, imitation learning requires specialized infrastructure and access to a human expert, and learning from intermediate forms of supervision (like binary preferences) is time-consuming and extracts little information from each human intervention. Can we overcome these challenges by building agents that learn from rich, interactive feedback instead? We propose a new supervision paradigm for interactive learning based on \"teachable\" decision-making systems that learn from structured advice provided by an external teacher. We begin by formalizing a class of human-in-the-loop decision making problems in which multiple forms of teacher-provided advice are available to a learner. We then describe a simple learning algorithm for these problems that first learns to interpret advice, then learns from advice to complete tasks even in the absence of human supervision. In puzzle-solving, navigation, and locomotion domains, we show that agents that learn from advice can acquire new skills with significantly less human supervision than standard reinforcement learning algorithms and often less than imitation learning.",
      "authors": [
        "Watkins, Olivia",
        "Darrell, Trevor",
        "Abbeel, Pieter",
        "Andreas, Jacob",
        "Gupta, Abhishek"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published at NeurIPS 2021",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-03-19",
      "selected": null,
      "title": "Teachable Reinforcement Learning via Advice Distillation",
      "urls": [
        "http://arxiv.org/abs/2203.11197v2",
        "http://arxiv.org/pdf/2203.11197v2",
        "http://arxiv.org/pdf/2203.11197.pdf"
      ]
    },
    {
      "abstract": "Cyber and cyber-physical systems equipped with machine learning algorithms such as autonomous cars share environments with humans. In such a setting, it is important to align system (or agent) behaviors with the preferences of one or more human users. We consider the case when an agent has to learn behaviors in an unknown environment. Our goal is to capture two defining characteristics of humans: i) a tendency to assess and quantify risk, and ii) a desire to keep decision making hidden from external parties. We incorporate cumulative prospect theory (CPT) into the objective of a reinforcement learning (RL) problem for the former. For the latter, we use differential privacy. We design an algorithm to enable an RL agent to learn policies to maximize a CPT-based objective in a privacy-preserving manner and establish guarantees on the privacy of value functions learned by the algorithm when rewards are sufficiently close. This is accomplished through adding a calibrated noise using a Gaussian process mechanism at each step. Through empirical evaluations, we highlight a privacy-utility tradeoff and demonstrate that the RL agent is able to learn behaviors that are aligned with that of a human user in the same environment in a privacy-preserving manner",
      "authors": [
        "Rajabi, Arezoo",
        "Ramasubramanian, Bhaskar",
        "Maruf, Abdullah Al",
        "Poovendran, Radha"
      ],
      "categories": null,
      "citations": null,
      "comments": "Submitted to conference. arXiv admin note: text overlap with\n  arXiv:2104.00540",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-03-18",
      "selected": null,
      "title": "Privacy-Preserving Reinforcement Learning Beyond Expectation",
      "urls": [
        "http://arxiv.org/pdf/2203.10165.pdf",
        "http://arxiv.org/pdf/2203.10165v1",
        "http://arxiv.org/abs/2203.10165v1"
      ]
    },
    {
      "abstract": "Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor's preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks.",
      "authors": [
        "Park, Jongjin",
        "Seo, Younggyo",
        "Shin, Jinwoo",
        "Lee, Honglak",
        "Abbeel, Pieter",
        "Lee, Kimin"
      ],
      "categories": null,
      "citations": 9,
      "comments": "Accepted to ICLR 2022",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ICLR 2022 - 10th International Conference on Learning Representations"
      },
      "publication_date": "2022-03-18",
      "selected": null,
      "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2203.10050.pdf",
        "http://arxiv.org/abs/2203.10050v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140724160&origin=inward",
        "http://arxiv.org/pdf/2203.10050v1"
      ]
    },
    {
      "abstract": "Identity recognition plays an important role in ensuring security in our daily life. Biometric-based (especially activity-based) approaches are favored due to their fidelity, universality, and resilience. However, most existing machine learning-based approaches rely on a traditional workflow where models are usually trained once for all, with limited involvement from end users in the process and neglecting the dynamic nature of the learning process. This makes the models static and cannot be updated in time, which usually leads to high false positive or false negative. Thus, in practice, an expert is desired to assist with providing high-quality observations and interpretation of model outputs. It is expedient to combine both advantages of human experts and the computational capability of computers to create a tight-coupling incremental learning process for better performance. In this study, we develop RLTIR, an interactive identity recognition approach based on reinforcement learning, to adjust the identification model by human guidance. We first build a base tree-structured identity recognition model, and an expert is introduced in the model for giving feedback upon model outputs. Then, the model is updated according to strategies that are automatically learned under a designated reinforcement learning framework. To the best of our knowledge, it is the very first attempt to combine human expert knowledge with model learning in the area of identity recognition. The experimental results show that the reinforced interactive identity recognition framework outperforms baseline methods with regard to recognition accuracy and robustness.",
      "authors": [
        "Qingyang Li",
        "Zhiwen Yu",
        "Lina Yao",
        "Bin Guo"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/JIOT.2021.3104024",
      "keywords": [
        "reinforcement learning (RL)",
        "Human feedback",
        "person identification"
      ],
      "number_of_pages": 12,
      "pages": "4464-4475",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2372-2541",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Internet of Things Journal"
      },
      "publication_date": "2022-03-15",
      "selected": null,
      "title": "RLTIR: Activity-Based Interactive Person Identification via Reinforcement Learning Tree",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126521680&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9511639"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hinneberg B.M."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroscience.2021.06.028",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "91-102",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03064522",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Neuroscience"
      },
      "publication_date": "2022-03-15",
      "selected": null,
      "title": "Acting in Temporal Contexts: On the Behavioral and Neurophysiological Consequences of Feedback Delays",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109462419&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Margraf L."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroscience.2021.04.016",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "4-19",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03064522",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Neuroscience"
      },
      "publication_date": "2022-03-15",
      "selected": null,
      "title": "Valence-dependent Neural Correlates of Augmented Feedback Processing in Extensive Motor Sequence Learning \u2013 Part I: Practice-related Changes of Feedback Processing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106369675&origin=inward"
      ]
    },
    {
      "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
      "authors": [
        "Gleave, Adam",
        "Irving, Geoffrey"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages main paper, 17 pages total",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-03-14",
      "selected": null,
      "title": "Uncertainty Estimation for Language Reward Models",
      "urls": [
        "http://arxiv.org/pdf/2203.07472.pdf",
        "http://arxiv.org/pdf/2203.07472v1",
        "http://arxiv.org/abs/2203.07472v1"
      ]
    },
    {
      "abstract": "<p>Impulsivity is defined as a trait-like tendency to engage in rash actions that are poorly thought out or expressed in an untimely manner. Previous research has found that impulsivity relates to deficits in decision making, in particular when it necessitates executive control or reward outcomes. Reinforcement learning (RL) relies on the ability to integrate reward or punishment outcomes to make good decisions, and has recently been shown to often recruit executive function; as such, it is unsurprising that impulsivity has been studied in the context of RL. However, how impulsivity relates to the mechanisms of RL remains unclear. We aimed to investigate the relationship between impulsivity and learning in a reward-driven learning task with probabilistic feedback and reversal known to recruit executive function. Based on prior literature in clinical populations, we predicted that higher impulsivity would be associated with poorer performance on the task, driven by more frequent switching following unrewarded outcomes. Our results did not support this prediction, but more advanced, trial-history dependent analyses revealed specific effects of impulsivity on switching behavior following consecutive unrewarded trials. Computational modeling captured group-level behavior, but not impulsivity results. Our results support previous findings highlighting the importance of sensitivity to negative outcomes in understanding how impulsivity relates to learning, but indicate that this may stem from more complex strategies than usually considered in computational models of learning. This should be an important target for future research.</p>",
      "authors": [
        "Zou, Amy R.",
        "Mu\u00f1oz Lopez, Daniela E.",
        "Johnson, Sheri L.",
        "Collins, Anne G. E."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyt.2022.800290",
      "keywords": [
        "computational modeling",
        "working memory",
        "reinforcement learning",
        "impulsivity",
        "Reversal Learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-0640",
        "publisher": "Frontiers Media SA",
        "sjr": 1.222,
        "snip": 1.265,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "Frontiers in Psychiatry"
      },
      "publication_date": "2022-03-14",
      "selected": null,
      "title": "Impulsivity Relates to Multi-Trial Choice Strategy in Probabilistic Reversal Learning",
      "urls": [
        "https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2022.800290/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127571626&origin=inward"
      ]
    },
    {
      "abstract": "Current Humanoid Service Robot (HSR) behaviours mainly rely on static models that cannot adapt dynamically to meet individual customer attitudes and preferences. In this work, we focus on empowering HSRs with adaptive feedback mechanisms driven by either implicit reward, by estimating facial affect, or explicit reward, by incorporating verbal responses of the human \u2018customer\u2019. To achieve this, we first create a custom dataset, annotated using crowd-sourced labels, to learn appropri-ate approach (positioning and movement) behaviours for a Robo-waiter. This dataset is used to pre-train a Reinforcement Learning (RL) agent to learn behaviours deemed socially appropriate for the robo-waiter. This model is later extended to include separate implicit and explicit reward mechanisms to allow for interactive learning and adaptation from user social feedback. We present a within-subjects Human-Robot Interaction (HRI) study with 21 participants implementing interactions between the robo-waiter and human customers implementing the above-mentioned model variations. Our results show that both explicit and implicit adaptation mechanisms enabled the adaptive robo-waiter to be rated as more enjoyable and sociable, and its positioning relative to the participants as more appropriate compared to using the pre-trained model or a randomised control implementation.",
      "authors": [
        "Emily McQuillin",
        "Nikhil Churamani",
        "Hatice Gunes"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/HRI53351.2022.9889395",
      "keywords": [
        "Explicit Feedback",
        "Implicit Feedback and Facial Affect",
        "Humanoid Robo-waiter",
        "Reinforcement Learning"
      ],
      "number_of_pages": 10,
      "pages": "541-550",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-0732-8",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
      },
      "publication_date": "2022-03-07",
      "selected": null,
      "title": "Learning Socially Appropriate Robo-waiter Behaviours through Real-time User Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9889395",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85133526078&origin=inward"
      ]
    },
    {
      "abstract": "Objectives:According to the Procedural Deficit Hypothesis, abnormalities in corticostriatal pathways could account for the language-related deficits observed in developmental dyslexia. The same neural network has also been implicated in the ability to learn contingencies based on trial and error (i.e., reinforcement learning [RL]). On this basis, the present study tested the assumption that dyslexic individuals would be impaired in RL compared with neurotypicals in two different tasks.Methods:In a probabilistic selection task, participants were required to learn reinforcement contingencies based on probabilistic feedback. In an implicit transitive inference task, participants were also required to base their decisions on reinforcement histories, but feedback was deterministic and stimulus pairs were partially overlapping, such that participants were required to learn hierarchical relations.Results:Across tasks, results revealed that although the ability to learn from positive/negative feedback did not differ between the two groups, the learning of reinforcement contingencies was poorer in the dyslexia group compared with the neurotypicals group. Furthermore, in novel test pairs where previously learned information was presented in new combinations, dyslexic individuals performed similarly to neurotypicals.Conclusions:Taken together, these results suggest that learning of reinforcement contingencies occurs less robustly in individuals with developmental dyslexia. Inferences for the neuro-cognitive mechanisms of developmental dyslexia are discussed.",
      "authors": [
        "Atheer Odah Massarwe",
        "Noyli Nissan",
        "Yafit Gabay"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1017/S1355617721000266",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "270-280",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13556177",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of the International Neuropsychological Society"
      },
      "publication_date": "2022-03-07",
      "selected": null,
      "title": "Atypical Reinforcement Learning in Developmental Dyslexia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103834514&origin=inward"
      ]
    },
    {
      "abstract": "Current Humanoid Service Robot (HSR) behaviours mainly rely on static models that cannot adapt dynamically to meet individual customer attitudes and preferences. In this work, we focus on empowering HSRs with adaptive feedback mechanisms driven by either implicit reward, by estimating facial affect, or explicit reward, by incorporating verbal responses of the human 'customer'. To achieve this, we first create a custom dataset, annotated using crowd-sourced labels, to learn appropriate approach (positioning and movement) behaviours for a Robo-waiter. This dataset is used to pre-train a Reinforcement Learning (RL) agent to learn behaviours deemed socially appropriate for the robo-waiter. This model is later extended to include separate implicit and explicit reward mechanisms to allow for interactive learning and adaptation from user social feedback. We present a within-subjects Human-Robot Interaction (HRI) study with 21 participants implementing interactions between the robo-waiter and human customers implementing the above-mentioned model variations. Our results show that both explicit and implicit adaptation mechanisms enabled the adaptive robo-waiter to be rated as more enjoyable and sociable, and its positioning relative to the participants as more appropriate compared to using the pre-trained model or a randomised control implementation.",
      "authors": [
        "Emily McQuillin",
        "Nikhil Churamani",
        "Hatice Gunes"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3523760.3523831",
      "keywords": [
        "facial affect",
        "humanoid robo-waiter",
        "implicit feedback",
        "reinforcement learning",
        "explicit feedback"
      ],
      "number_of_pages": 10,
      "pages": "541-550",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2022-03-07",
      "selected": null,
      "title": "Learning Socially Appropriate Robo-waiter Behaviours through Real-time User Feedback",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3523760.3523831"
      ]
    },
    {
      "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
      "authors": [
        "Ouyang, Long",
        "Wu, Jeff",
        "Jiang, Xu",
        "Almeida, Diogo",
        "Wainwright, Carroll L.",
        "Mishkin, Pamela",
        "Zhang, Chong",
        "Agarwal, Sandhini",
        "Slama, Katarina",
        "Ray, Alex",
        "Schulman, John",
        "Hilton, Jacob",
        "Kelton, Fraser",
        "Miller, Luke",
        "Simens, Maddie",
        "Askell, Amanda",
        "Welinder, Peter",
        "Christiano, Paul",
        "Leike, Jan",
        "Lowe, Ryan"
      ],
      "categories": null,
      "citations": 557,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-03-04",
      "selected": null,
      "title": "Training language models to follow instructions with human feedback",
      "urls": [
        "http://arxiv.org/pdf/2203.02155.pdf",
        "http://arxiv.org/pdf/2203.02155v1",
        "http://arxiv.org/abs/2203.02155v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163147262&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Glazer J."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.13981",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2022-03-01",
      "selected": null,
      "title": "Outcome valence and stimulus frequency affect neural responses to rewards and punishments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120156128&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Guo Z."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1142/S1752890922300011",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 0.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17528909",
        "publisher": "World Scientific",
        "sjr": 0.162,
        "snip": 0.516,
        "subject_areas": [
          "Artificial Intelligence",
          "Computer Vision and Pattern Recognition",
          "Control and Optimization"
        ],
        "title": "Journal of Uncertain Systems"
      },
      "publication_date": "2022-03-01",
      "selected": null,
      "title": "Survey of Reinforcement Learning based on Human Prior Knowledge",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129001500&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Guo B.C."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.19721/j.cnki.1001-7372.2022.03.013",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "153-165",
      "publication": {
        "category": "Journal",
        "cite_score": 3.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10017372",
        "publisher": "Xi'an Highway University",
        "sjr": 0.549,
        "snip": 1.18,
        "subject_areas": [
          "Mechanical Engineering",
          "Civil and Structural Engineering",
          "Transportation"
        ],
        "title": "Zhongguo Gonglu Xuebao/China Journal of Highway and Transport"
      },
      "publication_date": "2022-03-01",
      "selected": null,
      "title": "Decision Making Method for Control Right Transition of Human- machine Shared Driving Based on Driver-vehicle Risk State",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127879762&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bishara A."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/bmb/ldac001",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "15-32",
      "publication": {
        "category": "Journal",
        "cite_score": 10.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00071420",
        "publisher": "Oxford University Press",
        "sjr": 1.593,
        "snip": 2.175,
        "subject_areas": [
          "Medicine (all)"
        ],
        "title": "British Medical Bulletin"
      },
      "publication_date": "2022-03-01",
      "selected": null,
      "title": "Considerations for the implementation of machine learning into acute care settings",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127729661&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhang Z."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/cae.22472",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "550-563",
      "publication": {
        "category": "Journal",
        "cite_score": 5.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10613773",
        "publisher": "John Wiley & Sons Inc.",
        "sjr": 0.651,
        "snip": 1.306,
        "subject_areas": [
          "Computer Science (all)",
          "Education",
          "Engineering (all)"
        ],
        "title": "Computer Applications in Engineering Education"
      },
      "publication_date": "2022-03-01",
      "selected": null,
      "title": "A novel animation authoring framework for the virtual teacher performing experiment in mixed reality",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118769844&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dixon-Gordon K.L."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jpsychires.2021.12.016",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "126-134",
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223956",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.554,
        "snip": 1.364,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "Journal of Psychiatric Research"
      },
      "publication_date": "2022-03-01",
      "selected": null,
      "title": "Learning from gain and loss: Links to suicide risk",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122637049&origin=inward"
      ]
    },
    {
      "abstract": "We investigate whether naturalistic emotional human feedback can be directly exploited as a reward signal for training artificial agents via interactive human-in-the-loop reinforcement learning. To answer this question, we devise an experimental setting inspired by animal training, in which human test subjects interactively teach an emulated drone agent their desired command-action-mapping by providing emotional feedback on the drone's action selections. We present a first empirical proof-of-concept study and analysis confirming that human facial emotion expression can be directly exploited as reward signal in such interactive learning settings. Thereby, we contribute empirical findings towards more naturalistic and intuitive forms of reinforcement learning especially designed for non-expert users.",
      "authors": [
        "Pollak, Manuela",
        "Salfinger, Andrea",
        "Hummel, Karin Anna"
      ],
      "categories": null,
      "citations": null,
      "comments": "6 pages, Accepted at the AAAI-22 Workshop on Interactive Machine\n  Learning",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-02-19",
      "selected": null,
      "title": "Teaching Drones on the Fly: Can Emotional Feedback Serve as Learning Signal for Training Artificial Agents?",
      "urls": [
        "http://arxiv.org/pdf/2202.09634.pdf",
        "http://arxiv.org/abs/2202.09634v2",
        "http://arxiv.org/pdf/2202.09634v2"
      ]
    },
    {
      "abstract": "Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak \u00e0 Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and \u2018snowflake\u2019 configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained \u2018droplets\u2019 on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied. A newly designed control architecture uses deep reinforcement learning to learn to command the coils of a tokamak, and successfully stabilizes a wide variety of fusion plasma configurations.",
      "authors": [
        "Degrave, Jonas",
        "Felici, Federico",
        "Buchli, Jonas",
        "Neunert, Michael",
        "Tracey, Brendan",
        "Carpanese, Francesco",
        "Ewalds, Timo",
        "Hafner, Roland",
        "Abdolmaleki, Abbas",
        "de las Casas, Diego",
        "Donner, Craig",
        "Fritz, Leslie",
        "Galperti, Cristian",
        "Huber, Andrea",
        "Keeling, James",
        "Tsimpoukelli, Maria",
        "Kay, Jackie",
        "Merle, Antoine",
        "Moret, Jean-Marc",
        "Noury, Seb",
        "Pesamosca, Federico",
        "Pfau, David",
        "Sauter, Olivier",
        "Sommariva, Cristian",
        "Coda, Stefano",
        "Duval, Basil",
        "Fasoli, Ambrogio",
        "Kohli, Pushmeet",
        "Kavukcuoglu, Koray",
        "Hassabis, Demis",
        "Riedmiller, Martin"
      ],
      "categories": null,
      "citations": 252,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41586-021-04301-9",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "414-419",
      "publication": {
        "category": "Journal",
        "cite_score": 83.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00280836",
        "publisher": "Nature Publishing Group",
        "sjr": 20.957,
        "snip": 11.591,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Nature"
      },
      "publication_date": "2022-02-17",
      "selected": null,
      "title": "Magnetic control of tokamak plasmas through deep reinforcement learning",
      "urls": [
        "https://www.nature.com/articles/s41586-021-04301-9.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124776829&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Joo S.h."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijdrr.2021.102780",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "22124209",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.132,
        "snip": 1.546,
        "subject_areas": [
          "Safety Research",
          "Geotechnical Engineering and Engineering Geology",
          "Geology"
        ],
        "title": "International Journal of Disaster Risk Reduction"
      },
      "publication_date": "2022-02-15",
      "selected": null,
      "title": "Road-reconstruction after multi-locational flooding in multi-agent deep RL with the consideration of human mobility - Case study: Western Japan flooding in 2018 -",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122754981&origin=inward"
      ]
    },
    {
      "abstract": "Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (e.g., webcam images of eye gaze) can be challenging, especially when it involves inferring the user's desired action in the absence of a natural 'default' interface. Reinforcement learning from online user feedback on the system's performance presents a natural solution to this problem, and enables the interface to adapt to individual users. However, this approach tends to require a large amount of human-in-the-loop training data, especially when feedback is sparse. We propose a hierarchical solution that learns efficiently from sparse user feedback: we use offline pre-training to acquire a latent embedding space of useful, high-level robot behaviors, which, in turn, enables the system to focus on using online user feedback to learn a mapping from user inputs to desired high-level behaviors. The key insight is that access to a pre-trained policy enables the system to learn more from sparse rewards than a na\\\"ive RL algorithm: using the pre-trained policy, the system can make use of successful task executions to relabel, in hindsight, what the user actually meant to do during unsuccessful executions. We evaluate our method primarily through a user study with 12 participants who perform tasks in three simulated robotic manipulation domains using a webcam and their eye gaze: flipping light switches, opening a shelf door to reach objects inside, and rotating a valve. The results show that our method successfully learns to map 128-dimensional gaze features to 7-dimensional joint torques from sparse rewards in under 10 minutes of online training, and seamlessly helps users who employ different gaze strategies, while adapting to distributional shift in webcam inputs, tasks, and environments.",
      "authors": [
        "Chen, Sean",
        "Gao, Jensen",
        "Reddy, Siddharth",
        "Berseth, Glen",
        "Dragan, Anca D.",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to IEEE Conference on Robotics and Automation (ICRA) 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-02-05",
      "selected": null,
      "title": "ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2202.02465.pdf",
        "http://arxiv.org/pdf/2202.02465v1",
        "http://arxiv.org/abs/2202.02465v1"
      ]
    },
    {
      "abstract": "Building autonomous vehicles (AVs) is a complex problem, but enabling them to operate in the real world where they will be surrounded by human-driven vehicles (HVs) is extremely challenging. Prior works have shown the possibilities of creating inter-agent cooperation between a group of AVs that follow a social utility. Such altruistic AVs can form alliances and affect the behavior of HVs to achieve socially desirable outcomes. We identify two major challenges in the co-existence of AVs and HVs. First, social preferences and individual traits of a given human driver, e.g., selflessness and aggressiveness are unknown to an AV, and it is almost impossible to infer them in real-time during a short AV-HV interaction. Second, contrary to AVs that are expected to follow a policy, HVs do not necessarily follow a stationary policy and therefore are extremely hard to predict. To alleviate the above-mentioned challenges, we formulate the mixed-autonomy problem as a multi-agent reinforcement learning (MARL) problem and propose a decentralized framework and reward function for training cooperative AVs. Our approach enables AVs to learn the decision-making of HVs implicitly from experience, optimizes for a social utility while prioritizing safety and allowing adaptability; robustifying altruistic AVs to different human behaviors and constraining them to a safe action space. Finally, we investigate the robustness, safety and sensitivity of AVs to various HVs behavioral traits and present the settings in which the AVs can learn cooperative policies that are adaptable to different situations.",
      "authors": [
        "Valiente, Rodolfo",
        "Toghi, Behrad",
        "Pedarsani, Ramtin",
        "Fallah, Yaser P."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-02-02",
      "selected": null,
      "title": "Robustness and Adaptability of Reinforcement Learning based Cooperative Autonomous Driving in Mixed-autonomy Traffic",
      "urls": [
        "http://arxiv.org/pdf/2202.00881.pdf",
        "http://arxiv.org/abs/2202.00881v1",
        "http://arxiv.org/pdf/2202.00881v1"
      ]
    },
    {
      "abstract": "Recommender Systems for the IoT (RSIoT) aim for interactive item recommendations. Most existing methods focus on user feedback and have limitations in dealing with dynamic environments. Deep Reinforcement Learning (DRL) can deal with dynamic environments and conduct...",
      "authors": [
        "Altulyan, May S.",
        "Huang, Chaoran",
        "Yao, Lina",
        "Wang, Xianzhi",
        "Kanhere, Salil"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-3-030-97546-3_32",
      "keywords": [
        "Recommender system",
        "IoT",
        "Deep reinforcement learning"
      ],
      "number_of_pages": 12,
      "pages": "393-404",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-3-030-97545-6",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "AI 2021: Advances in Artificial Intelligence: 34th Australasian Joint Conference, AI 2021, Sydney, NSW, Australia, February 2\u20134, 2022, Proceedings"
      },
      "publication_date": "2022-02-02",
      "selected": null,
      "title": "Deep Reinforcement Learning for Dynamic Things of Interest Recommendation in Intelligent Ambient Environment",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-030-97546-3_32.pdf",
        "https://dl.acm.org/doi/10.1007/978-3-030-97546-3_32",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127115692&origin=inward"
      ]
    },
    {
      "abstract": "Neuroscience research has illuminated the mechanisms supporting learning from reward feedback, demonstrating a critical role for the striatum and midbrain dopamine system. However, in humans, short-term working memory that is dependent on frontal and parietal cortices can also play an important role, particularly in commonly used paradigms in which learning is relatively condensed in time. Given the growing use of reward-based learning tasks in translational studies in computational psychiatry, it is important to understand the extent of the influence of&nbsp;working memory and also&nbsp;how core&nbsp;gradual learning mechanisms can be better isolated. In our experiments, we manipulated the spacing between repetitions along with a post-learning delay preceding a test phase. We found that learning was slower for stimuli repeated after a long delay (spaced-trained) compared to those repeated immediately (massed-trained), likely reflecting the remaining contribution of feedback learning mechanisms when working memory is not available. For massed learning,&nbsp;brief interruptions led to drops in subsequent performance,&nbsp;and individual differences in working memory capacity positively correlated with overall&nbsp;performance. Interestingly, when tested&nbsp;after a delay period but not immediately, relative preferences decayed in the massed condition and increased in the spaced condition. Our results provide additional support for a large role of working memory in reward-based learning in temporally condensed designs. We suggest that spacing training within or between sessions is a promising approach to better isolate and understand mechanisms supporting gradual reward-based learning, with particular importance for understanding potential learning dysfunctions in addiction and psychiatric disorders.",
      "authors": [
        "Wimmer, G. Elliott",
        "Poldrack, Russell A."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13421-021-01233-7",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "312-324",
      "publication": {
        "category": "Journal",
        "cite_score": 4.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0090502X",
        "publisher": "Springer New York",
        "sjr": 1.025,
        "snip": 1.262,
        "subject_areas": [
          "Arts and Humanities (miscellaneous)",
          "Experimental and Cognitive Psychology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Memory and Cognition"
      },
      "publication_date": "2022-02-01",
      "selected": null,
      "title": "Reward learning and working memory: Effects of massed versus spaced training and post-learning delay period",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13421-021-01233-7.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114857455&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "James R."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.humov.2021.102896",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01679457",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Human Movement Science"
      },
      "publication_date": "2022-02-01",
      "selected": null,
      "title": "The nature of savings associated with a visuomotor adaptation task that involves one arm or both arms",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119527490&origin=inward"
      ]
    },
    {
      "abstract": "This paper proposes an interactive dialog system, called AidIR, to aid information retrieval. AidIR allows users to retrieve information on diseases resulting from coronaviruses and diseases transmitted by vector mosquitoes with natural language interaction and Line chat media. In a subjective evaluation, we asked 20 users to rate the intuitiveness, usability, and user experience of AidIR with a range between \u22122 and 2. Moreover, we also asked these users to answer yes\u2013no questions to evaluate AidIR and provide feedback. The average scores of intuitiveness, usability, and user experience are 0.8, 0.8, and 1.05, respectively. The yes\u2013no questions demonstrated that AidIR is better than systems using the graphical user interface in mobile phones and single-turn dialog systems. According to user feedback, AidIR is more convenient for information retrieval. Moreover, we designed a new loss function to jointly train a BERT model for domain classification and sequence label tasks. The accuracy of both tasks is 92%. Finally, we trained the dialog policy network with supervised learning tasks and deployed the reinforcement learning algorithm to allow AidIR to continue learning the dialog policy.",
      "authors": [
        "Wang, Da-Jinn",
        "Chen, Tsong-Yi",
        "Su, Chia-Yi"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/app12041875",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2076-3417",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Applied Sciences (Switzerland)"
      },
      "publication_date": "2022-02-01",
      "selected": null,
      "title": "AidIR: An Interactive Dialog System to Aid Disease Information Retrieval",
      "urls": [
        "https://www.mdpi.com/2076-3417/12/4/1875/pdf?version=1645021404",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125816913&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu Y."
      ],
      "categories": null,
      "citations": 47,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2020.3029587",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "853-865",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2022-02-01",
      "selected": null,
      "title": "Adaptive Tracking Control for Perturbed Strict-Feedback Nonlinear Systems Based on Optimized Backstepping Technique",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124054895&origin=inward"
      ]
    },
    {
      "abstract": "Interaction and cooperation with humans are overarching aspirations of\nartificial intelligence (AI) research. Recent studies demonstrate that AI\nagents trained with deep reinforcement learning are capable of collaborating\nwith humans. These studies primarily evaluate human compatibility through\n\"objective\" metrics such as task performance, obscuring potential variation in\nthe levels of trust and subjective preference that different agents garner. To\nbetter understand the factors shaping subjective preferences in human-agent\ncooperation, we train deep reinforcement learning agents in Coins, a two-player\nsocial dilemma. We recruit participants for a human-agent cooperation study and\nmeasure their impressions of the agents they encounter. Participants'\nperceptions of warmth and competence predict their stated preferences for\ndifferent agents, above and beyond objective performance metrics. Drawing\ninspiration from social science and biology research, we subsequently implement\na new \"partner choice\" framework to elicit revealed preferences: after playing\nan episode with an agent, participants are asked whether they would like to\nplay the next round with the same agent or to play alone. As with stated\npreferences, social perception better predicts participants' revealed\npreferences than does objective performance. Given these results, we recommend\nhuman-agent interaction researchers routinely incorporate the measurement of\nsocial perception and subjective preferences into their studies.",
      "authors": [
        "Kevin R. McKee",
        "Xuechunzi Bai",
        "Susan T. Fiske"
      ],
      "categories": null,
      "citations": 0,
      "comments": "Proceedings of the 21st International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2022)",
      "databases": [
        "ACM",
        "arXiv"
      ],
      "doi": "10.5555/3535850.3535951",
      "keywords": [
        "human-agent interaction",
        "warmth",
        "partner choice",
        "social perception",
        "preferences",
        "human-agent cooperation",
        "competence"
      ],
      "number_of_pages": 10,
      "pages": "898-907",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450392136",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems"
      },
      "publication_date": "2022-01-31",
      "selected": null,
      "title": "Warmth and competence in human-agent cooperation",
      "urls": [
        "http://arxiv.org/pdf/2201.13448v2",
        "http://arxiv.org/abs/2201.13448v2",
        "https://dl.acm.org/doi/10.5555/3535850.3535951",
        "http://dx.doi.org/10.5555/3535850.3535951"
      ]
    },
    {
      "abstract": "Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here, we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders, and successfully won the majority vote. By optimizing for human preferences, Democratic AI may be a promising method for value-aligned policy innovation.",
      "authors": [
        "Koster, Raphael",
        "Balaguer, Jan",
        "Tacchetti, Andrea",
        "Weinstein, Ari",
        "Zhu, Tina",
        "Hauser, Oliver",
        "Williams, Duncan",
        "Campbell-Gillingham, Lucy",
        "Thacker, Phoebe",
        "Botvinick, Matthew",
        "Summerfield, Christopher"
      ],
      "categories": null,
      "citations": null,
      "comments": "18 pages, 4 figures, 54 pages including supplemental materials",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-01-27",
      "selected": null,
      "title": "Human-centered mechanism design with Democratic AI",
      "urls": [
        "http://arxiv.org/pdf/2201.11441v1",
        "http://arxiv.org/abs/2201.11441v1",
        "http://arxiv.org/pdf/2201.11441.pdf"
      ]
    },
    {
      "abstract": "Agents should avoid unsafe behaviour during both training and deployment. This typically requires a simulator and a procedural specification of unsafe behaviour. Unfortunately, a simulator is not always available, and procedurally specifying constraints can be difficult or impossible for many real-world tasks. A recently introduced technique, ReQueST, aims to solve this problem by learning a neural simulator of the environment from safe human trajectories, then using the learned simulator to efficiently learn a reward model from human feedback. However, it is yet unknown whether this approach is feasible in complex 3D environments with feedback obtained from real humans - whether sufficient pixel-based neural simulator quality can be achieved, and whether the human data requirements are viable in terms of both quantity and quality. In this paper we answer this question in the affirmative, using ReQueST to train an agent to perform a 3D first-person object collection task using data entirely from human contractors. We show that the resulting agent exhibits an order of magnitude reduction in unsafe behaviour compared to standard reinforcement learning.",
      "authors": [
        "Rahtz, Matthew",
        "Varma, Vikrant",
        "Kumar, Ramana",
        "Kenton, Zachary",
        "Legg, Shane",
        "Leike, Jan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-01-20",
      "selected": null,
      "title": "Safe Deep RL in 3D Environments using Human Feedback",
      "urls": [
        "http://arxiv.org/abs/2201.08102v2",
        "http://arxiv.org/pdf/2201.08102.pdf",
        "http://arxiv.org/pdf/2201.08102v2"
      ]
    },
    {
      "abstract": "Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods.",
      "authors": [
        "Tuyls, Jens",
        "Yao, Shunyu",
        "Kakade, Sham",
        "Narasimhan, Karthik"
      ],
      "categories": null,
      "citations": 6,
      "comments": "ICLR 2022 (Spotlight) - https://sites.google.com/princeton.edu/xtx",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ICLR 2022 - 10th International Conference on Learning Representations"
      },
      "publication_date": "2022-01-04",
      "selected": null,
      "title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85148655379&origin=inward",
        "http://arxiv.org/pdf/2201.01251v3",
        "http://arxiv.org/pdf/2201.01251.pdf",
        "http://arxiv.org/abs/2201.01251v3"
      ]
    },
    {
      "abstract": "Deep reinforcement learning has gathered much attention recently. Impressive\nresults were achieved in activities as diverse as autonomous driving, game\nplaying, molecular recombination, and robotics. In all these fields, computer\nprograms have taught themselves to solve difficult problems. They have learned\nto fly model helicopters and perform aerobatic manoeuvers such as loops and\nrolls. In some applications they have even become better than the best humans,\nsuch as in Atari, Go, poker and StarCraft. The way in which deep reinforcement\nlearning explores complex environments reminds us of how children learn, by\nplayfully trying out things, getting feedback, and trying again. The computer\nseems to truly possess aspects of human learning; this goes to the heart of the\ndream of artificial intelligence. The successes in research have not gone\nunnoticed by educators, and universities have started to offer courses on the\nsubject. The aim of this book is to provide a comprehensive overview of the\nfield of deep reinforcement learning. The book is written for graduate students\nof artificial intelligence, and for researchers and practitioners who wish to\nbetter understand deep reinforcement learning methods and their challenges. We\nassume an undergraduate-level of understanding of computer science and\nartificial intelligence; the programming language of this book is Python. We\ndescribe the foundations, the algorithms and the applications of deep\nreinforcement learning. We cover the established model-free and model-based\nmethods that form the basis of the field. Developments go quickly, and we also\ncover advanced topics: deep multi-agent reinforcement learning, deep\nhierarchical reinforcement learning, and deep meta learning.",
      "authors": [
        "Aske Plaat"
      ],
      "categories": null,
      "citations": null,
      "comments": "Revised version 2023, added description of Monte Carlo sampling and\n  N-step algorithm, improved explanation of on-policy and off-policy learning.\n  Preprint available by permission of Publisher",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1007/978-981-19-0638-1",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-01-04",
      "selected": null,
      "title": "Deep Reinforcement Learning, a textbook",
      "urls": [
        "http://arxiv.org/abs/2201.02135v5",
        "http://arxiv.org/pdf/2201.02135v5",
        "http://dx.doi.org/10.1007/978-981-19-0638-1"
      ]
    },
    {
      "abstract": "Socially aware robot navigation, where a robot is required to optimize its trajectory to maintain comfortable and compliant spatial interactions with humans in addition to reaching its goal without collisions, is a fundamental yet challenging task in the context of human-robot interaction. While existing learning-based methods have achieved better performance than the preceding model-based ones, they still have drawbacks: reinforcement learning depends on the handcrafted reward that is unlikely to effectively quantify broad social compliance, and can lead to reward exploitation problems; meanwhile, inverse reinforcement learning suffers from the need for expensive human demonstrations. In this paper, we propose a feedback-efficient active preference learning approach, FAPL, that distills human comfort and expectation into a reward model to guide the robot agent to explore latent aspects of social compliance. We further introduce hybrid experience learning to improve the efficiency of human feedback and samples, and evaluate benefits of robot behaviors learned from FAPL through extensive simulation experiments and a user study (N=10) employing a physical robot to navigate with human subjects in real-world scenarios. Source code and experiment videos for this work are available at:https://sites.google.com/view/san-fapl.",
      "authors": [
        "Wang, Ruiqi",
        "Wang, Weizheng",
        "Min, Byung-Cheol"
      ],
      "categories": null,
      "citations": null,
      "comments": "To appear in IROS 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2022-01-03",
      "selected": null,
      "title": "Feedback-efficient Active Preference Learning for Socially Aware Robot Navigation",
      "urls": [
        "http://arxiv.org/pdf/2201.00469.pdf",
        "http://arxiv.org/abs/2201.00469v4",
        "http://arxiv.org/pdf/2201.00469v4"
      ]
    },
    {
      "abstract": "Achieving stability and robustness is the primary goal of biped locomotion control. Recently, deep reinforcement learning (DRL) has attracted great attention as a general methodology for constructing biped control policies and demonstrated significant improvements over the previous state-of-the-art control methods. Although deep control policies are more advantageous compared with previous controller design approaches, many questions remain: Are deep control policies as robust as human walking? Does simulated walking involve strategies similar to human walking for maintaining balance? Does a particular gait pattern affect human and simulated walking similarly? What do deep policies learn to achieve improved gait stability? The goal of this study is to address these questions by evaluating the push-recovery stability of deep policies compared with those of human subjects and a previous feedback controller. Furthermore, we conducted experiments to evaluate the effectiveness of variants of DRL algorithms.",
      "authors": [
        "Park, Hwangpil",
        "Yu, Ri",
        "Lee, Yoonsang",
        "Lee, Kyungho",
        "Lee, Jehee"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s00371-021-02342-9",
      "keywords": [
        "Push-recovery stability",
        "Physically based simulation",
        "Biped locomotion",
        "Gait analysis",
        "Deep reinforcement learning"
      ],
      "number_of_pages": 15,
      "pages": "473-487",
      "publication": {
        "category": "Journal",
        "cite_score": 4.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0178-2789",
        "publisher": "Springer Verlag",
        "sjr": 0.581,
        "snip": 1.439,
        "subject_areas": [
          "Software",
          "Computer Vision and Pattern Recognition",
          "Computer Graphics and Computer-Aided Design"
        ],
        "title": "The Visual Computer"
      },
      "publication_date": "2022-01-03",
      "selected": null,
      "title": "Understanding the stability of deep control policies for biped locomotion",
      "urls": [
        "https://dl.acm.org/doi/10.1007/s00371-021-02342-9",
        "https://link.springer.com/content/pdf/10.1007/s00371-021-02342-9.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122148532&origin=inward"
      ]
    },
    {
      "abstract": "Interactive Reinforcement Learning (IRL) has shown promising results in decreasing the learning times of Reinforcement Learning algorithms by incorporating human feedback and advice. In particular, the integration of multimodal feedback channels such as speech and gestures into IRL systems can enable more versatile and natural interaction of everyday users. In this letter, we propose a novel approach to integrate human advice from multiple modalities into IRL algorithms. For each advice modality we assume an individual base classifier that outputs a categorical probability distribution and fuse these distributions using the Bayesian fusion method Independent Opinion Pool. While existing approaches rely on heuristic fusion, our Bayesian approach is theoretically founded and fully exploits the uncertainty represented in the distributions. Experimental evaluations in a simulated grid world scenario and on a real-world human-robot interaction task with a 7-DoF robot arm show that our method clearly outperforms the closest related approach for multimodal IRL. In particular, our novel approach is more robust against misclassifications of the modalities\u2019 individual base classifiers.",
      "authors": [
        "Susanne Trick",
        "Franziska Herbert",
        "Constantin A. Rothkopf",
        "Dorothea Koert"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/LRA.2022.3182100",
      "keywords": [
        "Human factors and human-in-the-loop",
        "reinforcement learning",
        "multi-modal perception for HRI"
      ],
      "number_of_pages": 8,
      "pages": "7558-7565",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2377-3774",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Interactive Reinforcement Learning With Bayesian Fusion of Multimodal Advice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132773396&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9794595"
      ]
    },
    {
      "abstract": "Smart home appliances can take command and act intelligently, making them suitable for implementing optimization techniques. Artificial intelligence (AI) based control of these smart devices enables demand-side management (DSM) of electricity consumption. By integrating human feedback and activity in the decision process, this work proposes a deep Reinforcement Learning (RL) method for managing smart devices to optimize electricity cost and comfort residents. Our contributions are twofold. Firstly, we incorporate human feedback in the objective function of our DSM technique that we name Home Energy Recommendation System (HERS). Secondly, we include human activity data in the RL state definition to enhance the energy optimization performance. We perform comprehensive experimental analyses to compare the proposed deep RL approach with existing approaches that lack the aforementioned critical decision-making features. The proposed model is robust to varying resident activities and preferences and applicable to a broad spectrum of homes with different resident profiles.",
      "authors": [
        "Salman Sadiq Shuvo",
        "Yasin Yilmaz"
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TSG.2022.3158814",
      "keywords": [
        "artificial intelligence",
        "customer comfort",
        "Home energy management",
        "deep reinforcement learning",
        "residents\u2019 activity label",
        "demand side management"
      ],
      "number_of_pages": 10,
      "pages": "2812-2821",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1949-3061",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Smart Grid"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Home Energy Recommendation System (HERS): A Deep Reinforcement Learning Method Based on Residents\u2019 Feedback and Activity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126295711&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9733029"
      ]
    },
    {
      "abstract": "Learning to solve complex manipulation tasks from visual observations is a dominant challenge for real-world robot learning. Although deep reinforcement learning algorithms have recently demonstrated impressive results in this context, they still require an impractical amount of time-consuming trial-and-error iterations. In this work, we consider the promising alternative paradigm of interactive learning in which a human teacher provides feedback to the policy during execution, as opposed to imitation learning where a pre-collected dataset of perfect demonstrations is used. Our proposed CEILing (Corrective and Evaluative Interactive Learning) framework combines both corrective and evaluative feedback from the teacher to train a stochastic policy in an asynchronous manner, and employs a dedicated mechanism to trade off human corrections with the robot\u2019s own experience. We present results obtained with our framework in extensive simulation and real-world experiments to demonstrate that CEILing can effectively solve complex robot manipulation tasks directly from raw images in less than one hour of real-world training.",
      "authors": [
        "Eugenio Chisari",
        "Tim Welschehold",
        "Joschka Boedecker",
        "Wolfram Burgard",
        "Abhinav Valada"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/LRA.2022.3145516",
      "keywords": [
        "imitation learning",
        "human factors and human-in-the-loop",
        "Deep learning in grasping and manipulation",
        "interactive learning",
        "machine learning for robot control"
      ],
      "number_of_pages": 8,
      "pages": "3695-3702",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2377-3774",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Correct Me If I am Wrong: Interactive Learning for Robotic Manipulation",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9691826",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123733184&origin=inward"
      ]
    },
    {
      "abstract": "We study the challenging problem of following natural language instructions on a mobile manipulator robot. This task is challenging because it requires the robot to integrate the semantics of the unconstrained natural language instructions with the robot\u2019s egocentric visual observations of the environment which are typically incomplete and noisy. To address these challenges, we propose a method that is able to use visible landmarks to more efficiently explore the environment in search of the objects described by the natural language instructions. Additionally, we propose using a pose adjustment policy during manipulation planning to help the robot recover from noisy visual observations. We show that this policy can be trained through experience with reinforcement learning as well as with human-in-the-loop feedback. We evaluate our approach on the popular ALFRED instruction following benchmark and show that these methods achieve state-of-the-art performance (35.41%) with a substantial (8.92% absolute) gap from prior work.",
      "authors": [
        "Michael Murray",
        "Maya Cakmak"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/LRA.2022.3178804",
      "keywords": [
        "natural dialog for HRI",
        "learning from experience",
        "Multi-modal perception for HRI",
        "Domestic robotics"
      ],
      "number_of_pages": 8,
      "pages": "6870-6877",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2377-3774",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Following Natural Language Instructions for Household Tasks With Landmark Guided Search and Reinforced Pose Adjustment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131769319&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9785410"
      ]
    },
    {
      "abstract": "Accurately and intelligently identifying faults of the planetary gearbox is essential in the safe and reliable operation and maintenance of the mechanical drive system. Recently, fault diagnosis of planetary gearbox has acquired tremendous progress, especially with the rising popularity of deep learning (DL). However, most methods are standard supervised learning where the input is directly mapped to a fault type, and with strong feedback. Also, their learning ways are static and unlike human learning that gradually acquires knowledge by interaction with the environment. To a certain extent, these deficiencies reduce the generalization and intelligence level of DL-based fault diagnosis methods. Besides, due to harsh working conditions, signals acquired often have strong noise and nonlinear features, leading to relatively low accuracy if raw signals are used as the input directly. Thus, this article proposes a new fault diagnosis method based on time-frequency representation and deep reinforcement learning (DRL). We first define fault diagnosis as a sequential decision-making problem in the classification Markov decision process. Next, the vibration signals are converted to uniform-sized TF maps by synchro-extracting transform to enhance the robustness of feature representation. Finally, a diagnosis agent is built and trained in the framework of DRL to learn the optimal classification policy automatically. Experimental results show that this method not only achieves better generalization and stability with an overall accuracy of over 99.5% in single-speed load cases but also outperforms others in multiwork conditions.",
      "authors": [
        "Hui Wang",
        "Jiawen Xu",
        "Chuang Sun",
        "Ruqiang Yan",
        "Xuefeng Chen"
      ],
      "categories": null,
      "citations": 43,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TMECH.2021.3076775",
      "keywords": [
        "deep reinforcement learning (DRL)",
        "Classification policy",
        "deep learning (DL)",
        "fault diagnosis",
        "planetary gearbox"
      ],
      "number_of_pages": 14,
      "pages": "985-998",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1941-014X",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE/ASME Transactions on Mechatronics"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Intelligent Fault Diagnosis for Planetary Gearbox Using Time-Frequency Representation and Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105071413&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9420305"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 0.7,
        "is_potentially_predatory": false,
        "isbn": "9783030821982",
        "issn": "23673370",
        "publisher": "Springer International Publishing AG",
        "sjr": 0.151,
        "snip": 0.19,
        "subject_areas": [
          "Computer Networks and Communications",
          "Signal Processing",
          "Control and Systems Engineering"
        ],
        "title": "Lecture Notes in Networks and Systems"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Intelligent Systems Conference, IntelliSys 2021",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113759128&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113212338&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113531014&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Elor A."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/SMARTCOMP55677.2022.00033",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "133-141",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665481526",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2022 IEEE International Conference on Smart Computing, SMARTCOMP 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Human Experiences in Teaching Robots: Understanding Agent Expressivity and Learning Effects through a Virtual Robot Arm",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136098993&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Duggins P."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "3453-3459",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 44th Annual Meeting of the Cognitive Science Society: Cognitive Diversity, CogSci 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Reinforcement Learning, Social Value Orientation, and Decision Making: Computational Models and Empirical Validation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146423470&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dutra P.V.M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/SBGAMES56371.2022.9961076",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665461566",
        "issn": "21596654",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Brazilian Symposium on Games and Digital Entertainment, SBGAMES"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Procedural Content Generation using Reinforcement Learning and Entropy Measure as Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143795904&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gupta N."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/0952813X.2022.2117422",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0952813X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Experimental and Theoretical Artificial Intelligence"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Development of human decision making model with consideration of human factors through reinforcement learning and prospect utility theory",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138404852&origin=inward"
      ]
    },
    {
      "abstract": "As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine\u2019s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, \u201cCan robots become competent and adaptive teammates by emulating human skill acquisition strategies?\u201d In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams.",
      "authors": [
        "David A. Handelman",
        "Corban G. Rivera",
        "Robert St. Amant",
        "Emma A. Holmes",
        "Andrew R. Badger",
        "Bryanna Y. Yeh"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1117/12.2618686",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781510662087",
        "issn": "0277786X",
        "publisher": "SPIE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications IV"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134162538&origin=inward",
        "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12113/121130I/Adaptive-human-robot-teaming-through-integrated-symbolic-and-subsymbolic-artificial/10.1117/12.2618686.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Shen X."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNSRE.2022.3210700",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "2834-2844",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15344320",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.26,
        "snip": 1.675,
        "subject_areas": [
          "Neuroscience (all)",
          "Biomedical Engineering",
          "Internal Medicine",
          "Rehabilitation"
        ],
        "title": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Intermediate Sensory Feedback Assisted Multi-Step Neural Decoding for Reinforcement Learning Based Brain-Machine Interfaces",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139849297&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ward F.R."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1759-1761",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "On Agent Incentives to Manipulate Human Feedback in Multi-Agent Reward Learning Scenarios",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134335959&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Manela B."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2021.10.011",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "260-270",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Curriculum learning with Hindsight Experience Replay for sequential object manipulation tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118883674&origin=inward"
      ]
    },
    {
      "abstract": "This work presents a model for improving transparency during robot learning tasks in Human-Robot Interaction scenarios. Our model puts the human in the learning loop by using two categories of robot\u2019s emotional/behavioural reactions, one associated with the...",
      "authors": [
        "Angelopoulos, Georgios",
        "Rossi, Alessandra",
        "L\u2019Arco, Gianluca",
        "Rossi, Silvia"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-031-24667-8_27",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "300-311",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Transparent Interactive Reinforcement Learning Using Emotional Behaviours",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149878885&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-031-24667-8_27.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gong Z."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Explicable Policy Search",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163145242&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Cichos F."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-323-85796-3.00005-6",
      "keywords": [],
      "number_of_pages": 32,
      "pages": "113-144",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780323857963",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Intelligent Nanotechnology: Merging Nanoscience and Artificial Intelligence"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Artificial intelligence (AI) enhanced nanomotors and active matter",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85151173639&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Jadoon M.A."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/VTC2022-Spring54318.2022.9860949",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350329285",
        "issn": "15502252",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Vehicular Technology Conference"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Collision Resolution with Deep Reinforcement Learning for Random Access in Machine-Type Communication",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137810482&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Geng L."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/UV56588.2022.10185519",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665474771",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "6th IEEE International Conference on Universal Village, UV 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Evaluation of Smart Home Systems and Novel UV-Oriented Solution for Integration, Resilience, Inclusiveness &amp; Sustainability",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85167832425&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Crochepierre L."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 4,
      "pages": "5900-5903",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781956792034",
        "issn": "10450823",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IJCAI International Joint Conference on Artificial Intelligence"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Interactive Reinforcement Learning for Symbolic Regression from Multi-Format Human-Preference Feedbacks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137852550&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huang Y."
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.arcontrol.2022.01.001",
      "keywords": [],
      "number_of_pages": 23,
      "pages": "273-295",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13675788",
        "publisher": "Elsevier Ltd.",
        "sjr": 3.343,
        "snip": 4.271,
        "subject_areas": [
          "Software",
          "Control and Systems Engineering"
        ],
        "title": "Annual Reviews in Control"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Reinforcement Learning for feedback-enabled cyber resilience",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123705509&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gerstgrasser M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ICLR 2022 - 10th International Conference on Learning Representations"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "CROWDPLAY: CROWDSOURCING HUMAN DEMONSTRATIONS FOR OFFLINE LEARNING",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85147670155&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Pasqualini L."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICMLA55696.2022.00099",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "573-578",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665462839",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 21st IEEE International Conference on Machine Learning and Applications, ICMLA 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Score vs. Winrate in Score-Based Games: Which Reward for Reinforcement Learning?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85152213422&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "B\u0131y\u0131k E."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/02783649211041652",
      "keywords": [],
      "number_of_pages": 23,
      "pages": "45-67",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02783649",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Robotics Research"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113640781&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liao Y.H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 13,
      "pages": "21-33",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Conversational AI for Positive-sum Retailing under Falsehood Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149133961&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "3rd International Conference on Artificial Intelligence in HCI, AI-HCI 2022 Held as Part of the 24th HCI International Conference, HCII 2022",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131127295&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhang J."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 16,
      "pages": "1010-1025",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Can Humans Be out of the Loop?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85164535639&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bennett D."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/rev0000294",
      "keywords": [],
      "number_of_pages": 29,
      "pages": "513-541",
      "publication": {
        "category": "Journal",
        "cite_score": 9.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0033295X",
        "publisher": "American Psychological Association",
        "sjr": 2.801,
        "snip": 3.144,
        "subject_areas": [
          "Psychology (all)"
        ],
        "title": "Psychological Review"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "A Model of Mood as Integrated Advantage",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132454753&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning is a machine learning technique that makes a decision based on a sequence of actions. This allows changing a game agent\u2019s behavior through feedback, such as rewards or penalties for their actions. Recent work has been demonstrating the...",
      "authors": [
        "Dias, Augusto",
        "Foleiss, Juliano",
        "Lopes, Rui Pedro"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-95305-8_10",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "127-139",
      "publication": {
        "category": "Book",
        "cite_score": 1.0,
        "is_potentially_predatory": false,
        "isbn": "9789819927883",
        "issn": "18650929",
        "publisher": "Springer Science and Business Media Deutschland GmbH",
        "sjr": 0.194,
        "snip": 0.241,
        "subject_areas": [
          "Computer Science (all)",
          "Mathematics (all)"
        ],
        "title": "International Conference on Neural Information Processing"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Reinforcement Learning in Tower Defense",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-030-95305-8_10.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124662559&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Norman S.L."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/braincomms/fcac264",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Brain Communications"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Targeting neuroplasticity to improve motor recovery after stroke: an artificial neural network model",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85144552461&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Han J.I."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNSRE.2022.3196468",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "2186-2197",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15344320",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.26,
        "snip": 1.675,
        "subject_areas": [
          "Neuroscience (all)",
          "Biomedical Engineering",
          "Internal Medicine",
          "Rehabilitation"
        ],
        "title": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Policy Design for an Ankle-Foot Orthosis Using Simulated Physical Human-Robot Interaction via Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135757325&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hsiung E."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/HRI53351.2022.9889498",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "807-811",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781538685549",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Learning Reward Functions from a Combination of Demonstration and Evaluative Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140738317&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kong D."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85163170629&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhu H."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1351-1358",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 44th Annual Meeting of the Cognitive Science Society: Cognitive Diversity, CogSci 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Simulated Language Learning from Communicative Goals and Linguistic Input",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146417134&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu R."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149774355&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Alzahrani M."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.2523/IPTC-22151-MS",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781613998335",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Petroleum Technology Conference, IPTC 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Novel Stuck Pipe Troubles Prediction Model Using Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85150614908&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350309799",
        "issn": "21640572",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE-RAS International Conference on Humanoid Robots"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "2022 IEEE-RAS 21st International Conference on Humanoid Robots, Humanoids 2022",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146334820&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sheidlower I."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1726-1728",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Environment Guided Interactive Reinforcement Learning: Learning from Binary Feedback in High-Dimensional Robot Task Environments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134296748&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Van Waveren S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/HRI53351.2022.9889361",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1182-1184",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781538685549",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Leveraging Non-Experts and Formal Methods to Automatically Correct Robot Failures",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140752171&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Roder F."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICDL53763.2022.9962207",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "170-177",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665413114",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 IEEE International Conference on Development and Learning, ICDL 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Grounding Hindsight Instructions in Multi-Goal Reinforcement Learning for Robotics",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143403930&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huang J."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-323-85648-5.00020-7",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "145-154",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780323856485",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Human-Centered Artificial Intelligence: Research and Applications"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Reinforcement learning in EEG-based human-robot interaction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137880727&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zou A.R."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "3571-3578",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 44th Annual Meeting of the Cognitive Science Society: Cognitive Diversity, CogSci 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Three systems interact in one-shot reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85146430379&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9784907764784",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2022 61st Annual Conference of the Society of Instrument and Control Engineers of Japan, SICE 2022"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "2022 61st Annual Conference of the Society of Instrument and Control Engineers of Japan, SICE 2022",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85141154685&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bewley T."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "118-126",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Interpretable Preference-based Reinforcement Learning with Tree-Structured Reward Functions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132186906&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "McKee K.R."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "898-907",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Warmth and Competence in Human-Agent Cooperation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134296131&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kazantzidis I."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1654-1656",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "How to Train Your Agent: Active Learning from Human Preferences and Justifications in Safety-Critical Environments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134306740&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665461566",
        "issn": "21596654",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Brazilian Symposium on Games and Digital Entertainment, SBGAMES"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "2022 21st Brazilian Symposium on Games and Digital Entertainment, SBGames 2022",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85143766507&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huang, Bowen",
        "Huang, Haoran",
        "Zhang, Shuting",
        "Zhang, Dingyue",
        "Shi, Qingya",
        "Liu, Jianzhou",
        "Guo, Junchao"
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7150/thno.77949",
      "keywords": [
        "early detection",
        "prognosis prediction",
        "Artificial intelligence",
        "pancreatic cancer",
        "machine learning"
      ],
      "number_of_pages": 24,
      "pages": "6931-6954",
      "publication": {
        "category": "Journal",
        "cite_score": 20.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1838-7640",
        "publisher": "Ivyspring International Publisher",
        "sjr": 2.381,
        "snip": 1.893,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Pharmacology, Toxicology and Pharmaceutics (miscellaneous)"
        ],
        "title": "Theranostics"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Artificial intelligence in pancreatic cancer",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140261886&origin=inward",
        "https://www.thno.org/v12p6931.pdf"
      ]
    },
    {
      "abstract": "Data visualization is a complex task that typically requires human expertise, acquired through a large number of professional working hours. The automatic generation of reasonable visualizations would be a good solution for inexperienced laypeople. However, existing approaches fall short since they are quite static and rely only on traditional supervised learning. This results in models which recommend a single visualization solely based on the dataset features. User preferences and goals are not taken into account. We propose a more flexible solution that is iteratively updated with the individual user's preferences and outputs a ranked list of visualizations for a given dataset.",
      "authors": [
        "Laurito,Walter",
        "H\u00f6llig,Jacqueline",
        "Lachowitzer,Jonas",
        "Thoma,Steffen",
        "Budde,Matthias",
        "Philipp,Patrick"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.18420/inf2022_27",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "301-305",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 0.5,
        "is_potentially_predatory": false,
        "isbn": "9783885797203",
        "issn": "16175468",
        "publisher": "Gesellschaft fur Informatik (GI)",
        "sjr": 0.173,
        "snip": 0.2,
        "subject_areas": [
          "Computer Science Applications"
        ],
        "title": "Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "AIDA-Vis \u2013 Automatic Data Visualization with Human Preferences",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139783626&origin=inward",
        "https://dl.gi.de/bitstreams/2811d619-6f3e-4400-b2a9-285ac483d962/download"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang M."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/01605682.2021.2015257",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "2741-2755",
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01605682",
        "publisher": "Taylor and Francis Ltd.",
        "sjr": 0.891,
        "snip": 1.267,
        "subject_areas": [
          "Modeling and Simulation",
          "Statistics, Probability and Uncertainty",
          "Strategy and Management",
          "Management Science and Operations Research"
        ],
        "title": "Journal of the Operational Research Society"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Consensus achievement strategy of opinion dynamics based on deep reinforcement learning with time constraint",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121763834&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huang L."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TIFS.2022.3189530",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "2585-2597",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15566013",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Information Forensics and Security"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "ADVERT: An Adaptive and Data-Driven Attention Enhancement Mechanism for Phishing Prevention",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134259178&origin=inward"
      ]
    },
    {
      "abstract": "Many studies focusing on improving Transmission Control Protocol (TCP) flow control realize a more effective use of bandwidth in data center networks. They are excellent ways to more effectively use the bandwidth between clients and back-end servers. However, these schemes cannot achieve the total optimization of bandwidth use for data center networks as they do not take into account the path design of TCP flows against a hierarchical complex structure of data center networks. To address this issue, this paper proposes a TCP flow management scheme specified a hierarchical complex data center network for effective bandwidth use. The proposed scheme dynamically controls the paths of TCP flows by reinforcement learning based on a hierarchical feedback model, which obtains an optimal TCP flow establishment policy even if both the network topology and link states are more complicated. In evaluation, the proposed scheme achieved more effective bandwidth use and reduced the probability of TCP incast up to 30% than the conventional TCP flow management schemes: Variant Load Balancing (VLB), Equal Cost Multi Path (ECMP), and Intelligent Forwarding Strategy Based on Reinforcement Learning (IFS-RL) in the complex data center network.",
      "authors": [
        "Mizutani, Kimihiro"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/s22020611",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14248220",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.764,
        "snip": 1.317,
        "subject_areas": [
          "Atomic and Molecular Physics, and Optics",
          "Information Systems",
          "Instrumentation",
          "Biochemistry",
          "Analytical Chemistry",
          "Electrical and Electronic Engineering"
        ],
        "title": "Sensors (Switzerland)"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Effective TCP Flow Management Based on Hierarchical Feedback Learning in Complex Data Center Network",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123662004&origin=inward",
        "https://www.mdpi.com/1424-8220/22/2/611/pdf?version=1642140877"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Tzeng S.T."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1878-1880",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Engineering Normative and Cognitive Agents with Emotions and Values",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134355078&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Choo S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-323-85648-5.00005-0",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "127-143",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780323856485",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Human-Centered Artificial Intelligence: Research and Applications"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Interactive reinforcement learning and error-related potential classification for implicit feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85137886468&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hayes W.M."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/xlm0001145",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02787393",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Experimental Psychology: Learning Memory and Cognition"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Reinforcement Learning In and Out of Context: The Effects of Attentional Focus",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85134067144&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Baumann S."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/15622975.2021.1995809",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "349-360",
      "publication": {
        "category": "Journal",
        "cite_score": 6.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15622975",
        "publisher": "Taylor and Francis Ltd.",
        "sjr": 0.872,
        "snip": 0.985,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "World Journal of Biological Psychiatry"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Differentiating brain function of punishment versus reward processing in conduct disorder with and without attention deficit hyperactivity disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119449343&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Marta D."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/LRA.2021.3128237",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "406-413",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Human-Feedback Shield Synthesis for Perceived Safety in Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121268554&origin=inward"
      ]
    },
    {
      "abstract": "A goal of computational psychiatry is to ground symptoms in basic mechanisms. Theory suggests that avoidance in anxiety disorders may reflect dysregulated mental simulation, a process for evaluating candidate actions. If so, these covert processes should have observable consequences: choices reflecting increased and biased deliberation. In two online general population samples, we examined how self-report symptoms of social anxiety disorder predict choices in a socially framed reinforcement learning task, the patent race, in which the pattern of choices reflects the content of deliberation. Using a computational model to assess learning strategy, we found that self-report social anxiety was indeed associated with increased deliberative evaluation. This effect was stronger for a particular subset of feedback (\u2018upward counterfactual\u2019) in one of the experiments, broadly matching the biased content of rumination in social anxiety disorder, and robust to controlling for other psychiatric symptoms. These results suggest a grounding of symptoms of social anxiety disorder in more basic neuro-computational mechanisms. Hunter et al. find evidence that people with higher self-reported social anxiety deliberate more in a socially framed reinforcement learning task.",
      "authors": [
        "Hunter, Lindsay E.",
        "Meer, Elana A.",
        "Gillan, Claire M.",
        "Hsu, Ming",
        "Daw, Nathaniel D."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41562-021-01180-y",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "146-154",
      "publication": {
        "category": "Journal",
        "cite_score": 26.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2397-3374",
        "publisher": "Nature Publishing Group",
        "sjr": 5.639,
        "snip": 6.703,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Social Psychology"
        ],
        "title": "Nature Human Behaviour"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Increased and biased deliberation in social anxiety",
      "urls": [
        "https://www.nature.com/articles/s41562-021-01180-y.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112676711&origin=inward"
      ]
    },
    {
      "abstract": "Minimally invasive surgery has a smaller incision area than traditional open surgery, which can greatly reduce damage to the human body and improve the utilization of medical devices. However, minimally invasive surgery also has disadvantages such as limited flexibility and operational characteristics. The interactive minimally invasive surgical robot system not only improves the stability, safety, and accuracy of minimally invasive surgery but also introduces force feedback in controlling the surgical robot, which is a new development direction in the field of minimally invasive surgery. This paper reviews the development status of interactive minimally invasive surgical robotic systems and key technologies to achieve human-robot interaction and finally provides an outlook and summary of its development. Fuzzy theory and reinforcement learning are introduced into the parameter adjustment process of the variable guide control model, and a human-robot interaction method for minimally invasive surgical robot posture adjustment is proposed.",
      "authors": [
        "Jiang, Fenglin",
        "Jia, Ruiyao",
        "Jiang, Xiujuan",
        "Cao, Fang",
        "Lei, Tingting",
        "Luo, Li"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "https://doi.org/10.1155/2022/9434725",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 3.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1687-5265",
        "publisher": "Hindawi Publishing Corporation",
        "sjr": 0.863,
        "snip": 1.023,
        "subject_areas": [
          "Computer Science (all)",
          "Neuroscience (all)",
          "Mathematics (all)"
        ],
        "title": "Computational Intelligence and Neuroscience"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Human-Machine Interaction Methods for Minimally Invasive Surgical Robotic Arms",
      "urls": [
        "https://downloads.hindawi.com/journals/cin/2022/9434725.pdf",
        "https://dl.acm.org/doi/10.1155/2022/9434725",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138213319&origin=inward"
      ]
    },
    {
      "abstract": "Existing robotics technology is still mostly limited to being used by expert programmers who can adapt the systems to new required conditions, but not flexible and adaptable by non-expert workers or end-users. Imitation Learning (IL) has obtained considerable attention as a potential direction for enabling all kinds of users to easily program the behavior of robots or virtual agents. Interactive Imitation Learning (IIL) is a branch of Imitation Learning (IL) where human feedback is provided intermittently during robot execution allowing an online improvement of the robot\u2019s behavior. In this monograph, research in IIL is presented and low entry barriers for new practitioners are facilitated by providing a survey of the field that unifies and structures it. In addition, awareness of its potential is raised, what has been accomplished and what are still open research questions being covered. Highlighted are the most relevant works in IIL in terms of human-robot interaction (i.e., types of feedback), interfaces (i.e., means of providing feedback), learning (i.e., models learned from feedback and function approximators), user experience (i.e., human perception about the learning process), applications, and benchmarks. Furthermore, similarities and differences between IIL and Reinforcement Learning (RL) are analyzed, providing a discussion on how the concepts offline, online, off-policy and on-policy learning should be transferred to IIL from the RL literature. Particular focus is given to robotic applications in the real world and their implications are discussed, and limitations and promising future areas of research are provided.",
      "authors": [
        "Carlos Celemin",
        "Rodrigo P\u00e9rez-Dattari",
        "Eugenio Chisari",
        "Giovanni Franzese",
        "Leandro de Souza Rosa",
        "Ravi Prakash",
        "Zlatan Ajanovi\u0107",
        "Marta Ferraz",
        "Abhinav Valada",
        "Jens Kober"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781638281276",
        "issn": null,
        "publisher": "now",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Interactive Imitation Learning in Robotics: A Survey"
      },
      "publication_date": "2022-01-01",
      "selected": null,
      "title": "Interactive Imitation Learning in Robotics: A Survey",
      "urls": [
        "https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9962784.pdf&bkn=9962783&pdfType=book"
      ]
    },
    {
      "abstract": "Inferring reward functions from demonstrations and pairwise preferences are auspicious approaches for aligning Reinforcement Learning (RL) agents with human intentions. However, state-of-the art methods typically focus on learning a single reward model, thus rendering it difficult to trade off different reward functions from multiple experts. We propose Multi-Objective Reinforced Active Learning (MORAL), a novel method for combining diverse demonstrations of social norms into a Pareto-optimal policy. Through maintaining a distribution over scalarization weights, our approach is able to interactively tune a deep RL agent towards a variety of preferences, while eliminating the need for computing multiple policies. We empirically demonstrate the effectiveness of MORAL in two scenarios, which model a delivery and an emergency task that require an agent to act in the presence of normative conflicts. Overall, we consider our research a step towards multi-objective RL with learned rewards, bridging the gap between current reward learning and machine ethics literature.",
      "authors": [
        "Peschl, Markus",
        "Zgonnikov, Arkady",
        "Oliehoek, Frans A.",
        "Siebert, Luciano C."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-12-30",
      "selected": null,
      "title": "MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning",
      "urls": [
        "http://arxiv.org/pdf/2201.00012.pdf",
        "http://arxiv.org/abs/2201.00012v1",
        "http://arxiv.org/pdf/2201.00012v1"
      ]
    },
    {
      "abstract": "To improve the intelligence of multiskilled human resource group scheduling, a multiskilled human resource group intelligent scheduling method based on reinforcement learning is proposed. It is divided into six steps to build an enterprise project group, mainly including the definition of the project group, project selection and evaluation and resource allocation of the project group. The focus is on the research and establishment of a project group selection evaluation model and resource allocation model. At the same time, during the implementation of the project group, the structure and plan of the project group are constantly adjusted through feedback information to improve the efficiency of multiple project implementation. Based on reinforcement learning, the scheduling direction of the multiskilled human resources group is determined. The selected reinforcement learning algorithm is the Q reinforcement learning method, that is, an offline instantaneous TDOA algorithm independent of the environmental model. A Kubernetes group intelligent scheduling model is constructed, and a simulated annealing genetic algorithm is designed to solve the group intelligent scheduling model to realize the group intelligent scheduling of multiskilled human resources. The test results show that the scheduling failure rate of this method is low in the cluster environment with sufficient and insufficient resources, indicating that this method has broad application prospects.",
      "authors": [
        "Ling Niu"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IAAI54625.2021.9699985",
      "keywords": [
        "Group intelligent scheduling",
        "Enterprise project group",
        "Multiskilled human resources",
        "Reinforcement learning"
      ],
      "number_of_pages": 7,
      "pages": "61-67",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-6581-6",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 IEEE International Conference on Industrial Application of Artificial Intelligence, IAAI 2021"
      },
      "publication_date": "2021-12-24",
      "selected": null,
      "title": "Multi Skill Human Resource Group Intelligent Scheduling Method Based on Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9699985",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125964425&origin=inward"
      ]
    },
    {
      "abstract": "The potential of reinforcement learning (RL) to deliver aligned and performant agents is partially bottlenecked by the reward engineering problem. One alternative to heuristic trial-and-error is preference-based RL (PbRL), where a reward function is inferred from sparse human feedback. However, prior PbRL methods lack interpretability of the learned reward structure, which hampers the ability to assess robustness and alignment. We propose an online, active preference learning algorithm that constructs reward functions with the intrinsically interpretable, compositional structure of a tree. Using both synthetic and human-provided feedback, we demonstrate sample-efficient learning of tree-structured reward functions in several environments, then harness the enhanced interpretability to explore and debug for alignment.",
      "authors": [
        "Bewley, Tom",
        "Lecue, Freddy"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted for publication at the 21st International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS 2022)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-12-20",
      "selected": null,
      "title": "Interpretable Preference-based Reinforcement Learning with Tree-Structured Reward Functions",
      "urls": [
        "http://arxiv.org/abs/2112.11230v1",
        "http://arxiv.org/pdf/2112.11230v1",
        "http://arxiv.org/pdf/2112.11230.pdf"
      ]
    },
    {
      "abstract": "To develop driving automation technologies for human, a human-centered methodology should be adopted for ensured safety and satisfactory user experience. Automated lane change decision in dense highway traffic is challenging, especially when considering the personalized preferences of different drivers. To fulfill human driver centered decision algorithm development, we carry out driver-in-the-loop experiments on a 6-Degree-of-Freedom driving simulator. Based on the analysis of the lane change data by drivers of three specific styles,personalization indicators are selected to describe the driver preferences in lane change decision. Then a deep reinforcement learning (RL) approach is applied to design human-like agents for automated lane change decision, with refined reward and loss functions to capture the driver preferences.The trained RL agents and benchmark agents are tested in a two-lane highway driving scenario, and by comparing the agents with the specific drivers at the same initial states of lane change, the statistics show that the proposed algorithm can guarantee higher consistency of lane change decision preferences. The driver personalization indicators and the proposed RL-based lane change decision algorithm are promising to contribute in automated lane change system developing.",
      "authors": [
        "Li, Daofei",
        "Liu, Ao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-12-17",
      "selected": null,
      "title": "Personalized Lane Change Decision Algorithm Using Deep Reinforcement Learning Approach",
      "urls": [
        "http://arxiv.org/pdf/2112.13646.pdf",
        "http://arxiv.org/pdf/2112.13646v1",
        "http://arxiv.org/abs/2112.13646v1"
      ]
    },
    {
      "abstract": "Treatment recommendation is a complex multi-faceted problem with many treatment goals considered by clini-cians and patients, e.g., optimizing the survival rate, mitigating negative impacts, reducing financial expenses, avoiding over-treatment, etc. Recently, deep reinforcement learning (RL) approaches have gained popularity for treatment recommendation. In this paper, we investigate preference-based reinforcement learning approaches for treatment recommendation, where the reward function is itself learned based on treatment goals, without requiring either expert demonstrations in advance or human involvement during policy learning. We first present an open sim-ulation platform11https://sites.google.com/view/tr-with-prl/ to model the evolution of two diseases, namely Cancer and Sepsis, and individuals' reactions to the received treatment. Secondly, we systematically examine preference-based RL for treatment recommendation via simulated experiments and observe high utility in the learned policy in terms of high survival rate and low side effects, with inferred rewards highly correlated to treatment goals. We further explore the transferability of inferred reward functions and guidelines for agent design to provide insights in achieving the right trade-off among various human objectives with preference-based RL approaches for treatment recommendation in the real world.",
      "authors": [
        "Nan Xu",
        "Nitin Kamra",
        "Yan Liu"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICKG52313.2021.00025",
      "keywords": [
        "Treatment Recommendation",
        "Healthcare",
        "Preference Learning",
        "Reinforcement Learning"
      ],
      "number_of_pages": 8,
      "pages": "117-124",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-3859-9",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 12th IEEE International Conference on Big Knowledge, ICBK 2021"
      },
      "publication_date": "2021-12-07",
      "selected": null,
      "title": "Treatment Recommendation with Preference-based Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9667722",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125099904&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wurm F."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_01777",
      "keywords": [],
      "number_of_pages": 20,
      "pages": "34-53",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2021-12-06",
      "selected": null,
      "title": "Task learnability modulates surprise but not valence processing for reinforcement learning in probabilistic choice tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121442578&origin=inward"
      ]
    },
    {
      "abstract": "<h3>Introduction</h3>\n<p>Achieving optimal diabetes control requires several daily self-management behaviours, especially adherence to medication. Evidence supports the use of text messages to support adherence, but there remains much opportunity to improve their effectiveness. One key limitation is that message content has been generic. By contrast, reinforcement learning is a machine learning method that can be used to identify individuals\u2019 patterns of responsiveness by observing their response to cues and then optimising them accordingly. Despite its demonstrated benefits outside of healthcare, its application to tailoring communication for patients has received limited attention. The objective of this trial is to test the impact of a reinforcement learning-based text messaging programme on adherence to medication for patients with type 2 diabetes.</p><h3>Methods and analysis</h3>\n<p>In the REinforcement learning to Improve Non-adherence For diabetes treatments by Optimising Response and Customising Engagement (REINFORCE) trial, we are randomising 60 patients with suboptimal diabetes control treated with oral diabetes medications to receive a reinforcement learning intervention or control. Subjects in both arms will receive electronic pill bottles to use, and those in the intervention arm will receive up to daily text messages. The messages will be individually adapted using a reinforcement learning prediction algorithm based on daily adherence measurements from the pill bottles. The trial\u2019s primary outcome is average adherence to medication over the 6-month follow-up period. Secondary outcomes include diabetes control, measured by glycated haemoglobin A1c, and self-reported adherence. In sum, the REINFORCE trial will evaluate the effect of personalising the framing of text messages for patients to support medication adherence and provide insight into how this could be adapted at scale to improve other self-management interventions.</p><h3>Ethics and dissemination</h3>\n<p>This study was approved by the Mass General Brigham Institutional Review Board (IRB) (USA). Findings will be disseminated through peer-reviewed journals, clinicaltrials.gov reporting and conferences.</p><h3>Trial registration number</h3>\n<p>Clinicaltrials.gov (NCT04473326).</p>",
      "authors": [
        "Julie C Lauffenburger",
        "Elad Yom-Tov",
        "Punam A Keller",
        "Marie E McDonnell",
        "Lily G Bessette",
        "Constance P Fontanet",
        "Ellen S Sears",
        "Erin Kim",
        "Kaitlin Hanken",
        "J Joseph Buckley",
        "Renee A Barlev",
        "Nancy Haff",
        "Niteesh K Choudhry"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1136/bmjopen-2021-052091",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "BMJ Open"
      },
      "publication_date": "2021-12-03",
      "selected": null,
      "title": "REinforcement learning to improve non-adherence for diabetes treatments by Optimising Response and Customising Engagement (REINFORCE): study protocol of a pragmatic randomised trial",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121008254&origin=inward"
      ]
    },
    {
      "abstract": "The technologies used in smart homes have recently improved to learn the user preferences from feedback in order to enhance the user convenience and quality of experience. Most smart homes learn a uniform model to represent the thermal preferences of users, which generally fails when the pool of occupants includes people with different sensitivities to temperature, for instance due to age and physiological factors. Thus, a smart home with a single optimal policy may fail to provide comfort when a new user with a different preference is integrated into the home. In this paper, we propose a Bayesian Reinforcement learning framework that can approximate the current occupant state in a partially observable smart home environment using its thermal preference, and then identify the occupant as a new user or someone is already known to the system. Our proposed framework can be used to identify users based on the temperature and humidity preferences of the occupant when performing different activities to enable personalization and improve comfort. We then compare the proposed framework with a baseline long short-term memory learner that learns the thermal preference of the user from the sequence of actions which it takes. We perform these experiments with up to 5 simulated human models each based on hierarchical reinforcement learning. The results show that our framework can approximate the belief state of the current user just by its temperature and humidity preferences across different activities with a high degree of accuracy.",
      "authors": [
        "Suman, Shashi",
        "Rivest, Francois",
        "Etemad, Ali"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-12-02",
      "selected": null,
      "title": "Towards Personalization of User Preferences in Partially Observable Smart Home Environments",
      "urls": [
        "http://arxiv.org/abs/2112.00971v4",
        "http://arxiv.org/pdf/2112.00971.pdf",
        "http://arxiv.org/pdf/2112.00971v4"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) and brain-computer interfaces (BCI) have experienced significant growth over the past decade. With rising interest in human-in-the-loop (HITL), incorporating human input with RL algorithms has given rise to the sub-field of interactive RL. Adjacently, the field of BCI has long been interested in extracting informative brain signals from neural activity for use in human-computer interactions. A key link between these fields lies in the interpretation of neural activity as feedback such that interactive RL approaches can be employed. We denote this new and emerging medium of feedback as intrinsic feedback. Despite intrinsic feedback's ability to be conveyed automatically and even unconsciously, proper exploration surrounding this key link has largely gone unaddressed by both communities. Thus, to help facilitate a deeper understanding and a more effective utilization, we provide a tutorial-style review covering the motivations, approaches, and open problems of intrinsic feedback and its foundational concepts.",
      "authors": [
        "Poole, Benjamin",
        "Lee, Minwoo"
      ],
      "categories": null,
      "citations": null,
      "comments": "Name change and vast rewrites of the paper",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-12-02",
      "selected": null,
      "title": "Towards Interactive Reinforcement Learning with Intrinsic Feedback",
      "urls": [
        "http://arxiv.org/abs/2112.01575v3",
        "http://arxiv.org/pdf/2112.01575.pdf",
        "http://arxiv.org/pdf/2112.01575v3"
      ]
    },
    {
      "abstract": "The last decades of research have gradually elucidated the complex functions of the dopamine system in the vertebrate brain. The multiple roles of dopamine in motor function, learning, attention, motivation, and the emotions have been difficult to reconcile. A broad and detailed understanding of the physiology of cerebral dopamine is of importance in understanding a range of human disorders. One of the core functions of dopamine involves the basal ganglia and the learning and execution of automatized sequences of movements. Speech is one of the most complex and highly automatized sequential motor behaviors, though the exact roles that the basal ganglia and dopamine play in speech have been difficult to determine. Stuttering is a speech disorder that has been hypothesized to be related to the functions of the basal ganglia and dopamine. The aim of this review was to provide an overview of the current understanding of the cerebral dopamine system, in particular the mechanisms related to motor learning and the execution of movement sequences. The primary aim was not to review research on speech and stuttering, but to provide a platform of neurophysiological mechanisms, which may be utilized for further research and theoretical development on speech, speech disorders, and other behavioral disorders. Stuttering and speech are discussed here only briefly. The review indicates that a primary mechanism for the automatization of movement sequences is the merging of isolated movements into chunks that can be executed as units. In turn, chunks can be utilized hierarchically, as building blocks of longer chunks. It is likely that these mechanisms apply also to speech, so that frequent syllables and words are produced as motor chunks. It is further indicated that the main learning principle for sequence learning is reinforcement learning, with the phasic release of dopamine as the primary teaching signal indicating successful sequences. It is proposed that the dynamics of the dopamine system constitute the main neural basis underlying the situational variability of stuttering.",
      "authors": [
        "Alm, Per A."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2021.661880",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2021-12-02",
      "selected": null,
      "title": "The Dopamine System and Automatization of Movement Sequences: A Review With Relevance for Speech and Stuttering",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121396237&origin=inward"
      ]
    },
    {
      "abstract": "The Nash equilibrium concept has previously been shown to be an important tool to understand human sensorimotor interactions, where different actors vie for minimizing their respective effort while engaging in a multi-agent motor task. However, it is not clear how such equilibria are reached. Here, we compare different reinforcement learning models to human behavior engaged in sensorimotor interactions with haptic feedback based on three classic games, including the prisoner\u2019s dilemma, and the symmetric and asymmetric matching pennies games. We find that a discrete analysis that reduces the continuous sensorimotor interaction to binary choices as in classical matrix games does not allow to distinguish between the different learning algorithms, but that a more detailed continuous analysis with continuous formulations of the learning algorithms and the game-theoretic solutions affords different predictions. In particular, we find that Q-learning with intrinsic costs that disfavor deviations from average behavior explains the observed data best, even though all learning algorithms equally converge to admissible Nash equilibrium solutions. We therefore conclude that it is important to study different learning algorithms for understanding sensorimotor interactions, as such behavior cannot be inferred from a game-theoretic analysis alone, that simply focuses on the Nash equilibrium concept, as different learning algorithms impose preferences on the set of possible equilibrium solutions due to the inherent learning dynamics.",
      "authors": [
        "Lindig-Le\u00f3n, Cecilia",
        "Schmid, Gerrit",
        "Braun, Daniel A."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-021-99428-0",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Nash equilibria in human sensorimotor interactions explained by Q-learning with intrinsic costs",
      "urls": [
        "https://www.nature.com/articles/s41598-021-99428-0.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117693203&origin=inward"
      ]
    },
    {
      "abstract": "Background: Current treatments for generalized anxiety disorder (GAD) often yield suboptimal outcomes, partly because of insufficient targeting of underlying psychological mechanisms (eg, avoidance reinforcement learning). Mindfulness training (MT) has shown efficacy for anxiety; yet, widespread adoption has been limited, partly because of the difficulty in scaling in-person\u2013based delivery. Digital therapeutics are emerging as potentially viable treatments; however, very few have been empirically validated.\nObjective: The aim of this study is to test the efficacy and mechanism of an app-delivered MT that was designed to target a potential mechanism of anxiety (reinforcement learning), based on which previous studies have shown concern regarding feedback and the perpetuation of anxiety through negative reinforcement.\nMethods: Individuals with GAD were recruited using social media advertisements and randomized during an in-person visit to receive treatment as usual (n=33) or treatment as usual+app\u2212delivered MT (Unwinding Anxiety; n=32). The latter was composed of 30 modules to be completed over a 2-month period. Associated changes in outcomes were assessed using self-report questionnaires 1 and 2 months after treatment initiation.\nResults: We randomized 65 participants in this study, and a modified intent-to-treat approach was used for analysis. The median number of modules completed by the MT group was 25.5 (IQR 17) out of 30; 46% (13/28) of the participants completed the program. In addition, the MT group demonstrated a significant reduction in anxiety (GAD-7) compared with the control group at 2 months (67% vs 14%; median change in GAD-7: \u20138.5 [IQR 6.5] vs \u20131.0 [IQR 5.0]; P&lt;.001; 95% CI 6-10). Increases in mindfulness at 1 month (nonreactivity subscale) mediated decreases in worry at 2 months (Penn State Worry Questionnaire; P=.02) and decreases in worry at 1 month mediated reductions in anxiety at 2 months (P=.03).\nConclusions: To our knowledge, this is the first report on the efficacy and mechanism of an app-delivered MT for GAD. These findings demonstrate the clinical efficacy of MT as a digital therapeutic for individuals with anxiety (number needed to treat=1.6). These results also link recent advances in our mechanistic understanding of anxiety with treatment development, showing that app-delivered MT targets key reinforcement learning pathways, resulting in tangible, clinically meaningful reductions in worry and anxiety. Evidence-based, mechanistically targeted digital therapeutics have the potential to improve health at a population level at a low cost.\nTrial Registration: ClinicalTrials.gov NCT03683472; https://clinicaltrials.gov/ct2/show/NCT03683472\n",
      "authors": [
        "Roy A."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.2196/26987",
      "keywords": [
        "generalized anxiety disorder",
        "mHealth",
        "mobile phone",
        "digital therapeutics",
        "anxiety",
        "worry",
        "mindfulness"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMIR Publications Inc., Toronto, Canada",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Medical Internet Research"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Clinical Efficacy and Psychological Mechanisms of an App-Based Digital Therapeutic for Generalized Anxiety Disorder: Randomized Controlled Trial",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120900524&origin=inward"
      ]
    },
    {
      "abstract": "Robotic assistance via motorized robotic arm manipulators can be of valuable assistance to individuals with upper-limb motor disabilities. Brain-computer interfaces (BCI) offer an intuitive means to control such assistive robotic manipulators. However, BCI performance may vary due to the non-stationary nature of the electroencephalogram (EEG) signals. It, hence, cannot be used safely for controlling tasks where errors may be detrimental to the user. Avoiding obstacles is one such task. As there exist many techniques to avoid obstacles in robotics, we propose to give the control to the robot to avoid obstacles and to leave to the user the choice of the robot behavior to do so a matter of personal preference as some users may be more daring while others more careful. We enable the users to train the robot controller to adapt its way to approach obstacles relying on BCI that detects error-related potentials (ErrP), indicative of the user\u2019s error expectation of the robot\u2019s current strategy to meet their preferences. Gaussian process-based inverse reinforcement learning, in combination with the ErrP-BCI, infers the user\u2019s preference and updates the obstacle avoidance controller so as to generate personalized robot trajectories. We validate the approach in experiments with thirteen able-bodied subjects using a robotic arm that picks up, places and avoids real-life objects. Results show that the algorithm can learn user\u2019s preference and adapt the robot behavior rapidly using less than five demonstrations not necessarily optimal. Teaching an assistive robotic manipulator to move objects in a cluttered table requires demonstrations from expert operators, but what if the experts are individuals with motor disabilities? Batzianoulis et al. propose a learning approach which combines robot autonomy and a brain-computer interfacing that decodes whether the generated trajectories meet the user\u2019s criteria, and show how their system enables the robot to learn individual user\u2019s preferred behaviors using less than five demonstrations that are not necessarily optimal.",
      "authors": [
        "Batzianoulis, Iason",
        "Iwane, Fumiaki",
        "Wei, Shupeng",
        "Correia, Carolina Gaspar Pinto Ramos",
        "Chavarriaga, Ricardo",
        "Mill\u00e1n, Jos\u00e9 del R.",
        "Billard, Aude"
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s42003-021-02891-8",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2399-3642",
        "publisher": "Springer Nature",
        "sjr": 2.251,
        "snip": 1.359,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Agricultural and Biological Sciences (all)",
          "Biochemistry, Genetics and Molecular Biology (all)"
        ],
        "title": "Communications Biology"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Customizing skills for assistive robotic manipulators, an inverse reinforcement learning approach with error-related potentials",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121430745&origin=inward",
        "https://www.nature.com/articles/s42003-021-02891-8.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning in humans and other animals is driven by reward prediction errors: deviations between the amount of reward or punishment initially expected and that which is obtained. Temporal difference methods of reinforcement learning generate this reward prediction error at the earliest time at which a revision in reward or punishment likelihood is signalled, for example by a conditioned stimulus. Midbrain dopamine neurons, believed to compute reward prediction errors, generate this signal in response to both conditioned and unconditioned stimuli, as predicted by temporal difference learning. Electroencephalographic recordings of human participants have suggested that a component named the feedback-related negativity (FRN) is generated when this signal is carried to the cortex. If this is so, the FRN should be expected to respond equivalently to conditioned and unconditioned stimuli. However, very few studies have attempted to measure the FRN\u2019s response to unconditioned stimuli. The present study attempted to elicit the FRN in response to a primary aversive stimulus (electric shock) using a design that varied reward prediction error while holding physical intensity constant. The FRN was strongly elicited, but earlier and more transiently than typically seen, suggesting that it may incorporate other processes than the midbrain dopamine system.",
      "authors": [
        "Stewardson, Harry J.",
        "Sambrook, Thomas D."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-021-99408-4",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Reward prediction error in the ERP following unconditioned aversive stimuli",
      "urls": [
        "https://www.nature.com/articles/s41598-021-99408-4.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116562630&origin=inward"
      ]
    },
    {
      "abstract": "Learning signals during reinforcement learning and cognitive control rely on valenced reward prediction errors (RPEs) and non-valenced salience prediction errors (PEs) driven by surprise magnitude. A core debate in reward learning focuses on whether valenced and non-valenced PEs can be isolated in the human electroencephalogram (EEG). We combine behavioral modeling and single-trial EEG regression to disentangle sequential PEs in an interval timing task dissociating outcome valence, magnitude, and probability. Multiple regression across temporal, spatial, and frequency dimensions characterized a spatio-tempo-spectral cascade from early valenced RPE value to non-valenced RPE magnitude, followed by outcome probability indexed by a late frontal positivity. Separating negative and positive outcomes revealed the valenced RPE value effect is an artifact of overlap between two non-valenced RPE magnitude responses: frontal theta feedback-related negativity on losses and posterior delta reward positivity on wins. These results reconcile longstanding debates on the sequence of components representing reward and salience PEs in the human EEG. Hoy et al. combine behavioral modeling and single-trial EEG regression to disentangle sequential prediction errors in an interval timing task, which dissociated outcome valence, magnitude, and probability in human participants. Their study reconciles debates on the sequence of components representing reward and salience prediction errors in the human EEG.",
      "authors": [
        "Hoy, Colin W.",
        "Steiner, Sheila C.",
        "Knight, Robert T."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s42003-021-02426-1",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2399-3642",
        "publisher": "Springer Nature",
        "sjr": 2.251,
        "snip": 1.359,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Agricultural and Biological Sciences (all)",
          "Biochemistry, Genetics and Molecular Biology (all)"
        ],
        "title": "Communications Biology"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Single-trial modeling separates multiple overlapping prediction errors during reward processing in human EEG",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111111696&origin=inward",
        "https://www.nature.com/articles/s42003-021-02426-1.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Cheng W."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.parco.2021.102855",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678191",
        "publisher": "Elsevier B.V.",
        "sjr": 0.392,
        "snip": 0.826,
        "subject_areas": [
          "Artificial Intelligence",
          "Software",
          "Hardware and Architecture",
          "Computer Networks and Communications",
          "Theoretical Computer Science",
          "Computer Graphics and Computer-Aided Design"
        ],
        "title": "Parallel Computing"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "AIOC<sup>2</sup>: A deep Q-learning approach to autonomic I/O congestion control in Lustre",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116529448&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Byrne K.A."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jbtep.2021.101676",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00057916",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.809,
        "snip": 0.872,
        "subject_areas": [
          "Arts and Humanities (miscellaneous)",
          "Experimental and Cognitive Psychology",
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Journal of Behavior Therapy and Experimental Psychiatry"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Examining the effect of depressive symptoms on habit formation and habit-breaking",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110714503&origin=inward"
      ]
    },
    {
      "abstract": "There has been a fundamental failure to translate preclinically supported research into clinically efficacious treatments for psychiatric disorders. One of the greatest impediments toward improving this species gap has been the difficulty of identifying translatable neurophysiological signals that are related to specific behavioral constructs. Here, we present evidence from three paradigms that were completed by humans and mice using analogous procedures, with each task eliciting candidate a priori defined electrophysiological signals underlying effortful motivation, reinforcement learning, and cognitive control. The effortful motivation was assessed using a progressive ratio breakpoint task, yielding a similar decrease in alpha-band activity over time in both species. Reinforcement learning was assessed via feedback in a probabilistic learning task with delta power significantly modulated by reward surprise in both species. Additionally, cognitive control was assessed in the five-choice continuous performance task, yielding response-locked theta power seen across species, and modulated by difficulty in humans. Together, these successes, and also the teachings from these failures, provide a roadmap towards the use of electrophysiology as a method for translating findings from the preclinical assays to the clinical settings.",
      "authors": [
        "Cavanagh, James F.",
        "Gregg, David",
        "Light, Gregory A.",
        "Olguin, Sarah L.",
        "Sharp, Richard F.",
        "Bismark, Andrew W.",
        "Bhakta, Savita G.",
        "Swerdlow, Neal R.",
        "Brigman, Jonathan L.",
        "Young, Jared W."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41398-021-01562-w",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 10.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2158-3188",
        "publisher": "Nature Publishing Group",
        "sjr": 2.148,
        "snip": 1.549,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Translational Psychiatry"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Electrophysiological biomarkers of behavioral dimensions from cross-species paradigms",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115392227&origin=inward",
        "https://www.nature.com/articles/s41398-021-01562-w.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang X."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TPAMI.2020.2972281",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "4205-4216",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01628828",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Vision-Language Navigation Policy Learning and Adaptation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118661432&origin=inward"
      ]
    },
    {
      "abstract": "Human operators have the trend of increasing physical and mental workloads when performing teleoperation tasks in uncertain and dynamic environments. In addition, their performances are influenced by subjective factors, potentially leading to operational errors or task failure. Although agent-based methods offer a promising solution to the above problems, the human experience and intelligence are necessary for teleoperation scenarios. In this paper, a truncated quantile critics reinforcement learning-based integrated framework is proposed for human\u2013agent teleoperation that encompasses training, assessment and agent-based arbitration. The proposed framework allows for an expert training agent, a bilateral training and cooperation process to realize the co-optimization of agent and human. It can provide efficient and quantifiable training feedback. Experiments have been conducted to train subjects with the developed algorithm. The performances of human\u2013human and human\u2013agent cooperation modes are also compared. The results have shown that subjects can complete the tasks of reaching and picking and placing with the assistance of an agent in a shorter operational time, with a higher success rate and less workload than human\u2013human cooperation.",
      "authors": [
        "Huang, Zebin",
        "Wang, Ziwei",
        "Bai, Weibang",
        "Huang, Yanpei",
        "Sun, Lichao",
        "Xiao, Bo",
        "Yeatman, Eric M."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/s21248341",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14248220",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.764,
        "snip": 1.317,
        "subject_areas": [
          "Atomic and Molecular Physics, and Optics",
          "Information Systems",
          "Instrumentation",
          "Biochemistry",
          "Analytical Chemistry",
          "Electrical and Electronic Engineering"
        ],
        "title": "Sensors (Switzerland)"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "A Novel Training and Collaboration Integrated Framework for Human\u2013Agent Teleoperation",
      "urls": [
        "https://www.mdpi.com/1424-8220/21/24/8341/pdf?version=1639472806",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121453820&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Reinen J.M."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.euroneuro.2021.08.002",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "89-100",
      "publication": {
        "category": "Journal",
        "cite_score": 8.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0924977X",
        "publisher": "Elsevier B.V.",
        "sjr": 1.442,
        "snip": 1.185,
        "subject_areas": [
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neurology (clinical)",
          "Pharmacology",
          "Pharmacology (medical)"
        ],
        "title": "European Neuropsychopharmacology"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Differential reinforcement learning responses to positive and negative information in unmedicated individuals with depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114680089&origin=inward"
      ]
    },
    {
      "abstract": "Real-world results show a significant improvement of inspection times with no image quality degradation.",
      "authors": [
        "Zvezdan Lon\u010darevi\u0107",
        "Andrej Gams",
        "Simon Reber\u0161ek",
        "Bojan Nemec",
        "Jure \u0160krabar",
        "Jure Skvar\u010d",
        "Ale\u0161 Ude"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.rcim.2021.102200",
      "keywords": [
        "Robot learning",
        "Industrial robotics",
        "Robot-supported quality inspection"
      ],
      "number_of_pages": 14,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0736-5845",
        "publisher": "Pergamon Press, Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Robotics and Computer-Integrated Manufacturing"
      },
      "publication_date": "2021-12-01",
      "selected": null,
      "title": "Specifying and optimizing robotic motion for visual quality inspection",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.rcim.2021.102200",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107979874&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Noel Jeygar Robert V."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/dac.4982",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10745351",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Communication Systems"
      },
      "publication_date": "2021-11-25",
      "selected": null,
      "title": "Effective cooperative spectrum sensing using deep recurrent reinforced learning-based Q-routing in multihop cognitive radio networks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115016009&origin=inward"
      ]
    },
    {
      "abstract": "Active Learning is a machine learning scenario in which methods are trained by iteratively submitting a query to a human expert and then taking into account his feedback for the following computations. The application of such paradigm to the anomaly detection task...",
      "authors": [
        "Angiulli, Fabrizio",
        "Fassetti, Fabio",
        "Ferragina, Luca",
        "Papaleo, Prospero"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-3-030-91608-4_40",
      "keywords": [
        "Anomaly detection",
        "Meta-feature extraction",
        "Active Learning",
        "Reinforcement learning"
      ],
      "number_of_pages": 9,
      "pages": "406-414",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-3-030-91607-7",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Intelligent Data Engineering and Automated Learning \u2013 IDEAL 2021: 22nd International Conference, IDEAL 2021, Manchester, UK, November 25\u201327, 2021, Proceedings"
      },
      "publication_date": "2021-11-25",
      "selected": null,
      "title": "Meta-feature Extraction Strategies for Active Anomaly Detection",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-030-91608-4_40.pdf",
        "https://dl.acm.org/doi/10.1007/978-3-030-91608-4_40",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126489797&origin=inward"
      ]
    },
    {
      "abstract": "Scheduling when, where, and under what conditions to re-charge an electric vehicle poses unique challenges absent in internal combustion vehicles. Charging scheduling of an electric vehicle for time- and cost-efficiency depends on many variables in a dynamic...",
      "authors": [
        "Lee, Xian-Long",
        "Yang, Hong-Tzer",
        "Tang, Wenjun",
        "Toosi, Adel N.",
        "Lam, Edward"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-3-030-91431-8_17",
      "keywords": [
        "Adaptive charging scheduling",
        "Electric vehicle",
        "Multi-agent systems",
        "Reinforcement learning"
      ],
      "number_of_pages": 14,
      "pages": "273-286",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-3-030-91430-1",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2021-11-22",
      "selected": null,
      "title": "An Adaptive Charging Scheduling for Electric Vehicles Using Multiagent Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120521943&origin=inward",
        "https://dl.acm.org/doi/10.1007/978-3-030-91431-8_17",
        "https://link.springer.com/content/pdf/10.1007/978-3-030-91431-8_17.pdf"
      ]
    },
    {
      "abstract": "The quadruped robot applying the expert skill learning system can learn and generate adaptive skills from a group of representative expert skills, screen and master these combined skills through deep reinforcement learning, so as to select different skills in different environments to move in stranger environment. This approach leverages the advantages of trained expert skills and the fast online synthesis of adaptive policies to generate responsive motor skills during the changing tasks. The cable tunnel unmanned inspection quadruped robot is equipped with a 5-DOF mechanical arm with a sensor module on the top to realize the cable condition detection. The combination of the intelligent sensors can make the robot obtain the ability of environment perception similar to human beings, so that the quadruped robot can complete the instructions safely and accurately in the cable tunnel environment, collect and analyse the environment and cable condition parameters, find problems and give feedback in time.",
      "authors": [
        "C. Wu",
        "Y. Zhou",
        "Y. Zhang",
        "H. Li",
        "X. Wang",
        "Z. Li",
        "Y. Xu",
        "P. Ni"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1049/icp.2022.0457",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "1995-1999",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-83953-605-2",
        "issn": null,
        "publisher": "IET",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "22nd International Symposium on High Voltage Engineering (ISH 2021)"
      },
      "publication_date": "2021-11-21",
      "selected": null,
      "title": "Analysis on the locomotion of cable tunnel inspection quadruped robot based on deep reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85174656246&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9800408"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Quintana M."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3486611.3492386",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "242-243",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450391146",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "BuildSys 2021 - Proceedings of the 2021 ACM International Conference on Systems for Energy-Efficient Built Environments"
      },
      "publication_date": "2021-11-17",
      "selected": null,
      "title": "Cohort-based personal comfort models for HVAC occupant-centric control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120986014&origin=inward"
      ]
    },
    {
      "abstract": "A promising approach to improve the robustness and exploration in Reinforcement Learning is collecting human feedback and that way incorporating prior knowledge of the target environment. It is, however, often too expensive to obtain enough feedback of good quality. To mitigate the issue, we aim to rely on a group of multiple experts (and non-experts) with different skill levels to generate enough feedback. Such feedback can therefore be inconsistent and infrequent. In this paper, we build upon prior work -- Advise, a Bayesian approach attempting to maximise the information gained from human feedback -- extending the algorithm to accept feedback from this larger group of humans, the trainers, while also estimating each trainer's reliability. We show how aggregating feedback from multiple trainers improves the total feedback's accuracy and make the collection process easier in two ways. Firstly, this approach addresses the case of some of the trainers being adversarial. Secondly, having access to the information about each trainer reliability provides a second layer of robustness and offers valuable information for people managing the whole system to improve the overall trust in the system. It offers an actionable tool for improving the feedback collection process or modifying the reward function design if needed. We empirically show that our approach can accurately learn the reliability of each trainer correctly and use it to maximise the information gained from the multiple trainers' feedback, even if some of the sources are adversarial.",
      "authors": [
        "Yamagata, Taku",
        "McConville, Ryan",
        "Santos-Rodriguez, Raul"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted NeurIPS 2021 Workshop on Safe and Robust Control of\n  Uncertain Systems. arXiv admin note: text overlap with arXiv:1908.06134",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-11-16",
      "selected": null,
      "title": "Reinforcement Learning with Feedback from Multiple Humans with Diverse Skills",
      "urls": [
        "http://arxiv.org/pdf/2111.08596.pdf",
        "http://arxiv.org/abs/2111.08596v1",
        "http://arxiv.org/pdf/2111.08596v1"
      ]
    },
    {
      "abstract": "A shortcoming of batch reinforcement learning is its requirement for rewards in data, thus not applicable to tasks without reward functions. Existing settings for lack of reward, such as behavioral cloning, rely on optimal demonstrations collected from humans. Unfortunately, extensive expertise is required for ensuring optimality, which hinder the acquisition of large-scale data for complex tasks. This paper addresses the lack of reward in a batch reinforcement learning setting by learning a reward function from preferences. Generating preferences only requires a basic understanding of a task. Being a mental process, generating preferences is faster than performing demonstrations. So preferences can be collected at scale from non-expert humans using crowdsourcing. This paper tackles a critical challenge that emerged when collecting data from non-expert humans: the noise in preferences. A novel probabilistic model is proposed for modelling the reliability of labels, which utilizes labels collaboratively. Moreover, the proposed model smooths the estimation with a learned reward function. Evaluation on Atari datasets demonstrates the effectiveness of the proposed model, followed by an ablation study to analyze the relative importance of the proposed ideas.",
      "authors": [
        "Zhang, Guoxi",
        "Kashima, Hisashi"
      ],
      "categories": null,
      "citations": null,
      "comments": "16 pages. Accepted by ECML-PKDD 2022",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-11-08",
      "selected": null,
      "title": "Batch Reinforcement Learning from Crowds",
      "urls": [
        "http://arxiv.org/pdf/2111.04279v2",
        "http://arxiv.org/pdf/2111.04279.pdf",
        "http://arxiv.org/abs/2111.04279v2"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",
      "authors": [
        "Lee, Kimin",
        "Smith, Laura",
        "Dragan, Anca",
        "Abbeel, Pieter"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS Datasets and Benchmarks Track 2021. Code is available at\n  https://github.com/rll-research/B-Pref",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-11-04",
      "selected": null,
      "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2111.03026v1",
        "http://arxiv.org/pdf/2111.03026.pdf",
        "http://arxiv.org/abs/2111.03026v1"
      ]
    },
    {
      "abstract": "Navigation under a grid world has been a classical and historical theme in reinforcement learning, which is viewed as a Markov decision process (MDP) in general. The literature to date has proven that the navigation task can be addressed that an agent achieves the target with a high success rate and avoids collision with obstacles. But essentially, they met with success in a specific environment while the agent cannot work in a new surrounding, meaning that it does not boast broad applicability. In state-of-the-art approaches, poor feedback and lack of adaptability to increasing state spaces remain a problem. In this paper, we propose a modified approach to solve a series of navigation problems under moderate and huge-sized surroundings. The problem is addressed with a deep reinforcement learning algorithm with a guided classifier. We address these issues by providing a reliable guided reward with a brain-guided classifier based on human brain signals (electroencephalography, EEG) and a convolutional neural network. This paper explores several experiments to show that our model with deep RL and the brain-guided classifier can solve these complex and significant practical challenges. Our method improves efficiency by about twice as much as traditional approaches such as DQN and Q-learning.",
      "authors": [
        "Chaohao Lin",
        "S M Shafiul Hasan",
        "Ou Bai"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/RCAE53607.2021.9638872",
      "keywords": [
        "Markov decision process",
        "electroencephalography (EEG)",
        "reinforcement learning",
        "deep Q learning"
      ],
      "number_of_pages": 6,
      "pages": "278-283",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-2731-9",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 4th International Conference on Robotics, Control and Automation Engineering, RCAE 2021"
      },
      "publication_date": "2021-11-04",
      "selected": null,
      "title": "Robotic Navigation with Human Brain Signals and Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123758739&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9638872"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lim T.V."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/ijnp/pyab041",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "867-878",
      "publication": {
        "category": "Journal",
        "cite_score": 7.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14611457",
        "publisher": "Oxford University Press",
        "sjr": 1.406,
        "snip": 1.251,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Pharmacology (medical)",
          "Pharmacology"
        ],
        "title": "International Journal of Neuropsychopharmacology"
      },
      "publication_date": "2021-11-01",
      "selected": null,
      "title": "Impaired learning from negative feedback in stimulant use disorder: Dopaminergic Modulation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118946698&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ebrahimi S."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.artmed.2021.102193",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 14.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09333657",
        "publisher": "Elsevier B.V.",
        "sjr": 1.443,
        "snip": 2.22,
        "subject_areas": [
          "Artificial Intelligence",
          "Medicine (miscellaneous)"
        ],
        "title": "Artificial Intelligence in Medicine"
      },
      "publication_date": "2021-11-01",
      "selected": null,
      "title": "A reinforcement learning approach for finding optimal policy of adaptive radiation therapy considering uncertain tumor biological response",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117249998&origin=inward"
      ]
    },
    {
      "abstract": "The anthropomorphic intelligence of autonomous driving has been a research hotspot in the world. However, current studies have not been able to reveal the mechanism of drivers' natural driving behaviors. Therefore, this thesis starts from the perspective of cognitive decision-making in the human brain, which is inspired by the regulation of dopamine feedback in the basal ganglia, and a reinforcement learning model is established to solve the brain-like intelligent decision-making problems in the process of interacting with the environment. In this thesis, first, a detailed bionic mechanism architecture based on basal ganglia was proposed by the consideration and analysis of its feedback regulation mechanism; second, the above mechanism was transformed into a reinforcement Q-learning model, so as to implement the learning and adaptation abilities of an intelligent vehicle for brain-like intelligent decision-making during car-following; finally, the feasibility and effectiveness of the proposed method were verified by the simulations and real vehicle tests.",
      "authors": [
        "Sun, Tianjun",
        "Gao, Zhenhai",
        "Chang, Zhiyong",
        "Zhao, Kehan"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s42235-021-00113-9",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "1439-1451",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "16726529",
        "publisher": "Springer Singapore",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Bionic Engineering"
      },
      "publication_date": "2021-11-01",
      "selected": null,
      "title": "Brain-like Intelligent Decision-making Based on Basal Ganglia and Its Application in Automatic Car-following",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119256657&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s42235-021-00113-9.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Loriette C."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neurol.2021.08.004",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1133-1144",
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00353787",
        "publisher": "Elsevier Masson s.r.l.",
        "sjr": 0.753,
        "snip": 1.093,
        "subject_areas": [
          "Neurology (clinical)",
          "Neurology"
        ],
        "title": "Revue Neurologique"
      },
      "publication_date": "2021-11-01",
      "selected": null,
      "title": "Neurofeedback for cognitive enhancement and intervention and brain plasticity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117369210&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gabay Y."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nlm.2021.107518",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10747427",
        "publisher": "Academic Press Inc.",
        "sjr": 0.986,
        "snip": 0.765,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neurobiology of Learning and Memory"
      },
      "publication_date": "2021-11-01",
      "selected": null,
      "title": "Delaying feedback compensates for impaired reinforcement learning in developmental dyslexia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116073834&origin=inward"
      ]
    },
    {
      "abstract": "Establishing robust dialogue policy with low computation cost is challenging, especially for multi-domain task-oriented dialogue management due to the high complexity in state and action spaces. The previous works mostly using the deterministic policy optimization only attain moderate performance. Meanwhile, state-of-the-art result that uses end-to-end approach is computationally demanding since it utilizes a large-scaled language model based on the generative pre-trained transformer-2 (GPT-2). In this study, a new learning procedure consisting of three learning stages is presented to improve multi-domain dialogue management with corrective guidance. Firstly, the behavior cloning with an auxiliary task is developed to build a robust pre-trained model by mitigating the causal confusion problem in imitation learning. Next, the pre-trained model is rectified by using reinforcement learning via the proximal policy optimization. Lastly, human-in-the-loop learning strategy is fulfilled to enhance the agent performance by directly providing corrective feedback from rule-based agent so that the agent is prevented to trap in confounded states. The experiments on end-to-end evaluation show that the proposed learning method achieves state-of-the-art result by performing nearly identical to the rule-based agent. This method outperforms the second place of 9th dialog system technology challenge (DSTC9) track 2 that uses GPT-2 as the core model in dialogue management.",
      "authors": [
        "Mahdin Rohmatillah",
        "Jen-Tzung Chien"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3459637.3482333",
      "keywords": [
        "imitation learning",
        "policy optimization",
        "behavior cloning",
        "reinforcement learning",
        "dialogue management",
        "human-in-the-loop"
      ],
      "number_of_pages": 10,
      "pages": "1548-1557",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450384469",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management"
      },
      "publication_date": "2021-10-30",
      "selected": null,
      "title": "Corrective Guidance and Learning for Dialogue Management",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119177751&origin=inward",
        "https://dl.acm.org/doi/10.1145/3459637.3482333"
      ]
    },
    {
      "abstract": "Road user behavior prediction is one of the most critical components in trajectory planning for autonomous driving, especially in urban scenarios involving traffic signals. In this paper, a hierarchical framework is proposed to predict vehicle behaviors at a signalized intersection, using the traffic signal information of the intersection. The framework is composed of two phases: a discrete intention prediction phase and a continuous trajectory prediction phase. In the discrete intention prediction phase, a Bayesian network is adopted to predict the vehicle's high-level intention, after that, maximum entropy inverse reinforcement learning is utilized to learn the human driving model offline; during the online trajectory prediction phase, a driver characteristic is designed and updated to capture the different driving preferences between human drivers. We applied the proposed framework to one of the most challenging scenarios in autonomous driving: the yellow light running scenario. Numerical experiment results are presented in the later part of the paper which show the viability of the method. The accuracy of the Bayesian network for discrete intention prediction is 91.1%, and the prediction results are getting more and more accurate as the yellow time elapses. The average Euclidean distance error in continuous trajectory prediction is only 0.85 m in the yellow light running scenario.",
      "authors": [
        "Yang, Zhen",
        "Zhang, Rusheng",
        "Liu, Henry X."
      ],
      "categories": null,
      "citations": null,
      "comments": "This work has been accepted to the IEEE 2021 International Conference\n  on Intelligent Transportation Systems",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-29",
      "selected": null,
      "title": "A hierarchical behavior prediction framework at signalized intersections",
      "urls": [
        "http://arxiv.org/abs/2110.15465v1",
        "http://arxiv.org/pdf/2110.15465v1",
        "http://arxiv.org/pdf/2110.15465.pdf"
      ]
    },
    {
      "abstract": "In this paper, an implementation of a human in the loop (HITL) technique for robot navigation in an indoor environment is described. The HITL technique is integrated into the reinforcement learning algorithms for mobile robot navigation. Reinforcement algorithms, specifically Q-learning and SARSA, are used combined with HITL since these algorithms are good in exploration and navigation. Turtlebot3 has been used as the robot for validating the algorithms by implementing the system using Robot Operating System and Gazebo. The robot-assisted with human feedback was found to be better in navigation task execution when compared to standard algorithms without using human in the loop. This is a work in progress and the next step of this research is exploring other reinforced learning methods and implementing them on a physical robot.",
      "authors": [
        "Manasa Mainampati",
        "Balasubramaniyan Chandrasekaran"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IEMCON53756.2021.9623127",
      "keywords": [
        "SARSA",
        "HITL",
        "Path planning",
        "ROS",
        "Reinforcement learning",
        "TurtleBot",
        "Q- Learning"
      ],
      "number_of_pages": 5,
      "pages": "0448-0452",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-0067-1",
        "issn": "2644-3155",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference, IEMCON 2021"
      },
      "publication_date": "2021-10-27",
      "selected": null,
      "title": "Implementation of Human in The Loop on the TurtleBot using Reinforced Learning methods and Robot Operating System (ROS)",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123581172&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9623127"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Pan H."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3459637.3481932",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "4055-4064",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450384469",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management"
      },
      "publication_date": "2021-10-26",
      "selected": null,
      "title": "Learning to Expand: Reinforced Response Expansion for Information-seeking Conversations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119195168&origin=inward"
      ]
    },
    {
      "abstract": "In this paper, the possibility of extending the Soft Actor Critic (SAC) with human control feedback is proposed and implemented for mobile robot navigation using Turtlebot3 Burger. The Human In The Loop (HITL) technique is integrated into the reinforcement learning algorithm for obstacle avoidance-based navigation. The algorithm is usually presented in the continuous action domain and for this work, the SAC with discrete action settings is proposed to suit the minimal discrete space. The human feedback assisting the robot with the discrete action settings has proved to be better in path planning and navigation when compared to the standard algorithms with the episode number.",
      "authors": [
        "Balasubramaniyan Chandrasekaran",
        "Manasa Mainampati"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICMRA53481.2021.9675564",
      "keywords": [
        "HITL",
        "Path planning",
        "SAC",
        "ROS",
        "Reinforcement learning",
        "TurtleBot",
        "SAC with Discrete Actions"
      ],
      "number_of_pages": 7,
      "pages": "19-25",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-2728-9",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 4th International Conference on Mechatronics, Robotics and Automation, ICMRA 2021"
      },
      "publication_date": "2021-10-22",
      "selected": null,
      "title": "A Human in the Loop Based Robotic System by Using Soft Actor Critic with Discrete Actions",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9675564",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125319471&origin=inward"
      ]
    },
    {
      "abstract": "Active inference is a unifying theory for perception and action resting upon the idea that the brain maintains an internal model of the world by minimizing free energy. From a behavioral perspective, active inference agents can be seen as self-evidencing beings that act to fulfill their optimistic predictions, namely preferred outcomes or goals. In contrast, reinforcement learning requires human-designed rewards to accomplish any desired outcome. Although active inference could provide a more natural self-supervised objective for control, its applicability has been limited because of the shortcomings in scaling the approach to complex environments. In this work, we propose a contrastive objective for active inference that strongly reduces the computational burden in learning the agent's generative model and planning future actions. Our method performs notably better than likelihood-based active inference in image-based tasks, while also being computationally cheaper and easier to train. We compare to reinforcement learning agents that have access to human-designed reward functions, showing that our approach closely matches their performance. Finally, we also show that contrastive methods perform significantly better in the case of distractors in the environment and that our method is able to generalize goals to variations in the background. Website and code: https://contrastive-aif.github.io/",
      "authors": [
        "Mazzaglia, Pietro",
        "Verbelen, Tim",
        "Dhoedt, Bart"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted as a conference paper at 35th Conference on Neural\n  Information Processing Systems (NeurIPS 2021)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-19",
      "selected": null,
      "title": "Contrastive Active Inference",
      "urls": [
        "http://arxiv.org/pdf/2110.10083.pdf",
        "http://arxiv.org/pdf/2110.10083v4",
        "http://arxiv.org/abs/2110.10083v4"
      ]
    },
    {
      "abstract": "Intelligent agents must be able to think fast and slow to perform elaborate manipulation tasks. Reinforcement Learning (RL) has led to many promising results on a range of challenging decision-making tasks. However, in real-world robotics, these methods still struggle, as they require large amounts of expensive interactions and have slow feedback loops. On the other hand, fast human-like adaptive control methods can optimize complex robotic interactions, yet fail to integrate multimodal feedback needed for unstructured tasks. In this work, we propose to factor the learning problem in a hierarchical learning and adaption architecture to get the best of both worlds. The framework consists of two components, a slow reinforcement learning policy optimizing the task strategy given multimodal observations, and a fast, real-time adaptive control policy continuously optimizing the motion, stability, and effort of the manipulator. We combine these components through a bio-inspired action space that we call AFORCE. We demonstrate the new action space on a contact-rich manipulation task on real hardware and evaluate its performance on three simulated manipulation tasks. Our experiments show that AFORCE drastically improves sample efficiency while reducing energy consumption and improving safety.",
      "authors": [
        "Ulmer, Maximilian",
        "Aljalbout, Elie",
        "Schwarz, Sascha",
        "Haddadin, Sami"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-19",
      "selected": null,
      "title": "Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space",
      "urls": [
        "http://arxiv.org/pdf/2110.09904.pdf",
        "http://arxiv.org/abs/2110.09904v2",
        "http://arxiv.org/pdf/2110.09904v2"
      ]
    },
    {
      "abstract": "Daily decision-making circumstances often include risk and uncertainty, and the decision-maker&#x2019;s risk preference constantly influences the course of action taken. Modeling how individuals make decisions in real-world situations remains a key challenge, and the use of machine learning techniques to model and augment individuals&#x2019; decision-making processes has garnered little attention. In this paper, we propose a new framework to evaluate the feasibility of inferring risk attitudes from human behavior modeled using the Reinforcement Learning (RL) model. The RL-based model framework is then generalized by including a situational or contextual variable that is indicative of cognitive biases observed in human decision-making, using an equipment maintenance scenario as a case study. Interestingly, when compared to human participants with working experience, the mean performance of a trained RL agent bears some semblance to the surveyed human participants. Riding on such strong empirical results, we also discover that the risk-seeking group of human participants are 9 times more likely to engage in risk-seeking associated actions. Besides, our exploratory simulations show that near optimal performance can be consistently achieved with several enhancements to state-of-the-art advantage actor-critic RL algorithms under the situational context of BUSY and IDLE. Overall, our proposed RL-based model framework can categorically infer the user-defined human risk attitude, and the trained RL agent demonstrates its rational decision-making prowess by outperforming the majority of human participants.",
      "authors": [
        "Kevin Shen Hoong Ong",
        "Wang Wenbo",
        "Thomas Friedrichs",
        "Dusit Niyato"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/SMC52423.2021.9658936",
      "keywords": [
        "decision making",
        "context",
        "decision support",
        "Cognitive computing",
        "deep reinforcement learning",
        "risk"
      ],
      "number_of_pages": 7,
      "pages": "3114-3120",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3703-7",
        "issn": "1062-922X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics"
      },
      "publication_date": "2021-10-17",
      "selected": null,
      "title": "Augmented Human Intelligence for Decision Making in Maintenance Risk Taking Tasks using Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9658936",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124293102&origin=inward",
        "https://dl.acm.org/doi/10.1109/SMC52423.2021.9658936"
      ]
    },
    {
      "abstract": "Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train \"human-aware\" agents (\"behavioral cloning play\", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.",
      "authors": [
        "Strouse, DJ",
        "McKee, Kevin R.",
        "Botvinick, Matt",
        "Hughes, Edward",
        "Everett, Richard"
      ],
      "categories": null,
      "citations": 38,
      "comments": "Accepted at NeurIPS 2021 (spotlight)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 14,
      "pages": "14502-14515",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2021-10-15",
      "selected": null,
      "title": "Collaborating with Humans without Human Data",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132038465&origin=inward",
        "http://arxiv.org/pdf/2110.08176.pdf",
        "http://arxiv.org/abs/2110.08176v2",
        "http://arxiv.org/pdf/2110.08176v2"
      ]
    },
    {
      "abstract": "Providing Reinforcement Learning (RL) agents with human feedback can dramatically improve various aspects of learning. However, previous methods require human observer to give inputs explicitly (e.g., press buttons, voice interface), burdening the human in the loop of RL agent\u2019s learning process. Further, providing explicit human advise (feedback) continuously is not always possible or too restrictive, e.g., autonomous driving, disabled rehabilitation, etc. In this work, we investigate capturing human\u2019s intrinsic reactions as implicit (and natural) feedback through EEG in the form of error-related potentials (ErrP), providing a natural and direct way for humans to improve the RL agent learning. As such, the human intelligence can be integrated via implicit feedback with RL algorithms to accelerate the learning of RL agent. We develop three reasonably complex 2D discrete navigational games to experimentally evaluate the overall performance of the proposed work. And the motivation of using ErrPs as feedbacks is also verified by subjective experiments. Major contributions of our work are as follows, (i) we propose and experimentally validate the zero-shot learning of ErrPs, where the ErrPs can be learned for one game, and transferred to other unseen games, (ii) we propose a novel RL framework for integrating implicit human feedbacks via ErrPs with RL agent, improving the label efficiency and robustness to human mistakes, and (iii) compared to prior works, we scale the application of ErrPs to reasonably complex environments, and demonstrate the significance of our approach for accelerated learning through real user experiments.",
      "authors": [
        "Duo Xu",
        "Mohit Agarwal",
        "Ekansh Gupta",
        "Faramarz Fekri",
        "Raghupathy Sivakumar"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.neucom.2021.06.064",
      "keywords": [
        "Error-potentials (ErrPs)",
        "Human-agent interaction",
        "Reinforcement Learning",
        "Sample efficiency"
      ],
      "number_of_pages": 15,
      "pages": "139-153",
      "publication": {
        "category": "Journal",
        "cite_score": 10.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0925-2312",
        "publisher": "Elsevier B.V.",
        "sjr": 1.481,
        "snip": 1.853,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neurocomputing"
      },
      "publication_date": "2021-10-14",
      "selected": null,
      "title": "Accelerating Reinforcement Learning using EEG-based implicit human feedback",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.neucom.2021.06.064",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110690103&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang Z."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/IECON48115.2021.9589570",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665435543",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IECON Proceedings (Industrial Electronics Conference)"
      },
      "publication_date": "2021-10-13",
      "selected": null,
      "title": "Multiple-Pilot Collaboration for Advanced Remote Intervention using Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119520730&origin=inward"
      ]
    },
    {
      "abstract": "This paper presents an RL-based method to improve the performance of real-time 3D human pose estimation as a positioning feedback for DermDrone which is a micro sized quadrotor designed MetaOptima to capture high resolution full body images for dermatology application. The camera viewpoint is identified as the key parameter in the accuracy of monocular 3D human pose estimation. We present a deep reinforcement learning based method for determining the best viewpoint given the flight trajectory. Our goal is to present a reliable and accurate positioning feedback for DermDrone using a 3D human pose estimation algorithm. DQN and its variants (Double DQN, and Dueling DQN) were employed and their performances were investigated by conducting several simulations. The results confirm that RL-based viewpoint selection improve the performance of 3D human pose estimation.",
      "authors": [
        "Mojtaba Ahangar Arzati",
        "Siamak Arzanpour"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.23919/ICCAS52745.2021.9649799",
      "keywords": [
        "Next-Best-View",
        "Drone",
        "Viewpoint Selection",
        "Autonomous Navigation",
        "Reinforcement Learning",
        "Human Pose Estimation"
      ],
      "number_of_pages": 10,
      "pages": "544-553",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-1832-4",
        "issn": "1598-7833",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 21st International Conference on Control, Automation and Systems (ICCAS)"
      },
      "publication_date": "2021-10-12",
      "selected": null,
      "title": "Viewpoint Selection for DermDrone using Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124221336&origin=inward",
        "https://dl.acm.org/doi/10.23919/ICCAS52745.2021.9649799",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9649799"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning, where humans actively assist during an agent's learning process, has the promise to alleviate the sample complexity challenges of practical algorithms. However, the inner workings and state of the robot are typically hidden from the teacher when humans provide feedback. To create a common ground between the human and the learning robot, in this paper, we propose an Augmented Reality (AR) system that reveals the hidden state of the learning to the human users. This paper describes our system's design and implementation and concludes with a discussion on two directions for future work which we are pursuing: 1) use of our system in AI education activities at the K-12 level; and 2) development of a framework for an AR-based human-in-the-loop reinforcement learning, where the human teacher can see sensory and cognitive representations of the robot overlaid in the real world.",
      "authors": [
        "Zhang, Ziyi",
        "Akai-Nettey, Samuel Micah",
        "Addo, Adonai",
        "Rogers, Chris",
        "Sinapov, Jivko"
      ],
      "categories": null,
      "citations": null,
      "comments": "AAAI-FSS 2021 (arXiv:2109.10836)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-10",
      "selected": null,
      "title": "An Augmented Reality Platform for Introducing Reinforcement Learning to K-12 Students with Robots",
      "urls": [
        "http://arxiv.org/abs/2110.04697v1",
        "http://arxiv.org/pdf/2110.04697v1",
        "http://arxiv.org/pdf/2110.04697.pdf"
      ]
    },
    {
      "abstract": "Text-based image retrieval has seen considerable progress in recent years. However, the performance of existing methods suffers in real life since the user is likely to provide an incomplete description of an image, which often leads to results filled with false positives that fit the incomplete description. In this work, we introduce the partial-query problem and extensively analyze its influence on text-based image retrieval. Previous interactive methods tackle the problem by passively receiving users\u2019 feedback to supplement the incomplete query iteratively, which is time-consuming and requires heavy user effort. Instead, we propose a novel retrieval framework that conducts the interactive process in an Ask-and-Confirm fashion, where AI actively searches for discriminative details missing in the current query, and users only need to confirm AI\u2019s proposal. Specifically, we propose an object-based interaction to make the interactive retrieval more user-friendly and present a reinforcement-learning-based policy to search for discriminative objects. Furthermore, since fully-supervised training is often infeasible due to the difficulty of obtaining human-machine dialog data, we present a weakly-supervised training strategy that needs no human-annotated dialogs other than a text-image dataset. Experiments show that our framework significantly improves the performance of text-based image retrieval. Code is available at https://github.com/CuthbertCai/Ask-Confirm.",
      "authors": [
        "Guanyu Cai",
        "Jun Zhang",
        "Xinyang Jiang",
        "Yifei Gong",
        "Lianghua He",
        "Fufu Yu",
        "Pai Peng",
        "Xiaowei Guo",
        "Feiyue Huang",
        "Xing Sun"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICCV48922.2021.00185",
      "keywords": [
        "Vision + language",
        "Image and video retrieval"
      ],
      "number_of_pages": 10,
      "pages": "1815-1824",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-4804-5",
        "issn": "1550-5499",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the IEEE International Conference on Computer Vision"
      },
      "publication_date": "2021-10-10",
      "selected": null,
      "title": "Ask&Confirm: Active Detail Enriching for Cross-Modal Retrieval with Partial Query",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9710243",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127732440&origin=inward"
      ]
    },
    {
      "abstract": "Personalized therapy, in which a therapeutic practice is adapted to an individual patient, leads to better health outcomes. Typically, this is accomplished by relying on a therapist's training and intuition along with feedback from a patient. While there exist approaches to automatically adapt therapeutic content to a patient, they rely on hand-authored, pre-defined rules, which may not generalize to all individuals. In this paper, we propose an approach to automatically adapt therapeutic content to patients based on physiological measures. We implement our approach in the context of arachnophobia exposure therapy, and rely on experience-driven procedural content generation via reinforcement learning (EDPCGRL) to generate virtual spiders to match an individual patient. In this initial implementation, and due to the ongoing pandemic, we make use of virtual or artificial humans implemented based on prior arachnophobia psychology research. Our EDPCGRL method is able to more quickly adapt to these virtual humans with high accuracy in comparison to existing, search-based EDPCG approaches.",
      "authors": [
        "Mahmoudi-Nejad, Athar",
        "Guzdial, Matthew",
        "Boulanger, Pierre"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 3 figures, AIIDE 2021 Poster",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Artificial Intelligence",
          "Learning",
          "Computers and Society"
        ],
        "title": "Proceedings of the 17th AAAI Conference on Artificial Intelligence\n  and Interactive Digital Entertainment 2021 (AIIDE-21)"
      },
      "publication_date": "2021-10-07",
      "selected": null,
      "title": "Arachnophobia Exposure Therapy using Experience-driven Procedural Content Generation via Reinforcement Learning (EDPCGRL)",
      "urls": [
        "http://arxiv.org/pdf/2110.04146.pdf",
        "http://arxiv.org/abs/2110.04146v1",
        "http://arxiv.org/pdf/2110.04146v1"
      ]
    },
    {
      "abstract": "Learning to solve complex manipulation tasks from visual observations is a dominant challenge for real-world robot learning. Although deep reinforcement learning algorithms have recently demonstrated impressive results in this context, they still require an impractical amount of time-consuming trial-and-error iterations. In this work, we consider the promising alternative paradigm of interactive learning in which a human teacher provides feedback to the policy during execution, as opposed to imitation learning where a pre-collected dataset of perfect demonstrations is used. Our proposed CEILing (Corrective and Evaluative Interactive Learning) framework combines both corrective and evaluative feedback from the teacher to train a stochastic policy in an asynchronous manner, and employs a dedicated mechanism to trade off human corrections with the robot's own experience. We present results obtained with our framework in extensive simulation and real-world experiments to demonstrate that CEILing can effectively solve complex robot manipulation tasks directly from raw images in less than one hour of real-world training.",
      "authors": [
        "Chisari, Eugenio",
        "Welschehold, Tim",
        "Boedecker, Joschka",
        "Burgard, Wolfram",
        "Valada, Abhinav"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted for publication in RA-L. Video, code and models available at\n  http://ceiling.cs.uni-freiburg.de/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-07",
      "selected": null,
      "title": "Correct Me if I am Wrong: Interactive Learning for Robotic Manipulation",
      "urls": [
        "http://arxiv.org/pdf/2110.03316.pdf",
        "http://arxiv.org/pdf/2110.03316v2",
        "http://arxiv.org/abs/2110.03316v2"
      ]
    },
    {
      "abstract": "Current research in dialogue systems is focused on conversational assistants working on short conversations in either task-oriented or open domain settings. In this paper, we focus on improving task-based conversational assistants online, primarily those working on document-type conversations (e.g., emails) whose contents may or may not be completely related to the assistant's task. We propose \"NARLE\" a deep reinforcement learning (RL) framework for improving the natural language understanding (NLU) component of dialogue systems online without the need to collect human labels for customer data. The proposed solution associates user emotion with the assistant's action and uses that to improve NLU models using policy gradients. For two intent classification problems, we empirically show that using reinforcement learning to fine tune the pre-trained supervised learning models improves performance up to 43%. Furthermore, we demonstrate the robustness of the method to partial and noisy implicit feedback.",
      "authors": [
        "Zhou, Ruijie",
        "Deshmukh, Soham",
        "Greer, Jeremiah",
        "Lee, Charles"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-05",
      "selected": null,
      "title": "NaRLE: Natural Language Models using Reinforcement Learning with Emotion Feedback",
      "urls": [
        "http://arxiv.org/abs/2110.02148v1",
        "http://arxiv.org/pdf/2110.02148.pdf",
        "http://arxiv.org/pdf/2110.02148v1"
      ]
    },
    {
      "abstract": "Mapping natural language instructions to programs that computers can process is a fundamental challenge. Existing approaches focus on likelihood-based training or using reinforcement learning to fine-tune models based on a single reward. In this paper, we pose program generation from language as Inverse Reinforcement Learning. We introduce several interpretable reward components and jointly learn (1) a reward function that linearly combines them, and (2) a policy for program generation. Fine-tuning with our approach achieves significantly better performance than competitive methods using Reinforcement Learning (RL). On the VirtualHome framework, we get improvements of up to 9.0% on the Longest Common Subsequence metric and 14.7% on recall-based metrics over previous work on this framework (Puig et al., 2018). The approach is data-efficient, showing larger gains in performance in the low-data regime. Generated programs are also preferred by human evaluators over an RL-based approach, and rated higher on relevance, completeness, and human-likeness.",
      "authors": [
        "Ghosh, Sayan",
        "Srivastava, Shashank"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted at Findings of EMNLP 2021",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-02",
      "selected": null,
      "title": "Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2110.00842v1",
        "http://arxiv.org/pdf/2110.00842v1",
        "http://arxiv.org/pdf/2110.00842.pdf"
      ]
    },
    {
      "abstract": "The current research focus in Robot-Assisted Minimally Invasive Surgery (RAMIS) is directed towards increasing the level of robot autonomy, to place surgeons in a supervisory position. Although Learning from Demonstrations (LfD) approaches are among the preferred ways for an autonomous surgical system to learn expert gestures, they require a high number of demonstrations and show poor generalization to the variable conditions of the surgical environment. In this work, we propose an LfD methodology based on Generative Adversarial Imitation Learning (GAIL) that is built on a Deep Reinforcement Learning (DRL) setting. GAIL combines generative adversarial networks to learn the distribution of expert trajectories with a DRL setting to ensure generalisation of trajectories providing human-like behaviour. We consider automation of tissue retraction, a common RAMIS task that involves soft tissues manipulation to expose a region of interest. In our proposed methodology, a small set of expert trajectories can be acquired through the da Vinci Research Kit (dVRK) and used to train the proposed LfD method inside a simulated environment. Results indicate that our methodology can accomplish the tissue retraction task with human-like behaviour while being more sample-efficient than the baseline DRL method. Towards the end, we show that the learnt policies can be successfully transferred to the real robotic platform and deployed for soft tissue retraction on a synthetic phantom.",
      "authors": [
        "Pore, Ameya",
        "Tagliabue, Eleonora",
        "Piccinelli, Marco",
        "Dall'Alba, Diego",
        "Casals, Alicia",
        "Fiorini, Paolo"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted in IEEE International Symposium of Medical Robotics (ISMR\n  2021)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-10-01",
      "selected": null,
      "title": "Learning from Demonstrations for Autonomous Soft-tissue Retraction",
      "urls": [
        "http://arxiv.org/pdf/2110.00336v1",
        "http://arxiv.org/abs/2110.00336v1",
        "http://arxiv.org/pdf/2110.00336.pdf"
      ]
    },
    {
      "abstract": "A longstanding goal of artificial intelligence is to create artificial agents capable of learning to perform tasks that require sequential decision making. Importantly, while it is the artificial agent that learns and acts, it is still up to humans to specify the particular task to be performed. Classical task-specification approaches typically involve humans providing stationary reward functions or explicit demonstrations of the desired tasks. However, there has recently been a great deal of research energy invested in exploring alternative ways in which humans may guide learning agents that may, e.g., be more suitable for certain tasks or require less human effort. This survey provides a high-level overview of five recent machine learning frameworks that primarily rely on human guidance apart from pre-specified reward functions or conventional, step-by-step action demonstrations. We review the motivation, assumptions, and implementation of each framework, and we discuss possible future research directions.",
      "authors": [
        "Zhang, Ruohan",
        "Torabi, Faraz",
        "Warnell, Garrett",
        "Stone, Peter"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10458-021-09514-w",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13872532",
        "publisher": "Springer Netherlands",
        "sjr": 0.927,
        "snip": 2.046,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Autonomous Agents and Multi-Agent Systems"
      },
      "publication_date": "2021-10-01",
      "selected": null,
      "title": "Recent advances in leveraging human guidance for sequential decision-making tasks",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10458-021-09514-w.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108833554&origin=inward"
      ]
    },
    {
      "abstract": "Deep reinforcement learning has achieved significant success in many fields, but will confront sampling efficiency and safety problems when applying to robot control in the real world. Sim-to-real transfer learning was proposed to make use of samples in the simulation and overcome the gap between simulation and real world. In this paper, we focus on improving Progressive Neural Network &#x2014; an effective sim-to-real learning method, by proposing Interactive Progressive Network Learning (IPNL). IPNL integrates progressive network and interactive reinforcement learning (interactive RL) which learns from evaluative feedback provided by an observing human trainer. We test our method using five RL tasks with discrete or continuous actions in OpenAI Gym and a sinusoids curve following task with AUV simulator on the Gazebo platform. Our results suggest that while Progressive Network has good performance when transferring from tasks with low-dimensional state space to those with high-dimensional one but has little effect for transferring from high-dimensional tasks to low-dimensional ones, IPNL allows an agent to learn a more stable policy with better performance faster for both cases. More importantly, our further analysis indicate that there is a synergy between Progressive Network and interactive RL for improving the agent&#x2019;s learning. Our results in the path following of AUV shed light on the potential of applying our method in the real world tasks.",
      "authors": [
        "Rongshun Juan",
        "Jie Huang",
        "Randy Gomez",
        "Keisuke Nakamura",
        "Qixin Sha",
        "Bo He",
        "Guangliang Li"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/IROS51168.2021.9636061",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1281-1288",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2021-09-27",
      "selected": null,
      "title": "Shaping Progressive Net of Reinforcement Learning for Policy Transfer with Human Evaluative Feedback",
      "urls": [
        "https://dl.acm.org/doi/10.1109/IROS51168.2021.9636061",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124344900&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9636061"
      ]
    },
    {
      "abstract": "Deep reinforcement learning (deep RL) has emerged as an effective tool for developing controllers for legged robots. However, vanilla deep RL often requires a tremendous amount of training samples and is not feasible for achieving robust behaviors. Instead, researchers have investigated a novel policy architecture by incorporating human experts' knowledge, such as Policies Modulating Trajectory Generators (PMTG). This architecture builds a recurrent control loop by combining a parametric trajectory generator (TG) and a feedback policy network to achieve more robust behaviors. To take advantage of human experts' knowledge but eliminate time-consuming interactive teaching, researchers have investigated a novel architecture, Policies Modulating Trajectory Generators (PMTG), which builds a recurrent control loop by combining a parametric trajectory generator (TG) and a feedback policy network to achieve more robust behaviors using intuitive prior knowledge. In this work, we propose Policies Modulating Finite State Machine (PM-FSM) by replacing TGs with contact-aware finite state machines (FSM), which offer more flexible control of each leg. Compared with the TGs, FSMs offer high-level management on each leg motion generator and enable a flexible state arrangement, which makes the learned behavior less vulnerable to unseen perturbations or challenging terrains. This invention offers an explicit notion of contact events to the policy to negotiate unexpected perturbations. We demonstrated that the proposed architecture could achieve more robust behaviors in various scenarios, such as challenging terrains or external perturbations, on both simulated and real robots. The supplemental video can be found at: https://youtu.be/78cboMqTkJQ.",
      "authors": [
        "Liu, Ren",
        "Sontakke, Nitish",
        "Ha, Sehoon"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-09-26",
      "selected": null,
      "title": "PM-FSM: Policies Modulating Finite State Machine for Robust Quadrupedal Locomotion",
      "urls": [
        "http://arxiv.org/pdf/2109.12696v2",
        "http://arxiv.org/abs/2109.12696v2",
        "http://arxiv.org/pdf/2109.12696.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, we argue that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, making human-in-the-loop RL more accessible, including learning from demonstrations, learning from feedback, or curriculum learning. In addition, all experiments can be conducted over the internet without any additional software needed on the client\u2019s computer, making experiments at scale significantly easier.",
      "authors": [
        "Taylor, Matthew E.",
        "Nissen, Nicholas",
        "Wang, Yuan",
        "Navidi, Neda"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s00521-021-06375-y",
      "keywords": [
        "Human subject studies",
        "Human-AI interaction",
        "Crowdsourcing",
        "Open-source software",
        "Human-in-the-loop AI",
        "Reinforcement learning"
      ],
      "number_of_pages": 11,
      "pages": "23429-23439",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0941-0643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2021-09-19",
      "selected": null,
      "title": "Improving reinforcement learning with human assistance: an argument for human subject studies with HIPPO Gym",
      "urls": [
        "https://dl.acm.org/doi/10.1007/s00521-021-06375-y",
        "https://link.springer.com/content/pdf/10.1007/s00521-021-06375-y.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115119924&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Fede S.J."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuropsychologia.2021.107957",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00283932",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.995,
        "snip": 1.03,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neuropsychologia"
      },
      "publication_date": "2021-09-17",
      "selected": null,
      "title": "Charity preferences and perceived impact moderate charitable giving and associated neural response",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110454687&origin=inward"
      ]
    },
    {
      "abstract": "We consider the problem of learning the behavioral preferences of an expert engaged in a task from noisy and partially-observable demonstrations. This is motivated by real-world applications such as a line robot learning from observing a human worker, where some observations are occluded by environmental objects that cannot be removed. Furthermore, robotic perception tends to be imperfect and noisy. Previous techniques for inverse reinforcement learning (IRL) take the approach of either omitting the missing portions or inferring it as part of expectation-maximization, which tends to be slow and prone to local optima. We present a new method that generalizes the well-known Bayesian maximum-a-posteriori (MAP) IRL method by marginalizing the occluded portions of the trajectory. This is additionally extended with an observation model to account for perception noise. We show that the marginal MAP (MMAP) approach significantly improves on the previous IRL technique under occlusion in both formative evaluations on a toy problem and in a summative evaluation on an onion sorting line task by a robot.",
      "authors": [
        "Suresh, Prasanth Sengadu",
        "Doshi, Prashant"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-09-16",
      "selected": null,
      "title": "Marginal MAP Estimation for Inverse RL under Occlusion with Observer Noise",
      "urls": [
        "http://arxiv.org/abs/2109.07788v1",
        "http://arxiv.org/pdf/2109.07788.pdf",
        "http://arxiv.org/pdf/2109.07788v1"
      ]
    },
    {
      "abstract": "Fluid human-agent communication is essential for the future of human-in-the-loop reinforcement learning. An agent must respond appropriately to feedback from its human trainer even before they have significant experience working together. Therefore, it is important that learning agents respond well to various feedback schemes human trainers are likely to provide. This work analyzes the COnvergent Actor-Critic by Humans (COACH) algorithm under three different types of feedback-policy feedback, reward feedback, and advantage feedback. For these three feedback types, we find that COACH can behave sub-optimally. We propose a variant of COACH, episodic COACH (E-COACH), which we prove converges for all three types. We compare our COACH variant with two other reinforcement-learning algorithms: Q-learning and TAMER.",
      "authors": [
        "Shah, Ishaan",
        "Halpern, David",
        "Asadi, Kavosh",
        "Littman, Michael L."
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted into ICML 2021 workshops Human-AI Collaboration in\n  Sequential Decision-Making and Human in the Loop Learning",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-09-15",
      "selected": null,
      "title": "Convergence of a Human-in-the-Loop Policy-Gradient Algorithm With Eligibility Trace Under Reward, Policy, and Advantage Feedback",
      "urls": [
        "http://arxiv.org/pdf/2109.07054v1",
        "http://arxiv.org/abs/2109.07054v1",
        "http://arxiv.org/pdf/2109.07054.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Llobera J."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1098/rsos.210537",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Royal Society Open Science"
      },
      "publication_date": "2021-09-15",
      "selected": null,
      "title": "Evaluating participant responses to a virtual reality experience using reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118945027&origin=inward"
      ]
    },
    {
      "abstract": "Low-resource Relation Extraction (LRE) aims to extract relation facts from limited labeled corpora when human annotation is scarce. Existing works either utilize self-training scheme to generate pseudo labels that will cause the gradual drift problem, or leverage meta-learning scheme which does not solicit feedback explicitly. To alleviate selection bias due to the lack of feedback loops in existing LRE learning paradigms, we developed a Gradient Imitation Reinforcement Learning method to encourage pseudo label data to imitate the gradient descent direction on labeled data and bootstrap its optimization capability through trial and error. We also propose a framework called GradLRE, which handles two major scenarios in low-resource relation extraction. Besides the scenario where unlabeled data is sufficient, GradLRE handles the situation where no unlabeled data is available, by exploiting a contextualized augmentation method to generate data. Experimental results on two public datasets demonstrate the effectiveness of GradLRE on low resource relation extraction when comparing with baselines.",
      "authors": [
        "Hu, Xuming",
        "Zhang, Chenwei",
        "Yang, Yawen",
        "Li, Xiaohe",
        "Lin, Li",
        "Wen, Lijie",
        "Yu, Philip S."
      ],
      "categories": null,
      "citations": 37,
      "comments": "In EMNLP 2021 as a long paper. Code and data available at\n  https://github.com/THU-BPM/GradLRE",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "2737-2746",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781955917094",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings"
      },
      "publication_date": "2021-09-14",
      "selected": null,
      "title": "Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction",
      "urls": [
        "http://arxiv.org/pdf/2109.06415.pdf",
        "http://arxiv.org/pdf/2109.06415v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123977559&origin=inward",
        "http://arxiv.org/abs/2109.06415v1"
      ]
    },
    {
      "abstract": "Touch is commonly used to mediate human-machine interactions, notably in the setting of Digital Musical Instruments (DMIs), where touch screens are prevalent. The lack of rich haptic feedback has an impact on the richness and quality of the interaction. Piezoelectric transducers can be integrated to induce localized vibrations over a stiff surface to reestablish the correct haptic exchange. For instance, the time-reversal method has been successfully used to create a localized peak on a thin-rigid surface by using one or several actuators. This paper presents a deep learning approach to optimize the time-reversed signals on a single-actuator setup. With only one transducer, the amplitude and contrast ratio are increased and the desired position of the localized peak is assured to be exact. It is shown that Reinforcement Learning can be applied to optimize a pre-trained Neural Network to achieve similar or even better results when compared with the state-of-the-art approach using time-reversed impulse responses.",
      "authors": [
        "Camilo Hernandez-Mejia",
        "Marc Favier",
        "Xiaotao Ren",
        "Paolo Germano",
        "Yves Perriard"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IUS52206.2021.9593749",
      "keywords": [
        "piezoelectric transducer",
        "time- reversal",
        "Deep learning",
        "surface haptics"
      ],
      "number_of_pages": 5,
      "pages": "1-5",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-4777-5",
        "issn": "1948-5719",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 IEEE International Ultrasonics Symposium (IUS)"
      },
      "publication_date": "2021-09-11",
      "selected": null,
      "title": "Reinforcement Learning and Hardware in the Loop for Localized Vibrotactile Feedback in Haptic Surfaces",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9593749",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122869743&origin=inward"
      ]
    },
    {
      "abstract": "In the first step, a 1-DOF power assist robotic system (PARS) was developed for object manipulation with it, and the dynamics for human-robot co-manipulation of objects was derived reflecting human cognition (weight perception). Then, an admittance control scheme with position feedback and velocity controller was derived from the weight-perception-based dynamics. In a user study, human subjects lifted objects with the system. An evaluation scheme was developed to evaluate human-robot interaction (HRI) and co-manipulation performance. A reinforcement learning method was implemented to learn the admittance control parameters resulting in satisfactory HRI and manipulation performance. The results showed that inclusion of weight perception in the dynamics and the learning control were effective to produce satisfactory HRI and performance. In the second step, a novel variable admittance feedforward adaptive control algorithm was proposed, which helped further improve the HRI and manipulation performance. Then, effectiveness of the adaptive feedforward learning control method was validated using a multi-DOF PARS for manipulating heavy objects.",
      "authors": [
        "S. M. Mizanoor Rahman",
        "Ryojun Ikeura"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICHMS53169.2021.9582629",
      "keywords": [
        "power assist robot",
        "learning control",
        "feedforward control",
        "cognition",
        "object manipulation"
      ],
      "number_of_pages": 7,
      "pages": "1-7",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-0171-5",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2021 IEEE International Conference on Human-Machine Systems, ICHMS 2021"
      },
      "publication_date": "2021-09-08",
      "selected": null,
      "title": "Cognitive Feedforward Learning Control for Object Manipulation with a Power Assist Robotic System",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118958230&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9582629"
      ]
    },
    {
      "abstract": "In this paper we tackle motion planning in industrial human-robot cooperative scenarios modeled as a reinforcement learning problem solved in a simulated environment. The agent learns the most effective policy to reach the designated target position while avoiding collisions with a human, performing a pick and place task in the robot workspace, and with fixed obstacles. The policy acts as a feedback motion planner (or reactive motion planner), therefore at each time-step it senses the surrounding environment and computes the action to be performed. In this work a novel formulation of the action that guarantees the trajectory derivatives continuity is proposed to create smooth trajectories that are necessary for maximizing the human trust in the robot. The action is defined as the sub-trajectory the agent must follow for the duration of a time-step, therefore the complete trajectory is the concatenation of all the trajectories computed at each time-step. The proposed method does not require to infer the action the human is currently performing and/or foresee the space occupied by the human. Indeed, during the training phase in a simulated environment the agent experience how the human behaves in the specific scenario, therefore it learns the policy that best adapts to the human actions and movements. The proposed method is finally applied in a scenario of human-robot cooperative pick and place.",
      "authors": [
        "Giorgio Nicola",
        "Stefano Ghidoni"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ETFA45728.2021.9613505",
      "keywords": [
        "Motion Planning",
        "Human Robot Cooperation",
        "Deep Reinforcement Learning"
      ],
      "number_of_pages": 7,
      "pages": "01-07",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-2990-7",
        "issn": "19460740",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )"
      },
      "publication_date": "2021-09-07",
      "selected": null,
      "title": "Deep Reinforcement Learning for Motion Planning in Human Robot cooperative Scenarios",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9613505",
        "https://dl.acm.org/doi/10.1109/ETFA45728.2021.9613505",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122944080&origin=inward"
      ]
    },
    {
      "abstract": "Multi-agent inverse reinforcement learning (MIRL) can be used to learn reward functions from agents in social environments. To model realistic social dynamics, MIRL methods must account for suboptimal human reasoning and behavior. Traditional formalisms of game theory provide computationally tractable behavioral models, but assume agents have unrealistic cognitive capabilities. This research identifies and compares mechanisms in MIRL methods which a) handle noise, biases and heuristics in agent decision making and b) model realistic equilibrium solution concepts. MIRL research is systematically reviewed to identify solutions for these challenges. The methods and results of these studies are analyzed and compared based on factors including performance accuracy, efficiency, and descriptive quality. We found that the primary methods for handling noise, biases and heuristics in MIRL were extensions of Maximum Entropy (MaxEnt) IRL to multi-agent settings. We also found that many successful solution concepts are generalizations of the traditional Nash Equilibrium (NE). These solutions include the correlated equilibrium, logistic stochastic best response equilibrium and entropy regularized mean field NE. Methods which use recursive reasoning or updating also perform well, including the feedback NE and archive multi-agent adversarial IRL. Success in modeling specific biases and heuristics in single-agent IRL and promising results using a Theory of Mind approach in MIRL imply that modeling specific biases and heuristics may be useful. Flexibility and unbiased inference in the identified alternative solution concepts suggest that a solution concept which has both recursive and generalized characteristics may perform well at modeling realistic social interactions.",
      "authors": [
        "Bergerson, Sage"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-09-02",
      "selected": null,
      "title": "Multi-Agent Inverse Reinforcement Learning: Suboptimal Demonstrations and Alternative Solution Concepts",
      "urls": [
        "http://arxiv.org/abs/2109.01178v1",
        "http://arxiv.org/pdf/2109.01178v1",
        "http://arxiv.org/pdf/2109.01178.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu J."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.engappai.2021.104362",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 12.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09521976",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.729,
        "snip": 2.271,
        "subject_areas": [
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Engineering Applications of Artificial Intelligence"
      },
      "publication_date": "2021-09-01",
      "selected": null,
      "title": "Spiking neural network-based multi-task autonomous learning for mobile robots",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109948263&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bhangal S."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.13853",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2021-09-01",
      "selected": null,
      "title": "Learning to deal with delayed outcomes: EEG oscillatory and slow potentials during the prefeedback interval",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107526012&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Suzuki S."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/pcn.13279",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "277-285",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13231316",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychiatry and Clinical Neurosciences"
      },
      "publication_date": "2021-09-01",
      "selected": null,
      "title": "Psychiatric symptoms influence reward-seeking and loss-avoidance decision-making through common and distinct computational processes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110304267&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Han D."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/desc.13069",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1363755X",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.962,
        "snip": 1.911,
        "subject_areas": [
          "Developmental and Educational Psychology",
          "Cognitive Neuroscience"
        ],
        "title": "Developmental Science"
      },
      "publication_date": "2021-09-01",
      "selected": null,
      "title": "The impact of errors in infant development: Falling like a baby",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099065653&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Koenig S."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.brat.2021.103917",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00057967",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.787,
        "snip": 1.66,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Behaviour Research and Therapy"
      },
      "publication_date": "2021-09-01",
      "selected": null,
      "title": "An attentional perspective on differential fear conditioning in chronic pain: The informational value of safety cues.",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111082419&origin=inward"
      ]
    },
    {
      "abstract": "<p>Atrial fibrillation (AF) is the most common cardiac arrhythmia and currently affects more than 650,000 people in the United Kingdom alone. Catheter ablation (CA) is the only AF treatment with a long-term curative effect as it involves destroying arrhythmogenic tissue in the atria. However, its success rate is suboptimal, approximately 50% after a 2-year follow-up, and this high AF recurrence rate warrants significant improvements. Image-guidance of CA procedures have shown clinical promise, enabling the identification of key patient anatomical and pathological (such as fibrosis) features of atrial tissue, which require ablation. However, the latter approach still suffers from a lack of functional information and the need to interpret structures in the images by a clinician. Deep learning plays an increasingly important role in biomedicine, facilitating efficient diagnosis and treatment of clinical problems. This study applies deep reinforcement learning in combination with patient imaging (to provide structural information of the atria) and image-based modelling (to provide functional information) to design patient-specific CA strategies to guide clinicians and improve treatment success rates. To achieve this, patient-specific 2D left atrial (LA) models were derived from late-gadolinium enhancement (LGE) MRI scans of AF patients and were used to simulate patient-specific AF scenarios. Then a reinforcement Q-learning algorithm was created, where an ablating agent moved around the 2D LA, applying CA lesions to terminate AF and learning through feedback imposed by a reward policy. The agent achieved 84% success rate in terminating AF during training and 72% success rate in testing. Finally, AF recurrence rate was measured by attempting to re-initiate AF in the 2D atrial models after CA with 11% recurrence showing a great improvement on the existing therapies. Thus, reinforcement Q-learning algorithms can predict successful CA strategies from patient MRI data and help to improve the patient-specific guidance of CA therapy.</p>",
      "authors": [
        "Muizniece, Laila",
        "Bertagnoli, Adrian",
        "Qureshi, Ahmed",
        "Zeidan, Aya",
        "Roy, Aditi",
        "Muffoletto, Marica",
        "Aslanidi, Oleg"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fphys.2021.733139",
      "keywords": [
        "reinforcement learning",
        "Patient Imaging",
        "Atrial Fibrillation",
        "deep learning",
        "Catheter Ablation"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.1,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-042X",
        "publisher": "Frontiers Media SA",
        "sjr": 1.028,
        "snip": 1.197,
        "subject_areas": [
          "Physiology",
          "Physiology (medical)"
        ],
        "title": "Frontiers in Physiology"
      },
      "publication_date": "2021-08-25",
      "selected": null,
      "title": "Reinforcement Learning to Improve Image-Guidance of Ablation Therapy for Atrial Fibrillation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114649481&origin=inward",
        "https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2021.733139/pdf"
      ]
    },
    {
      "abstract": "The edge-cloud hybrid environment requires complex deployment strategies to enable the smart Internet-of-Things (IoT) system. However, current service deployment strategies use simple, generalized heuristics and ignore the heterogeneous characteristics in the edge-cloud hybrid environment. In this article, we devise a method to find a microservice-based service deployment strategy that can reduce the average waiting time of IoT devices in the hybrid environment. For this purpose, we first propose a microservice-based deployment problem (MSDP) based on the heterogeneous and dynamic characteristics in the edge-cloud hybrid environment, including heterogeneity of edge server capacities, dynamic geographical information of IoT devices, and changing device preference for applications and complex application structures. We then propose a multiple buffer deep deterministic policy gradient (MB_DDPG) to provide more preferable service deployment solutions. Our algorithm leverages reinforcement learning and neural network to learn a deployment strategy without any human instruction. Therefore, the service provider can make full use of limited resources to improve the Quality of Service (QoS). Finally, we implement MB_DDPG based on real-world data sets and some synthetic data, and we also implement another two algorithms, genetic algorithm and random algorithm, as a contrast. The experimental results demonstrate that MB_DDPG is able to learn a preferable strategy which, in terms of average waiting time, outperforms genetic algorithm and the random algorithm by 32% and 44%, respectively.",
      "authors": [
        "Lulu Chen",
        "Yangchuan Xu",
        "Zhihui Lu",
        "Jie Wu",
        "Keke Gai",
        "Patrick C. K. Hung",
        "Meikang Qiu"
      ],
      "categories": null,
      "citations": 46,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/JIOT.2020.3014970",
      "keywords": [
        "reinforcement learning",
        "Edge-cloud hybrid environment",
        "microservice deployment",
        "smart Internet-of-Things (IoT) system"
      ],
      "number_of_pages": 13,
      "pages": "12610-12622",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2372-2541",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Internet of Things Journal"
      },
      "publication_date": "2021-08-15",
      "selected": null,
      "title": "IoT Microservice Deployment in Edge-Cloud Hybrid Environment Using Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099587770&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9162056"
      ]
    },
    {
      "abstract": "\"High Quality Related Search Query Suggestions\" task aims at recommending search queries which are real, accurate, diverse, relevant and engaging. Obtaining large amounts of query-quality human annotations is expensive. Prior work on supervised query suggestion models suffered from selection and exposure bias, and relied on sparse and noisy immediate user-feedback (e.g., clicks), leading to low quality suggestions. Reinforcement Learning techniques employed to reformulate a query using terms from search results, have limited scalability to large-scale industry applications. To recommend high quality related search queries, we train a Deep Reinforcement Learning model to predict the query a user would enter next. The reward signal is composed of long-term session-based user feedback, syntactic relatedness and estimated naturalness of generated query. Over the baseline supervised model, our proposed approach achieves a significant relative improvement in terms of recommendation diversity (3%), down-stream user-engagement (4.2%) and per-sentence word repetitions (82%).",
      "authors": [
        "Bodigutla, Praveen Kumar"
      ],
      "categories": null,
      "citations": null,
      "comments": "Multi-Armed Bandits and Reinforcement Learning: Advancing Decision\n  Making in E-Commerce and Beyond at KDD 2021",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-08-10",
      "selected": null,
      "title": "High Quality Related Search Query Suggestions using Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2108.04452v1",
        "http://arxiv.org/pdf/2108.04452.pdf",
        "http://arxiv.org/abs/2108.04452v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Deng W."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN50785.2021.9515537",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "749-756",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665404921",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 30th IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2021"
      },
      "publication_date": "2021-08-08",
      "selected": null,
      "title": "Using socially assistive robot feedback to reinforce infant leg movement acceleration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115070998&origin=inward"
      ]
    },
    {
      "abstract": "The capability to interactively learn from human feedback would enable agents in new settings. For example, even novice users could train service robots in new tasks naturally and interactively. Human-in-the-loop Reinforcement Learning (HRL) combines human feedback and Reinforcement Learning (RL) techniques. State-of-the-art interactive learning techniques suffer from slow learning speed, thus leading to a frustrating experience for the human. We approach this problem by extending the HRL framework TAMER for evaluative feedback with the possibility to enhance human feedback with two different types of counterfactual explanations (action and state based). We experimentally show that our extensions improve the speed of learning.",
      "authors": [
        "Karalus, Jakob",
        "Lindner, Felix"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-08-03",
      "selected": null,
      "title": "Accelerating the Learning of TAMER with Counterfactual Explanations",
      "urls": [
        "http://arxiv.org/pdf/2108.01358.pdf",
        "http://arxiv.org/pdf/2108.01358v2",
        "http://arxiv.org/abs/2108.01358v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gerhardsson A."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/jsr.13236",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09621105",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Sleep Research"
      },
      "publication_date": "2021-08-01",
      "selected": null,
      "title": "Does insufficient sleep affect how you learn from reward or punishment? Reinforcement learning after 2 nights of sleep restriction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096647789&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Brady W.J."
      ],
      "categories": null,
      "citations": 57,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1126/sciadv.abe5641",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Science Advances"
      },
      "publication_date": "2021-08-01",
      "selected": null,
      "title": "How social learning amplifies moral outrage expression in online social networks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112525606&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mitchell B.A."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/1741-2552/aba6d9",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17412560",
        "publisher": "IOP Publishing Ltd.",
        "sjr": 1.135,
        "snip": 1.25,
        "subject_areas": [
          "Biomedical Engineering",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Journal of Neural Engineering"
      },
      "publication_date": "2021-08-01",
      "selected": null,
      "title": "Motor adaptation via distributional learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111118238&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Veselic S."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.yhbeh.2021.105022",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0018506X",
        "publisher": "Academic Press Inc.",
        "sjr": 1.091,
        "snip": 1.068,
        "subject_areas": [
          "Endocrine and Autonomic Systems",
          "Behavioral Neuroscience",
          "Endocrinology"
        ],
        "title": "Hormones and Behavior"
      },
      "publication_date": "2021-08-01",
      "selected": null,
      "title": "A causal role of estradiol in human reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109797293&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bradshaw A.R."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/1747021821999663",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "1344-1359",
      "publication": {
        "category": "Journal",
        "cite_score": 3.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17470218",
        "publisher": "SAGE Publications Inc.",
        "sjr": 0.877,
        "snip": 1.026,
        "subject_areas": [
          "Physiology",
          "Experimental and Cognitive Psychology",
          "Psychology (all)",
          "Physiology (medical)",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Quarterly Journal of Experimental Psychology"
      },
      "publication_date": "2021-08-01",
      "selected": null,
      "title": "Instrumental learning in social interactions: Trait learning from faces and voices",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109171220&origin=inward"
      ]
    },
    {
      "abstract": "Social species rely on the ability to modulate feedback-monitoring in social contexts to adjust one\u2019s actions and obtain desired outcomes. When being awarded positive outcomes during a gambling task, feedback-monitoring is attenuated when strangers are rewarded, as less value is assigned to the awarded outcome. This difference in feedback-monitoring can be indexed by an event-related potential (ERP) component known as the Reward Positivity (RewP), whose amplitude is enhanced when receiving positive feedback. While the degree of familiarity influences the RewP, little is known about how the RewP and reinforcement learning are affected when gambling on behalf of familiar versus nonfamiliar agents, such as robots. This question becomes increasingly important given that robots may be used as teachers and/or social companions in the near future, with whom children and adults will interact with for short or long periods of time. In the present study, we examined whether feedback-monitoring when gambling on behalf of oneself compared with a robot is impacted by whether participants have familiarized themselves with the robot before the task. We expected enhanced RewP amplitude for self versus other for those who did not familiarize with the robot and that self\u2013other differences in the RewP would be attenuated for those who familiarized with the robot. Instead, we observed that the RewP was larger when familiarization with the robot occurred, which corresponded to overall worse learning outcomes. We additionally observed an enhanced P3 effect for the high-familiarity condition, which suggests an increased motivation to reward. These findings suggest that familiarization with robots may cause a positive motivational effect, which positively affects RewP amplitudes, but interferes with learning.",
      "authors": [
        "Abubshait, Abdulaziz",
        "Beatty, Paul J.",
        "McDonald, Craig G.",
        "Hassall, Cameron D.",
        "Krigolson, Olav E.",
        "Wiese, Eva"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-021-00895-9",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "763-775",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2021-08-01",
      "selected": null,
      "title": "A win-win situation: Does familiarity with a social robot modulate feedback monitoring and learning?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103627414&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-021-00895-9.pdf"
      ]
    },
    {
      "abstract": "When applying autonomous driving technology in human-crewed vehicles, it is essential to consider the personal driving style with ensuring not only safety but also the driver\u2019s preference. Reinforcement learning (RL) has attracted much attention in the field of autonomous driving; however, it requires a finely tuned reward function. A method for tasks that are difficult to design reward functions, such as reproducing a personal driving style, is inverse reinforcement learning (IRL). Although IRL is commonly applied to the estimation of human and animal intentions, most previous methods require high computational costs to compute inner loop RL. For the problem of inner loop RL, Logistic Regression-Based IRL (LogReg-IRL), which does not require RL for reward estimation, is available. Moreover, LogReg-IRL can compute a value function as well as a reward function of the driver\u2019s own. Therefore, this paper proposes a method to estimate the latent driving preferences (called driving style) of a driver using the rewards and values obtained by applying LogReg-IRL. Several experimental results show that the proposed method could reproduce the original trajectory and quantify the driver\u2019s implicit preference.",
      "authors": [
        "Kishikawa, Daiko",
        "Arai, Sachiyo"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10015-021-00682-2",
      "keywords": [
        "Deep inverse reinforcement learning",
        "Driving style",
        "Autonomous driving"
      ],
      "number_of_pages": 9,
      "pages": "338-346",
      "publication": {
        "category": "Journal",
        "cite_score": 1.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1433-5298",
        "publisher": "Springer Japan",
        "sjr": 0.281,
        "snip": 0.598,
        "subject_areas": [
          "Artificial Intelligence",
          "Biochemistry, Genetics and Molecular Biology (all)"
        ],
        "title": "Artificial Life and Robotics"
      },
      "publication_date": "2021-08-01",
      "selected": null,
      "title": "Estimation of personal driving style via deep inverse reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103154177&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10015-021-00682-2.pdf",
        "https://dl.acm.org/doi/10.1007/s10015-021-00682-2"
      ]
    },
    {
      "abstract": "API search involves finding components in an API that are relevant to a programming task. For example, a programmer may need a function in a C library that opens a new network connection, then another function that sends data across that connection. Unfortunately, programmers often have trouble finding the API components that they need. A strong scientific consensus is emerging towards developing interactive tool support that responds to conversational feedback, emulating the experience of asking a fellow human programmer for help. A major barrier to creating these interactive tools is implementing dialogue management for API search. Dialogue management involves determining how a system should respond to user input, such as whether to ask a clarification question or to display potential results. In this paper, we present a dialogue manager for interactive API search that considers search results and dialogue history to select efficient actions. We implement two dialogue policies: a hand-crafted policy and a policy optimized via reinforcement learning. We perform a synthetics evaluation and a human evaluation comparing the policies to a generic single-turn, top-N policy used by source code search engines.",
      "authors": [
        "Eberhart, Zachary",
        "McMillan, Collin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-07-26",
      "selected": null,
      "title": "Dialogue Management for Interactive API Search",
      "urls": [
        "http://arxiv.org/pdf/2107.12317v1",
        "http://arxiv.org/pdf/2107.12317.pdf",
        "http://arxiv.org/abs/2107.12317v1"
      ]
    },
    {
      "abstract": "Playtesting is an essential step in the game design process. Game designers use the feedback from playtests to refine their designs. Game designers may employ procedural personas to automate the playtesting process. In this paper, we present two approaches to improve automated playtesting. First, we propose developing persona, which allows a persona to progress to different goals. In contrast, the procedural persona is fixed to a single goal. Second, a human playtester knows which paths she has tested before, and during the consequent tests, she may test different paths. However, Reinforcement Learning (RL) agents disregard these previous paths. We propose a novel methodology that we refer to as Alternative Path Finder (APF). We train APF with previous paths and employ APF during the training of an RL agent. APF modulates the reward structure of the environment while preserving the agent's goal. When evaluated, the agent generates a different trajectory that achieves the same goal. We use the General Video Game Artificial Intelligence (GVG-AI) and VizDoom frameworks to test our proposed methodologies. We use Proximal Policy Optimization (PPO) RL agent during experiments. First, we compare the playtest data generated by developing and procedural persona. Our experiments show that developing persona provides better insight into the game and how different players would play. Second, we present the alternative paths found using APF and argue why traditional RL agents cannot learn those paths.",
      "authors": [
        "Ariyurek, Sinan",
        "Surer, Elif",
        "Betin-Can, Aysu"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-07-26",
      "selected": null,
      "title": "Playtesting: What is Beyond Personas",
      "urls": [
        "http://arxiv.org/abs/2107.11965v2",
        "http://arxiv.org/pdf/2107.11965v2",
        "http://arxiv.org/pdf/2107.11965.pdf"
      ]
    },
    {
      "abstract": "Demonstration-guided reinforcement learning (RL) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task demonstrations. Prior approaches for demonstration-guided RL treat every new task as an independent learning problem and attempt to follow the provided demonstrations step-by-step, akin to a human trying to imitate a completely unseen behavior by following the demonstrator's exact muscle movements. Naturally, such learning will be slow, but often new behaviors are not completely unseen: they share subtasks with behaviors we have previously learned. In this work, we aim to exploit this shared subtask structure to increase the efficiency of demonstration-guided RL. We first learn a set of reusable skills from large offline datasets of prior experience collected across many tasks. We then propose Skill-based Learning with Demonstrations (SkiLD), an algorithm for demonstration-guided RL that efficiently leverages the provided demonstrations by following the demonstrated skills instead of the primitive actions, resulting in substantial performance improvements over prior demonstration-guided RL approaches. We validate the effectiveness of our approach on long-horizon maze navigation and complex robot manipulation tasks.",
      "authors": [
        "Pertsch, Karl",
        "Lee, Youngwoon",
        "Wu, Yue",
        "Lim, Joseph J."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 11,
      "pages": "729-739",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2021-07-21",
      "selected": null,
      "title": "Demonstration-Guided Reinforcement Learning with Learned Skills",
      "urls": [
        "http://arxiv.org/pdf/2107.10253v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171797404&origin=inward",
        "http://arxiv.org/pdf/2107.10253.pdf",
        "http://arxiv.org/abs/2107.10253v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chaput R."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3461702.3462515",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "13-23",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450384735",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AIES 2021 - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society"
      },
      "publication_date": "2021-07-21",
      "selected": null,
      "title": "A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112403429&origin=inward"
      ]
    },
    {
      "abstract": "Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully; they might be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and allowing them to achieve their goals better. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach in three different multi-agent environments where another agent's success depends on altruistic behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them.",
      "authors": [
        "Franzmeyer, Tim",
        "Malinowski, Mateusz",
        "Henriques, Jo\u00e3o F."
      ],
      "categories": null,
      "citations": null,
      "comments": "ICLR 2022 Spotlight Presentation",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-07-20",
      "selected": null,
      "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards",
      "urls": [
        "http://arxiv.org/pdf/2107.09598v4",
        "http://arxiv.org/pdf/2107.09598.pdf",
        "http://arxiv.org/abs/2107.09598v4"
      ]
    },
    {
      "abstract": "Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance.",
      "authors": [
        "Siu, Ho Chit",
        "Pena, Jaime D.",
        "Chen, Edenna",
        "Zhou, Yutai",
        "Lopez, Victor J.",
        "Palko, Kyle",
        "Chang, Kimberlee C.",
        "Allen, Ross E."
      ],
      "categories": null,
      "citations": 15,
      "comments": "Accepted for publication at NeurIPS 2021",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 13,
      "pages": "16183-16195",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2021-07-15",
      "selected": null,
      "title": "Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi",
      "urls": [
        "http://arxiv.org/pdf/2107.07630v3",
        "http://arxiv.org/pdf/2107.07630.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131831243&origin=inward",
        "http://arxiv.org/abs/2107.07630v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Moran R."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cub.2021.03.091",
      "keywords": [],
      "number_of_pages": null,
      "pages": "2747-2756.e6",
      "publication": {
        "category": "Journal",
        "cite_score": 12.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09609822",
        "publisher": "Cell Press",
        "sjr": 2.806,
        "snip": 1.81,
        "subject_areas": [
          "Neuroscience (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Agricultural and Biological Sciences (all)"
        ],
        "title": "Current Biology"
      },
      "publication_date": "2021-07-12",
      "selected": null,
      "title": "Efficiency and prioritization of inference-based credit assignment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107020509&origin=inward"
      ]
    },
    {
      "abstract": "The sex hormone estradiol has recently gained attention in human decision-making research. Animal studies have already shown that estradiol promotes dopaminergic transmission and thus supports reward-seeking behavior and aspects of addiction. In humans, natural variations of estradiol across the menstrual cycle modulate the ability to learn from direct performance feedback (\u201cmodel-free\u201d learning). However, it remains unclear whether estradiol also influences more complex \u201cmodel-based\u201d contributions to reinforcement learning. Here, 41 women were tested twice \u2013 in the low and high estradiol state of the follicular phase of their menstrual cycle \u2013 with a two-step decision task designed to separate model-free from model-based learning. The results showed that in the high estradiol state women relied more heavily on model-free learning, and accomplished reduced performance gains, particularly during the more volatile periods of the task that demanded increased learning effort. In contrast, model-based control remained unaltered by the influence of hormonal state across the group. Yet, when accounting for individual differences in the genetic proxy of the COMT-Val158Met polymorphism (rs4680), we observed that only the participants homozygote for the methionine allele (n = 12; with putatively higher prefrontal dopamine) experienced a decline in model-based control when facing volatile reward probabilities. This group also showed the increase in suboptimal model-free control, while the carriers of the valine allele remained unaffected by the rise in endogenous estradiol. Taken together, these preliminary findings suggest that endogenous estradiol may affect the balance between model-based and model-free control, and particularly so in women with a high prefrontal baseline dopamine capacity and in situations of increased environmental volatility.",
      "authors": [
        "Diekhof, Esther K.",
        "Geana, Andra",
        "Ohm, Frederike",
        "Doll, Bradley B.",
        "Frank, Michael J."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2021.658769",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2021-07-08",
      "selected": null,
      "title": "The Straw That Broke the Camel\u2019s Back: Natural Variations in 17\u03b2-Estradiol and COMT-Val158Met Genotype Interact in the Modulation of Model-Free and Model-Based Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111029961&origin=inward"
      ]
    },
    {
      "abstract": "Most of the natural and man-made objects that we daily encounter undergo permanent or plastic deformation when the applied force exceeds a certain limit. Furthermore, we frequently use this material property to shape objects to desired forms. Current haptic-enabled VR simulators, however, are limited to interaction with elastic objects. In this paper, we aim to provide a real like haptic experience of manipulation with virtual plastic objects. An end-to-end framework is developed enabling the user to collect the haptic feedback data from real material, building the model, and render it in real-time FEM simulation. We model the plastic flow as a closed-loop block-box controller, which is optimized through inverse reinforcement learning trying to mimic the real deformation. The neural network-based controller in our model allows modeling plasticity with arbitrary complexity. To evaluate the proposed system, data from three real materials with various properties were captured and tested. The experimental results revealed the force feedback to be at the reasonable level of realism having relative error below than the human just noticeable difference of force perception.",
      "authors": [
        "Arsen Abdulali",
        "Seokhee Jeon"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/WHC49131.2021.9517181",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "115-120",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-4806-2",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 IEEE World Haptics Conference, WHC 2021"
      },
      "publication_date": "2021-07-06",
      "selected": null,
      "title": "Data-driven Haptic Modeling of Plastic Flow via Inverse Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9517181",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115140148&origin=inward"
      ]
    },
    {
      "abstract": "Digitization and remote connectivity have enlarged the attack surface and made cyber systems more vulnerable. As attackers become increasingly sophisticated and resourceful, mere reliance on traditional cyber protection, such as intrusion detection, firewalls, and encryption, is insufficient to secure the cyber systems. Cyber resilience provides a new security paradigm that complements inadequate protection with resilience mechanisms. A Cyber-Resilient Mechanism (CRM) adapts to the known or zero-day threats and uncertainties in real-time and strategically responds to them to maintain critical functions of the cyber systems in the event of successful attacks. Feedback architectures play a pivotal role in enabling the online sensing, reasoning, and actuation process of the CRM. Reinforcement Learning (RL) is an essential tool that epitomizes the feedback architectures for cyber resilience. It allows the CRM to provide sequential responses to attacks with limited or without prior knowledge of the environment and the attacker. In this work, we review the literature on RL for cyber resilience and discuss cyber resilience against three major types of vulnerabilities, i.e., posture-related, information-related, and human-related vulnerabilities. We introduce three application domains of CRMs: moving target defense, defensive cyber deception, and assistive human security technologies. The RL algorithms also have vulnerabilities themselves. We explain the three vulnerabilities of RL and present attack models where the attacker targets the information exchanged between the environment and the agent: the rewards, the state observations, and the action commands. We show that the attacker can trick the RL agent into learning a nefarious policy with minimum attacking effort. Lastly, we discuss the future challenges of RL for cyber security and resilience and emerging applications of RL-based CRMs.",
      "authors": [
        "Huang, Yunhan",
        "Huang, Linan",
        "Zhu, Quanyan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-07-02",
      "selected": null,
      "title": "Reinforcement Learning for Feedback-Enabled Cyber Resilience",
      "urls": [
        "http://arxiv.org/pdf/2107.00783v2",
        "http://arxiv.org/pdf/2107.00783.pdf",
        "http://arxiv.org/abs/2107.00783v2"
      ]
    },
    {
      "abstract": "Telerobotic systems must adapt to new environmental conditions and deal with high uncertainty caused by long-time delays. As one of the best alternatives to human-level intelligence, Reinforcement Learning (RL) may offer a solution to cope with these issues. This paper proposes to integrate RL with the Model Mediated Teleoperation (MMT) concept. The teleoperator interacts with a simulated virtual environment, which provides instant feedback. Whereas feedback from the real environment is delayed, feedback from the model is instantaneous, leading to high transparency. The MMT is realized in combination with an intelligent system with two layers. The first layer utilizes Dynamic Movement Primitives (DMP) which accounts for certain changes in the avatar environment. And, the second layer addresses the problems caused by uncertainty in the model using RL methods. Augmented reality was also provided to fuse the avatar device and virtual environment models for the teleoperator. Implemented on DLR's Exodex Adam hand-arm haptic exoskeleton, the results show RL methods are able to find different solutions when changes are applied to the object position after the demonstration. The results also show DMPs to be effective at adapting to new conditions where there is no uncertainty involved.",
      "authors": [
        "Beik-Mohammadi, Hadi",
        "Kerzel, Matthias",
        "Pleintinger, Benedikt",
        "Hulin, Thomas",
        "Reisich, Philipp",
        "Schmidt, Annika",
        "Pereira, Aaron",
        "Wermter, Stefan",
        "Lii, Neal Y."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-07-01",
      "selected": null,
      "title": "Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time Delays Using Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2107.00359.pdf",
        "http://arxiv.org/abs/2107.00359v1",
        "http://arxiv.org/pdf/2107.00359v1"
      ]
    },
    {
      "abstract": "Despite the advances in the autonomous driving domain, autonomous vehicles (AVs) are still inefficient and limited in terms of cooperating with each other or coordinating with vehicles operated by humans. A group of autonomous and human-driven vehicles (HVs) which work together to optimize an altruistic social utility -- as opposed to the egoistic individual utility -- can co-exist seamlessly and assure safety and efficiency on the road. Achieving this mission without explicit coordination among agents is challenging, mainly due to the difficulty of predicting the behavior of humans with heterogeneous preferences in mixed-autonomy environments. Formally, we model an AV's maneuver planning in mixed-autonomy traffic as a partially-observable stochastic game and attempt to derive optimal policies that lead to socially-desirable outcomes using a multi-agent reinforcement learning framework. We introduce a quantitative representation of the AVs' social preferences and design a distributed reward structure that induces altruism into their decision making process. Our altruistic AVs are able to form alliances, guide the traffic, and affect the behavior of the HVs to handle competitive driving scenarios. As a case study, we compare egoistic AVs to our altruistic autonomous agents in a highway merging setting and demonstrate the emerging behaviors that lead to a noticeable improvement in the number of successful merges as well as the overall traffic flow and safety.",
      "authors": [
        "Toghi, Behrad",
        "Valiente, Rodolfo",
        "Sadigh, Dorsa",
        "Pedarsani, Ramtin",
        "Fallah, Yaser P."
      ],
      "categories": null,
      "citations": null,
      "comments": "Under Review in an IEEE Journal",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-07-01",
      "selected": null,
      "title": "Social Coordination and Altruism in Autonomous Driving",
      "urls": [
        "http://arxiv.org/abs/2107.00200v4",
        "http://arxiv.org/pdf/2107.00200.pdf",
        "http://arxiv.org/pdf/2107.00200v4"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Robinson A.H."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/adb.12999",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13556215",
        "publisher": "Wiley-Blackwell",
        "sjr": 1.049,
        "snip": 0.941,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Psychiatry and Mental Health",
          "Pharmacology"
        ],
        "title": "Addiction Biology"
      },
      "publication_date": "2021-07-01",
      "selected": null,
      "title": "Are methamphetamine users compulsive? Faulty reinforcement learning, not inflexibility, underlies decision making in people with methamphetamine use disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099055912&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Parrell B."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_01742",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "1470-1486",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2021-07-01",
      "selected": null,
      "title": "A potential role for reinforcement learning in speech production",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110550790&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kambara H."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2021.01.030",
      "keywords": [],
      "number_of_pages": 20,
      "pages": "179-198",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2021-07-01",
      "selected": null,
      "title": "Computational reproductions of external force field adaption without assuming desired trajectories",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102499801&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rawls E."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cortex.2021.03.012",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "26-39",
      "publication": {
        "category": "Journal",
        "cite_score": 7.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00109452",
        "publisher": "Masson SpA",
        "sjr": 1.303,
        "snip": 1.241,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Cortex"
      },
      "publication_date": "2021-07-01",
      "selected": null,
      "title": "The aversion positivity: Mediofrontal cortical potentials reflect parametric aversive prediction errors and drive behavioral modification following negative reinforcement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105340179&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wan S."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2020.3011559",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "3287-3292",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2021-07-01",
      "selected": null,
      "title": "Human-in-the-Loop Low-Shot Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091298622&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huang F.Y."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.2101954118",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2021-06-29",
      "selected": null,
      "title": "Preferences for nutrients and sensory food qualities identify biological sources of economic values in monkeys",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108346866&origin=inward"
      ]
    },
    {
      "abstract": "In this paper, we consider a type of image quality assessment (IQA) as a task-specific measurement, which can be used to select images that are more amenable to a given target task, such as image classification or segmentation. We propose to train simultaneously two...",
      "authors": [
        "Saeed, Shaheer U.",
        "Fu, Yunguan",
        "Baum, Zachary M. C.",
        "Yang, Qianye",
        "Rusu, Mirabela",
        "Fan, Richard E.",
        "Sonn, Geoffrey A.",
        "Barratt, Dean C.",
        "Hu, Yipeng"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-3-030-78191-0_58",
      "keywords": [
        "Task amenability",
        "Medical image quality assessment",
        "Deep learning",
        "Reinforcement learning"
      ],
      "number_of_pages": 12,
      "pages": "755-766",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-3-030-78190-3",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Information Processing in Medical Imaging: 27th International Conference, IPMI 2021, Virtual Event, June 28\u2013June 30, 2021, Proceedings"
      },
      "publication_date": "2021-06-28",
      "selected": null,
      "title": "Learning Image Quality Assessment by Reinforcing Task Amenable Data Selection",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111408307&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-030-78191-0_58.pdf",
        "https://dl.acm.org/doi/10.1007/978-3-030-78191-0_58"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sachdeva E."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3449639.3459387",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "163-171",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450383509",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "GECCO 2021 - Proceedings of the 2021 Genetic and Evolutionary Computation Conference"
      },
      "publication_date": "2021-06-26",
      "selected": null,
      "title": "MAEDyS: Multiagent evolution via dynamic skill selection",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110075846&origin=inward"
      ]
    },
    {
      "abstract": "Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity -- the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.",
      "authors": [
        "Laidlaw, Cassidy",
        "Russell, Stuart"
      ],
      "categories": null,
      "citations": 4,
      "comments": "Accepted at NeurIPS 2021 (Spotlight)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 14,
      "pages": "15070-15083",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2021-06-19",
      "selected": null,
      "title": "Uncertain Decisions Facilitate Better Preference Learning",
      "urls": [
        "http://arxiv.org/pdf/2106.10394.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132029865&origin=inward",
        "http://arxiv.org/pdf/2106.10394v2",
        "http://arxiv.org/abs/2106.10394v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu Y."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuron.2021.04.014",
      "keywords": [],
      "number_of_pages": null,
      "pages": "2009-2024.e6",
      "publication": {
        "category": "Journal",
        "cite_score": 26.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08966273",
        "publisher": "Cell Press",
        "sjr": 7.736,
        "snip": 3.346,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Neuron"
      },
      "publication_date": "2021-06-16",
      "selected": null,
      "title": "A cortical circuit mechanism for structural knowledge-based flexible sensorimotor decision-making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108001030&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hammerstrom M.R."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.brainres.2021.147393",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00068993",
        "publisher": "Elsevier B.V.",
        "sjr": 0.854,
        "snip": 0.766,
        "subject_areas": [
          "Neuroscience (all)",
          "Molecular Biology",
          "Neurology (clinical)",
          "Developmental Biology"
        ],
        "title": "Brain Research"
      },
      "publication_date": "2021-06-15",
      "selected": null,
      "title": "What happens when right means wrong? The impact of conflict arising from competing feedback responses",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102481363&origin=inward"
      ]
    },
    {
      "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
      "authors": [
        "Lee, Kimin",
        "Smith, Laura",
        "Abbeel, Pieter"
      ],
      "categories": null,
      "citations": 10,
      "comments": "ICML 2021. First two authors contributed equally. Website:\n  https://sites.google.com/view/icml21pebble Code:\n  https://github.com/pokaxpoka/B_Pref",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 12,
      "pages": "6152-6163",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713845065",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2021-06-09",
      "selected": null,
      "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
      "urls": [
        "http://arxiv.org/pdf/2106.05091v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85161299499&origin=inward",
        "http://arxiv.org/pdf/2106.05091.pdf",
        "http://arxiv.org/abs/2106.05091v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Heider Y."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/nag.3196",
      "keywords": [],
      "number_of_pages": 26,
      "pages": "1212-1237",
      "publication": {
        "category": "Journal",
        "cite_score": 7.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03639061",
        "publisher": "John Wiley and Sons Ltd",
        "sjr": 1.429,
        "snip": 1.485,
        "subject_areas": [
          "Mechanics of Materials",
          "Materials Science (all)",
          "Geotechnical Engineering and Engineering Geology",
          "Computational Mechanics"
        ],
        "title": "International Journal for Numerical and Analytical Methods in Geomechanics"
      },
      "publication_date": "2021-06-01",
      "selected": null,
      "title": "An offline multi-scale unsaturated poromechanics model enabled by self-designed/self-improved neural networks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100873431&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rogers M.P."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.amjsurg.2020.11.023",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "1298-1299",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00029610",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "American Journal of Surgery"
      },
      "publication_date": "2021-06-01",
      "selected": null,
      "title": "The present and future state of machine learning for predictive analytics in surgery",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096537044&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Peters K.Z."
      ],
      "categories": null,
      "citations": 39,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.tins.2021.02.001",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "464-477",
      "publication": {
        "category": "Journal",
        "cite_score": 25.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01662236",
        "publisher": "Elsevier Ltd.",
        "sjr": 4.784,
        "snip": 3.343,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Trends in Neurosciences"
      },
      "publication_date": "2021-06-01",
      "selected": null,
      "title": "Modulating the Neuromodulators: Dopamine, Serotonin, and the Endocannabinoid System",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101868515&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huang H."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.11999/JEIT191035",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1781-1788",
      "publication": {
        "category": "Journal",
        "cite_score": 1.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10095896",
        "publisher": "Science Press",
        "sjr": 0.219,
        "snip": 0.509,
        "subject_areas": [
          "Electrical and Electronic Engineering"
        ],
        "title": "Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology"
      },
      "publication_date": "2021-06-01",
      "selected": null,
      "title": "Cognitive Emotional Interaction Model of Robot Based on Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108001063&origin=inward"
      ]
    },
    {
      "abstract": "The tabletop robot Haru, used for affective telepresence research, enables a teleoperator to communicate affects from a distance. The robot&#x2019;s expressiveness offers myriad ways of communicating affects through the execution of emotive routines. The teleoperator reacts to input modalities such as the user&#x2019;s facial expression, gestures and speech-based intent as perceived by the robot&#x2019;s perception system. However, due to the sheer number of routines to select from, the task of choosing the appropriate or the most preferred routine is becoming cumbersome. In this paper, we propose a human-in-the-loop reinforcement learning mechanism in which an agent learns the teleoperator&#x2019;s selection preference as a function of the input modalities and aids the routine selection process by narrowing it to n-best optimal choices. Our experimental results show that with only a few number of interactions from the teleoperator, the system can learn to recommend optimal routine behaviors for all perceived modalities, which greatly reduces the workload of the teleoperator.",
      "authors": [
        "Yurii Vasylkiv",
        "Zhen Ma",
        "Guangliang Li",
        "Eleanor Sandry",
        "Heike Brock",
        "Keisuke Nakamura",
        "Irani Pourang",
        "Randy Gomez"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA48506.2021.9560755",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "2026-2032",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7396-2",
        "issn": "1050-4729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2021-05-30",
      "selected": null,
      "title": "Automating Behavior Selection for Affective Telepresence Robot",
      "urls": [
        "https://dl.acm.org/doi/10.1109/ICRA48506.2021.9560755",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119893851&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9560755"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) is an effective technique for training decision-making agents through interactions with their environment. The advent of deep learning has been associated with highly notable successes with sequential decision making problems - such as defeating some of the highest-ranked human players at Go. In digital advertising, real-time bidding (RTB) is a common method of allocating advertising inventory through real-time auctions. Bidding strategies need to incorporate logic for dynamically adjusting parameters in order to deliver pre-assigned campaign goals. Here we discuss techniques toward using RL to train bidding agents. As a campaign metric we particularly focused on viewability: the percentage of inventory which goes on to be viewed by an end user. This paper is presented as a survey of techniques and experiments which we developed through the course of this research. We discuss expanding our training data to include edge cases by training on simulated interactions. We discuss the experimental results comparing the performance of several promising RL algorithms, and an approach to hyperparameter optimization of an actor/critic training pipeline through Bayesian optimization. Finally, we present live-traffic tests of some of our RL agents against a rule-based feedback-control approach, demonstrating the potential for this method as well as areas for further improvement. This paper therefore presents an arrangement of our findings in this quickly developing field, and ways that it can be applied to an RTB use case.",
      "authors": [
        "Tashman, Michael",
        "Hoffman, John",
        "Xie, Jiayi",
        "Ye, Fengdan",
        "Morsali, Atefeh",
        "Winikor, Lee",
        "Gerami, Rouzbeh"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-05-21",
      "selected": null,
      "title": "Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2105.10587v1",
        "http://arxiv.org/pdf/2105.10587.pdf",
        "http://arxiv.org/abs/2105.10587v1"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665412667",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 11th International Conference on Information Science and Technology, ICIST 2021"
      },
      "publication_date": "2021-05-21",
      "selected": null,
      "title": "2021 11th International Conference on Information Science and Technology, ICIST 2021",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107914274&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Olszewska J.I."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/SACI51354.2021.9465564",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "67-72",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728195445",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "SACI 2021 - IEEE 15th International Symposium on Applied Computational Intelligence and Informatics, Proceedings"
      },
      "publication_date": "2021-05-19",
      "selected": null,
      "title": "Human Computer Interaction Feedback Based-On Data Visualization Using MVAR and NN",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113848870&origin=inward"
      ]
    },
    {
      "abstract": "Accurate and precise terrain estimation is a difficult problem for robot locomotion in real-world environments. Thus, it is useful to have systems that do not depend on accurate estimation to the point of fragility. In this paper, we explore the limits of such an approach by investigating the problem of traversing stair-like terrain without any external perception or terrain models on a bipedal robot. For such blind bipedal platforms, the problem appears difficult (even for humans) due to the surprise elevation changes. Our main contribution is to show that sim-to-real reinforcement learning (RL) can achieve robust locomotion over stair-like terrain on the bipedal robot Cassie using only proprioceptive feedback. Importantly, this only requires modifying an existing flat-terrain training RL framework to include stair-like terrain randomization, without any changes in reward function. To our knowledge, this is the first controller for a bipedal, human-scale robot capable of reliably traversing a variety of real-world stairs and other stair-like disturbances using only proprioception.",
      "authors": [
        "Siekmann, Jonah",
        "Green, Kevin",
        "Warila, John",
        "Fern, Alan",
        "Hurst, Jonathan"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to RSS 2021. Submission video available at\n  https://youtu.be/MPhEmC6b6XU and video of a supplemental robustness test at\n  https://youtu.be/nuhHiKEtaZQ",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-05-18",
      "selected": null,
      "title": "Blind Bipedal Stair Traversal via Sim-to-Real Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2105.08328v1",
        "http://arxiv.org/abs/2105.08328v1",
        "http://arxiv.org/pdf/2105.08328.pdf"
      ]
    },
    {
      "abstract": "The heavy traffic and related issues have always been concerns for modern cities. With the help of deep learning and reinforcement learning, people have proposed various policies to solve these traffic-related problems, such as smart traffic signal control systems and taxi dispatching systems. People usually validate these policies in a city simulator, since directly applying them in the real city introduces real cost. However, these policies validated in the city simulator may fail in the real city if the simulator is significantly different from the real world. To tackle this problem, we need to build a real-like traffic simulation system. Therefore, in this paper, we propose to learn the human routing model, which is one of the most essential part in the traffic simulator. This problem has two major challenges. First, human routing decisions are determined by multiple factors, besides the common time and distance factor. Second, current historical routes data usually covers just a small portion of vehicles, due to privacy and device availability issues. To address these problems, we propose a theory-guided residual network model, where the theoretical part can emphasize the general principles for human routing decisions (e.g., fastest route), and the residual part can capture drivable condition preferences (e.g., local road or highway). Since the theoretical part is composed of traditional shortest path algorithms that do not need data to train, our residual network can learn human routing models from limited data. We have conducted extensive experiments on multiple real-world datasets to show the superior performance of our model, especially with small data. Besides, we have also illustrated why our model is better at recovering real routes through case studies.",
      "authors": [
        "Liu, Chang",
        "Zheng, Guanjie",
        "Li, Zhenhui"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-05-18",
      "selected": null,
      "title": "Learning to Route via Theory-Guided Residual Network",
      "urls": [
        "http://arxiv.org/pdf/2105.08279v2",
        "http://arxiv.org/abs/2105.08279v2",
        "http://arxiv.org/pdf/2105.08279.pdf"
      ]
    },
    {
      "abstract": "While current autonomous navigation systems allow robots to successfully drive themselves from one point to another in specific environments, they typically require extensive manual parameter re-tuning by human robotics experts in order to function in new environments. Furthermore, even for just one complex environment, a single set of fine-tuned parameters may not work well in different regions of that environment. These problems prohibit reliable mobile robot deployment by non-expert users. As a remedy, we propose Adaptive Planner Parameter Learning (APPL), a machine learning framework that can leverage non-expert human interaction via several modalities -- including teleoperated demonstrations, corrective interventions, and evaluative feedback -- and also unsupervised reinforcement learning to learn a parameter policy that can dynamically adjust the parameters of classical navigation systems in response to changes in the environment. APPL inherits safety and explainability from classical navigation systems while also enjoying the benefits of machine learning, i.e., the ability to adapt and improve from experience. We present a suite of individual APPL methods and also a unifying cycle-of-learning scheme that combines all the proposed methods in a framework that can improve navigation performance through continual, iterative human interaction and simulation training.",
      "authors": [
        "Xiao, Xuesu",
        "Wang, Zizhao",
        "Xu, Zifan",
        "Liu, Bo",
        "Warnell, Garrett",
        "Dhamankar, Gauraang",
        "Nair, Anirudh",
        "Stone, Peter"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-05-17",
      "selected": null,
      "title": "APPL: Adaptive Planner Parameter Learning",
      "urls": [
        "http://arxiv.org/abs/2105.07620v3",
        "http://arxiv.org/pdf/2105.07620.pdf",
        "http://arxiv.org/pdf/2105.07620v3"
      ]
    },
    {
      "abstract": "Cutting-edge Human-Computer Interaction (HCI) technologies embedded with Machine Learning (ML) will cause a paradigm shift in various domains, including manufacturing and developing facilities and services for professional and personal use. ML implemented HCIs can help people overcome societal challenges brought about by the COVID-19 pandemic. We introduce a system for people to perform physical exercises at home. This system is intended to help a range of demographics, from non-critical physical therapy patients to experienced weightlifters. More specifically, we propose a method to assess the difficulty of an exercise for visual exercise tracking systems. Pose estimation tracks exercises and reinforcement learning provides autonomous feedback to the user (patient/athlete). This information is processed largely on the client side, allowing the application to run smoothly anywhere in the world.",
      "authors": [
        "Ishan Ranasinghe",
        "Ram Dantu",
        "Mark V. Albert",
        "Sam Watts",
        "Ruben Ocana"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Fitts\u2019s law",
        "Index of Difficulty",
        "Human-Computer Interaction",
        "Pose Estimation",
        "Reinforcement Learning"
      ],
      "number_of_pages": 4,
      "pages": "1054-1057",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-9041-9",
        "issn": "1573-0077",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the IM 2021 - 2021 IFIP/IEEE International Symposium on Integrated Network Management"
      },
      "publication_date": "2021-05-17",
      "selected": null,
      "title": "Cyber-Physiotherapy: Rehabilitation to Training",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113632926&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9464036"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning provides a way for agents to learn to solve tasks from evaluative feedback provided by a human user. Previous research showed that humans give copious feedback early in training but very sparsely thereafter. In this paper, we investigate the potential of agent learning from trainers' facial expressions via interpreting them as evaluative feedback. To do so, we implemented TAMER which is a popular interactive reinforcement learning method in a reinforcement-learning benchmark problem --- Infinite Mario, and conducted the first large-scale study of TAMER involving 561 participants. With designed CNN-RNN model, our analysis shows that telling trainers to use facial expressions and competition can improve the accuracies for estimating positive and negative feedback using facial expressions. In addition, our results with a simulation experiment show that learning solely from predicted feedback based on facial expressions is possible and using strong/effective prediction models or a regression method, facial responses would significantly improve the performance of agents. Furthermore, our experiment supports previous studies demonstrating the importance of bi-directional feedback and competitive elements in the training interface.",
      "authors": [
        "Guangliang Li",
        "Hamdi Dibeklio\u011flu",
        "Shimon Whiteson",
        "Hayley Hung"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3463952.3464219",
      "keywords": [
        "implicit feedback",
        "interactive reinforcement learning",
        "facial feedback"
      ],
      "number_of_pages": 3,
      "pages": "1735-1737",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450383073",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems"
      },
      "publication_date": "2021-05-03",
      "selected": null,
      "title": "Facial Feedback for Reinforcement Learning: A Case Study and Offline Analysis Using the TAMER Framework",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3463952.3464219"
      ]
    },
    {
      "abstract": "In this work, we propose a framework that enables a human to teach a robot a new task by interactively providing it with unlabeled instructions. We ground the meaning of instruction signals in the task-learning process, and use them simultaneously for guiding the latter. We implement our framework as a modular architecture, named TICS (Task-Instruction-Contingency-Shaping) that combines different information sources: a predefined reward function, human evaluative feedback and unlabeled instructions. This approach provides a novel perspective for robotic task learning that lies between Reinforcement Learning and Supervised Learning paradigms. We evaluate our framework both in simulation and with a real robot. The experimental results demonstrate the effectiveness of our framework in accelerating the task-learning process and in reducing the number of required teaching signals.",
      "authors": [
        "Anis Najar",
        "Olivier Sigaud",
        "Mohamed Chetouani"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3463952.3464220",
      "keywords": [
        "human-robot interaction",
        "reinforcement learning",
        "unlabeled instructions",
        "interactive machine learning"
      ],
      "number_of_pages": 2,
      "pages": "1738-1739",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450383073",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems"
      },
      "publication_date": "2021-05-03",
      "selected": null,
      "title": "Teaching a Robot with Unlabeled Instructions: The TICS Architecture",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3463952.3464220"
      ]
    },
    {
      "abstract": "Peer-reviewed scientific journal publishing basic neuroscience research in the areas of neuronal plasticity, learning and memory",
      "authors": [
        "Farah Bader",
        "Martin Wiener"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1101/lm.053108.120",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "171-177",
      "publication": {
        "category": "Journal",
        "cite_score": 4.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10720502",
        "publisher": "Cold Spring Harbor Laboratory Press",
        "sjr": 0.926,
        "snip": 0.754,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Learning and Memory"
      },
      "publication_date": "2021-05-01",
      "selected": null,
      "title": "Awareness of errors and feedback in human time estimation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104379527&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rommerskirchen L."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.13789",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2021-05-01",
      "selected": null,
      "title": "The reward positivity reflects the integrated value of temporally threefold-layered decision outcomes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101813653&origin=inward"
      ]
    },
    {
      "abstract": "As more machine learning agents interact with humans, it is increasingly a prospect that an agent trained to perform a task optimally, using only a measure of task performance as feedback, can violate societal norms for acceptable behavior or cause harm. Value alignment is a property of intelligent agents wherein they solely pursue non-harmful behaviors or human-beneficial goals. We introduce an approach to value-aligned reinforcement learning, in which we train an agent with two reward signals: a standard task performance reward, plus a normative behavior reward. The normative behavior reward is derived from a value-aligned prior model previously shown to classify text as normative or non-normative. We show how variations on a policy shaping technique can balance these two sources of reward and produce policies that are both effective and perceived as being more normative. We test our value-alignment technique on three interactive text-based worlds; each world is designed specifically to challenge agents with a task as well as provide opportunities to deviate from the task to engage in normative and/or altruistic behavior.",
      "authors": [
        "Nahian, Md Sultan Al",
        "Frazier, Spencer",
        "Harrison, Brent",
        "Riedl, Mark"
      ],
      "categories": null,
      "citations": null,
      "comments": "(Nahian and Frazier contributed equally to this work)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-04-19",
      "selected": null,
      "title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior",
      "urls": [
        "http://arxiv.org/pdf/2104.09469v1",
        "http://arxiv.org/abs/2104.09469v1",
        "http://arxiv.org/pdf/2104.09469.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Don H.J."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/xlm0000896",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "1311-1327",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02787393",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Experimental Psychology: Learning Memory and Cognition"
      },
      "publication_date": "2021-04-19",
      "selected": null,
      "title": "Frequency Effects in Action Versus Value Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109027760&origin=inward"
      ]
    },
    {
      "abstract": "Deep reinforcement learning (DRL) has achieved great successes in many simulated tasks. The sample inefficiency problem makes applying traditional DRL methods to real-world robots a great challenge. Generative Adversarial Imitation Learning (GAIL) -- a general model-free imitation learning method, allows robots to directly learn policies from expert trajectories in large environments. However, GAIL shares the limitation of other imitation learning methods that they can seldom surpass the performance of demonstrations. In this paper, to address the limit of GAIL, we propose GAN-Based Interactive Reinforcement Learning (GAIRL) from demonstration and human evaluative feedback by combining the advantages of GAIL and interactive reinforcement learning. We tested our proposed method in six physics-based control tasks, ranging from simple low-dimensional control tasks -- Cart Pole and Mountain Car, to difficult high-dimensional tasks -- Inverted Double Pendulum, Lunar Lander, Hopper and HalfCheetah. Our results suggest that with both optimal and suboptimal demonstrations, a GAIRL agent can always learn a more stable policy with optimal or close to optimal performance, while the performance of the GAIL agent is upper bounded by the performance of demonstrations or even worse than it. In addition, our results indicate the reason that GAIRL is superior over GAIL is the complementary effect of demonstrations and human evaluative feedback.",
      "authors": [
        "Huang, Jie",
        "Juan, Rongshun",
        "Gomez, Randy",
        "Nakamura, Keisuke",
        "Sha, Qixin",
        "He, Bo",
        "Li, Guangliang"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-04-14",
      "selected": null,
      "title": "GAN-Based Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback",
      "urls": [
        "http://arxiv.org/pdf/2104.06600v1",
        "http://arxiv.org/abs/2104.06600v1",
        "http://arxiv.org/pdf/2104.06600.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Deng Z."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.knosys.2021.106854",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 12.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09507051",
        "publisher": "Elsevier B.V.",
        "sjr": 2.065,
        "snip": 2.578,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Management Information Systems",
          "Information Systems and Management"
        ],
        "title": "Knowledge-Based Systems"
      },
      "publication_date": "2021-04-06",
      "selected": null,
      "title": "Build complementary models on human feedback for simulation to the real world",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100974934&origin=inward"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning agents use human feedback or instruction to help them learn in complex environments. Often, this feedback comes in the form of a discrete signal that is either positive or negative. While informative, this information can be difficult to generalize on its own. In this work, we explore how natural language advice can be used to provide a richer feedback signal to a reinforcement learning agent by extending policy shaping, a well-known Interactive reinforcement learning technique. Usually policy shaping employs a human feedback policy to help an agent to learn more about how to achieve its goal. In our case, we replace this human feedback policy with policy generated based on natural language advice. We aim to inspect if the generated natural language reasoning provides support to a deep reinforcement learning agent to decide its actions successfully in any given environment. So, we design our model with three networks: first one is the experience driven, next is the advice generator and third one is the advice driven. While the experience driven reinforcement learning agent chooses its actions being influenced by the environmental reward, the advice driven neural network with generated feedback by the advice generator for any new state selects its actions to assist the reinforcement learning agent to better policy shaping.",
      "authors": [
        "Tasrin, Tasmia",
        "Nahian, Md Sultan Al",
        "Perera, Habarakadage",
        "Harrison, Brent"
      ],
      "categories": null,
      "citations": null,
      "comments": "7 pages, 6 figures, The 34th International FLAIRS Conference, 2021",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-04-04",
      "selected": null,
      "title": "Influencing Reinforcement Learning through Natural Language Guidance",
      "urls": [
        "http://arxiv.org/pdf/2104.01506.pdf",
        "http://arxiv.org/pdf/2104.01506v2",
        "http://arxiv.org/abs/2104.01506v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Fr\u00f6mer R."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7554/ELIFE.62825",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "eLife"
      },
      "publication_date": "2021-04-01",
      "selected": null,
      "title": "Response-based outcome predictions and confidence regulate feedback processing and learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85105707248&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rubin J.E."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/ejn.14745",
      "keywords": [],
      "number_of_pages": 20,
      "pages": "2234-2253",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0953816X",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.044,
        "snip": 0.891,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "European Journal of Neuroscience"
      },
      "publication_date": "2021-04-01",
      "selected": null,
      "title": "The credit assignment problem in cortico-basal ganglia-thalamic networks: A review, a problem and a possible solution",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085077512&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement Learning (RL) provides effective results with an agent learning from a stand-alone reward function. However, it presents unique challenges with large amounts of environment states and action spaces, as well as in the determination of rewards. Imitation Learning (IL) offers a promising solution for those challenges using a teacher. In IL, the learning process can take advantage of human-sourced assistance and/or control over the agent and environment. A human teacher and an agent learner are considered in this study. The teacher takes part in the agent\u2019s training towards dealing with the environment, tackling a specific objective, and achieving a predefined goal. This paper proposes a novel approach combining IL with different types of RL methods, namely, state-action-reward-state-action (SARSA) and Asynchronous Advantage Actor\u2013Critic Agents (A3C), to overcome the problems of both stand-alone systems. How to effectively leverage the teacher\u2019s feedback\u2014be it direct binary or indirect detailed\u2014for the agent learner to learn sequential decision-making policies is addressed. The results of this study on various OpenAI-Gym environments show that this algorithmic method can be incorporated with different combinations, and significantly decreases both human endeavors and tedious exploration process.",
      "authors": [
        "Navidi, Neda",
        "Landry, Rene"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/app11073068",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2076-3417",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Applied Sciences (Switzerland)"
      },
      "publication_date": "2021-04-01",
      "selected": null,
      "title": "New Approach in Human-AI Interaction by Reinforcement-Imitation Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103567382&origin=inward",
        "https://www.mdpi.com/2076-3417/11/7/3068/pdf?version=1617938093"
      ]
    },
    {
      "abstract": "Purpose Existing virtual agents (VAs) present in dialogue systems are either information retrieval based or static goal-driven. However, in real-world situations, end-users might not have a known and fixed goal beforehand for the task, i.e., they may upgrade/downgrade/update their goal components in real-time to maximize their utility values. Existing VAs are unable to handle such dynamic goal-oriented situations.   Methodology Due to the absence of any related dialogue dataset where such choice deviations are present, we have created a conversational dataset called Deviation adapted Virtual Agent(DevVA), with the manual annotation of its corresponding intents, slots, and sentiment labels. A Dynamic Goal Driven Dialogue Agent (DGDVA) has been developed by incorporating a Dynamic Goal Driven Module (GDM) on top of a deep reinforcement learning based dialogue manager. In the course of a conversation, the user sentiment provides grounded feedback about agent behavior, including goal serving action. User sentiment appears to be an appropriate indicator for goal discrepancy that guides the agent to complete the user\u2019s desired task with gratification. The negative sentiment expressed by the user about an aspect of the provided choice is treated as a discrepancy that is being resolved by the GDM depending upon the observed discrepancy and current dialogue state. The goal update capability and the VA\u2019s interactiveness trait enable end-users to accomplish their desired task satisfactorily.   Findings The obtained experimental results illustrate that DGDVA can handle dynamic goals with maximum user satisfaction and a significantly higher success rate. The interaction drives the user to decide its final goal through the latent specification of possible choices and information retrieved and provided by the dialogue agent. Through the experimental results (qualitative and quantitative), we firmly conclude that the proposed sentiment-aware VA adapts users\u2019 dynamic behavior for its goal setting with substantial efficacy in terms of primary objective i.e., task success rate (0.88).   Practical implications In real world, it can be argued that many people do not have a predefined and fixed goal for tasks such as online shopping, movie booking & restaurant booking, etc. They tend to explore the available options first which are aligned with their minimum requirements and then decide one amongst them. The DGDVA provides maximum user satisfaction as it enables them to accomplish a dynamic goal that leads to additional utilities along with the essential ones.   Originality To the best of our knowledge, this is the first effort towards the development of A Dynamic Goal Adapted Task-Oriented Dialogue Agent that can serve user goals dynamically until the user is satisfied.",
      "authors": [
        "Abhisek Tiwari",
        "Tulika Saha",
        "Sriparna Saha",
        "Shubhashis Sengupta",
        "Anutosh Maitra",
        "Roshni Ramnani",
        "Pushpak Bhattacharyya"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0249030",
      "keywords": [
        "Neurons",
        "Vanilla",
        "Learning",
        "Language",
        "Decision making",
        "Graphs",
        "Memory",
        "Natural language"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2021-04-01",
      "selected": null,
      "title": "A dynamic goal adapted task oriented dialogue agent",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103779996&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0249030&type=printable"
      ]
    },
    {
      "abstract": "Thanks to the rapid growth in wearable technologies, monitoring complex human\ncontext becomes feasible, paving the way to develop human-in-the-loop IoT\nsystems that naturally evolve to adapt to the human and environment state\nautonomously. Nevertheless, a central challenge in designing such personalized\nIoT applications arises from human variability. Such variability stems from the\nfact that different humans exhibit different behaviors when interacting with\nIoT applications (intra-human variability), the same human may change the\nbehavior over time when interacting with the same IoT application (inter-human\nvariability), and human behavior may be affected by the behaviors of other\npeople in the same environment (multi-human variability). To that end, we\npropose FaiR-IoT, a general reinforcement learning-based framework for adaptive\nand fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three\nlevels of reinforcement learning agents interact to continuously learn human\npreferences and maximize the system's performance and fairness while taking\ninto account the intra-, inter-, and multi-human variability. We validate the\nproposed framework on two applications, namely (i) Human-in-the-Loop Automotive\nAdvanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House.\nResults obtained on these two applications validate the generality of FaiR-IoT\nand its ability to provide a personalized experience while enhancing the\nsystem's performance by 40%-60% compared to non-personalized systems and\nenhancing the fairness of the multi-human systems by 1.5 orders of magnitude.",
      "authors": [
        "Salma Elmalaki"
      ],
      "categories": null,
      "citations": null,
      "comments": "14 pages",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1145/3450268.3453525",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Multiagent Systems",
          "Learning"
        ],
        "title": "IoTDI 2021: Proceedings of the International Conference on\n  Internet-of-Things Design and Implementation"
      },
      "publication_date": "2021-03-30",
      "selected": null,
      "title": "FaiR-IoT: Fairness-aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT",
      "urls": [
        "http://dx.doi.org/10.1145/3450268.3453525",
        "http://arxiv.org/abs/2103.16033v1",
        "http://arxiv.org/pdf/2103.16033v1"
      ]
    },
    {
      "abstract": "The inputs and preferences of human users are important considerations in situations where these users interact with autonomous cyber or cyber-physical systems. In these scenarios, one is often interested in aligning behaviors of the system with the preferences of one or more human users. Cumulative prospect theory (CPT) is a paradigm that has been empirically shown to model a tendency of humans to view gains and losses differently. In this paper, we consider a setting where an autonomous agent has to learn behaviors in an unknown environment. In traditional reinforcement learning, these behaviors are learned through repeated interactions with the environment by optimizing an expected utility. In order to endow the agent with the ability to closely mimic the behavior of human users, we optimize a CPT-based cost. We introduce the notion of the CPT-value of an action taken in a state, and establish the convergence of an iterative dynamic programming-based approach to estimate this quantity. We develop two algorithms to enable agents to learn policies to optimize the CPT-vale, and evaluate these algorithms in environments where a target state has to be reached while avoiding obstacles. We demonstrate that behaviors of the agent learned using these algorithms are better aligned with that of a human user who might be placed in the same environment, and is significantly improved over a baseline that optimizes an expected utility.",
      "authors": [
        "Ramasubramanian, Bhaskar",
        "Niu, Luyao",
        "Clark, Andrew",
        "Poovendran, Radha"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-03-29",
      "selected": null,
      "title": "Reinforcement Learning Beyond Expectation",
      "urls": [
        "http://arxiv.org/abs/2104.00540v1",
        "http://arxiv.org/pdf/2104.00540.pdf",
        "http://arxiv.org/pdf/2104.00540v1"
      ]
    },
    {
      "abstract": "This paper presents a personalized adaptive cruise control (PACC) design that\ncan learn driver behavior and adaptively control the semi-autonomous vehicle\n(SAV) in the car-following scenario, and investigates its impacts on mixed\ntraffic. In mixed traffic where the SAV and human-driven vehicles share the\nroad, the SAV's driver can choose a PACC tuning that better fits the driver's\npreferred driving behaviors. The individual driver's preferences are learned\nthrough the inverse reinforcement learning (IRL) approach by recovering a\nunique cost function from the driver's demonstrated driving data that best\nexplains the observed driving style. The proposed PACC design plans the motion\nof the SAV by minimizing the learned unique cost function considering the short\npreview information of the preceding human-driven vehicle. The results reveal\nthat the learned driver model can identify and replicate the personalized\ndriving behaviors accurately and consistently when following the preceding\nvehicle in a variety of traffic conditions. Furthermore, we investigated the\nimpacts of the PACC with different drivers on mixed traffic by considering time\nheadway, gap distance, and fuel economy assessments. A statistical\ninvestigation shows that the impacts of the PACC on mixed traffic vary among\ntested drivers due to their intrinsic driving preferences.",
      "authors": [
        "Mehmet Ozkan",
        "Yao Ma"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to 2021 American Control Conference (ACC)",
      "databases": [
        "arXiv"
      ],
      "doi": "10.23919/ACC50511.2021.9482812",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-03-26",
      "selected": null,
      "title": "Personalized Adaptive Cruise Control and Impacts on Mixed Traffic",
      "urls": [
        "http://arxiv.org/pdf/2103.14705v1",
        "http://dx.doi.org/10.23919/ACC50511.2021.9482812",
        "http://arxiv.org/abs/2103.14705v1"
      ]
    },
    {
      "abstract": "Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.",
      "authors": [
        "Xianbo Mo",
        "Shunquan Tan",
        "Bin Li",
        "Jiwu Huang"
      ],
      "categories": null,
      "citations": null,
      "comments": "accepted by TIFS",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1109/TIFS.2021.3104140",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-03-25",
      "selected": null,
      "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography",
      "urls": [
        "http://dx.doi.org/10.1109/TIFS.2021.3104140",
        "http://arxiv.org/abs/2103.13689v2",
        "http://arxiv.org/pdf/2103.13689v2"
      ]
    },
    {
      "abstract": "Identity recognition plays an important role in ensuring security in our daily life. Biometric-based (especially activity-based) approaches are favored due to their fidelity, universality, and resilience. However, most existing machine learning-based approaches rely on a traditional workflow where models are usually trained once for all, with limited involvement from end-users in the process and neglecting the dynamic nature of the learning process. This makes the models static and can not be updated in time, which usually leads to high false positive or false negative. Thus, in practice, an expert is desired to assist with providing high-quality observations and interpretation of model outputs. It is expedient to combine both advantages of human experts and the computational capability of computers to create a tight-coupling incremental learning process for better performance. In this study, we develop RLTIR, an interactive identity recognition approach based on reinforcement learning, to adjust the identification model by human guidance. We first build a base tree-structured identity recognition model. And an expert is introduced in the model for giving feedback upon model outputs. Then, the model is updated according to strategies that are automatically learned under a designated reinforcement learning framework. To the best of our knowledge, it is the very first attempt to combine human expert knowledge with model learning in the area of identity recognition. The experimental results show that the reinforced interactive identity recognition framework outperforms baseline methods with regard to recognition accuracy and robustness.",
      "authors": [
        "Li, Qingyang",
        "Yu, Zhiwen",
        "Yao, Lina",
        "Guo, Bin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-03-20",
      "selected": null,
      "title": "RLTIR: Activity-based Interactive Person Identification based on Reinforcement Learning Tree",
      "urls": [
        "http://arxiv.org/pdf/2103.11104.pdf",
        "http://arxiv.org/pdf/2103.11104v1",
        "http://arxiv.org/abs/2103.11104v1"
      ]
    },
    {
      "abstract": "<p>The decisions we make are sometimes influenced by interactions with other agents. Previous studies have suggested that the prefrontal cortex plays an important role in decision-making and that the dopamine system underlies processes of motivation, motor preparation, and reinforcement learning. However, the physiological mechanisms underlying how the prefrontal cortex and the dopaminergic system are involved in decision-making remain largely unclear. The present study aimed to determine how decision strategies influence event-related potentials (ERPs). We also tested the effect of levodopa, a dopamine precursor, on decision-making and ERPs in a randomized double-blind placebo-controlled investigation. The subjects performed a matching-pennies task against an opposing virtual computer player by choosing between right and left targets while their ERPs were recorded. According to the rules of the matching-pennies task, the subject won the trial when they chose the same side as the opponent, and lost otherwise. We set three different task rules: (1) with the alternation (ALT) rule, the computer opponent made alternating choices of right and left in sequential trials; (2) with the random (RAND) rule, the opponent randomly chose between right and left; and (3) with the GAME rule, the opponent analyzed the subject\u2019s past choices to predict the subject\u2019s next choice, and then chose the opposite side. A sustained medial ERP became more negative toward the time of the subject\u2019s target choice. A biphasic potential appeared when the opponent\u2019s choice was revealed after the subject\u2019s response. The ERPs around the subject\u2019s choice were greater in RAND and GAME than in ALT, and the negative peak was enhanced by levodopa. In addition to these medial ERPs, we observed lateral frontal ERPs tuned to the choice direction. The signals emerged around the choice period selectively in RAND and GAME when levodopa was administered. These results suggest that decision processes are modulated by the dopamine system when a complex and strategic decision is required, which may reflect decision updating with dopaminergic prediction error signals.</p>",
      "authors": [
        "Chang, Fang-Yu",
        "Wiratman, Winnugroho",
        "Ugawa, Yoshikazu",
        "Kobayashi, Shunsuke"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2021.552750",
      "keywords": [
        "Executive Function",
        "Bereitschaftspotential (BP)",
        "Mixed-Strategy",
        "Game theory",
        "Feedback",
        "decision-making",
        "readiness potential",
        "Event-related potentials",
        "high-density EEG",
        "Prefrontal Cortex",
        "Parkinson Disease",
        "Levodopa"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2021-03-19",
      "selected": null,
      "title": "Event-Related Potentials During Decision-Making in a Mixed-Strategy Game",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2021.552750/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103554813&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Staley J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3434074.3447164",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "225-228",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450382908",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2021-03-08",
      "selected": null,
      "title": "Contingency detection in multi-agent interactions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102751081&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Yu H."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3434074.3447207",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "430-433",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450382908",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2021-03-08",
      "selected": null,
      "title": "Active feedback learning with rich feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102744085&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kessler Faulkner T.A."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3434074.3446361",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "577-579",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450382908",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2021-03-08",
      "selected": null,
      "title": "Interactive reinforcement learning from imperfect teachers",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102763600&origin=inward"
      ]
    },
    {
      "abstract": "When a robot is deployed to learn a new task in a \"real-word\" environment, there may be multiple teachers and therefore multiple sources of feedback. Furthermore, there may be multiple optimal solutions for a given task and teachers may have preferences among those various solutions. We present an Interactive Reinforcement Learning (I-RL) algorithm, Multi-Teacher Activated Policy Shaping (M-TAPS), which addresses the problem of learning from multiple teachers and leverages differences between them as a means to explore the environment. We show that this algorithm can significantly increase an agent's robustness to the environment and quickly adopt to a teacher's preferences. Finally, we present a formal model for comparing human teachers and constructed oracle teachers and the way that they provide feedback to a robot.",
      "authors": [
        "Isaac S. Sheidlower",
        "Elaine Schaertl Short"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3434074.3447189",
      "keywords": [
        "human-robot interaction",
        "interactive reinforcement learning"
      ],
      "number_of_pages": 5,
      "pages": "344-348",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450382908",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2021-03-08",
      "selected": null,
      "title": "When Oracles Go Wrong: Using Preferences as a Means to Explore",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102735335&origin=inward",
        "https://dl.acm.org/doi/10.1145/3434074.3447189"
      ]
    },
    {
      "abstract": "Machine learning is a fast-growing field with various applications in artificial intelligence and data science. Recently, a new machine learning program have been integrated into the Israeli high school computer science curriculum and thus we added a new machine learning module to the Methods of Teaching Computer Science (MTCS) course, which is part of the teachers' preparation program. This machine learning module provides us a unique opportunity to teach both pedagogy and content with the same subject matter. After teaching the basics of machine learning, we asked the students to find similarities between human learning theories and machine learning algorithms. Students identified several interesting parallels: (a) Supervised learning is similar to behavioral learning as the machine learns to connect training examples (stimuli) with labels (behavior). Also, the learning is based on minimizing error (punishment) function, (b) Reinforcement learning is similar to behavioral learning as learning is based on feedback from the environment, (c) Constructivism can be identified in the iterative convergence of the algorithm; the inner model improves each iteration based on the current knowledge, and (d) Social learning is reflected in clustering as each cluster affects the learning of the other clusters. In our talk, we present the idea that computational mental models may be used to reinforce pedagogical mental models and vice versa.",
      "authors": [
        "Koby Mike",
        "Rinat B. Rosenberg-Kima"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1145/3408877.3439550",
      "keywords": [
        "learning theories",
        "machine learning",
        "teachers training"
      ],
      "number_of_pages": 1,
      "pages": "1368",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450380621",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 52nd ACM Technical Symposium on Computer Science Education"
      },
      "publication_date": "2021-03-05",
      "selected": null,
      "title": "Teaching Machine Learning to Computer Science Preservice Teachers: Human vs. Machine Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3408877.3439550"
      ]
    },
    {
      "abstract": "Text-based image retrieval has seen considerable progress in recent years. However, the performance of existing methods suffers in real life since the user is likely to provide an incomplete description of an image, which often leads to results filled with false positives that fit the incomplete description. In this work, we introduce the partial-query problem and extensively analyze its influence on text-based image retrieval. Previous interactive methods tackle the problem by passively receiving users' feedback to supplement the incomplete query iteratively, which is time-consuming and requires heavy user effort. Instead, we propose a novel retrieval framework that conducts the interactive process in an Ask-and-Confirm fashion, where AI actively searches for discriminative details missing in the current query, and users only need to confirm AI's proposal. Specifically, we propose an object-based interaction to make the interactive retrieval more user-friendly and present a reinforcement-learning-based policy to search for discriminative objects. Furthermore, since fully-supervised training is often infeasible due to the difficulty of obtaining human-machine dialog data, we present a weakly-supervised training strategy that needs no human-annotated dialogs other than a text-image dataset. Experiments show that our framework significantly improves the performance of text-based image retrieval. Code is avaiable at https://github.com/CuthbertCai/Ask-Confirm.",
      "authors": [
        "Cai, Guanyu",
        "Zhang, Jun",
        "Jiang, Xinyang",
        "Gong, Yifei",
        "He, Lianghua",
        "Yu, Fufu",
        "Peng, Pai",
        "Guo, Xiaowei",
        "Huang, Feiyue",
        "Sun, Xing"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted by ICCV2021",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-03-02",
      "selected": null,
      "title": "Ask&Confirm: Active Detail Enriching for Cross-Modal Retrieval with Partial Query",
      "urls": [
        "http://arxiv.org/abs/2103.01654v2",
        "http://arxiv.org/pdf/2103.01654.pdf",
        "http://arxiv.org/pdf/2103.01654v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Samyoun S."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.smhl.2020.100171",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Smart Health"
      },
      "publication_date": "2021-03-01",
      "selected": null,
      "title": "iWash: A smartwatch handwashing quality assessment and reminder system with real-time feedback in the context of infectious disease",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098208393&origin=inward"
      ]
    },
    {
      "abstract": "Slow-timescale (tonic) changes in dopamine (DA) contribute to a wide variety of processes in reinforcement learning, interval timing, and other domains. Furthermore, changes in tonic DA exert distinct effects depending on when they occur (e.g., during learning vs. performance) and what task the subject is performing (e.g., operant vs. classical conditioning). Two influential theories of tonic DA\u2014the average reward theory and the Bayesian theory in which DA controls precision\u2014have each been successful at explaining a subset of empirical findings. But how the same DA signal performs two seemingly distinct functions without creating crosstalk is not well understood. Here we reconcile the two theories under the unifying framework of \u2018rational inattention,\u2019 which (1) conceptually links average reward and precision, (2) outlines how DA manipulations affect this relationship, and in so doing, (3) captures new empirical phenomena. In brief, rational inattention asserts that agents can increase their precision in a task (and thus improve their performance) by paying a cognitive cost. Crucially, whether this cost is worth paying depends on average reward availability, reported by DA. The monotonic relationship between average reward and precision means that the DA signal contains the information necessary to retrieve the precision. When this information is needed after the task is performed, as presumed by Bayesian inference, acute manipulations of DA will bias behavior in predictable ways. We show how this framework reconciles a remarkably large collection of experimental findings. In reinforcement learning, the rational inattention framework predicts that learning from positive and negative feedback should be enhanced in high and low DA states, respectively, and that DA should tip the exploration-exploitation balance toward exploitation. In interval timing, this framework predicts that DA should increase the speed of the internal clock and decrease the extent of interference by other temporal stimuli during temporal reproduction (the central tendency effect). Finally, rational inattention makes the new predictions that these effects should be critically dependent on the controllability of rewards, that post-reward delays in intertemporal choice tasks should be underestimated, and that average reward manipulations should affect the speed of the clock\u2014thus capturing empirical findings that are unexplained by either theory alone. Our results suggest that a common computational repertoire may underlie the seemingly heterogeneous roles of DA.",
      "authors": [
        "John G. Mikhael",
        "Lucy Lai",
        "Samuel J. Gershman"
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1008659",
      "keywords": [
        "Learning",
        "Behavior",
        "Decision making",
        "Simulation and modeling",
        "Arms",
        "Entropy",
        "Animal behavior",
        "Control theory"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2021-03-01",
      "selected": null,
      "title": "Rational inattention and tonic dopamine",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103513244&origin=inward",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1008659&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Garcia B."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1098/rstb.2019.0665",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 12.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09628436",
        "publisher": "The Royal Society",
        "sjr": 2.084,
        "snip": 1.629,
        "subject_areas": [
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Agricultural and Biological Sciences (all)"
        ],
        "title": "Philosophical Transactions of the Royal Society B: Biological Sciences"
      },
      "publication_date": "2021-03-01",
      "selected": null,
      "title": "The description-experience gap: A challenge for the neuroeconomics of decision-making under uncertainty",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099541676&origin=inward"
      ]
    },
    {
      "abstract": "Smart homes are becoming increasingly popular as a result of advances in machine learning and cloud computing. Devices, such as smart thermostats and speakers, are now capable of learning from user feedback and adaptively adjust their settings to human preferences. Nonetheless, these devices might in turn impact human behavior. To investigate the potential impacts of smart homes on human behavior, we simulate a series of hierarchical-reinforcement learning-based human models capable of performing various activities\u2014namely, setting temperature and humidity for thermal comfort inside a Q-Learning-based smart home model. We then investigate the possibility of the human models\u2019 behaviors being altered as a result of the smart home and the human model adapting to one another. For our human model, the activities are based on hierarchical-reinforcement learning. This allows the human to learn how long it must continue a given activity and decide when to leave it. We then integrate our human model in the environment along with the smart home model and perform rigorous experiments considering various scenarios involving a model of a single human and models of two different humans with the smart home. Our experiments show that with the smart home, the human model can exhibit unexpected behaviors such as frequent changing of activities and an increase in the time required to modify the thermal preferences. With two human models, we interestingly observe that certain combinations of models result in normal behaviors, while other combinations exhibit the same unexpected behaviors as those observed from the single human experiment.",
      "authors": [
        "Shashi Suman",
        "Ali Etemad",
        "Francois Rivest"
      ],
      "categories": null,
      "citations": 8,
      "comments": "in IEEE Transactions on Artificial Intelligence",
      "databases": [
        "IEEE",
        "arXiv",
        "Scopus"
      ],
      "doi": "10.1109/TAI.2021.3127483",
      "keywords": [
        "Q-Learning",
        "smart home",
        "hierarchical reinforce ment learning",
        "AI-human interaction"
      ],
      "number_of_pages": 14,
      "pages": "567-580",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2691-4581",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Artificial Intelligence"
      },
      "publication_date": "2021-02-26",
      "selected": null,
      "title": "Potential Impacts of Smart Homes on Human Behavior: A Reinforcement Learning Approach",
      "urls": [
        "http://arxiv.org/abs/2102.13307v3",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9612040",
        "http://arxiv.org/pdf/2102.13307v3",
        "http://dx.doi.org/10.1109/TAI.2021.3127483",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85135324200&origin=inward"
      ]
    },
    {
      "abstract": "Persuasive argumentation depends on multiple aspects, which include not only the content of the individual arguments, but also the way they are presented. The presentation of arguments is crucial \u2013 in particular in the context of dialogical argumentation. However, the effects of different discussion styles on the listener are hard to isolate in human dialogues. In order to demonstrate and investigate various styles of argumentation, we propose a multi-agent system in which different aspects of persuasion can be modelled and investigated separately. Our system utilizes argument structures extracted from text-based reviews for which a minimal bias of the user can be assumed. The persuasive dialogue is modelled as a dialogue game for argumentation that was motivated by the objective to enable both natural and flexible interactions between the agents. In order to support a comparison of factual against affective persuasion approaches, we implemented two fundamentally different strategies for both agents: The logical policy utilizes deep Reinforcement Learning in a multi-agent setup to optimize the strategy with respect to the game formalism and the available argument. In contrast, the emotional policy selects the next move in compliance with an agent emotion that is adapted to user feedback to persuade on an emotional level. The resulting interaction is presented to the user via virtual avatars and can be rated through an intuitive interface.",
      "authors": [
        "Niklas Rach",
        "Klaus Weber",
        "Yuchi Yang",
        "Stefan Ultes",
        "Elisabeth Andr\u00e9",
        "Wolfgang Minker"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1515/itit-2020-0050",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "17-30",
      "publication": {
        "category": "Journal",
        "cite_score": 3.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "16112776",
        "publisher": "De Gruyter Oldenbourg",
        "sjr": 0.289,
        "snip": 0.454,
        "subject_areas": [
          "Computer Science (all)"
        ],
        "title": "IT - Information Technology"
      },
      "publication_date": "2021-02-23",
      "selected": null,
      "title": "EVA 2.0: Emotional and rational multimodal argumentation between virtual agents",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104456531&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Xu M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3760/cma.j.cn321463-20200611-00172",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "107-114",
      "publication": {
        "category": "Journal",
        "cite_score": 0.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10075232",
        "publisher": "Chinese Medical Journals Publishing House Co.Ltd",
        "sjr": 0.101,
        "snip": 0.025,
        "subject_areas": [
          "Radiology, Nuclear Medicine and Imaging",
          "Gastroenterology"
        ],
        "title": "Chinese Journal of Digestive Endoscopy"
      },
      "publication_date": "2021-02-20",
      "selected": null,
      "title": "Evaluation of performance measurement system of gastrointestinal endoscopy based on deep learning (with video)",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160289752&origin=inward"
      ]
    },
    {
      "abstract": "In this paper, we consider a type of image quality assessment as a task-specific measurement, which can be used to select images that are more amenable to a given target task, such as image classification or segmentation. We propose to train simultaneously two neural networks for image selection and a target task using reinforcement learning. A controller network learns an image selection policy by maximising an accumulated reward based on the target task performance on the controller-selected validation set, whilst the target task predictor is optimised using the training set. The trained controller is therefore able to reject those images that lead to poor accuracy in the target task. In this work, we show that the controller-predicted image quality can be significantly different from the task-specific image quality labels that are manually defined by humans. Furthermore, we demonstrate that it is possible to learn effective image quality assessment without using a ``clean'' validation set, thereby avoiding the requirement for human labelling of images with respect to their amenability for the task. Using $6712$, labelled and segmented, clinical ultrasound images from $259$ patients, experimental results on holdout data show that the proposed image quality assessment achieved a mean classification accuracy of $0.94\\pm0.01$ and a mean segmentation Dice of $0.89\\pm0.02$, by discarding $5\\%$ and $15\\%$ of the acquired images, respectively. The significantly improved performance was observed for both tested tasks, compared with the respective $0.90\\pm0.01$ and $0.82\\pm0.02$ from networks without considering task amenability. This enables image quality feedback during real-time ultrasound acquisition among many other medical imaging applications.",
      "authors": [
        "Saeed, Shaheer U.",
        "Fu, Yunguan",
        "Baum, Zachary M. C.",
        "Yang, Qianye",
        "Rusu, Mirabela",
        "Fan, Richard E.",
        "Sonn, Geoffrey A.",
        "Barratt, Dean C.",
        "Hu, Yipeng"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted at IPMI 2021 (The 27th international conference on\n  Information Processing in Medical Imaging)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-02-15",
      "selected": null,
      "title": "Learning image quality assessment by reinforcing task amenable data selection",
      "urls": [
        "http://arxiv.org/pdf/2102.07615.pdf",
        "http://arxiv.org/abs/2102.07615v1",
        "http://arxiv.org/pdf/2102.07615v1"
      ]
    },
    {
      "abstract": "<p>Prior research has used an Interactive Landslide Simulator (ILS) tool to investigate human decision making against landslide risks. It has been found that repeated feedback in the ILS tool about damages due to landslides causes an improvement in human decisions against landslide risks. However, little is known on how theories of learning from feedback (e.g., reinforcement learning) would account for human decisions in the ILS tool. The primary goal of this paper is to account for human decisions in the ILS tool via computational models based upon reinforcement learning and to explore the model mechanisms involved when people make decisions in the ILS tool. Four different reinforcement-learning models were developed and evaluated in their ability to capture human decisions in an experiment involving two conditions in the ILS tool. The parameters of an Expectancy-Valence (EV) model, two Prospect-Valence-Learning models (PVL and PVL-2), a combination EV-PU model, and a random model were calibrated to human decisions in the ILS tool across the two conditions. Later, different models with their calibrated parameters were generalized to data collected in an experiment involving a new condition in ILS. When generalized to this new condition, the PVL-2 model\u2019s parameters of both damage-feedback conditions outperformed all other RL models (including the random model). We highlight the implications of our results for decision making against landslide risks.</p>",
      "authors": [
        "Chaturvedi, Pratik",
        "Dutt, Varun"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyg.2020.499422",
      "keywords": [
        "Expectancy-valence model",
        "Prospect-Valence-Learning model",
        "reinforcement learning",
        "decision-making",
        "Interactive Landslide Simulator",
        "damage-feedback"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-1078",
        "publisher": "Frontiers Media SA",
        "sjr": 0.891,
        "snip": 1.422,
        "subject_areas": [
          "Psychology (all)"
        ],
        "title": "Frontiers in Psychology"
      },
      "publication_date": "2021-02-10",
      "selected": null,
      "title": "Understanding Human Decision Making in an Interactive Landslide Simulator Tool via Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101544912&origin=inward",
        "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.499422/pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, this article argues that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, again lowering the bar so that more researchers can quickly investigate different ways that human teachers could assist RL agents, including learning from demonstrations, learning from feedback, or curriculum learning.",
      "authors": [
        "Taylor, Matthew E.",
        "Nissen, Nicholas",
        "Wang, Yuan",
        "Navidi, Neda"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-02-02",
      "selected": null,
      "title": "Improving Reinforcement Learning with Human Assistance: An Argument for Human Subject Studies with HIPPO Gym",
      "urls": [
        "http://arxiv.org/pdf/2102.02639.pdf",
        "http://arxiv.org/abs/2102.02639v1",
        "http://arxiv.org/pdf/2102.02639v1"
      ]
    },
    {
      "abstract": "<p><bold>Introduction:</bold> Many adults do not reach the recommended physical activity (PA) guidelines, which can lead to serious health problems. A promising method to increase PA is the use of smartphone PA applications. However, despite the development and evaluation of multiple PA apps, it remains unclear how to develop and design engaging and effective PA apps. Furthermore, little is known on ways to harness the potential of artificial intelligence for developing personalized apps. In this paper, we describe the design and development of the Playful data-driven Active Urban Living (PAUL): a personalized PA application.</p><p><bold>Methods:</bold> The two-phased development process of the PAUL apps rests on principles from the behavior change model; the Integrate, Design, Assess, and Share (IDEAS) framework; and the behavioral intervention technology (BIT) model. During the first phase, we explored whether location-specific information on performing PA in the built environment is an enhancement to a PA app. During the second phase, the other modules of the app were developed. To this end, we first build the theoretical foundation for the PAUL intervention by performing a literature study. Next, a focus group study was performed to translate the theoretical foundations and the needs and wishes in a set of user requirements. Since the participants indicated the need for reminders at a for-them-relevant moment, we developed a self-learning module for the timing of the reminders. To initialize this module, a data-mining study was performed with historical running data to determine good situations for running.</p><p><bold>Results:</bold> The results of these studies informed the design of a personalized mobile health (mHealth) application for running, walking, and performing strength exercises. The app is implemented as a set of modules based on the persuasive strategies \u201cmonitoring of behavior,\u201d \u201cfeedback,\u201d \u201cgoal setting,\u201d \u201creminders,\u201d \u201crewards,\u201d and \u201cproviding instruction.\u201d An architecture was set up consisting of a smartphone app for the user, a back-end server for storage and adaptivity, and a research portal to provide access to the research team.</p><p><bold>Conclusions:</bold> The interdisciplinary research encompassing psychology, human movement sciences, computer science, and artificial intelligence has led to a theoretically and empirically driven leisure time PA application. In the current phase, the feasibility of the PAUL app is being assessed.</p>",
      "authors": [
        "Sporrel, Karlijn",
        "De Boer, R\u00e9mi D. D.",
        "Wang, Shihan",
        "Nibbeling, Nicky",
        "Simons, Monique",
        "Deutekom, Marije",
        "Castro, Paula C.",
        "Dourado, Victor Zuniga",
        "Kr\u00f6se, Ben"
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpubh.2020.528472",
      "keywords": [
        "mHealth",
        "behavior intervention design",
        "data-mining",
        "reinforcement learning",
        "Just-in-time adaptive interventions",
        "Persuasive Technology",
        "behavior change",
        "physical activity"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2296-2565",
        "publisher": "Frontiers Media SA",
        "sjr": 1.125,
        "snip": 1.374,
        "subject_areas": [
          "Public Health, Environmental and Occupational Health"
        ],
        "title": "Frontiers in Public Health"
      },
      "publication_date": "2021-02-02",
      "selected": null,
      "title": "The Design and Development of a Personalized Leisure Time Physical Activity Application Based on Behavior Change Theories, End-User Perceptions, and Principles From Empirical Data Mining",
      "urls": [
        "https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2020.528472/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100727209&origin=inward"
      ]
    },
    {
      "abstract": "Oxytocin has been found to play an important role in human social cognition and social interaction. Over the last two decades, surge studies have been conducted to investigate how oxytocin impacts other-oriented processes, such as trust and generosity (Zak et al. in PLoS ONE 2(11):e1128, 2007); however, the examination of the effect of oxytocin on self-related processes was relatively inadequate. Appropriate and efficient social interactions require both self- and other-related information processing. Recent studies have found that intranasal oxytocin (IN-OT) influences the self-related process, although the results have been mixed. The computational process underlying the effects of IN-OT on self-processing remains unknown. We aim to investigate the effect of IN-OT on self-oriented learning across different contexts (self-other independent vs. self-other dependent) and uncover the process by which IN-OT affects dynamic behavior changes. We performed two double-blind, placebo-controlled studies and used reinforcement learning theory to integrate action and related feedback for participants\u2019 behaviors. In study 1, IN-OT decreased self-oriented reward learning when self-oriented learning and prosocial (other-oriented) learning were assessed separately. These effects were partially due to the OT-related increase in choice variability during self-oriented learning. In study 2, IN-OT also decreased learning performance during self-oriented reward learning when self-related and other-related rewards were present together. These effects occurred at an early stage of the learning process and could be moderated by the participants\u2019 social value orientation. Our findings show that OT attenuates the process of self-oriented learning and provides an underlying computational process. Our findings shed new light on the dynamics of IN-OT\u2019s effects on human self-oriented learning processes. For future studies on OT effects on self-oriented learning, individual factors such as social value orientation should be taken into consideration in research development and analysis.",
      "authors": [
        "Liao, Zhijun",
        "Huang, Liqin",
        "Luo, Siyang"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00213-020-05694-7",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "461-474",
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00333158",
        "publisher": "Springer Verlag",
        "sjr": 1.05,
        "snip": 0.934,
        "subject_areas": [
          "Pharmacology"
        ],
        "title": "Psychopharmacology"
      },
      "publication_date": "2021-02-01",
      "selected": null,
      "title": "Intranasal oxytocin decreases self-oriented learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095718099&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s00213-020-05694-7.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Williams C.C."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.13722",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2021-02-01",
      "selected": null,
      "title": "The ERP, frequency, and time\u2013frequency correlates of feedback processing: Insights from a large sample study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096799378&origin=inward"
      ]
    },
    {
      "abstract": "We propose a new approach towards emotional natural language generation using bidirectional seq2seq model. Our goal is to generate emotionally relevant language that accommodates the emotional tone of the prior context. To incorporate emotional information, we train our own embeddings appended with emotion values through valence, arousal and dominance scores. We use a reinforcement-learning framework, which is tuned using policy gradient method. Two of the internal rewards in our reinforcement learning framework, viz. Ease of Answering and Semantic Coherence are based on prior state-of-the-art. We propose a new internal reward, Emotional Intelligence, computed by minimizing the affective dissonance between the source and generated text. We also train a separate external reward analyzer to predict the rewards as well as to maximize the expected rewards (both internal and external). We evaluate the system on two common corpora used for Natural Language Generation tasks: the Cornell Movie Dialog and Yelp Restaurant Review Corpus. We report standard evaluation metrics including BLEU, ROUGE-L and perplexity as well as human evaluation to validate our approach. We demonstrate the ability of proposed model to generate emotionally appropriate responses on both corpora.",
      "authors": [
        "Srinivasan, Vidhushini",
        "Santhanam, Sashank",
        "Shaikh, Samira"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10844-020-00626-5",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "189-206",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09259902",
        "publisher": "Springer US",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Intelligent Information Systems"
      },
      "publication_date": "2021-02-01",
      "selected": null,
      "title": "Using reinforcement learning with external rewards for open-domain natural language generation",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10844-020-00626-5.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095714759&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Stocco A."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/cogs.12941",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03640213",
        "publisher": "Wiley-Blackwell",
        "sjr": 1.057,
        "snip": 1.257,
        "subject_areas": [
          "Artificial Intelligence",
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive Science"
      },
      "publication_date": "2021-02-01",
      "selected": null,
      "title": "Individual Differences in Reward-Based Learning Predict Fluid Reasoning Abilities",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101540948&origin=inward"
      ]
    },
    {
      "abstract": "BackgroundSeveral studies have reported diminished learning from non-social outcomes in depressed individuals. However, it is not clear how depression impacts learning from social feedback. Notably, mood disorders are commonly associated with deficits in social functioning, which raises the possibility that potential impairments in social learning may negatively affect real-life social experiences in depressed subjects.MethodsNinety-two participants with high (HD; N = 40) and low (LD; N = 52) depression scores were recruited. Subjects performed a learning task, during which they received monetary outcomes or social feedback which they were told came from other people. Additionally, participants answered questions about their everyday social experiences. Computational models were fit to the data and model parameters were related to social experience measures.ResultsHD subjects reported a reduced quality and quantity of social experiences compared to LD controls, including an increase in the amount of time spent in negative social situations. Moreover, HD participants showed lower learning rates than LD subjects in the social condition of the task. Interestingly, across all participants, reduced social learning rates predicted higher amounts of time spent in negative social situations, even when depression scores were controlled for.ConclusionThese findings indicate that deficits in social learning may affect the quality of everyday social experiences. Specifically, the impaired ability to use social feedback to appropriately update future actions, which was observed in HD subjects, may lead to suboptimal interpersonal behavior in real life. This, in turn, may evoke negative feedback from others, thus bringing about more unpleasant social encounters.",
      "authors": [
        "Anna-Lena Frey",
        "Michael J. Frank",
        "Ciara McCabe"
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1017/S0033291719003222",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "408-415",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00332917",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychological Medicine"
      },
      "publication_date": "2021-02-01",
      "selected": null,
      "title": "Social reinforcement learning as a predictor of real-life experiences in individuals with high and low depressive symptomatology",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076621124&origin=inward"
      ]
    },
    {
      "abstract": "Compared with obstacle avoidance in open environment, collision-free path planning for duct-enter task is often challenged by narrow and complex space inside ducts. For obstacle avoidance, redundant robot is usually applied for this task. The motion of redundant robot can be decoupled to end-effector motion and self-motion. Current methods for duct-enter task are not robust due to the difficulty to properly define the self-motion. This difficulty mainly comes from two aspects: the definition of distances from robot to obstacles and the fusion of multiple data. In this work, we adapt the ideas underlying the success of human to handling this kind tasks, variable optimization strategies and learning, for one robust path planner. Proposed planner applies reinforcement learning skills to learn proper self-motion and achieves robust planning. For achieving robust behavior, state-action planner is creatively designed with three especially designed strategies. Firstly, optimization function, the kernel part of self-motion, is considered as part of action. Instead of taking every joint motion, this strategy embeds reinforcement learning skills on self-motion, reducing the search domain to null space of redundant robot. Secondly, robot end orientation is taken into action. For duct-enter task, robot end link is the motion starter for exploring movement just like the snake head. The orientation of robot end link when passing through some position can be referred by following links. Hence the second strategy can accelerate exploring by reduce the null space to possible redundant robot manifold. Thirdly, path guide point is also added into action part. This strategy can divide one long distance task into several short distance tasks, reducing the task difficulty. After these creative designs, the planner has been trained with reinforcement learning skills. With the feedback of robot and environment state, proposed planner can choose proper optimization strategies, just like the human brain, for avoiding collision between robot body and target duct. Compared with two general methods, Virtual Axis method with orientation Guidance and Virtual Axis, experiment results show that the success rate is separately improved by 5.9% and 49.7%. And two different situation experiments are carried out on proposed planner. Proposed planner achieves 100% success rate in the situation with constant start point and achieves 98.7% success rate in the situation with random start point meaning that the proposed planner can handle the perturbation of start point and goal point. The experiments proves the robustness of proposed planner.",
      "authors": [
        "Hua, Xiaotong",
        "Wang, Guolei",
        "Xu, Jing",
        "Chen, Ken"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10845-020-01582-1",
      "keywords": [
        "Path planning",
        "Self-motion",
        "Reinforcement learning",
        "Obstacle avoidance"
      ],
      "number_of_pages": 12,
      "pages": "471-482",
      "publication": {
        "category": "Journal",
        "cite_score": 17.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0956-5515",
        "publisher": "Springer Netherlands",
        "sjr": 2.16,
        "snip": 2.91,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Industrial and Manufacturing Engineering"
        ],
        "title": "Journal of Intelligent Manufacturing"
      },
      "publication_date": "2021-02-01",
      "selected": null,
      "title": "Reinforcement learning-based collision-free path planner for redundant robot in narrow duct",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10845-020-01582-1.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085362011&origin=inward",
        "https://dl.acm.org/doi/10.1007/s10845-020-01582-1"
      ]
    },
    {
      "abstract": "Although deep reinforcement learning has led to breakthroughs in many difficult domains, these successes have required an ever-increasing number of samples, affording only a shrinking segment of the AI community access to their development. Resolution of these limitations requires new, sample-efficient methods. To facilitate research in this direction, we propose this second iteration of the MineRL Competition. The primary goal of the competition is to foster the development of algorithms which can efficiently leverage human demonstrations to drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments. To that end, participants compete under a limited environment sample-complexity budget to develop systems which solve the MineRL ObtainDiamond task in Minecraft, a sequential decision making environment requiring long-term planning, hierarchical control, and efficient exploration methods. The competition is structured into two rounds in which competitors are provided several paired versions of the dataset and environment with different game textures and shaders. At the end of each round, competitors submit containerized versions of their learning algorithms to the AIcrowd platform where they are trained from scratch on a hold-out dataset-environment pair for a total of 4-days on a pre-specified hardware platform. In this follow-up iteration to the NeurIPS 2019 MineRL Competition, we implement new features to expand the scale and reach of the competition. In response to the feedback of the previous participants, we introduce a second minor track focusing on solutions without access to environment interactions of any kind except during test-time. Further we aim to prompt domain agnostic submissions by implementing several novel competition mechanics including action-space randomization and desemantization of observations and actions.",
      "authors": [
        "Guss, William H.",
        "Castro, Mario Ynocente",
        "Devlin, Sam",
        "Houghton, Brandon",
        "Kuno, Noboru Sean",
        "Loomis, Crissman",
        "Milani, Stephanie",
        "Mohanty, Sharada",
        "Nakata, Keisuke",
        "Salakhutdinov, Ruslan",
        "Schulman, John",
        "Shiroshita, Shinya",
        "Topin, Nicholay",
        "Ummadisingu, Avinash",
        "Vinyals, Oriol"
      ],
      "categories": null,
      "citations": null,
      "comments": "37 pages, initial submission, accepted at NeurIPS. arXiv admin note:\n  substantial text overlap with arXiv:1904.10079",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2021-01-26",
      "selected": null,
      "title": "The MineRL 2020 Competition on Sample Efficient Reinforcement Learning using Human Priors",
      "urls": [
        "http://arxiv.org/pdf/2101.11071.pdf",
        "http://arxiv.org/abs/2101.11071v1",
        "http://arxiv.org/pdf/2101.11071v1"
      ]
    },
    {
      "abstract": "<p>Cognitive flexibility helps us to navigate through our ever-changing environment and has often been examined by reversal learning paradigms. Performance in reversal learning can be modeled using computational modeling which allows for the specification of biologically plausible models to infer psychological mechanisms. Although such models are increasingly used in cognitive neuroscience, developmental approaches are still scarce. Additionally, though most reversal learning paradigms have a comparable design regarding timing and feedback contingencies, the type of feedback differs substantially between studies. The present study used hierarchical Gaussian filter modeling to investigate cognitive flexibility in reversal learning in children and adolescents and the effect of various feedback types. The results demonstrate that children make more overall errors and regressive errors (when a previously learned response rule is chosen instead of the new correct response after the initial shift to the new correct target), but less perseverative errors (when a previously learned response set continues to be used despite a reversal) adolescents. Analyses of the extracted model parameters of the winning model revealed that children seem to use new and conflicting information less readily than adolescents to update their stimulus-reward associations. Furthermore, more subclinical rigidity in everyday life (parent-ratings) is related to less explorative choice behavior during the probabilistic reversal learning task. Taken together, this study provides first-time data on the development of the underlying processes of cognitive flexibility using computational modeling.</p>",
      "authors": [
        "Weiss, Eileen Oberwelland",
        "Kruppa, Jana A.",
        "Fink, Gereon R.",
        "Herpertz-Dahlmann, Beate",
        "Konrad, Kerstin",
        "Schulte-R\u00fcther, Martin"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2020.536596",
      "keywords": [
        "development",
        "cognitive flexibility",
        "feedback processing",
        "Reinforcement Leaning",
        "executive functioning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2021-01-18",
      "selected": null,
      "title": "Developmental Differences in Probabilistic Reversal Learning: A Computational Modeling Approach",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2020.536596/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100537384&origin=inward"
      ]
    },
    {
      "abstract": "In this study, we apply Deep Reinforcement Learning for handling full-dimensional 7 degrees of freedom arm reaching, and demonstrate the relations among motion error, energy, and synergy emergence during the learning process, to reveal the mechanism of employing motor synergy. Although synergy information has never been encoded into the reward function, the synergy effect naturally emerges, leading to a similar situation as human motion learning. To the best of our knowledge, this is a pioneer study verifying a concurrent relation between the error-energy index and synergy development in DRL for multi-directional reaching tasks. In addition, our proposed feedback-augmented DRL controller shows better capability over DRL only in terms of error-energy index.",
      "authors": [
        "Jihui Han",
        "Jiazheng Chai",
        "Mitsuhiro Hayashibe"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IEEECONF49454.2021.9382755",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "78-82",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7659-8",
        "issn": "2474-2317",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 IEEE/SICE International Symposium on System Integration, SII 2021"
      },
      "publication_date": "2021-01-11",
      "selected": null,
      "title": "Emergence of Motor Synergy in Multi-directional Reaching with Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103740890&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9382755"
      ]
    },
    {
      "abstract": "Human demonstrations can provide trustful samples to train reinforcement learning algorithms for robots to learn complex behaviors in real-world environments. However, obtaining sufficient demonstrations may be impractical because many behaviors are difficult for humans to demonstrate. A more practical approach is to replace human demonstrations by human queries, i.e., preference-based reinforcement learning. One key limitation of the existing algorithms is the need for a significant amount of human queries because a large number of labeled data is needed to train neural networks for the approximation of a continuous, high-dimensional reward function. To reduce and minimize the need for human queries, we propose a new GAN-assisted human preference-based reinforcement learning approach that uses a generative adversarial network (GAN) to learn human preferences and then replace the role of human in assigning preferences. The adversarial neural network is simple and only has a binary output, hence requiring much less human queries to train. Moreover, a maximum entropy based reinforcement learning algorithm is designed to shape the loss towards the desired regions or away from the undesired regions. To show the effectiveness of the proposed approach, we present some studies on complex robotic tasks without access to the environment reward in a typical MuJoCo robot locomotion environment. The obtained results show our method can achieve a reduction of about 99.8% human time without performance sacrifice.",
      "authors": [
        "Huixin Zhan",
        "Feng Tao",
        "Yongcan Cao"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/LRA.2021.3063927",
      "keywords": [
        "reinforcement learning",
        "Generative adversarial network (GAN)",
        "human preferences"
      ],
      "number_of_pages": 8,
      "pages": "3545-3552",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2377-3774",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Human-Guided Robot Behavior Learning: A GAN-Assisted Preference-Based Reinforcement Learning Approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102297100&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9369902"
      ]
    },
    {
      "abstract": "The current reward learning from human preferences could be used to resolve complex reinforcement learning (RL) tasks without access to a reward function by defining a single fixed preference between pairs of trajectory segments. However, the judgment of preferences between trajectories is not dynamic and still requires human input over thousands of iterations. In this study, we proposed a weak human preference supervision framework, for which we developed a human preference scaling model that naturally reflects the human perception of the degree of weak choices between trajectories and established a human-demonstration estimator through supervised learning to generate the predicted preferences for reducing the number of human inputs. The proposed weak human preference supervision framework can effectively solve complex RL tasks and achieve higher cumulative rewards in simulated robot locomotion\u2014MuJoCo games\u2014relative to the single fixed human preferences. Furthermore, our established human-demonstration estimator requires human feedback only for less than 0.01% of the agent\u2019s interactions with the environment and significantly reduces the cost of human inputs by up to 30% compared with the existing approaches. To present the flexibility of our approach, we released a video (https://youtu.be/jQPe1OILT0M) showing comparisons of the behaviors of agents trained on different types of human input. We believe that our naturally inspired human preferences with weakly supervised learning are beneficial for precise reward learning and can be applied to state-of-the-art RL systems, such as human-autonomy teaming systems.",
      "authors": [
        "Zehong Cao",
        "KaiChiu Wong",
        "Chin-Teng Lin"
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2021.3084198",
      "keywords": [
        "weak human preferences",
        "supervised learning",
        "Deep reinforcement learning (RL)",
        "scaling"
      ],
      "number_of_pages": 10,
      "pages": "5369-5378",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162-2388",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Weak Human Preference Supervision for Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111004869&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9448304"
      ]
    },
    {
      "abstract": "The high request for autonomous human\u2013robot interaction (HRI), combined with the potential of machine learning (ML) techniques, allow us to deploy ML mechanisms in robot control. However, the use of ML can make robots\u2019 behavior unclear to the observer during the learning phase. Recently, transparency in HRI has been investigated to make such interactions more comprehensible. In this work, we propose a model to improve the transparency during reinforcement learning (RL) tasks for HRI scenarios: the model supports transparency by having the robot show nonverbal emotional-behavioral cues. Our model considered human feedback as the reward of the RL algorithm and it presents emotional-behavioral responses based on the progress of the robot learning. The model is managed only by the temporal-difference error. We tested the architecture in a teaching scenario with the iCub humanoid robot. The results highlight that when the robot expresses its emotional-behavioral response, the human teacher is able to understand its learning process better. Furthermore, people prefer to interact with an expressive robot as compared to a mechanical one. Movement-based signals proved to be more effective in revealing the internal state of the robot than facial expressions. In particular, gaze movements were effective in showing the robot's next intentions. In contrast, communicating uncertainty through robot movements sometimes led to action misinterpretation, highlighting the importance of balancing transparency and the legibility of the robot goal. We also found a reliable temporal window in which to register teachers\u2019 feedback that can be used by the robot as a reward.",
      "authors": [
        "Marco Matarese",
        "Alessandra Sciutti",
        "Francesco Rea",
        "Silvia Rossi"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/THMS.2021.3116119",
      "keywords": [
        "Human\u2013robot interaction",
        "reinforcement learning (RL)",
        "humanoid robot",
        "transparency",
        "social robotics"
      ],
      "number_of_pages": 12,
      "pages": "578-589",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2168-2305",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Human-Machine Systems"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Toward Robots\u2019 Behavioral Transparency of Temporal Difference Reinforcement Learning With a Human Teacher",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118249923&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9580746"
      ]
    },
    {
      "abstract": "Full-dimensional natural arm manipulation is a challenging task in the field of model-based control due to its high degree of freedom and unknown dynamics of the given system. Deep reinforcement learning (DRL) offers a promising model-free approach for handling high-dimensional robotics problems. Although impressive results for the arm manipulation task have been reported, it still remains an open problem on how we can create human-like synergetic reaching motion using learning algorithms. In this study, we apply DRL for managing full-dimensional arm manipulation in a simulation study, and verify the relations among motion error, energy, and synergy emergence, to reveal the mechanism of employing motor synergy. Although synergy information has never been encoded into the reward function, the synergy naturally emerges along with feedforward control, leading to a similar situation as human motion learning. To the best of our knowledge, this is a pioneer study demonstrating the error and energy optimization issue exists behind the motor synergy employment in DRL for reaching tasks. In addition, our proposed feedback-augmented DRL controller shows better capability over DRL in terms of synergy development and the coupled criteria of error-energy index. This implies that feedback control can support the learning process under redundancy by voiding unnecessary random exploration.",
      "authors": [
        "Jihui Han",
        "Jiazheng Chai",
        "Mitsuhiro Hayashibe"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TMRB.2021.3056924",
      "keywords": [
        "reaching task",
        "human motor learning",
        "motor synergy",
        "Deep learning in robotics",
        "redundancy"
      ],
      "number_of_pages": 12,
      "pages": "498-509",
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2576-3202",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 0.864,
        "snip": 1.233,
        "subject_areas": [
          "Biomedical Engineering",
          "Artificial Intelligence",
          "Computer Science Applications",
          "Human-Computer Interaction",
          "Control and Optimization"
        ],
        "title": "IEEE Transactions on Medical Robotics and Bionics"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Synergy Emergence in Deep Reinforcement Learning for Full-Dimensional Arm Manipulation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115872799&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9345796"
      ]
    },
    {
      "abstract": "Reinforcement learning is an unsupervised learning algorithm, where learning is based upon feedback from the environment. Prior research has proposed cognitive (e.g., Instance-based Learning or IBL) and statistical (Q-learning) reinforcement learning algorithms. However, an evaluation of these algorithms in a single dynamic environment has not been explored. In this paper, a comparison between the statistical Q-learning algorithm and the cognitive IBL algorithm is presented. A well-known environment, \u201cFrozen Lake,\u201d is used to train, generalize, and scale Q-learning and IBL algorithms. For generalizing, the Q-learning and IBL agents were trained on one version of the Frozen Lake and tested on a permuted version of the same environment. For scaling, the two algorithms were tested on a larger version of the Frozen Lake environment. Results revealed that the IBL algorithm used less training time and generalized better to different environment variants. The IBL algorithm was also able to show scalability by retaining its superior performance in the larger environment. These results indicate that the IBL algorithm could be proposed as an alternative to the standard reinforcement learning algorithms based on dynamic programming such as Q-learning. The inclusion of human factors (such as memory) in the IBL algorithm makes it suitable for robust learning in complex and dynamic environments.",
      "authors": [
        "Anmol Gupta",
        "Partha Pratim Roy",
        "Varun Dutt"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2021.3117855",
      "keywords": [
        "dynamic environment",
        "Q-learning",
        "frozen lake",
        "instance-based learning",
        "cognitive modeling",
        "Reinforcement learning",
        "openAI"
      ],
      "number_of_pages": 16,
      "pages": "138775-138790",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Evaluation of Instance-Based Learning and Q-Learning Algorithms in Dynamic Environments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117587140&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9558862"
      ]
    },
    {
      "abstract": "API search involves finding components in an API that are relevant to a programming task. For example, a programmer may need a function in a C library that opens a new network connection, then another function that sends data across that connection. Unfortunately, programmers often have trouble finding the API components that they need. A strong scientific consensus is emerging towards developing interactive tool support that responds to conversational feedback, emulating the experience of asking a fellow human programmer for help. A major barrier to creating these interactive tools is implementing dialogue management for API search. Dialogue management involves determining how a system should respond to user input, such as whether to ask a clarification question or to display potential results. In this paper, we present a dialogue manager for interactive API search that considers search results and dialogue history to select efficient actions. We implement two dialogue policies: a hand-crafted policy and a policy optimized via reinforcement learning. We perform a synthetics evaluation and a human evaluation comparing the policies to a generic single-turn, top-N policy used by source code search engines.",
      "authors": [
        "Zachary Eberhart",
        "Collin McMillan"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICSME52107.2021.00031",
      "keywords": [
        "on-demand documentation",
        "API search",
        "interactive dialogue systems",
        "reinforcement learning",
        "dialogue policy"
      ],
      "number_of_pages": 12,
      "pages": "274-285",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-2883-5",
        "issn": "1063-6773",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Dialogue Management for Interactive API Search",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9609185",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123375249&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Najar A."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 2,
      "pages": "1726-1727",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Teaching a robot with unlabeled instructions: The TICS architecture",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112259907&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "23340754",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "34th International Florida Artificial Intelligence Research Society Conference, FLAIRS-34 2021",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85176852018&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Marzuki A.A."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1001/jamanetworkopen.2021.36195",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "JAMA Network Open"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Association of Environmental Uncertainty with Altered Decision-making and Learning Mechanisms in Youths with Obsessive-Compulsive Disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120359678&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Munhall K.G."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/9781119184096.ch4",
      "keywords": [],
      "number_of_pages": 25,
      "pages": "97-121",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781119184096",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "The Handbook of Speech Perception"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Perceptual control of speech",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121483147&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Agrawal V."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "2843-2849",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 43rd Annual Meeting of the Cognitive Science Society: Comparative Cognition: Animal Minds, CogSci 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Tracking what matters: A decision-variable account of human behavior in bandit tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131398425&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "23rd International Conference on Principles and Practice of Multi-Agent Systems, PRIMA 2020",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102742857&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Watkins O."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 14,
      "pages": "6920-6933",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Teachable Reinforcement Learning via Advice Distillation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131879989&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wei T."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 2,
      "pages": "15919-15920",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713835974",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "35th AAAI Conference on Artificial Intelligence, AAAI 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Extending Policy Shaping to Continuous State Spaces (Student Abstract)",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130097113&origin=inward"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning (IRL) agents use human feedback or instruction to help them learn in complex environments. Often, this feedback comes in the form of a discrete signal that\u2019s either positive or negative. While informative, this information can be difficult to generalize on its own. In this work, we explore how natural language advice can be used to provide a richer feedback signal to a reinforcement learning agent by extending policy shaping, a well-known IRL technique. Usually policy shaping employs a human feedback policy to help an agent to learn more about how to achieve its goal. In our case, we replace this human feedback policy with policy generated based on natural language advice. We aim to inspect if the generated natural language reasoning provides support to a deep RL agent to decide its actions successfully in any given environment. So, we design our model with three networks: first one is the experience driven, next is the advice generator and third one is the advice driven. While the experience driven RL agent chooses its actions being influenced by the environmental reward, the advice driven neural network with generated feedback by the advice generator for any new state selects its actions to assist the RL agent to better policy shaping.",
      "authors": [
        "Tasmia Tasrin",
        "Md Sultan Al Nahian",
        "Habarakadage Perera",
        "Brent Harrison"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.32473/flairs.v34i1.128472",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "23340754",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Influencing Reinforcement Learning through Natural Language Guidance",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131130368&origin=inward",
        "https://journals.flvc.org/FLAIRS/article/download/128472/130088"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sumers T.R."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "6002-6010",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713835974",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "35th AAAI Conference on Artificial Intelligence, AAAI 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Learning Rewards from Linguistic Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125786611&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kreutzer J."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "37-43",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781954085756",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "SPNLP 2021 - 5th Workshop on Structured Prediction for NLP, Proceedings of the Workshop"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130056052&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Paduraru C."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ASEW52652.2021.00024",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "65-72",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665435833",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Automated game testing using computer vision methods",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85125674883&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Guan L."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 13,
      "pages": "21885-21897",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85131831797&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ranasinghe I."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/CIC52973.2021.00012",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "11-15",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665416252",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2021 IEEE 7th International Conference on Collaboration and Internet Computing, CIC 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "A Collaborative and Adaptive Feedback System for Physical Exercises",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126833005&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lu X."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "16084-16086",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713835974",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "35th AAAI Conference on Artificial Intelligence, AAAI 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "ACAT-G: An Interactive Learning Framework for Assisted Response Generation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85130040995&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sun Y."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/AIAM54119.2021.00063",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "276-279",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665417327",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture, AIAM 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Performance of Reinforcement Learning on Traditional Video Games",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127262524&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ji C.X."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "305-314",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AMIA ... Annual Symposium proceedings. AMIA Symposium"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Trajectory Inspection: A Method for Iterative Clinician-Driven Design of Reinforcement Learning Studies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110615555&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang X."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "1259-1268",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85171799590&origin=inward"
      ]
    },
    {
      "abstract": "To mitigate the physical burden of disabled people, we propose an approach that a robot could perceive the implied action intentions of disabled people by their selected objects. This article presents a framework for recognizing and learning human intentions based on...",
      "authors": [
        "Liu, Yan",
        "Yao, Yufeng",
        "Peng, Haoqi",
        "Liu, Yaxin"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-89134-3_32",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "345-356",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "An MRF-Based Intention Recognition Framework for WMRA with Selected Objects as Contextual Clues",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-030-89134-3_32.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118132979&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bernardoni F."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/abn0000690",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "736-747",
      "publication": {
        "category": "Journal",
        "cite_score": 11.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0021843X",
        "publisher": "American Psychological Association",
        "sjr": 2.031,
        "snip": 2.532,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "Journal of Abnormal Psychology"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "More by Stick Than by Carrot: A Reinforcement Learning Style Rooted in the Medial Frontal Cortex in Anorexia Nervosa",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119609153&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Li G."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1723-1725",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Facial feedback for reinforcement learning: A case study and offline analysis using the TAMER framework",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112344889&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780998508252",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of ICCM 2021 - 19th International Conference on Cognitive Modelling"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Proceedings of ICCM 2021 - 19th International Conference on Cognitive Modelling",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180634943&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Cao Z."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/adb.12873",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13556215",
        "publisher": "Wiley-Blackwell",
        "sjr": 1.049,
        "snip": 0.941,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Psychiatry and Mental Health",
          "Pharmacology"
        ],
        "title": "Addiction Biology"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Aberrant reward prediction errors in young adult at-risk alcohol users",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078680191&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Shi W."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 15,
      "pages": "3478-3492",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781955917100",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Findings of the Association for Computational Linguistics, Findings of ACL: EMNLP 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85129189084&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781954085756",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "SPNLP 2021 - 5th Workshop on Structured Prediction for NLP, Proceedings of the Workshop"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "SPNLP 2021 - 5th Workshop on Structured Prediction for NLP, Proceedings of the Workshop",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85138389276&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665444125",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2021 International Automatic Control Conference, CACS 2021"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "2021 International Automatic Control Conference, CACS 2021",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123947704&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liang R."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "26-32",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781954085657",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ECNLP 2021 - 4th Workshop on e-Commerce and NLP, Proceedings"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Turn-Level User Satisfaction Estimation in E-commerce Customer Service",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123302544&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Palidis D.J."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00696.2020",
      "keywords": [],
      "number_of_pages": 21,
      "pages": "47-67",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Null effects of levodopa on reward- And error-based motor adaptation, savings, and anterograde interference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85109181849&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Treesatayapun C."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/16168658.2021.1943887",
      "keywords": [],
      "number_of_pages": 23,
      "pages": "368-390",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "16168658",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Fuzzy Information and Engineering"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Output Feedback Controller for a Class of Unknown Nonlinear Discrete Time Systems Using Fuzzy Rules Emulated Networks and Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108424233&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Seo J."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 17,
      "pages": "1-17",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781624106095",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AIAA Scitech 2021 Forum"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Development of an artificial intelligence system for structural design using reinforcement learning: Proof of concept",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100079068&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 1.0,
        "is_potentially_predatory": false,
        "isbn": "9789819927883",
        "issn": "18650929",
        "publisher": "Springer Science and Business Media Deutschland GmbH",
        "sjr": 0.194,
        "snip": 0.241,
        "subject_areas": [
          "Computer Science (all)",
          "Mathematics (all)"
        ],
        "title": "International Conference on Neural Information Processing"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "7th International Conference on Life System Modeling and Simulation, LSMS 2021, and the 7th International Conference on Intelligent Computing for Sustainable Energy and Environment, ICSEE 2021",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118976719&origin=inward"
      ]
    },
    {
      "abstract": "In everyday life, humans face environments that feature uncertain and volatile or changing situations. Efficient adaptive behaviour must take into account uncertainty and volatility. Previous models of adaptive behaviour involve inferences about volatility that rely on complex and often intractable computations. Because such computations are presumably implausible biologically, it is unclear how humans develop efficient adaptive behaviours in such environments. Here, we demonstrate a counterintuitive result: simple, low-level inferences confined to uncertainty can produce near-optimal adaptive behaviour, regardless of the environmental volatility, assuming imprecisions in computation that conform to the psychophysical Weber law. We further show empirically that this Weber-imprecision model explains human behaviour in volatile environments better than optimal adaptive models that rely on high-level inferences about volatility, even when considering biologically plausible approximations of such models, as well as non-inferential models like adaptive reinforcement learning. Findling et al. present the Weber-imprecision model of decision-making, which operates on imprecise representations of uncertainty. It produces efficient adaptive behaviour regardless of environmental volatility and fits human behaviour better than optimal adaptive models.",
      "authors": [
        "Findling, Charles",
        "Chopin, Nicolas",
        "Koechlin, Etienne"
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41562-020-00971-z",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "99-112",
      "publication": {
        "category": "Journal",
        "cite_score": 26.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2397-3374",
        "publisher": "Nature Publishing Group",
        "sjr": 5.639,
        "snip": 6.703,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Social Psychology"
        ],
        "title": "Nature Human Behaviour"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Imprecise neural computations as a source of adaptive behaviour in volatile environments",
      "urls": [
        "https://www.nature.com/articles/s41562-020-00971-z.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095694929&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "McKendrick M."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/anae.15274",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "171-181",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00032409",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Anaesthesia"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "The use of artificial intelligence and robotics in regional anaesthesia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099106444&origin=inward"
      ]
    },
    {
      "abstract": "The paper presents a hierarchical spike timing neural network model developed in NEST simulator that aims to reproduce the eye movements\u2019 behavior of humans during decision making. It includes the main visual information processing structures starting from...",
      "authors": [
        "Koprinkova-Hristova, Petia",
        "Bocheva, Nadejda"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-71616-5_14",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "139-153",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9783030716158",
        "issn": "1860949X",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Annual Meeting of the Bulgarian Section of SIAM"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Spike Timing Neural Model of Eye Movement Motor Response with Reinforcement Learning",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-030-71616-5_14.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104788923&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Siekmann J."
      ],
      "categories": null,
      "citations": 42,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.15607/RSS.2021.XVII.061",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780992374778",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Robotics: Science and Systems"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Blind Bipedal Stair Traversal via Sim-to-Real Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121654573&origin=inward"
      ]
    },
    {
      "abstract": "Deep learning methods are derived and inspired from the structure and activities of a brain. In an intricate state, learning from the past experiences helps human to accomplish the task in efficient way. This paper addresses such deep learning approaches practiced in...",
      "authors": [
        "Vyas, Dhaval R.",
        "Markana, Anilkumar",
        "Padhiyar, Nitin"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-981-15-9953-8_11",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "117-130",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789811599521",
        "issn": "21945357",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent Human Systems Integration"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Robotic Grasp Synthesis Using Deep Learning Approaches: A Survey",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-981-15-9953-8_11.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85103295190&origin=inward"
      ]
    },
    {
      "abstract": "In both military and commercial domains, tasks are increasingly entrusted to autonomous systems and robots. These artificial intelligence (AI) systems are expected to be safe and intelligent, adapt to changing environments, and interact with other actors, both automated and human. In this paper, we present a framework and corresponding analytics for developing AI agents that possess (1) cognitive skills, including the ability to perform counter-factual reasoning and self-assessment and to exhibit human-like curiosity, biases, and errors; (2) the ability to learn complex tasks quickly with limited feedback; (3) the ability to coordinate and co-learn with human or AI teammates; and (4) the ability to function well over long time horizons (e.g. hours or days). Our framework is based on the theory of adaptive behavior called active inference, which was developed in computational neuroscience and psychology. Together with learnable deep factorized representations, the active inference provides the objective function, high-capacity predictions, and scalable computational mechanisms that enable AI agents to execute four processes fundamental to human cognition: learning, perception, planning, and simulation. We demonstrate the advantages of our AI solution in the domain of planning multi-agent maneuvers for executing area control missions. Our model achieves faster learning compared to the reinforcement learning baseline, producing faster point accumulation and game win rate.",
      "authors": [
        "Georgiy Levchuk",
        "Lisa Lucia",
        "Louis Penafiel"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1117/12.2589259",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781510662087",
        "issn": "0277786X",
        "publisher": "SPIE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications IV"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Towards adaptive and curious artificial agents: learning, perception, and planning in dynamic uncertain environments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107852635&origin=inward",
        "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11751/117510D/Towards-adaptive-and-curious-artificial-agents--learning-perception-and/10.1117/12.2589259.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sch\u00fcller T."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nicl.2021.102746",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "NeuroImage: Clinical"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Performance monitoring in obsessive\u2013compulsive disorder: Insights from internal capsule/nucleus accumbens deep brain stimulation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108993556&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kruijne W."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/neco_a_01339",
      "keywords": [],
      "number_of_pages": 40,
      "pages": "1-40",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08997667",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Neural Computation"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Flexible working memory through selective gating and attentional tagging",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098452264&origin=inward"
      ]
    },
    {
      "abstract": "The value learning process has been investigated using decision-making tasks with a correct answer specified by the external environment (externally guided decision-making, EDM). In EDM, people are required to adjust their choices based on feedback, and the learning process is generally explained by the reinforcement learning (RL) model. In addition to EDM, value is learned through internally guided decision-making (IDM), in which no correct answer defined by external circumstances is available, such as preference judgment. In IDM, it has been believed that the value of the chosen item is increased and that of the rejected item is decreased (choice-induced preference change; CIPC). An RL-based model called the choice-based learning (CBL) model had been proposed to describe CIPC, in which the values of chosen and/or rejected items are updated as if own choice were the correct answer. However, the validity of the CBL model has not been confirmed by fitting the model to IDM behavioral data. The present study aims to examine the CBL model in IDM. We conducted simulations, a preference judgment task for novel contour shapes, and applied computational model analyses to the behavioral data. The results showed that the CBL model with both the chosen and rejected value\u2019s updated were a good fit for the IDM behavioral data compared to the other candidate models. Although previous studies using subjective preference ratings had repeatedly reported changes only in one of the values of either the chosen or rejected items, we demonstrated for the first time both items\u2019 value changes were based solely on IDM choice behavioral data with computational model analyses.",
      "authors": [
        "Jianhong Zhu",
        "Junya Hashimoto",
        "Kentaro Katahira",
        "Makoto Hirakawa",
        "Takashi Nakao"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0244434",
      "keywords": [
        "Learning",
        "Behavior",
        "Human learning",
        "Simulation and modeling",
        "Decision making",
        "Reaction time",
        "Symmetry",
        "Autocorrelation"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "Computational modeling of choice-induced preference change: A Reinforcement-Learning-based approach",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0244434&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099432091&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789811599521",
        "issn": "21945357",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent Human Systems Integration"
      },
      "publication_date": "2021-01-01",
      "selected": null,
      "title": "5th International Conference on Advanced Machine Learning Technologies and Applications, AMLTA 2020",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087015117&origin=inward"
      ]
    },
    {
      "abstract": "Persuasion dialogue systems reflect the machine's ability to make strategic moves beyond verbal communication, and therefore differentiate themselves from task-oriented or open-domain dialogue systems and have their own unique values. However, the repetition and inconsistency problems still persist in dialogue response generation and could substantially impact user experience and impede the persuasion outcome. Besides, although reinforcement learning (RL) approaches have achieved big success in strategic tasks such as games, they require a sophisticated user simulator to provide real-time feedback to the dialogue system, which limits the application of RL on persuasion dialogues. To address these issues towards a better persuasion dialogue system, we apply RL to refine a language model baseline without user simulators, and distill sentence-level information about repetition, inconsistency, and task relevance through rewards. Moreover, to better accomplish the persuasion task, the model learns from human demonstration to imitate human persuasion behavior and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.",
      "authors": [
        "Shi, Weiyan",
        "Li, Yu",
        "Sahay, Saurav",
        "Yu, Zhou"
      ],
      "categories": null,
      "citations": null,
      "comments": "EMNLP 2021 Findings",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-12-31",
      "selected": null,
      "title": "Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration",
      "urls": [
        "http://arxiv.org/abs/2012.15375v2",
        "http://arxiv.org/pdf/2012.15375v2",
        "http://arxiv.org/pdf/2012.15375.pdf"
      ]
    },
    {
      "abstract": "We introduce the concept of a multi-principal assistance game (MPAG), and circumvent an obstacle in social choice theory, Gibbard's theorem, by using a sufficiently collegial preference inference mechanism. In an MPAG, a single agent assists N human principals who may have widely different preferences. MPAGs generalize assistance games, also known as cooperative inverse reinforcement learning games. We analyze in particular a generalization of apprenticeship learning in which the humans first perform some work to obtain utility and demonstrate their preferences, and then the robot acts to further maximize the sum of human payoffs. We show in this setting that if the game is sufficiently collegial, i.e. if the humans are responsible for obtaining a sufficient fraction of the rewards through their own actions, then their preferences are straightforwardly revealed through their work. This revelation mechanism is non-dictatorial, does not limit the possible outcomes to two alternatives, and is dominant-strategy incentive-compatible.",
      "authors": [
        "Fickinger, Arnaud",
        "Zhuang, Simon",
        "Critch, Andrew",
        "Hadfield-Menell, Dylan",
        "Russell, Stuart"
      ],
      "categories": null,
      "citations": null,
      "comments": "arXiv admin note: text overlap with arXiv:2007.09540",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-12-29",
      "selected": null,
      "title": "Multi-Principal Assistance Games: Definition and Collegial Mechanisms",
      "urls": [
        "http://arxiv.org/pdf/2012.14536.pdf",
        "http://arxiv.org/pdf/2012.14536v1",
        "http://arxiv.org/abs/2012.14536v1"
      ]
    },
    {
      "abstract": "Humans learn a motor skill (MS) through practice and experience and then may retain it for its recruitment, which is effective in rapid response for novel contexts. For a MS to be recruited for novel contexts, its recruitment range must be extended. In addressing this issue, we hypothesized that a MS is dynamically modulated according to feedback context to expand its recruitment range into novel contexts, which do not involve the learning of a MS. The following two sub-issues are considered. We previously demonstrated that the learned MS could be recruited in novel contexts through its modulation, which is driven by dynamically regulating the synergistic redundancy between muscles according to feedback context. However, this modulation is trained in the dynamics under the MS learning context. Learning a MS in a specific condition naturally causes movement deviation from the desired state when the MS is executed in a novel context. We hypothesized that this deviation can be reduced with the additional modulation of a MS, which tunes the MS-produced muscle activities with using the feedback gain signals driven by the deviation from the desired state. Based on this hypothesis, we newly propose the feedback gain signals-driven tuning model of a learned MS for its robust recruitment. This model is based on the neurophysiological architecture in the cortico-basal ganglia circuit, in which a MS is plausibly retained as it was learned and is then recruited by tuning its muscle control signals according to feedback context. In this study, through computational simulation, we show that the proposed model may be used to neurophysiologically describe the recruitment of a learned MS in novel contexts.",
      "authors": [
        "Min, Kyuengbo",
        "Lee, Jongho",
        "Kakei, Shinji"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2020.457682",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2020-12-23",
      "selected": null,
      "title": "Dynamic Modulation of a Learned Motor Skill for Its Recruitment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099054702&origin=inward"
      ]
    },
    {
      "abstract": "Generating useful network summaries is a challenging and important problem with several applications like sensemaking, visualization, and compression. However, most of the current work in this space do not take human feedback into account while generating summaries. Consider an intelligence analysis scenario, where the analyst is exploring a similarity network between documents. The analyst can express her agreement/disagreement with the visualization of the network summary via iterative feedback, e.g. closing or moving documents (\"nodes\") together. How can we use this feedback to improve the network summary quality? In this paper, we present NetReAct, a novel interactive network summarization algorithm which supports the visualization of networks induced by text corpora to perform sensemaking. NetReAct incorporates human feedback with reinforcement learning to summarize and visualize document networks. Using scenarios from two datasets, we show how NetReAct is successful in generating high-quality summaries and visualizations that reveal hidden patterns better than other non-trivial baselines.",
      "authors": [
        "Amiri, Sorour E.",
        "Adhikari, Bijaya",
        "Wenskovitch, John",
        "Rodriguez, Alexander",
        "Dowling, Michelle",
        "North, Chris",
        "Prakash, B. Aditya"
      ],
      "categories": null,
      "citations": null,
      "comments": "Presented at NeuRIPS 2020 HAMLETS",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-12-22",
      "selected": null,
      "title": "NetReAct: Interactive Learning for Network Summarization",
      "urls": [
        "http://arxiv.org/pdf/2012.11821v1",
        "http://arxiv.org/pdf/2012.11821.pdf",
        "http://arxiv.org/abs/2012.11821v1"
      ]
    },
    {
      "abstract": "Providing explanations is considered an imperative ability for an AI agent in a human-robot teaming framework. The right explanation provides the rationale behind an AI agent's decision-making. However, to maintain the human teammate's cognitive demand to comprehend the provided explanations, prior works have focused on providing explanations in a specific order or intertwining the explanation generation with plan execution. Moreover, these approaches do not consider the degree of details required to share throughout the provided explanations. In this work, we argue that the agent-generated explanations, especially the complex ones, should be abstracted to be aligned with the level of details the human teammate desires to maintain the recipient's cognitive load. Therefore, learning a hierarchical explanations model is a challenging task. Moreover, the agent needs to follow a consistent high-level policy to transfer the learned teammate preferences to a new scenario while lower-level detailed plans are different. Our evaluation confirmed the process of understanding an explanation, especially a complex and detailed explanation, is hierarchical. The human preference that reflected this aspect corresponded exactly to creating and employing abstraction for knowledge assimilation hidden deeper in our cognitive process. We showed that hierarchical explanations achieved better task performance and behavior interpretability while reduced cognitive load. These results shed light on designing explainable agents utilizing reinforcement learning and planning across various domains.",
      "authors": [
        "Zakershahrak, Mehrdad",
        "Ghodratnama, Samira"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-12-22",
      "selected": null,
      "title": "Are We On The Same Page? Hierarchical Explanation Generation for Planning Tasks in Human-Robot Teaming using Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2012.11792v2",
        "http://arxiv.org/abs/2012.11792v2",
        "http://arxiv.org/pdf/2012.11792.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Levi-Aharoni H."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/1741-2552/abc8d8",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17412560",
        "publisher": "IOP Publishing Ltd.",
        "sjr": 1.135,
        "snip": 1.25,
        "subject_areas": [
          "Biomedical Engineering",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Journal of Neural Engineering"
      },
      "publication_date": "2020-12-16",
      "selected": null,
      "title": "The value\u2013complexity trade-off for reinforcement learning based brain\u2013computer interfaces",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099334924&origin=inward"
      ]
    },
    {
      "abstract": "Recent advances in Artificial Intelligence (AI) have revived the quest for agents able to acquire an open-ended repertoire of skills. However, although this ability is fundamentally related to the characteristics of human intelligence, research in this field rarely considers the processes that may have guided the emergence of complex cognitive capacities during the evolution of the species. Research in Human Behavioral Ecology (HBE) seeks to understand how the behaviors characterizing human nature can be conceived as adaptive responses to major changes in the structure of our ecological niche. In this paper, we propose a framework highlighting the role of environmental complexity in open-ended skill acquisition, grounded in major hypotheses from HBE and recent contributions in Reinforcement learning (RL). We use this framework to highlight fundamental links between the two disciplines, as well as to identify feedback loops that bootstrap ecological complexity and create promising research directions for AI researchers.",
      "authors": [
        "Nisioti, Eleni",
        "Moulin-Frier, Cl\u00e9ment"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-12-15",
      "selected": null,
      "title": "Grounding Artificial Intelligence in the Origins of Human Behavior",
      "urls": [
        "http://arxiv.org/pdf/2012.08564v2",
        "http://arxiv.org/pdf/2012.08564.pdf",
        "http://arxiv.org/abs/2012.08564v2"
      ]
    },
    {
      "abstract": "Preference-based Reinforcement Learning (PbRL) replaces reward values in traditional reinforcement learning by preferences to better elicit human opinion on the target objective, especially when numerical reward values are hard to design or interpret. Despite promising results in applications, the theoretical understanding of PbRL is still in its infancy. In this paper, we present the first finite-time analysis for general PbRL problems. We first show that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL. If preferences are stochastic, and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that are able to identify the best policy up to accuracy \u03b5 with high probability. Our method explores the state space by navigating to under-explored states, and solves PbRL using a combination of dueling bandits and policy search. Experiments show the efficacy of our method when it is applied to real-world problems.",
      "authors": [
        "Yichong Xu",
        "Ruosong Wang",
        "Lin F. Yang",
        "Aarti Singh",
        "Artur Dubrawski"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3495724.3497301",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "18784-18794",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713829546",
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 34th International Conference on Neural Information Processing Systems"
      },
      "publication_date": "2020-12-06",
      "selected": null,
      "title": "Preference-based reinforcement learning with finite-time guarantees",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3495724.3497301"
      ]
    },
    {
      "abstract": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about\u2014summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles [22], producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.",
      "authors": [
        "Nisan Stiennon",
        "Long Ouyang",
        "Jeff Wu",
        "Daniel M. Ziegler",
        "Ryan Lowe",
        "Chelsea Voss",
        "Alec Radford",
        "Dario Amodei",
        "Paul Christiano"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3495724.3495977",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "3008-3021",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713829546",
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 34th International Conference on Neural Information Processing Systems"
      },
      "publication_date": "2020-12-06",
      "selected": null,
      "title": "Learning to summarize from human feedback",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3495724.3495977"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Byrne K.A."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cognition.2020.104448",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00100277",
        "publisher": "Elsevier B.V.",
        "sjr": 1.691,
        "snip": 1.813,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Cognitive Neuroscience",
          "Language and Linguistics",
          "Linguistics and Language"
        ],
        "title": "Cognition"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Acute stress enhances tolerance of uncertainty during decision-making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090583535&origin=inward"
      ]
    },
    {
      "abstract": "The Wisconsin Card Sorting Test (WCST) is considered a gold standard for the assessment of cognitive flexibility. On the WCST, repeating a sorting category following negative feedback is typically treated as indicating reduced cognitive flexibility. Therefore such responses are referred to as \u2018perseveration\u2019 errors. Recent research suggests that the propensity for perseveration errors is modulated by response demands: They occur less frequently when their commitment repeats the previously executed response. Here, we propose parallel reinforcement-learning models of card sorting performance, which assume that card sorting performance can be conceptualized as resulting from model-free reinforcement learning at the level of responses that occurs in parallel with model-based reinforcement learning at the categorical level. We compared parallel reinforcement-learning models with purely model-based reinforcement learning, and with the state-of-the-art attentional-updating model. We analyzed data from 375 participants who completed a computerized WCST. Parallel reinforcement-learning models showed best predictive accuracies for the majority of participants. Only parallel reinforcement-learning models accounted for the modulation of perseveration propensity by response demands. In conclusion, parallel reinforcement-learning models provide a new theoretical perspective on card sorting and it offers a suitable framework for discerning individual differences in latent processes that subserve behavioral flexibility.",
      "authors": [
        "Steinke, Alexander",
        "Lange, Florian",
        "Kopp, Bruno"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-020-72407-7",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Parallel model-based and model-free reinforcement learning for card sorting performance",
      "urls": [
        "https://www.nature.com/articles/s41598-020-72407-7.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091317490&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Yang X."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2020.08.004",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "30-42",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Event-driven H<inf>\u221e</inf> control with critic learning for nonlinear systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089823320&origin=inward"
      ]
    },
    {
      "abstract": "Cognitive inflexibility is a well-documented, yet non-specific corollary of many neurological diseases. Computational modeling of covert cognitive processes supporting cognitive flexibility may provide progress toward nosologically specific aspects of cognitive inflexibility. We review computational models of the Wisconsin Card Sorting Test (WCST), which represents a gold standard for the clinical assessment of cognitive flexibility. A parallel reinforcement-learning (RL) model provides the best conceptualization of individual trial-by-trial WCST responses among all models considered. Clinical applications of the parallel RL model suggest that patients with Parkinson\u2019s disease (PD) and patients with amyotrophic lateral sclerosis (ALS) share a non-specific covert cognitive symptom: bradyphrenia. Impaired stimulus-response learning appears to occur specifically in patients with PD, whereas haphazard responding seems to occur specifically in patients with ALS. Computational modeling hence possesses the potential to reveal nosologically specific profiles of covert cognitive symptoms, which remain undetectable by traditionally applied behavioral methods. The present review exemplifies how computational neuropsychology may advance the assessment of cognitive flexibility. We discuss implications for neuropsychological assessment and directions for future research.",
      "authors": [
        "Steinke, Alexander",
        "Kopp, Bruno"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/brainsci10121000",
      "keywords": [],
      "number_of_pages": 24,
      "pages": "1-24",
      "publication": {
        "category": "Journal",
        "cite_score": 3.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2076-3425",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.752,
        "snip": 0.938,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Brain Sciences"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Toward a Computational Neuropsychology of Cognitive Flexibility",
      "urls": [
        "https://www.mdpi.com/2076-3425/10/12/1000/pdf?version=1608194523",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097819117&origin=inward"
      ]
    },
    {
      "abstract": "Reward delivery in reinforcement learning tasks elicits increased beta power in the human EEG over frontal areas of the scalp but it is unclear whether these 20\u201330&nbsp;Hz oscillations directly facilitate reward learning. We previously proposed that frontal beta is not specific to reward processing but rather reflects the role of prefrontal cortex in maintaining and transferring task-related information to other brain areas. To test this proposal, we had subjects perform a reinforcement learning task followed by a memory recall task in which subjects were asked to recall stimuli associated either with reward feedback (Reward Recall condition) or error feedback (Error Recall condition). We trained a classifier on post-feedback beta power in the Reward Recall condition to discriminate trials associated with reward feedback from those associated with error feedback and then tested the classifier on post-feedback beta power in the Error Recall condition. Crucially, the model classified error-related beta in the Error Recall condition as reward-related. The model also predicted stimulus recall from post-feedback beta power irrespective of feedback valence and task condition. These results indicate that post-feedback beta power is not specific to reward processing but rather reflects a more general task-related process.",
      "authors": [
        "HajiHosseini, Azadeh",
        "Hutcherson, Cendri A.",
        "Holroyd, Clay B."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-020-72128-x",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Beta oscillations following performance feedback predict subsequent recall of task-relevant information",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090961653&origin=inward",
        "https://www.nature.com/articles/s41598-020-72128-x.pdf"
      ]
    },
    {
      "abstract": "The formation of cooperative groups of agents with limited information-processing capabilities to solve complex problems together is a fundamental building principle that cuts through multiple scales in biology from groups of cells to groups of humans. Here, we study an experimental paradigm where a group of humans is joined together to solve a common sensorimotor task that cannot be achieved by a single agent but relies on the cooperation of the group. In particular, each human acts as a neuron-like binary decision-maker that determines in each moment of time whether to be active or not. Inspired by the population vector method for movement decoding, each neuron-like decision-maker is assigned a preferred movement direction that the decision-maker is ignorant about. From the population vector reflecting the group activity, the movement of a cursor is determined, and the task for the group is to steer the cursor into a predefined target. As the preferred movement directions are unknown and players are not allowed to communicate, the group has to learn a control strategy on the fly from the shared visual feedback. Performance is analyzed by learning speed and accuracy, action synchronization, and group coherence. We study four different computational models of the observed behavior, including a perceptron model, a reinforcement learning model, a Bayesian inference model and a Thompson sampling model that efficiently approximates Bayes optimal behavior. The Bayes and especially the Thompson model excel in predicting the human group behavior compared to the other models, suggesting that internal models are crucial for adaptive coordination. We discuss benefits and limitations of our paradigm regarding a better understanding of distributed information processing.",
      "authors": [
        "Schmid, Gerrit",
        "Braun, Daniel A."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-020-64091-4",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Human group coordination in a sensorimotor task with neuron-like decision-making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084963134&origin=inward",
        "https://www.nature.com/articles/s41598-020-64091-4.pdf"
      ]
    },
    {
      "abstract": "When confronted with information that challenges our beliefs, we must often learn from error in order to successfully navigate the world. Past studies in reinforcement learning and educational psychology have linked prediction error, a measure of surprise, to successful learning from feedback. However, there are substantial individual differences in belief-updating success, and the psychological factors that influence belief updating remain unclear. Here, we identify a novel factor that may predict belief updating: right-wing authoritarianism (RWA), which is characterized by a desire for order, structure, and preservation of social norms. We hypothesized that because people who score high on RWA are motivated to preserve entrenched beliefs, they may often fail to successfully update their beliefs when confronted with new information. Using a novel paradigm, we challenged participants\u2019 false beliefs and misconceptions to elicit prediction error. In two studies, we found consistent evidence that high-RWA individuals were less successful at correcting their false beliefs. Relative to low-RWA individuals, high-RWA individuals were less likely to revise beliefs in response to prediction error. We argue that RWA is associated with a relatively closed-minded cognitive style that negatively influences belief updating.",
      "authors": [
        "Sinclair, Alyssa H.",
        "Stanley, Matthew L.",
        "Seli, Paul"
      ],
      "categories": null,
      "citations": 25,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13423-020-01767-y",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "1348-1361",
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10699384",
        "publisher": "Springer New York",
        "sjr": 1.783,
        "snip": 2.034,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Arts and Humanities (miscellaneous)"
        ],
        "title": "Psychonomic Bulletin and Review"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Closed-minded cognition: Right-wing authoritarianism is negatively related to belief updating following prediction error",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088632922&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13423-020-01767-y.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hartmann H."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/jne.12917",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09538194",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroendocrinology"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Preliminary evidence for an association between intake of high-fat high-sugar diet, variations in peripheral dopamine precursor availability and dopamine-dependent cognition in humans",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097008056&origin=inward"
      ]
    },
    {
      "abstract": "Interactive NLP is a promising paradigm to close the gap between automatic NLP systems and the human upper bound. Preference-based interactive learning has been successfully applied, but the existing methods require several thousand interaction rounds even in simulations with perfect user feedback. In this paper, we study preference-based interactive summarisation. To reduce the number of interaction rounds, we propose the Active Preference-based ReInforcement Learning (APRIL) framework. APRIL uses active learning to query the user, preference learning to learn a summary ranking function from the preferences, and neural Reinforcement learning to efficiently search for the (near-)optimal summary. Our results show that users can easily provide reliable preferences over summaries and that APRIL outperforms the state-of-the-art preference-based interactive method in both simulation and real-user experiments.",
      "authors": [
        "Gao, Yang",
        "Meyer, Christian M.",
        "Gurevych, Iryna"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10791-019-09367-8",
      "keywords": [
        "Preference learning",
        "Reinforcement learning",
        "Interactive Natural Language Processing",
        "Document summarisation",
        "Active learning"
      ],
      "number_of_pages": 31,
      "pages": "555-585",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1386-4564",
        "publisher": "Kluwer Academic Publishers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Information Retrieval Journal"
      },
      "publication_date": "2020-12-01",
      "selected": null,
      "title": "Preference-based interactive multi-document summarisation",
      "urls": [
        "https://dl.acm.org/doi/10.1007/s10791-019-09367-8",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076042325&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10791-019-09367-8.pdf"
      ]
    },
    {
      "abstract": "Cognitive control, the ability of a system to adapt to the demands of a task, is an integral part of cognition. A widely accepted fact about cognitive control is that it is context-sensitive: Adults and children alike infer information about a task's demands from contextual cues and use these inferences to learn from ambiguous cues. However, the precise way in which people use contextual cues to guide adaptation to a new task remains poorly understood. This work connects the context-sensitive nature of cognitive control to a method for meta-learning with context-conditioned adaptation. We begin by identifying an essential difference between human learning and current approaches to meta-learning: In contrast to humans, existing meta-learning algorithms do not make use of task-specific contextual cues but instead rely exclusively on online feedback in the form of task-specific labels or rewards. To remedy this, we introduce a framework for using contextual information about a task to guide the initialization of task-specific models before adaptation to online feedback. We show how context-conditioned meta-learning can capture human behavior in a cognitive task and how it can be scaled to improve the speed of learning in various settings, including few-shot classification and low-sample reinforcement learning. Our work demonstrates that guiding meta-learning with task information can capture complex, human-like behavior, thereby deepening our understanding of cognitive control.",
      "authors": [
        "Dubey, Rachit",
        "Grant, Erin",
        "Luo, Michael",
        "Narasimhan, Karthik",
        "Griffiths, Thomas"
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-11-27",
      "selected": null,
      "title": "Connecting Context-specific Adaptation in Humans to Meta-learning",
      "urls": [
        "http://arxiv.org/abs/2011.13782v2",
        "http://arxiv.org/pdf/2011.13782.pdf",
        "http://arxiv.org/pdf/2011.13782v2"
      ]
    },
    {
      "abstract": "This paper provides a human-machine collaborative control framework, including artificial intelligence decision systems, human-level control, arbiter judgment, and learning of autonomous boundary, so that human suggestions are incorporated into the training process of decisions, assisting agents to learn quickly control decision tasks. Based on the model-free deep reinforcement learning algorithm HITL-AC, the human feedback (reward or punishment) is connected with the reward of the agent, so that the agent continuously tries to find a better boundary during the system's operation, avoiding defects of pre-fixed boundary. This formulation improves the data efficiency of reinforcement learning and plays a guiding role in seeking human intervention when the agent is in an uncertain environmental state during the test use phase. The fourth section of the paper gives a training demonstration of a realtime environment (bipedal walker). Compared with existing standard reinforcement learning methods that do not consider boundary concepts, the method with boundary information mentioned in this article can accelerate the process of agent reinforcement learning during the training phase, and seek human help when guiding the dangerous state of the agent during the test phase. And the real-time optimization algorithm (HITL-AC) for the boundary is better than the fixed value algorithm (HITL-FIX). This is beneficial for solving real-world problems, further proving the feasibility and effectiveness of the proposed framework and method.",
      "authors": [
        "Qianqian Zhang",
        "Yun-Bo Zhao",
        "Yu Kang"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ANZCC50923.2020.9318326",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "160-165",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-9993-1",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 Australian and New Zealand Control Conference, ANZCC 2020"
      },
      "publication_date": "2020-11-26",
      "selected": null,
      "title": "Autonomous Boundary of Human-Machine Collaboration System Based on Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9318326",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100503965&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728199924",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 Australian and New Zealand Control Conference, ANZCC 2020"
      },
      "publication_date": "2020-11-26",
      "selected": null,
      "title": "2020 Australian and New Zealand Control Conference, ANZCC 2020",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100510768&origin=inward"
      ]
    },
    {
      "abstract": "Information-seeking conversation systems are increasingly popular in real-world applications, especially for e-commerce companies. To retrieve appropriate responses for users, it is necessary to compute the matching degrees between candidate responses and users' queries with historical dialogue utterances. As the contexts are usually much longer than responses, it is thus necessary to expand the responses (usually short) with richer information. Recent studies on pseudo-relevance feedback (PRF) have demonstrated its effectiveness in query expansion for search engines, hence we consider expanding response using PRF information. However, existing PRF approaches are either based on heuristic rules or require heavy manual labeling, which are not suitable for solving our task. To alleviate this problem, we treat the PRF selection for response expansion as a learning task and propose a reinforced learning method that can be trained in an end-to-end manner without any human annotations. More specifically, we propose a reinforced selector to extract useful PRF terms to enhance response candidates and a BERT-based response ranker to rank the PRF-enhanced responses. The performance of the ranker serves as a reward to guide the selector to extract useful PRF terms, which boosts the overall task performance. Extensive experiments on both standard benchmarks and commercial datasets prove the superiority of our reinforced PRF term selector compared with other potential soft or hard selection methods. Both case studies and quantitative analysis show that our model is capable of selecting meaningful PRF terms to expand response candidates and also achieving the best results compared with all baselines on a variety of evaluation metrics. We have also deployed our method on online production in an e-commerce company, which shows a significant improvement over the existing online ranking system.",
      "authors": [
        "Pan, Haojie",
        "Chen, Cen",
        "Wang, Chengyu",
        "Qiu, Minghui",
        "Yang, Liu",
        "Ji, Feng",
        "Huang, Jun"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-11-25",
      "selected": null,
      "title": "Learning to Expand: Reinforced Pseudo-relevance Feedback Selection for Information-seeking Conversations",
      "urls": [
        "http://arxiv.org/abs/2011.12771v2",
        "http://arxiv.org/pdf/2011.12771.pdf",
        "http://arxiv.org/pdf/2011.12771v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Tsuda B."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.2009591117",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "29872-29882",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2020-11-24",
      "selected": null,
      "title": "A modeling framework for adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096889382&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Eckstein M.K."
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.1912330117",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "29381-29389",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2020-11-24",
      "selected": null,
      "title": "Computational evidence for hierarchically structured reinforcement learning in humans",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096174268&origin=inward"
      ]
    },
    {
      "abstract": "This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.",
      "authors": [
        "Kumar, Ramana",
        "Uesato, Jonathan",
        "Ngo, Richard",
        "Everitt, Tom",
        "Krakovna, Victoria",
        "Legg, Shane"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-11-17",
      "selected": null,
      "title": "REALab: An Embedded Perspective on Tampering",
      "urls": [
        "http://arxiv.org/pdf/2011.08820v1",
        "http://arxiv.org/abs/2011.08820v1",
        "http://arxiv.org/pdf/2011.08820.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "\u5f20\u51ef\u4e50",
        "\u5218\u5a77\u5a77",
        "\u5218\u7bb4",
        "\u5e84\u5bc5",
        "\u67f4\u8273\u6770"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.11834/jig.200251",
      "keywords": [
        "\u60c5\u7eea\u8c03\u8282",
        "\u4eba\u673a\u4ea4\u4e92(HCI)",
        "\u591a\u6a21\u6001",
        "\u673a\u5668\u5b66\u4e60",
        "\u60c5\u611f\u8ba1\u7b97"
      ],
      "number_of_pages": 14,
      "pages": "2451-2464",
      "publication": {
        "category": "Journal",
        "cite_score": 1.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10068961",
        "publisher": "Editorial and Publishing Board of JIG",
        "sjr": 0.175,
        "snip": 0.521,
        "subject_areas": [
          "Artificial Intelligence",
          "Human-Computer Interaction",
          "Computer Vision and Pattern Recognition",
          "Computer Graphics and Computer-Aided Design"
        ],
        "title": "Journal of Image and Graphics"
      },
      "publication_date": "2020-11-16",
      "selected": null,
      "title": "\u9762\u5411\u60c5\u7eea\u8c03\u8282\u7684\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u6280\u672f",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097236998&origin=inward"
      ]
    },
    {
      "abstract": "Current hearing aid fittings are carried out using prescriptive compression settings. These settings are derived from group averages but do not account for individual differences or preferences. In a previous work, we developed a human-in-the-loop deep reinforcement learning compression approach to set the compression ratios across a number of frequency bands. These compression ratios were compared to those of the widely used and accepted DSL-v5 hearing aid prescription to determine if incorporating user preference impacted word recognition performance. A pilot clinical study of this comparison for four participants is reported in this paper. The clinical testing results obtained strongly support the hypothesis that the personalized compression settings do not negatively impact word recognition or audibility compared to the prescriptive compression settings. The ability to provide a personalized amplification strategy with no degradation in hearing performance would be of benefit in hearing aids or other assistive listening technologies.",
      "authors": [
        "Sara Akbarzadeh",
        "Nasim Alamdari",
        "Christina Campbell",
        "Edward Lobarinas",
        "Nasser Kehtarnavaz"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/DCAS51144.2020.9330651",
      "keywords": [
        "personalized compression",
        "clinical testing of word recognition",
        "deep reinforcement learning"
      ],
      "number_of_pages": 2,
      "pages": "1-2",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-8511-8",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 IEEE 14th Dallas Circuits and Systems Conference (DCAS)"
      },
      "publication_date": "2020-11-15",
      "selected": null,
      "title": "Word Recognition Clinical Testing of Personalized Deep Reinforcement Learning Compression",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9330651"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Akbarzadeh S."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/DCAS51144.2020.9330640",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728185101",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2020 IEEE Dallas Circuits and Systems Conference, DCAS 2020"
      },
      "publication_date": "2020-11-15",
      "selected": null,
      "title": "Word recognition clinical testing of personalized deep reinforcement learning compression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100875647&origin=inward"
      ]
    },
    {
      "abstract": "Humans quickly solve tasks in novel systems with complex dynamics, without requiring much interaction. While deep reinforcement learning algorithms have achieved tremendous success in many complex tasks, these algorithms need a large number of samples to learn meaningful policies. In this paper, we present a task for navigating a marble to the center of a circular maze. While this system is very intuitive and easy for humans to solve, it can be very difficult and inefficient for standard reinforcement learning algorithms to learn meaningful policies. We present a model that learns to move a marble in the complex environment within minutes of interacting with the real system. Learning consists of initializing a physics engine with parameters estimated using data from the real system. The error in the physics engine is then corrected using Gaussian process regression, which is used to model the residual between real observations and physics engine simulations. The physics engine augmented with the residual model is then used to control the marble in the maze environment using a model-predictive feedback over a receding horizon. To the best of our knowledge, this is the first time that a hybrid model consisting of a full physics engine along with a statistical function approximator has been used to control a complex physical system in real-time using nonlinear model-predictive control (NMPC).",
      "authors": [
        "Ota, Kei",
        "Jha, Devesh K.",
        "Romeres, Diego",
        "van Baar, Jeroen",
        "Smith, Kevin A.",
        "Semitsu, Takayuki",
        "Oiki, Tomoaki",
        "Sullivan, Alan",
        "Nikovski, Daniel",
        "Tenenbaum, Joshua B."
      ],
      "categories": null,
      "citations": null,
      "comments": "Under submission",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-11-14",
      "selected": null,
      "title": "Data-Efficient Learning for Complex and Real-Time Physical Problem Solving using Augmented Simulation",
      "urls": [
        "http://arxiv.org/pdf/2011.07193.pdf",
        "http://arxiv.org/abs/2011.07193v2",
        "http://arxiv.org/pdf/2011.07193v2"
      ]
    },
    {
      "abstract": "Inverse reinforcement learning (IRL) is a common technique for inferring human preferences from data. Standard IRL techniques tend to assume that the human demonstrator is stationary, that is that their policy $\\pi$ doesn't change over time. In practice, humans interacting with a novel environment or performing well on a novel task will change their demonstrations as they learn more about the environment or task. We investigate the consequences of relaxing this assumption of stationarity, in particular by modelling the human as learning. Surprisingly, we find in some small examples that this can lead to better inference than if the human was stationary. That is, by observing a demonstrator who is themselves learning, a machine can infer more than by observing a demonstrator who is noisily rational. In addition, we find evidence that misspecification can lead to poor inference, suggesting that modelling human learning is important, especially when the human is facing an unfamiliar environment.",
      "authors": [
        "Giles, Harry",
        "Chan, Lawrence"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to the 2020 NeurIPS HAMLETS workshop",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-11-11",
      "selected": null,
      "title": "Accounting for Human Learning when Inferring Human Preferences",
      "urls": [
        "http://arxiv.org/pdf/2011.05596.pdf",
        "http://arxiv.org/abs/2011.05596v2",
        "http://arxiv.org/pdf/2011.05596v2"
      ]
    },
    {
      "abstract": "Multi-agent reinforcement learning (MARL) has shown recent success in increasingly complex fixed-team zero-sum environments. However, the real world is not zero-sum nor does it have fixed teams; humans face numerous social dilemmas and must learn when to cooperate and when to compete. To successfully deploy agents into the human world, it may be important that they be able to understand and help in our conflicts. Unfortunately, selfish MARL agents typically fail when faced with social dilemmas. In this work, we show evidence of emergent direct reciprocity, indirect reciprocity and reputation, and team formation when training agents with randomized uncertain social preferences (RUSP), a novel environment augmentation that expands the distribution of environments agents play in. RUSP is generic and scalable; it can be applied to any multi-agent environment without changing the original underlying game dynamics or objectives. In particular, we show that with RUSP these behaviors can emerge and lead to higher social welfare equilibria in both classic abstract social dilemmas like Iterated Prisoner's Dilemma as well in more complex intertemporal environments.",
      "authors": [
        "Baker, Bowen"
      ],
      "categories": null,
      "citations": 7,
      "comments": "to be published in NeurIPS 2020 proceedings",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2020-11-10",
      "selected": null,
      "title": "Emergent Reciprocity and Team Formation from Randomized Uncertain Social Preferences",
      "urls": [
        "http://arxiv.org/pdf/2011.05373.pdf",
        "http://arxiv.org/pdf/2011.05373v1",
        "http://arxiv.org/abs/2011.05373v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108433410&origin=inward"
      ]
    },
    {
      "abstract": "Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning (RL) setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions.",
      "authors": [
        "Kreutzer, Julia",
        "Riezler, Stefan",
        "Lawrence, Carolin"
      ],
      "categories": null,
      "citations": null,
      "comments": "5th Workshop on Structured Prediction for NLP at ACL 2021 Previously\n  named \"Learning from Human Feedback: Challenges for Real-World Reinforcement\n  Learning in NLP\" and presented at Challenges of Real-World RL Workshop at\n  NeurIPS 2020",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-11-04",
      "selected": null,
      "title": "Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks",
      "urls": [
        "http://arxiv.org/pdf/2011.02511v3",
        "http://arxiv.org/pdf/2011.02511.pdf",
        "http://arxiv.org/abs/2011.02511v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhao Z."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.brainres.2020.146979",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00068993",
        "publisher": "Elsevier B.V.",
        "sjr": 0.854,
        "snip": 0.766,
        "subject_areas": [
          "Neuroscience (all)",
          "Molecular Biology",
          "Neurology (clinical)",
          "Developmental Biology"
        ],
        "title": "Brain Research"
      },
      "publication_date": "2020-11-01",
      "selected": null,
      "title": "Dynamic changes of brain networks during feedback-related processing of reinforcement learning in schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086567574&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mukherjee D."
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/abn0000641",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "810-823",
      "publication": {
        "category": "Journal",
        "cite_score": 11.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0021843X",
        "publisher": "American Psychological Association",
        "sjr": 2.031,
        "snip": 2.532,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "Journal of Abnormal Psychology"
      },
      "publication_date": "2020-11-01",
      "selected": null,
      "title": "Reward and punishment reversal-learning in major depressive disorder.",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091818646&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Taschereau-Dumouchel V."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.tics.2020.09.006",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "856-858",
      "publication": {
        "category": "Journal",
        "cite_score": 30.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13646613",
        "publisher": "Elsevier Ltd.",
        "sjr": 5.615,
        "snip": 6.454,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Trends in Cognitive Sciences"
      },
      "publication_date": "2020-11-01",
      "selected": null,
      "title": "Could Brain Decoding Machines Change Our Minds?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091680224&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kildahl N."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.addbeh.2020.106496",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03064603",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.328,
        "snip": 1.371,
        "subject_areas": [
          "Toxicology",
          "Medicine (miscellaneous)",
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Addictive Behaviors"
      },
      "publication_date": "2020-11-01",
      "selected": null,
      "title": "Individual differences in learning during decision-making may predict specific harms associated with gambling",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086590314&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zha D."
      ],
      "categories": null,
      "citations": 27,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICDM50108.2020.00086",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "771-780",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728183169",
        "issn": "15504786",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Data Mining, ICDM"
      },
      "publication_date": "2020-11-01",
      "selected": null,
      "title": "Meta-AAD: Active anomaly detection with deep reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100901648&origin=inward"
      ]
    },
    {
      "abstract": "We are interested in the design of autonomous robot behaviors that learn the preferences of users over continued interactions, with the goal of efficiently executing navigation behaviors in a way that the user expects. In this paper, we discuss our work in progress to modify a general model for robot navigation behaviors in an exploration task on a per-user basis using preference-based reinforcement learning. The novel contribution of this approach is that it combines reinforcement learning, motion planning, and natural language processing to allow an autonomous agent to learn from sustained dialogue with a human teammate as opposed to one-off instructions.",
      "authors": [
        "Hayes, Cory",
        "Marge, Matthew"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted for publication at AI-HRI 2020 (arXiv:2010.13830)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-10-30",
      "selected": null,
      "title": "Towards Preference Learning for Autonomous Ground Robot Navigation Tasks",
      "urls": [
        "http://arxiv.org/pdf/2010.16361.pdf",
        "http://arxiv.org/abs/2010.16361v2",
        "http://arxiv.org/pdf/2010.16361v2"
      ]
    },
    {
      "abstract": "In this article, the authors present a novel method to learn the personalized tactic of discretionary lane-change initiation for fully autonomous vehicles through human-computer interactions. Instead of learning from human-driving demonstrations, a reinforcement learning technique is employed to learn how to initiate lane changes from traffic context, the action of a self-driving vehicle, and in-vehicle user feedback. The proposed offline algorithm rewards the action-selection strategy when the user gives positive feedback and penalizes it when negative feedback. Also, a multi-dimensional driving scenario is considered to represent a more realistic lane-change trade-off. The results show that the lane-change initiation model obtained by this method can reproduce the personal lane-change tactic, and the performance of the customized models (average accuracy 86.1%) is much better than that of the non-customized models (average accuracy 75.7%). This method allows continuous improvement of customization for users during fully autonomous driving even without human-driving experience, which will significantly enhance the user acceptance of high-level autonomy of self-driving vehicles.",
      "authors": [
        "Liu, Zhuoxi",
        "Wang, Zheng",
        "Yang, Bo",
        "Nakano, Kimihiko"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-10-29",
      "selected": null,
      "title": "Learning Personalized Discretionary Lane-Change Initiation for Fully Autonomous Driving Based on Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2010.15372v1",
        "http://arxiv.org/pdf/2010.15372.pdf",
        "http://arxiv.org/pdf/2010.15372v1"
      ]
    },
    {
      "abstract": "Human demonstrations can provide trustful samples to train reinforcement learning algorithms for robots to learn complex behaviors in real-world environments. However, obtaining sufficient demonstrations may be impractical because many behaviors are difficult for humans to demonstrate. A more practical approach is to replace human demonstrations by human queries, i.e., preference-based reinforcement learning. One key limitation of the existing algorithms is the need for a significant amount of human queries because a large number of labeled data is needed to train neural networks for the approximation of a continuous, high-dimensional reward function. To reduce and minimize the need for human queries, we propose a new GAN-assisted human preference-based reinforcement learning approach that uses a generative adversarial network (GAN) to actively learn human preferences and then replace the role of human in assigning preferences. The adversarial neural network is simple and only has a binary output, hence requiring much less human queries to train. Moreover, a maximum entropy based reinforcement learning algorithm is designed to shape the loss towards the desired regions or away from the undesired regions. To show the effectiveness of the proposed approach, we present some studies on complex robotic tasks without access to the environment reward in a typical MuJoCo robot locomotion environment. The obtained results show our method can achieve a reduction of about 99.8% human time without performance sacrifice.",
      "authors": [
        "Zhan, Huixin",
        "Tao, Feng",
        "Cao, Yongcan"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-10-15",
      "selected": null,
      "title": "Human-guided Robot Behavior Learning: A GAN-assisted Preference-based Reinforcement Learning Approach",
      "urls": [
        "http://arxiv.org/pdf/2010.07467.pdf",
        "http://arxiv.org/abs/2010.07467v1",
        "http://arxiv.org/pdf/2010.07467v1"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781665404792",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 23rd IEEE International Symposium on Measurement and Control in Robotics, ISMCR 2020"
      },
      "publication_date": "2020-10-15",
      "selected": null,
      "title": "2020 23rd IEEE International Symposium on Measurement and Control in Robotics, ISMCR 2020",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85126430020&origin=inward"
      ]
    },
    {
      "abstract": "During human-robot interaction, errors will occur. Hence, understanding the effects of interaction errors and especially the effect of prior knowledge on robot learning performance is relevant to develop appropriate approaches for learning under natural interaction conditions, since future robots will continue to learn based on what they have already learned. In this study, we investigated interaction errors that occurred under two learning conditions, i.e., in the case that the robot learned without prior knowledge (cold-start learning) and in the case that the robot had prior knowledge (warm-start learning). In our human-robot interaction scenario, the robot learns to assign the correct action to a current human intention (gesture). Gestures were not predefined but the robot had to learn their meaning. We used a contextual-bandit approach to maximize the expected payoff by updating (a) the current human intention (gesture) and (b) the current human intrinsic feedback after each action selection of the robot. As an intrinsic evaluation of the robot behavior we used the error-related potential (ErrP) in the human electroencephalogram as reinforcement signal. Either gesture errors (human intentions) can be misinterpreted by incorrectly captured gestures or errors in the ErrP classification (human feedback) can occur. We investigated these two types of interaction errors and their effects on the learning process. Our results show that learning and its online adaptation was successful under both learning conditions (except for one subject in cold-start learning). Furthermore, warm-start learning achieved faster convergence, while cold-start learning was less affected by online changes in the current context.",
      "authors": [
        "Kim, Su Kyoung",
        "Kirchner, Elsa Andrea",
        "Schlo\u00dfm\u00fcller, Lukas",
        "Kirchner, Frank"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/frobt.2020.558531",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2296-9144",
        "publisher": "Frontiers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Robotics and AI"
      },
      "publication_date": "2020-10-15",
      "selected": null,
      "title": "Errors in Human-Robot Interactions and Their Effects on Robot Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094680107&origin=inward"
      ]
    },
    {
      "abstract": "How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions. A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions. We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty. We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.",
      "authors": [
        "Jaques, Natasha",
        "Shen, Judy Hanwen",
        "Ghandeharioun, Asma",
        "Ferguson, Craig",
        "Lapedriza, Agata",
        "Jones, Noah",
        "Gu, Shixiang Shane",
        "Picard, Rosalind"
      ],
      "categories": null,
      "citations": 18,
      "comments": "To appear in EMNLP 2020 (long paper)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 19,
      "pages": "3985-4003",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781952148606",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference"
      },
      "publication_date": "2020-10-12",
      "selected": null,
      "title": "Human-centric Dialog Training via Offline Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2010.05848v1",
        "http://arxiv.org/pdf/2010.05848.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108137300&origin=inward",
        "http://arxiv.org/pdf/2010.05848v1"
      ]
    },
    {
      "abstract": "We investigate a deep reinforcement learning (RL) architecture that supports explaining why a learned agent prefers one action over another. The key idea is to learn action-values that are directly represented via human-understandable properties of expected futures. This is realized via the embedded self-prediction (ESP)model, which learns said properties in terms of human provided features. Action preferences can then be explained by contrasting the future properties predicted for each action. To address cases where there are a large number of features, we develop a novel method for computing minimal sufficient explanations from anESP. Our case studies in three domains, including a complex strategy game, show that ESP models can be effectively learned and support insightful explanations.",
      "authors": [
        "Lin, Zhengxian",
        "Lam, Kim-Ho",
        "Fern, Alan"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published (Oral) at ICLR 2021",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-10-11",
      "selected": null,
      "title": "Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions",
      "urls": [
        "http://arxiv.org/pdf/2010.05180v2",
        "http://arxiv.org/pdf/2010.05180.pdf",
        "http://arxiv.org/abs/2010.05180v2"
      ]
    },
    {
      "abstract": "In this article, the authors present a novel method to learn the personalized tactic of discretionary lane-change initiation for fully autonomous vehicles through human-computer interactions. Instead of learning from human-driving demonstrations, a reinforcement learning technique is employed to learn how to initiate lane changes from traffic context, the action of a self-driving vehicle, and in-vehicle user&#x2019;s feedback. The proposed offline algorithm rewards the action-selection strategy when the user gives positive feedback and penalizes it when negative feedback. Also, a multi-dimensional driving scenario is considered to represent a more realistic lane-change trade-off. The results show that the lane-change initiation model obtained by this method can reproduce the personal lane-change tactic, and the performance of the customized models (average accuracy 86.1%) is much better than that of the non-customized models (average accuracy 75.7%). This method allows continuous improvement of customization for users during fully autonomous driving even without human-driving experience, which will significantly enhance the user acceptance of high-level autonomy of self-driving vehicles.",
      "authors": [
        "Zhuoxi Liu",
        "Zheng Wang",
        "Bo Yang",
        "Kimihiko Nakano"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/SMC42975.2020.9283222",
      "keywords": [
        "human-machine interface",
        "lane-change initiation",
        "reinforcement learning",
        "personalized model",
        "autonomous driving"
      ],
      "number_of_pages": 7,
      "pages": "457-463",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3703-7",
        "issn": "1062-922X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics"
      },
      "publication_date": "2020-10-11",
      "selected": null,
      "title": "Learning Personalized Discretionary Lane-Change Initiation for Fully Autonomous Driving Based on Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098875413&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9283222",
        "https://dl.acm.org/doi/10.1109/SMC42975.2020.9283222"
      ]
    },
    {
      "abstract": "This work aims at improving the workers&#x2019; wellbeing by providing them with skill-based personalized assistance in the context of physical Human-Robot Collaboration (pHRC). Past researches usually assume that each person will respond equally to assistance and therefore do not update their assistance policy online. However, since the focus of our work is on humans in pHRC, intra- and inter-individual variations are to be considered. Thus, we propose a new hybrid approach that combines reinforcement learning and a symbolic approach based on an ontology to guide humans towards skills improvement using solely internal robot data without any additional sensor. The advantage of this combination is to handle constant adaptation of users needs while reducing the learning process. This reduction is insured by the use of a knowledge base to choose the most suitable assistance, as well as a pre-training of the learning algorithm in simulation. In addition, including human feedback in the learning algorithm speeds up learning and ensures that unwanted assistance is not provided to the operator. Finally, since acquiring a skill involves both theory and practice, we offer two types of assistance, textual advice, along with a change of the robot behavior. We have demonstrated through simulations and a real-world experimentation that our approach leads the learner more quickly to the mastery of skills and thus eases the on-the- job training.",
      "authors": [
        "Katleen Blanchet",
        "Amel Bouzeghoub",
        "Selma Kchir",
        "Olivier Lebec"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/SMC42975.2020.9283469",
      "keywords": [
        "Human-in-the-Loop",
        "Human-Centered Reinforcement Learning",
        "Real-World Robotic Application",
        "Physical Human-Robot Collaboration",
        "Q-Learning",
        "Human Profiling",
        "Ontology",
        "Robot Assistance",
        "Profile Oriented Adaptation",
        "Human-Robot Symbiosis"
      ],
      "number_of_pages": 7,
      "pages": "4281-4287",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3703-7",
        "issn": "1062-922X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics"
      },
      "publication_date": "2020-10-11",
      "selected": null,
      "title": "How to Guide Humans Towards Skills Improvement in Physical Human-Robot Collaboration Using Reinforcement Learning?",
      "urls": [
        "https://dl.acm.org/doi/10.1109/SMC42975.2020.9283469",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098872493&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9283469"
      ]
    },
    {
      "abstract": "Driving behavior modeling is of great importance for designing safe, smart, and personalized autonomous driving systems. In this paper, an internal reward function-based driving model that emulates the human's decision-making mechanism is utilized. To infer the reward function parameters from naturalistic human driving data, we propose a structural assumption about human driving behavior that focuses on discrete latent driving intentions. It converts the continuous behavior modeling problem to a discrete setting and thus makes maximum entropy inverse reinforcement learning (IRL) tractable to learn reward functions. Specifically, a polynomial trajectory sampler is adopted to generate candidate trajectories considering high-level intentions and approximate the partition function in the maximum entropy IRL framework. An environment model considering interactive behaviors among the ego and surrounding vehicles is built to better estimate the generated trajectories. The proposed method is applied to learn personalized reward functions for individual human drivers from the NGSIM highway driving dataset. The qualitative results demonstrate that the learned reward functions are able to explicitly express the preferences of different drivers and interpret their decisions. The quantitative results reveal that the learned reward functions are robust, which is manifested by only a marginal decline in proximity to the human driving trajectories when applying the reward function in the testing conditions. For the testing performance, the personalized modeling method outperforms the general modeling approach, significantly reducing the modeling errors in human likeness (a custom metric to gauge accuracy), and these two methods deliver better results compared to other baseline methods.",
      "authors": [
        "Huang, Zhiyu",
        "Wu, Jingda",
        "Lv, Chen"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-10-07",
      "selected": null,
      "title": "Driving Behavior Modeling using Naturalistic Human Driving Data with Inverse Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2010.03118v4",
        "http://arxiv.org/pdf/2010.03118.pdf",
        "http://arxiv.org/abs/2010.03118v4"
      ]
    },
    {
      "abstract": "<p>The neurotransmitter dopamine is implicated in diverse functions, including reward processing, reinforcement learning, and cognitive control. The tendency to discount future rewards over time has long been discussed in the context of potential dopaminergic modulation. Here we examined the effect of a single dose of the D2 receptor antagonist haloperidol (2 mg) on temporal discounting in healthy female and male human participants. Our approach extends previous pharmacological studies in two ways. First, we applied combined temporal discounting drift diffusion models to examine choice dynamics. Second, we examined dopaminergic modulation of reward magnitude effects on temporal discounting. Hierarchical Bayesian parameter estimation revealed that the data were best accounted for by a temporal discounting drift diffusion model with nonlinear trialwise drift rate scaling. This model showed good parameter recovery, and posterior predictive checks revealed that it accurately reproduced the relationship between decision conflict and response times in individual participants. We observed reduced temporal discounting and substantially faster nondecision times under haloperidol compared with placebo. Discounting was steeper for low versus high reward magnitudes, but this effect was largely unaffected by haloperidol. Results were corroborated by model-free analyses and modeling via more standard approaches. We previously reported elevated caudate activation under haloperidol in this sample of participants, supporting the idea that haloperidol elevated dopamine neurotransmission (e.g., by blocking inhibitory feedback via presynaptic D2 auto-receptors). The present results reveal that this is associated with an augmentation of both lower-level (nondecision time) and higher-level (temporal discounting) components of the decision process.</p><p><b>SIGNIFICANCE STATEMENT</b> Dopamine is implicated in reward processing, reinforcement learning, and cognitive control. Here we examined the effects of a single dose of the D2 receptor antagonist haloperidol on temporal discounting and choice dynamics during the decision process. We extend previous studies by applying computational modeling using the drift diffusion model, which revealed that haloperidol reduced the nondecision time and reduced impulsive choice compared with placebo. These findings are compatible with a haloperidol-induced increase in striatal dopamine (e.g., because of a presynaptic mechanism). Our data provide novel insights into the contributions of dopamine to value-based decision-making and highlight how comprehensive model-based analyses using sequential sampling models can inform the effects of pharmacological modulation on choice processes.</p>",
      "authors": [
        "Ben Wagner",
        "Mareike Clos",
        "Tobias Sommer",
        "Jan Peters"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.0592-20.2020",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "7936-7948",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2020-10-07",
      "selected": null,
      "title": "Dopaminergic Modulation of Human Intertemporal Choice: A Diffusion Model Analysis Using the D2-Receptor Antagonist Haloperidol",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092750277&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ernst B."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bandc.2020.105610",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02782626",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Brain and Cognition"
      },
      "publication_date": "2020-10-01",
      "selected": null,
      "title": "The effect of feedback novelty on neural correlates of feedback processing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089076291&origin=inward"
      ]
    },
    {
      "abstract": "Foraging animals have to evaluate, compare and select food patches in order to increase their fitness. Understanding what drives foraging decisions requires careful manipulation of the value of alternative options while monitoring animals choices. Value-based decision-making tasks in combination with formal learning models have provided both an experimental and theoretical framework to study foraging decisions in lab settings. While these approaches were successfully used in the past to understand what drives choices in mammals, very little work has been done on fruit flies. This is despite the fact that fruit flies have served as model organism for many complex behavioural paradigms. To fill this gap we developed a single-animal, trial-based decision making task, where freely walking flies experienced optogenetic sugar-receptor neuron stimulation. We controlled the value of available options by manipulating the probabilities of optogenetic stimulation. We show that flies integrate reward history of chosen options and forget value of unchosen options. We further discover that flies assign higher values to rewards experienced early in the behavioural session, consistent with formal reinforcement learning models. Finally, we also show that the probabilistic rewards affect walking trajectories of flies, suggesting that accumulated value is controlling the navigation vector of flies in a graded fashion. These findings establish the fruit fly as a model organism to explore the genetic and circuit basis of reward foraging decisions.",
      "authors": [
        "Sophie E. Seidenbecher",
        "Joshua I. Sanders",
        "Anne C. von Philipsborn",
        "Duda Kvitsiani"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0239616",
      "keywords": [
        "Neurons",
        "Learning",
        "Foraging",
        "Behavior",
        "Decision making",
        "Animal behavior",
        "Optogenetics",
        "Genetics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2020-10-01",
      "selected": null,
      "title": "Reward foraging task and model-based analysis reveal how fruit flies learn value of available options",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0239616&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092225838&origin=inward"
      ]
    },
    {
      "abstract": "Decision making relies on the interplay between two distinct learning mechanisms, namely habitual model-free learning and goal-directed model-based learning. Recent literature suggests that this interplay is significantly shaped by the environmental structure as represented by an internal model. We employed a modified two-stage but one-decision Markov decision task to investigate how two internal models differing in the predictability of stage transitions influence the neural correlates of feedback processing. Our results demonstrate that fronto-central theta and the feedback-related negativity (FRN), two correlates of reward prediction errors in the medial frontal cortex, are independent of the internal representations of the environmental structure. In contrast, centro-parietal delta and the P3, two correlates possibly reflecting feedback evaluation in working memory, were highly susceptible to the underlying internal model. Model-based analyses of single-trial activity showed a comparable pattern, indicating that while the computation of unsigned reward prediction errors is represented by theta and the FRN irrespective of the internal models, the P3 adapts to the internal representation of an environment. Our findings further substantiate the assumption that the feedback-locked components under investigation reflect distinct mechanisms of feedback processing and that different internal models selectively influence these mechanisms.",
      "authors": [
        "Wurm, Franz",
        "Ernst, Benjamin",
        "Steinhauser, Marco"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-020-00820-6",
      "keywords": [],
      "number_of_pages": 20,
      "pages": "1070-1089",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2020-10-01",
      "selected": null,
      "title": "The influence of internal models on feedback-related brain activity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089558402&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-020-00820-6.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhou D."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cobeha.2020.09.007",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "125-134",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Current Opinion in Behavioral Sciences"
      },
      "publication_date": "2020-10-01",
      "selected": null,
      "title": "The growth and form of knowledge networks by kinesthetic curiosity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093121778&origin=inward"
      ]
    },
    {
      "abstract": "We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based \"literal\" and \"pragmatic\" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the \"pragmatic\" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.",
      "authors": [
        "Sumers, Theodore R.",
        "Ho, Mark K.",
        "Hawkins, Robert D.",
        "Narasimhan, Karthik",
        "Griffiths, Thomas L."
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages, 4 figures. AAAI '21",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-09-30",
      "selected": null,
      "title": "Learning Rewards from Linguistic Feedback",
      "urls": [
        "http://arxiv.org/pdf/2009.14715v3",
        "http://arxiv.org/pdf/2009.14715.pdf",
        "http://arxiv.org/abs/2009.14715v3"
      ]
    },
    {
      "abstract": "Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors, such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the semantic difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from semantic changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets.",
      "authors": [
        "Shi, Xiangxi",
        "Yang, Xu",
        "Gu, Jiuxiang",
        "Joty, Shafiq",
        "Cai, Jianfei"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Computer Vision and Pattern Recognition"
        ],
        "title": "ECCV2020"
      },
      "publication_date": "2020-09-30",
      "selected": null,
      "title": "Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning",
      "urls": [
        "http://arxiv.org/pdf/2009.14352.pdf",
        "http://arxiv.org/abs/2009.14352v1",
        "http://arxiv.org/pdf/2009.14352v1"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) is a promising field to enhance robotic autonomy and decision making capabilities for space robotics, something which is challenging with traditional techniques due to stochasticity and uncertainty within the environment. RL can be used to enable lunar cave exploration with infrequent human feedback, faster and safer lunar surface locomotion or the coordination and collaboration of multi-robot systems. However, there are many hurdles making research challenging for space robotic applications using RL and machine learning, particularly due to insufficient resources for traditional robotics simulators like CoppeliaSim. Our solution to this is an open source modular platform called Reinforcement Learning for Simulation based Training of Robots, or RL STaR, that helps to simplify and accelerate the application of RL to the space robotics research field. This paper introduces the RL STaR platform, and how researchers can use it through a demonstration.",
      "authors": [
        "Blum, Tamir",
        "Paillet, Gabin",
        "Laine, Mickael",
        "Yoshida, Kazuya"
      ],
      "categories": null,
      "citations": null,
      "comments": "3 figures, 1 table",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-09-21",
      "selected": null,
      "title": "RL STaR Platform: Reinforcement Learning for Simulation based Training of Robots",
      "urls": [
        "http://arxiv.org/pdf/2009.09595v1",
        "http://arxiv.org/pdf/2009.09595.pdf",
        "http://arxiv.org/abs/2009.09595v1"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning proposes the use of externally sourced information in order to speed up the learning process. When interacting with a learner agent, humans may provide either evaluative or informative advice. Prior research has focused on the effect of human-sourced advice by including real-time feedback on the interactive reinforcement learning process, specifically aiming to improve the learning speed of the agent, while minimising the time demands on the human. This work focuses on answering which of two approaches, evaluative or informative, is the preferred instructional approach for humans. Moreover, this work presents an experimental setup for a human trial designed to compare the methods people use to deliver advice in terms of human engagement. The results obtained show that users giving informative advice to the learner agents provide more accurate advice, are willing to assist the learner agent for a longer time, and provide more advice per episode. Additionally, self-evaluation from participants using the informative approach has indicated that the agent\u2019s ability to follow the advice is higher, and therefore, they feel their own advice to be of higher accuracy when compared to people providing evaluative advice.",
      "authors": [
        "Bignold, Adam",
        "Cruz, Francisco",
        "Dazeley, Richard",
        "Vamplew, Peter",
        "Foale, Cameron"
      ],
      "categories": null,
      "citations": 3,
      "comments": "23 pages, 15 figures",
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1007/s00521-021-06850-6",
      "keywords": [
        "Evaluative and informative advice",
        "Policy shaping",
        "Reward shaping",
        "Interactive reinforcement learning",
        "User study",
        "Assisted reinforcement learning"
      ],
      "number_of_pages": 16,
      "pages": "18215-18230",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0941-0643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Multiagent Systems",
          "Artificial Intelligence",
          "Software",
          "Human-Computer Interaction"
        ],
        "title": "Neural Computing and Applications, 1-16 (2022)"
      },
      "publication_date": "2020-09-21",
      "selected": null,
      "title": "Human engagement providing evaluative and informative advice for interactive reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122825590&origin=inward",
        "https://dl.acm.org/doi/10.1007/s00521-021-06850-6",
        "https://link.springer.com/content/pdf/10.1007/s00521-021-06850-6.pdf",
        "http://arxiv.org/abs/2009.09575v2",
        "http://arxiv.org/pdf/2009.09575v2",
        "http://dx.doi.org/10.1007/s00521-021-06850-6"
      ]
    },
    {
      "abstract": "Cyber-physical and ambient systems surround the human user with applications that should be tailored as possible to her/his preferences and the current situation. We propose to build them automatically and on the fly by composition of software components present at the time in the environment, but without prior expression of the user's needs or process specification or composition model. In order to produce knowledge useful for automatic composition in the absence of an initial guideline, we have developed a generic solution based on lifelong online reinforcement learning. It is decentralized within a multi-agent system where agents learn incrementally from user feedback to satisfy her/him. Different use cases have been experimented in which applications, adapted to the user and the situation, are composed and emerge automatically and continuously.",
      "authors": [
        "Walid Younes",
        "Fran\u00e7oise Adreit",
        "Sylvie Trouilhet",
        "Jean-Paul Arcangeli"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/WETICE49692.2020.00009",
      "keywords": [
        "Agent",
        "service discovery",
        "ambient intelligence",
        "selection and composition",
        "reinforcement learning",
        "online learning",
        "multi-agent system",
        "user feedback"
      ],
      "number_of_pages": 6,
      "pages": "3-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6976-7",
        "issn": "1524-4547",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)"
      },
      "publication_date": "2020-09-10",
      "selected": null,
      "title": "Agent-mediated application emergence through reinforcement learning from user feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9338484",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85100711696&origin=inward"
      ]
    },
    {
      "abstract": "The paper summarizes our efforts to develop a spike timing neural network model of dynamic visual information processing and decision making inspired by the available knowledge about how the human brain performs this complicated task. It consists of multiple layers with functionality corresponding to the main visual information processing structures starting from the early level of the visual system up to the areas responsible for decision making based on accumulated sensory evidence as well as the basal ganglia modulation due to the feedback from the environment. In the present work, we investigated age-related changes in the spike timing dependent plastic synapses of the model as a result of reinforcement learning.",
      "authors": [
        "Petia Koprinkova-Hristova",
        "Nadejda Bocheva"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.15439/2020F141",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "93-100",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-4147-3",
        "issn": "2300-5963",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2020 Federated Conference on Computer Science and Information Systems, FedCSIS 2020"
      },
      "publication_date": "2020-09-06",
      "selected": null,
      "title": "Age-related Spike Timing Dependent Plasticity of Brain-inspired Model of Visual Information Processing with Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9222902",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095727187&origin=inward",
        "pdf/141.pdf"
      ]
    },
    {
      "abstract": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.",
      "authors": [
        "Stiennon, Nisan",
        "Ouyang, Long",
        "Wu, Jeff",
        "Ziegler, Daniel M.",
        "Lowe, Ryan",
        "Voss, Chelsea",
        "Radford, Alec",
        "Amodei, Dario",
        "Christiano, Paul"
      ],
      "categories": null,
      "citations": 143,
      "comments": "NeurIPS 2020",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2020-09-02",
      "selected": null,
      "title": "Learning to summarize from human feedback",
      "urls": [
        "http://arxiv.org/pdf/2009.01325.pdf",
        "http://arxiv.org/abs/2009.01325v3",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108421820&origin=inward",
        "http://arxiv.org/pdf/2009.01325v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhou W."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1177/0361198120949875",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "915-925",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03611981",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Transportation Research Record"
      },
      "publication_date": "2020-09-01",
      "selected": null,
      "title": "Q-Learning-Based Coordinated Variable Speed Limit and Hard Shoulder Running Control Strategy to Reduce Travel Time at Freeway Corridor",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096130632&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "LeBlanc K.A."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/xhp0000791",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1001-1012",
      "publication": {
        "category": "Journal",
        "cite_score": 4.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00961523",
        "publisher": "American Psychological Association",
        "sjr": 1.108,
        "snip": 1.029,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Arts and Humanities (miscellaneous)"
        ],
        "title": "Journal of Experimental Psychology: Human Perception and Performance"
      },
      "publication_date": "2020-09-01",
      "selected": null,
      "title": "The role of visual error and reward feedback in learning to aim to an optimal movement endpoint",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086871543&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sadler J.R."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.physbeh.2020.112962",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00319384",
        "publisher": "Elsevier Inc.",
        "sjr": 0.753,
        "snip": 1.02,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience"
        ],
        "title": "Physiology and Behavior"
      },
      "publication_date": "2020-09-01",
      "selected": null,
      "title": "Network organization during probabilistic learning via taste outcomes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085522936&origin=inward"
      ]
    },
    {
      "abstract": "Reference tracking systems involve a plant that is stabilized by a local feedback controller and a command center that indicates the reference set-point the plant should follow. Typically, these systems are subject to limitations such as disturbances, systems delays, constraints, uncertainties, underperforming controllers, and unmodeled parameters that do not allow them to achieve the desired performance. In situations where it is not possible to redesign the closed-loop system, it is usual to incorporate a reference governor that instructs the system to follow a modified reference path such that the resultant path is close to the ideal one. Typically, strategies to design the reference governor need to know a model of the system, which can be an unfeasible task. In this paper, we propose a framework based on deep reinforcement learning that can learn a policy to generate a modified reference that improves the system&#x2019;s performance in a non-invasive and model-free fashion. To illustrate the effectiveness of our approach, we present two challenging cases in engineering: a flight control with a pilot model that includes human reaction delays, and a mean-field control problem for a massive number of space-heating devices. The proposed strategy successfully designs a reference signal that works even in situations that were not seen during the learning process.",
      "authors": [
        "Maria Angelica Arroyo",
        "Luis Felipe Giraldo"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "arXiv",
        "Scopus"
      ],
      "doi": "10.1109/CCTA48906.2021.9658973",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "956-961",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-3644-1",
        "issn": "2768-0762",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CCTA 2021 - 5th IEEE Conference on Control Technology and Applications"
      },
      "publication_date": "2020-08-31",
      "selected": null,
      "title": "Data-driven Outer-Loop Control Using Deep Reinforcement Learning for Trajectory Tracking",
      "urls": [
        "http://arxiv.org/pdf/2008.13732v1",
        "http://dx.doi.org/10.1109/CCTA48906.2021.9658973",
        "http://arxiv.org/abs/2008.13732v1",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9658973",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85124806396&origin=inward"
      ]
    },
    {
      "abstract": "To understand the function of the neocortex, which is a hierarchical distributed network, it is useful giving meaning to the signals transmitted between these areas from the computational viewpoint. The overall anatomical structure or organs related to this network, including the neocortex, thalamus, and basal ganglia, has been roughly revealed, and much physiological knowledge, though often fragmentary, is being accumulated. The computational theories involving the neocortex have also been developed considerably. By introducing the assumption \"The signals transmitted by interarea axonal projections of pyramidal cells in the neocortex carry different meanings for each cell type, common to all areas.\", derived from its nature as a distributed network in the neocortex, allows us to specify the computational meanings of interarea signals. In this paper, first, the types of signals exchanged between neocortical areas are investigated, taking into account biological constraints, and employing theories such as predictive coding, reinforcement learning, representation emulation theory, and BDI logic as theoretical starting points, two types of feedforward signals (observation and deviation) and three types of feedback signals (prediction, plan, and intention) are identified. Next, based on the anatomical knowledge of the neocortex and thalamus, the pathways connecting the areas are organized and summarized as three corticocortical pathways and two thalamocortical pathways. Using this summation as preparation, this paper proposes a hypothesis that gives meaning to each type of signals transmitted in the different pathways in the neocortex, from the viewpoint of their functions. This hypothesis reckons that the feedforward corticocortical pathway transmits observation signals, the feedback corticocortical pathway transmits prediction signals, and the corticothalamic pathway mediated by core relay cells transmits deviation signals. The thalamocortical pathway, which is mediated by matrix relay cells, would be responsible for transmitting the signals that activate a part of prediction signals as intentions, due to the reason that the nature of the other available feedback pathways are not sufficient for conveying plans and intentions as signals. The corticocortical pathway, which is projected from various IT cells to the first layer, would be responsible for transmitting signals that activate a part of prediction signals as plans.",
      "authors": [
        "Yamakawa, Hiroshi"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2020.00074",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2020-08-18",
      "selected": null,
      "title": "Revealing the Computational Meaning of Neocortical Interarea Signals",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090036437&origin=inward"
      ]
    },
    {
      "abstract": "Teaching by demonstrations and teaching by assigning rewards are two popular methods of knowledge transfer in humans. However, showing the right behaviour (by demonstration) may appear more natural to a human teacher than assessing the learner\u2019s performance and assigning a reward or punishment to it. In the context of robot learning, the preference between these two approaches has not been studied extensively. In this article, we propose a method that replaces the traditional method of reward assignment with action assignment (which is similar to providing a demonstration) in interactive reinforcement learning. The main purpose of the suggested action is to compute a reward by seeing if the suggested action was followed by the self-acting agent or not. We compared action assignment with reward assignment via a user study conducted over the web using a two-dimensional maze game. The logs of interactions showed that action assignment significantly improved users\u2019 ability to teach the right behaviour. The survey results showed that both action and reward assignment seemed highly natural and usable, reward assignment required more mental effort, repeatedly assigning rewards and seeing the agent disobey commands caused frustration in users, and many users desired to control the agent\u2019s behaviour directly.",
      "authors": [
        "Syed Ali Raza",
        "Mary-Anne Williams"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3404197",
      "keywords": [
        "reward shaping",
        "reinforcement learning",
        "learning from human teachers",
        "Interactive machine learning"
      ],
      "number_of_pages": 24,
      "pages": "1-24",
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1556-4665",
        "publisher": "Association for Computing Machinery (ACM)",
        "sjr": 0.487,
        "snip": 1.313,
        "subject_areas": [
          "Software",
          "Computer Science (miscellaneous)",
          "Control and Systems Engineering"
        ],
        "title": "ACM Transactions on Autonomous and Adaptive Systems"
      },
      "publication_date": "2020-08-04",
      "selected": null,
      "title": "Human Feedback as Action Assignment in Interactive Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3404197",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092378541&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Loftus T.J."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.surg.2020.04.049",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "253-266",
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00396060",
        "publisher": "Elsevier Inc.",
        "sjr": 1.16,
        "snip": 1.471,
        "subject_areas": [
          "Surgery"
        ],
        "title": "Surgery (United States)"
      },
      "publication_date": "2020-08-01",
      "selected": null,
      "title": "Decision analysis and reinforcement learning in surgical decision-making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086519362&origin=inward"
      ]
    },
    {
      "abstract": "Executive dysfunction is a well-documented, yet nonspecific corollary of various neurological diseases and psychiatric disorders. Here, we applied computational modeling of latent cognition for executive control in amyotrophic lateral sclerosis (ALS) patients. We utilized a parallel reinforcement learning model of trial-by-trial Wisconsin Card Sorting Test (WCST) behavior. Eighteen ALS patients and 21 matched healthy control participants were assessed on a computerized variant of the WCST (cWCST). ALS patients showed latent cognitive symptoms, which can be characterized as bradyphrenia and haphazard responding. A comparison with results from a recent computational Parkinson\u2019s disease (PD) study (Steinke et al., 2020, J Clin Med) suggests that bradyphrenia represents a disease-nonspecific latent cognitive symptom of ALS and PD patients alike. Haphazard responding seems to be a disease-specific latent cognitive symptom of ALS, whereas impaired stimulus-response learning seems to be a disease-specific latent cognitive symptom of PD. These data were obtained from the careful modeling of trial-by-trial behavior on the cWCST, and they suggest that computational cognitive neuropsychology provides nosologically specific indicators of latent facets of executive dysfunction in ALS (and PD) patients, which remain undiscoverable for traditional behavioral cognitive neuropsychology. We discuss implications for neuropsychological assessment, and we discuss opportunities for confirmatory computational brain imaging studies.",
      "authors": [
        "Steinke, Alexander",
        "Lange, Florian",
        "Seer, Caroline",
        "Petri, Susanne",
        "Kopp, Bruno"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/jcm9082605",
      "keywords": [],
      "number_of_pages": 24,
      "pages": "1-24",
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2077-0383",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.935,
        "snip": 1.179,
        "subject_areas": [
          "Medicine (all)"
        ],
        "title": "Journal of Clinical Medicine"
      },
      "publication_date": "2020-08-01",
      "selected": null,
      "title": "A Computational Study of Executive Dysfunction in Amyotrophic Lateral Sclerosis",
      "urls": [
        "https://www.mdpi.com/2077-0383/9/8/2605/pdf?version=1597150707",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091292379&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sharp M.E."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/brain/awaa182",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "2519-2531",
      "publication": {
        "category": "Journal",
        "cite_score": 20.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00068950",
        "publisher": "Oxford University Press",
        "sjr": 4.437,
        "snip": 3.147,
        "subject_areas": [
          "Neurology (clinical)"
        ],
        "title": "Brain"
      },
      "publication_date": "2020-08-01",
      "selected": null,
      "title": "Dopamine is associated with prioritization of reward-associated memories in Parkinson's disease",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089927376&origin=inward"
      ]
    },
    {
      "abstract": "Achieving stability and robustness is the primary goal of biped locomotion control. Recently, deep reinforce learning (DRL) has attracted great attention as a general methodology for constructing biped control policies and demonstrated significant improvements over the previous state-of-the-art. Although deep control policies have advantages over previous controller design approaches, many questions remain unanswered. Are deep control policies as robust as human walking? Does simulated walking use similar strategies as human walking to maintain balance? Does a particular gait pattern similarly affect human and simulated walking? What do deep policies learn to achieve improved gait stability? The goal of this study is to answer these questions by evaluating the push-recovery stability of deep policies compared to human subjects and a previous feedback controller. We also conducted experiments to evaluate the effectiveness of variants of DRL algorithms.",
      "authors": [
        "Park, Hwangpil",
        "Yu, Ri",
        "Lee, Yoonsang",
        "Lee, Kyungho",
        "Lee, Jehee"
      ],
      "categories": null,
      "citations": null,
      "comments": "11 pages, 12 figures and 3 tables",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-07-30",
      "selected": null,
      "title": "Understanding the Stability of Deep Control Policies for Biped Locomotion",
      "urls": [
        "http://arxiv.org/abs/2007.15242v1",
        "http://arxiv.org/pdf/2007.15242v1",
        "http://arxiv.org/pdf/2007.15242.pdf"
      ]
    },
    {
      "abstract": "The current reward learning from human preferences could be used to resolve complex reinforcement learning (RL) tasks without access to a reward function by defining a single fixed preference between pairs of trajectory segments. However, the judgement of preferences between trajectories is not dynamic and still requires human input over thousands of iterations. In this study, we proposed a weak human preference supervision framework, for which we developed a human preference scaling model that naturally reflects the human perception of the degree of weak choices between trajectories and established a human-demonstration estimator via supervised learning to generate the predicted preferences for reducing the number of human inputs. The proposed weak human preference supervision framework can effectively solve complex RL tasks and achieve higher cumulative rewards in simulated robot locomotion -- MuJoCo games -- relative to the single fixed human preferences. Furthermore, our established human-demonstration estimator requires human feedback only for less than 0.01\\% of the agent's interactions with the environment and significantly reduces the cost of human inputs by up to 30\\% compared with the existing approaches. To present the flexibility of our approach, we released a video (https://youtu.be/jQPe1OILT0M) showing comparisons of the behaviours of agents trained on different types of human input. We believe that our naturally inspired human preferences with weakly supervised learning are beneficial for precise reward learning and can be applied to state-of-the-art RL systems, such as human-autonomy teaming systems.",
      "authors": [
        "Cao, Zehong",
        "Wong, KaiChiu",
        "Lin, Chin-Teng"
      ],
      "categories": null,
      "citations": null,
      "comments": "Submitting to IEEE Transactions on Neural Networks and Learning\n  Systems",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-07-25",
      "selected": null,
      "title": "Weak Human Preference Supervision For Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2007.12904v2",
        "http://arxiv.org/pdf/2007.12904.pdf",
        "http://arxiv.org/abs/2007.12904v2"
      ]
    },
    {
      "abstract": "Standard reinforcement learning (RL) aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective -- learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free algorithm based on temporal difference learning and a near-greedy heuristic for action selection. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits good convergence properties and discovers meaningful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician/human-in-the-loop decision making, in which humans (e.g., clinicians, patients) can incorporate additional knowledge (e.g., side effects, patient preference) when selecting among near-equivalent actions.",
      "authors": [
        "Tang, Shengpu",
        "Modi, Aditya",
        "Sjoding, Michael W.",
        "Wiens, Jenna"
      ],
      "categories": null,
      "citations": 5,
      "comments": "ICML 2020. Code available at\n  https://github.com/shengpu1126/RL-Set-Valued-Policy",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "9329-9338",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713821120",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "37th International Conference on Machine Learning, ICML 2020"
      },
      "publication_date": "2020-07-24",
      "selected": null,
      "title": "Clinician-in-the-Loop Decision Making: Reinforcement Learning with Near-Optimal Set-Valued Policies",
      "urls": [
        "http://arxiv.org/pdf/2007.12678v1",
        "http://arxiv.org/abs/2007.12678v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098811001&origin=inward",
        "http://arxiv.org/pdf/2007.12678.pdf"
      ]
    },
    {
      "abstract": "Assistance games (also known as cooperative inverse reinforcement learning games) have been proposed as a model for beneficial AI, wherein a robotic agent must act on behalf of a human principal but is initially uncertain about the humans payoff function. This paper studies multi-principal assistance games, which cover the more general case in which the robot acts on behalf of N humans who may have widely differing payoffs. Impossibility theorems in social choice theory and voting theory can be applied to such games, suggesting that strategic behavior by the human principals may complicate the robots task in learning their payoffs. We analyze in particular a bandit apprentice game in which the humans act first to demonstrate their individual preferences for the arms and then the robot acts to maximize the sum of human payoffs. We explore the extent to which the cost of choosing suboptimal arms reduces the incentive to mislead, a form of natural mechanism design. In this context we propose a social choice method that uses shared control of a system to combine preference inference with social welfare optimization.",
      "authors": [
        "Fickinger, Arnaud",
        "Zhuang, Simon",
        "Hadfield-Menell, Dylan",
        "Russell, Stuart"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-07-19",
      "selected": null,
      "title": "Multi-Principal Assistance Games",
      "urls": [
        "http://arxiv.org/pdf/2007.09540.pdf",
        "http://arxiv.org/abs/2007.09540v1",
        "http://arxiv.org/pdf/2007.09540v1"
      ]
    },
    {
      "abstract": "The act of explaining across two parties is a feedback loop, where one provides information on what needs to be explained and the other provides an explanation relevant to this information. We apply a reinforcement learning framework which emulates this format by providing explanations based on the explainee's current mental model. We conduct novel online human experiments where explanations generated by various explanation methods are selected and presented to participants, using policies which observe participants' mental models, in order to optimize an interpretability proxy. Our results suggest that mental model-based policies (anchored in our proposed state representation) may increase interpretability over multiple sequential explanations, when compared to a random selection baseline. This work provides insight into how to select explanations which increase relevant information for users, and into conducting human-grounded experimentation to understand interpretability.",
      "authors": [
        "Yeung, Arnold YS",
        "Joshi, Shalmali",
        "Williams, Joseph Jay",
        "Rudzicz, Frank"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted into ICML 2020 Workshop on Human Interpretability in Machine\n  Learning (Spotlight)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-07-17",
      "selected": null,
      "title": "Sequential Explanations with Mental Model-Based Policies",
      "urls": [
        "http://arxiv.org/pdf/2007.09028.pdf",
        "http://arxiv.org/abs/2007.09028v1",
        "http://arxiv.org/pdf/2007.09028v1"
      ]
    },
    {
      "abstract": "Standard reinforcement learning (RL) aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective - learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free algorithm based on temporal difference learning and a near-greedy heuristic for action selection. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits good convergence properties and discovers meaning-ful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician/human-in-the-loop decision making, in which humans (e.g., clinicians, patients) can in-corporate additional knowledge (e.g., side effects, patient preference) when selecting among near-equivalent actions.",
      "authors": [
        "Shengpu Tang",
        "Aditya Modi",
        "Michael W. Sjoding",
        "Jenna Wiens"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3524938.3525808",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "9387-9396",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMLR.org",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 37th International Conference on Machine Learning"
      },
      "publication_date": "2020-07-13",
      "selected": null,
      "title": "Clinician-in-the-loop decision making: reinforcement learning with near-optimal set-valued policies",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3524938.3525808"
      ]
    },
    {
      "abstract": "This paper presents the design, control, and applications of a multi-segment soft robotic arm. In order to design a soft arm with large load capacity, several design principles are proposed by analyzing two kinds of buckling issues, under which we present a novel structure named Honeycomb Pneumatic Networks (HPN). Parameter optimization method, based on finite element method (FEM), is proposed to optimize HPN Arm design parameters. Through a quick fabrication process, several prototypes with different performance are made, one of which can achieve the transverse load capacity of 3 kg under 3 bar pressure. Next, considering different internal and external conditions, we develop three controllers according to different model precision. Specifically, based on accurate model, an open-loop controller is realized by combining piece-wise constant curvature (PCC) modeling method and machine learning method. Based on inaccurate model, a feedback controller, using estimated Jacobian, is realized in 3D space. A model-free controller, using reinforcement learning to learn a control policy rather than a model, is realized in 2D plane, with minimal training data. Then, these three control methods are compared on a same experiment platform to explore the applicability of different methods under different conditions. Lastly, we figure out that soft arm can greatly simplify the perception, planning, and control of interaction tasks through its compliance, which is its main advantage over the rigid arm. Through plentiful experiments in three interaction application scenarios, human-robot interaction, free space interaction task, and confined space interaction task, we demonstrate the potential application prospect of the soft arm.",
      "authors": [
        "Jiang, Hao",
        "Wang, Zhanchi",
        "Jin, Yusong",
        "Chen, Xiaotong",
        "Li, Peijin",
        "Gan, Yinghao",
        "Lin, Sen",
        "Chen, Xiaoping"
      ],
      "categories": null,
      "citations": null,
      "comments": "Submitted to The International Journal of Robotics Research in\n  January, 2019",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-07-08",
      "selected": null,
      "title": "Design, Control, and Applications of a Soft Robotic Arm",
      "urls": [
        "http://arxiv.org/pdf/2007.04047.pdf",
        "http://arxiv.org/abs/2007.04047v1",
        "http://arxiv.org/pdf/2007.04047v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Park J."
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1146/annurev-neuro-070918-050452",
      "keywords": [],
      "number_of_pages": 23,
      "pages": "485-507",
      "publication": {
        "category": "Book",
        "cite_score": 24.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0147006X",
        "publisher": "Annual Reviews Inc.",
        "sjr": 8.389,
        "snip": 3.679,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Annual Review of Neuroscience"
      },
      "publication_date": "2020-07-08",
      "selected": null,
      "title": "Basal Ganglia Circuits for Action Specification",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088208697&origin=inward"
      ]
    },
    {
      "abstract": "Robots are extending their presence in domestic environments every day, it being more common to see them carrying out tasks in home scenarios. In the future, robots are expected to increasingly perform more complex tasks and, therefore, be able to acquire experience from different sources as quickly as possible. A plausible approach to address this issue is interactive feedback, where a trainer advises a learner on which actions should be taken from specific states to speed up the learning process. Moreover, deep reinforcement learning has been recently widely used in robotics to learn the environment and acquire new skills autonomously. However, an open issue when using deep reinforcement learning is the excessive time needed to learn a task from raw input images. In this work, we propose a deep reinforcement learning approach with interactive feedback to learn a domestic task in a Human\u2013Robot scenario. We compare three different learning methods using a simulated robotic arm for the task of organizing different objects; the proposed methods are (i) deep reinforcement learning (DeepRL); (ii) interactive deep reinforcement learning using a previously trained artificial agent as an advisor (agent\u2013IDeepRL); and (iii) interactive deep reinforcement learning using a human advisor (human\u2013IDeepRL). We demonstrate that interactive approaches provide advantages for the learning process. The obtained results show that a learner agent, using either agent\u2013IDeepRL or human\u2013IDeepRL, completes the given task earlier and has fewer mistakes compared to the autonomous DeepRL approach.",
      "authors": [
        "Moreira, Ithan",
        "Rivas, Javier",
        "Cruz, Francisco",
        "Dazeley, Richard",
        "Ayala, Angel",
        "Fernandes, Bruno"
      ],
      "categories": null,
      "citations": 23,
      "comments": "In press journal Applied Sciences",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.3390/app10165574",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2076-3417",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Applied Sciences (Switzerland)"
      },
      "publication_date": "2020-07-07",
      "selected": null,
      "title": "Deep Reinforcement Learning with Interactive Feedback in a Human\u2013Robot Environment",
      "urls": [
        "https://www.mdpi.com/2076-3417/10/16/5574/pdf?version=1597216274",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089800888&origin=inward",
        "http://dx.doi.org/10.3390/app10165574",
        "http://arxiv.org/abs/2007.03363v2",
        "http://arxiv.org/pdf/2007.03363v2"
      ]
    },
    {
      "abstract": "Existing prescriptive compression strategies used in hearing aid fitting are designed based on gain averages from a group of users which are not necessarily optimal for a specific user. Nearly half of hearing aid users prefer settings that differ from the commonly prescribed settings. This paper presents a human-in-loop deep reinforcement learning approach that personalizes hearing aid compression to achieve improved hearing perception. The developed approach is designed to learn a specific user's hearing preferences in order to optimize compression based on the user's feedbacks. Both simulation and subject testing results are reported which demonstrate the effectiveness of the developed personalized compression.",
      "authors": [
        "Alamdari, Nasim",
        "Lobarinas, Edward",
        "Kehtarnavaz, Nasser"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-07-01",
      "selected": null,
      "title": "Personalization of Hearing Aid Compression by Human-In-Loop Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2007.00192v1",
        "http://arxiv.org/pdf/2007.00192v1",
        "http://arxiv.org/pdf/2007.00192.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rosokha Y."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/rest_a_00846",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "569-582",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00346535",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Review of Economics and Statistics"
      },
      "publication_date": "2020-07-01",
      "selected": null,
      "title": "Motivating innovation: The effect of loss aversion on the willingness to persist",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090754771&origin=inward"
      ]
    },
    {
      "abstract": "A model-free controller for a general class of output feedback nonlinear discrete-time systems is established by action-critic networks and reinforcement learning with human knowledge based on IF\u2013THEN rules. The action network is designed by a single input fuzzy-rules emulated network with the set of IF\u2013THEN rules utilized by the relation between control effort and plant\u2019s output such as IF the output is high THEN the control effort should be reduced. The critic network is constructed by a multi-input FREN (MiFREN) for estimating an unknown long-term cost function. The set of IF\u2013THEN rules for MiFREN is defined by the general knowledge of optimization such that IF the quadratic values of control effort and tracking error are high THEN the cost function should be high. The convergence of tracking error and bounded external signals can be guaranteed by Lyapunov direct method under general assumptions which are reasonable for practical plants. A computer simulation system is firstly provided to demonstrate the design method and the performance of the proposed controller. Furthermore, an experimental system with the prototype of DC-motor current control is conducted to show the effectiveness of the control scheme.",
      "authors": [
        "Treesatayapun, Chidentree"
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00521-019-04509-x",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "9761-9775",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09410643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2020-07-01",
      "selected": null,
      "title": "Knowledge-based reinforcement learning controller with fuzzy-rule network: experimental validation",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00521-019-04509-x.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074359827&origin=inward"
      ]
    },
    {
      "abstract": "Providing Reinforcement Learning (RL) agents with human feedback can dramatically improve various aspects of learning. However, previous methods require human observer to give inputs explicitly (e.g., press buttons, voice interface), burdening the human in the loop of RL agent's learning process. Further, it is sometimes difficult or impossible to obtain the explicit human advise (feedback), e.g., autonomous driving, disabled rehabilitation, etc. In this work, we investigate capturing human's intrinsic reactions as implicit (and natural) feedback through EEG in the form of error-related potentials (ErrP), providing a natural and direct way for humans to improve the RL agent learning. As such, the human intelligence can be integrated via implicit feedback with RL algorithms to accelerate the learning of RL agent. We develop three reasonably complex 2D discrete navigational games to experimentally evaluate the overall performance of the proposed work. Major contributions of our work are as follows, (i) we propose and experimentally validate the zero-shot learning of ErrPs, where the ErrPs can be learned for one game, and transferred to other unseen games, (ii) we propose a novel RL framework for integrating implicit human feedbacks via ErrPs with RL agent, improving the label efficiency and robustness to human mistakes, and (iii) compared to prior works, we scale the application of ErrPs to reasonably complex environments, and demonstrate the significance of our approach for accelerated learning through real user experiments.",
      "authors": [
        "Xu, Duo",
        "Agarwal, Mohit",
        "Gupta, Ekansh",
        "Fekri, Faramarz",
        "Sivakumar, Raghupathy"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-06-30",
      "selected": null,
      "title": "Accelerating Reinforcement Learning Agent with EEG-based Implicit Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/2006.16498.pdf",
        "http://arxiv.org/pdf/2006.16498v3",
        "http://arxiv.org/abs/2006.16498v3"
      ]
    },
    {
      "abstract": "As a massive number of the Internet of Things (IoT) devices are deployed, the security and privacy issues in IoT arouse more and more attention. The IoT attacks are causing tremendous loss to the IoT networks and even threatening human safety. Compared to traditional networks, IoT networks have unique characteristics, which make the attack detection more challenging. First, the heterogeneity of platforms, protocols, software, and hardware exposes various vulnerabilities. Second, in addition to the traditional high-rate attacks, the low-rate attacks are also extensively used by IoT attackers to obfuscate the legitimate and malicious traffic. These low-rate attacks are challenging to detect and can persist in the networks. Last, the attackers are evolving to be more intelligent and can dynamically change their attack strategies based on the environment feedback to avoid being detected, making it more challenging for the defender to discover a consistent pattern to identify the attack. In order to adapt to the new characteristics in IoT attacks, we propose a reinforcement learning-based attack detection model that can automatically learn and recognize the transformation of the attack pattern. Therefore, we can continuously detect IoT attacks with less human intervention. In this paper, we explore the crucial features of IoT traffics and utilize the entropy-based metrics to detect both the high-rate and low-rate IoT attacks. Afterward, we leverage the reinforcement learning technique to continuously adjust the attack detection threshold based on the detection feedback, which optimizes the detection and the false alarm rate. We conduct extensive experiments over a real IoT attack dataset and demonstrate the effectiveness of our IoT attack detection framework.",
      "authors": [
        "Gu, Tianbo",
        "Abhishek, Allaukik",
        "Fu, Hao",
        "Zhang, Huanle",
        "Basu, Debraj",
        "Mohapatra, Prasant"
      ],
      "categories": null,
      "citations": null,
      "comments": "11 pages, 8 figures, 2 tables, to appear in the 21st IEEE\n  International Symposium on a World of Wireless, Mobile and Multimedia\n  Networks (IEEE WoWMoM 2020)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-06-29",
      "selected": null,
      "title": "Towards Learning-automation IoT Attack Detection through Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2006.15826.pdf",
        "http://arxiv.org/abs/2006.15826v1",
        "http://arxiv.org/pdf/2006.15826v1"
      ]
    },
    {
      "abstract": "Human explanation (e.g., in terms of feature importance) has been recently used to extend the communication channel between human and agent in interactive machine learning. Under this setting, human trainers provide not only the ground truth but also some form of explanation. However, this kind of human guidance was only investigated in supervised learning tasks, and it remains unclear how to best incorporate this type of human knowledge into deep reinforcement learning. In this paper, we present the first study of using human visual explanations in human-in-the-loop reinforcement learning (HRL). We focus on the task of learning from feedback, in which the human trainer not only gives binary evaluative \"good\" or \"bad\" feedback for queried state-action pairs, but also provides a visual explanation by annotating relevant features in images. We propose EXPAND (EXPlanation AugmeNted feeDback) to encourage the model to encode task-relevant features through a context-aware data augmentation that only perturbs irrelevant features in human salient information. We choose five tasks, namely Pixel-Taxi and four Atari games, to evaluate the performance and sample efficiency of this approach. We show that our method significantly outperforms methods leveraging human explanation that are adapted from supervised learning, and Human-in-the-loop RL baselines that only utilize evaluative feedback.",
      "authors": [
        "Guan, Lin",
        "Verma, Mudit",
        "Guo, Sihang",
        "Zhang, Ruohan",
        "Kambhampati, Subbarao"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-06-26",
      "selected": null,
      "title": "Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation",
      "urls": [
        "http://arxiv.org/pdf/2006.14804.pdf",
        "http://arxiv.org/abs/2006.14804v5",
        "http://arxiv.org/pdf/2006.14804v5"
      ]
    },
    {
      "abstract": "It is hypothesized that the nonlinear muscle characteristic of biomechanical systems simplify control in the sense that the information the nervous system has to process is reduced through off-loading computation to the morphological structure. It has been proposed to quantify the required information with an information-entropy based approach, which evaluates the minimally required information to control a desired movement, i.e., control effort. The key idea is to compare the same movement but generated by different actuators, e.g., muscles and torque actuators, and determine which of the two morphologies requires less information to generate the same movement. In this work, for the first time, we apply this measure to numerical simulations of more complex human movements: point-to-point arm movements and walking. These models consider up to 24 control signals rendering the brute force approach of the previous implementation to search for the minimally required information futile. We therefore propose a novel algorithm based on the pattern search approach specifically designed to solve this constraint optimization problem. We apply this algorithm to numerical models, which include Hill-type muscle-tendon actuation as well as ideal torque sources acting directly on the joints. The controller for the point-to-point movements was obtained by deep reinforcement learning for muscle and torque actuators. Walking was controlled by proprioceptive neural feedback in the muscular system and a PD controller in the torque model. Results show that the neuromuscular models consistently require less information to successfully generate the movement than the torque-driven counterparts. These findings were consistent for all investigated controllers in our experiments, implying that this is a system property, not a controller property. The proposed algorithm to determine the control effort is more efficient than other standard optimization techniques and provided as open source.",
      "authors": [
        "Haeufle, Daniel F. B.",
        "Wochner, Isabell",
        "Holzm\u00fcller, David",
        "Driess, Danny",
        "G\u00fcnther, Michael",
        "Schmitt, Syn"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/frobt.2020.00077",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2296-9144",
        "publisher": "Frontiers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Robotics and AI"
      },
      "publication_date": "2020-06-24",
      "selected": null,
      "title": "Muscles Reduce Neuronal Information Load: Quantification of Control Effort in Biological vs. Robotic Pointing and Walking",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084270226&origin=inward"
      ]
    },
    {
      "abstract": "Intrinsic Human-In-The-Loop Reinforcement Learning (HITL-RL) is an approach to obtain the human feedback implicitly by capturing brain waves through the use of wearable electroencephalogram (EEG) headsets. It can significantly accelerate the training convergence of RL algorithms while reducing the burden placed on the humans involved in the training loop. While a human naturally observes the performance of an RL agent, any erroneous behavior of the agent can be recognized through the error-potentials (ErrP) in the EEG signal. This information can then be incorporated into the reward function of the RL algorithm to accelerate its learning. The detection accuracy of the error-potentials thus significantly affects the convergence time of the RL algorithm. The focus of this work is the reliable detection of error-potentials using the brain waves of the user detected using only an off-the-shelf EEG wearable. We first present a new error-potential decoding algorithm that leverages the spatial, temporal, and frequency-domain characteristics of the EEG signals. We develop three Atari-like game environments and recruit 25 volunteers for evaluation. The proposed algorithm achieves an accuracy performance of 73.71% (an improvement of 8.11% over the current state-of-the-art algorithm). We then show that a modified algorithm that intelligently discards low-confidence estimates is capable of boosting the accuracy to 79.51% (16.63% improvement).",
      "authors": [
        "Mohit Agarwal",
        "Shyam Krishnan Venkateswaran",
        "Raghupathy Sivakumar"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3396870.3400014",
      "keywords": [
        "EEG",
        "human in the loop (HITL)",
        "reinforcement learning",
        "error-potentials",
        "ErrPs",
        "brainwaves"
      ],
      "number_of_pages": 6,
      "pages": "25-30",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450380133",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "WearSys 2020 - Proceedings of the 6th ACM Workshop on Wearable Systems and Applications, Part of MobiSys 2020"
      },
      "publication_date": "2020-06-19",
      "selected": null,
      "title": "Human-in-the-loop RL with an EEG wearable headset: on effective use of brainwaves to accelerate learning",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3396870.3400014",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086708795&origin=inward"
      ]
    },
    {
      "abstract": "Editorial: The Embodied Brain: Computational Mechanisms of Integrated Sensorimotor Interactions With a Dynamic Environment",
      "authors": [
        "Senden, Mario",
        "Peters, Judith",
        "R\u00f6hrbein, Florian",
        "Deco, Gustavo",
        "Goebel, Rainer"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2020.00053",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2020-06-18",
      "selected": null,
      "title": "Editorial: The Embodied Brain: Computational Mechanisms of Integrated Sensorimotor Interactions With a Dynamic Environment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087289166&origin=inward"
      ]
    },
    {
      "abstract": "Preference-based Reinforcement Learning (PbRL) replaces reward values in traditional reinforcement learning by preferences to better elicit human opinion on the target objective, especially when numerical reward values are hard to design or interpret. Despite promising results in applications, the theoretical understanding of PbRL is still in its infancy. In this paper, we present the first finite-time analysis for general PbRL problems. We first show that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL. If preferences are stochastic, and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that are able to identify the best policy up to accuracy $\\varepsilon$ with high probability. Our method explores the state space by navigating to under-explored states, and solves PbRL using a combination of dueling bandits and policy search. Experiments show the efficacy of our method when it is applied to real-world problems.",
      "authors": [
        "Xu, Yichong",
        "Wang, Ruosong",
        "Yang, Lin F.",
        "Singh, Aarti",
        "Dubrawski, Artur"
      ],
      "categories": null,
      "citations": 7,
      "comments": "Thirty-fourth Conference on Neural Information Processing Systems\n  (NeurIPS 2020). Spotlight presentation",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2020-06-16",
      "selected": null,
      "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108448291&origin=inward",
        "http://arxiv.org/pdf/2006.08910v2",
        "http://arxiv.org/abs/2006.08910v2",
        "http://arxiv.org/pdf/2006.08910.pdf"
      ]
    },
    {
      "abstract": "Recommender systems have been attracting much attention from both academia and industry because of their ability to capture user interests and generate personalized item recommendations. As the life pace in contemporary society speeds up, traditional recommender systems are inevitably limited by their disconnected interaction styles and low adaptivity to users\u2019 evolving demands. Consequently, conversational recommender systems emerge as a prospective research area, where an intelligent dialogue agent is integrated with a recommender system. Conversational recommender systems possess the ability to accurately understand end-users\u2019 intent or request and generate human-like dialogue responses when performing recommendations. However, existing conversational recommender systems only allow the systems to ask users for more preference information, while users\u2019 further questions and concerns about the recommended items (e.g., enquiring the location of a recommended restaurant) can hardly be addressed. Though the recent task-oriented dialogue systems allow for two-way communications, they are not easy to train because of their high dependence on human guidance in terms of user intent recognition and system response generation. Hence, to enable two-way human-machine communications and tackle the challenges brought by manually crafted rules, we propose Conversational Recommender System with Adversarial Learning (CRSAL), a novel end-to-end system to tackle the task of conversational recommendation. In CRSAL, we innovatively design a fully statistical dialogue state tracker coupled with a neural policy agent to precisely capture each user\u2019s intent from limited dialogue data and generate conversational recommendation actions. We further develop an adversarial Actor-Critic reinforcement learning approach to adaptively refine the quality of generated system actions, thus ensuring coherent human-like dialogue responses. Extensive experiments on two benchmark datasets fully demonstrate the superiority of CRSAL on conversational recommendation tasks.",
      "authors": [
        "Xuhui Ren",
        "Hongzhi Yin",
        "Tong Chen",
        "Hao Wang",
        "Nguyen Quoc Viet Hung",
        "Zi Huang",
        "Xiangliang Zhang"
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3394592",
      "keywords": [
        "adversarial learning",
        "dialogue systems",
        "deep neural networks",
        "Conversational recommender systems"
      ],
      "number_of_pages": 40,
      "pages": "1-40",
      "publication": {
        "category": "Journal",
        "cite_score": 10.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1046-8188",
        "publisher": "Association for Computing Machinery (ACM)",
        "sjr": 1.864,
        "snip": 3.493,
        "subject_areas": [
          "Business, Management and Accounting (all)",
          "Information Systems",
          "Computer Science Applications"
        ],
        "title": "ACM Transactions on Information Systems"
      },
      "publication_date": "2020-06-13",
      "selected": null,
      "title": "CRSAL: Conversational Recommender Systems with Adversarial Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093981606&origin=inward",
        "https://dl.acm.org/doi/10.1145/3394592"
      ]
    },
    {
      "abstract": "The technological and scientific challenges involved in the development of autonomous vehicles (AVs) are currently of primary interest for many automobile companies and research labs. However, human-controlled vehicles are likely to remain on the roads for several decades to come and may share with AVs the traffic environments of the future. In such mixed environments, AVs should deploy human-like driving policies and negotiation skills to enable smooth traffic flow. To generate automated human-like driving policies, we introduce a model-free, deep reinforcement learning approach to imitate an experienced human driver's behavior. We study a static obstacle avoidance task on a two-lane highway road in simulation (Unity). Our control algorithm receives a stochastic feedback signal from two sources: a model-driven part, encoding simple driving rules, such as lane-keeping and speed control, and a stochastic, data-driven part, incorporating human expert knowledge from driving data. To assess the similarity between machine and human driving, we model distributions of track position and speed as Gaussian processes. We demonstrate that our approach leads to human-like driving policies.",
      "authors": [
        "Emuna, Ran",
        "Borowsky, Avinoam",
        "Biess, Armin"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-06-07",
      "selected": null,
      "title": "Deep Reinforcement Learning for Human-Like Driving Policies in Collision Avoidance Tasks of Self-Driving Cars",
      "urls": [
        "http://arxiv.org/pdf/2006.04218v2",
        "http://arxiv.org/abs/2006.04218v2",
        "http://arxiv.org/pdf/2006.04218.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Stewardson H.J."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2020.03.002",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "81-86",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2020-06-01",
      "selected": null,
      "title": "Evidence for parietal reward prediction errors using great grand average meta-analysis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084257903&origin=inward"
      ]
    },
    {
      "abstract": "What is the role of real-time control and learning in the formation of social conventions? To answer this question, we propose a computational model that matches human behavioral data in a social decision-making game that was analyzed both in discrete-time and continuous-time setups. Furthermore, unlike previous approaches, our model takes into account the role of sensorimotor control loops in embodied decision-making scenarios. For this purpose, we introduce the Control-based Reinforcement Learning (CRL) model. CRL is grounded in the Distributed Adaptive Control (DAC) theory of mind and brain, where low-level sensorimotor control is modulated through perceptual and behavioral learning in a layered structure. CRL follows these principles by implementing a feedback control loop handling the agent\u2019s reactive behaviors (pre-wired reflexes), along with an Adaptive Layer that uses reinforcement learning to maximize long-term reward. We test our model in a multi-agent game-theoretic task in which coordination must be achieved to find an optimal solution. We show that CRL is able to reach human-level performance on standard game-theoretic metrics such as efficiency in acquiring rewards and fairness in reward distribution.",
      "authors": [
        "Ismael T. Freire",
        "Clement Moulin-Frier",
        "Marti Sanchez-Fibla",
        "Xerxes D. Arsiwalla",
        "Paul F. M. J. Verschure"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0234434",
      "keywords": [
        "Ballistics",
        "Video games",
        "Game theory",
        "Learning",
        "Human performance",
        "Decision making",
        "Sensory perception",
        "Animal sociality"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2020-06-01",
      "selected": null,
      "title": "Modeling the formation of social conventions from embodied real-time interactions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086935020&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0234434&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Xu B."
      ],
      "categories": null,
      "citations": 40,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TMI.2019.2962013",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1930-1941",
      "publication": {
        "category": "Journal",
        "cite_score": 20.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02780062",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 2.872,
        "snip": 3.532,
        "subject_areas": [
          "Software",
          "Computer Science Applications",
          "Electrical and Electronic Engineering",
          "Radiological and Ultrasound Technology"
        ],
        "title": "IEEE Transactions on Medical Imaging"
      },
      "publication_date": "2020-06-01",
      "selected": null,
      "title": "Attention by Selection: A Deep Selective Attention Approach to Breast Cancer Classification",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077288699&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "K\u00fchnel A."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.euroneuro.2020.03.023",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "17-29",
      "publication": {
        "category": "Journal",
        "cite_score": 8.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0924977X",
        "publisher": "Elsevier B.V.",
        "sjr": 1.442,
        "snip": 1.185,
        "subject_areas": [
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neurology (clinical)",
          "Pharmacology",
          "Pharmacology (medical)"
        ],
        "title": "European Neuropsychopharmacology"
      },
      "publication_date": "2020-06-01",
      "selected": null,
      "title": "Stimulation of the vagus nerve reduces learning in a go/no-go reinforcement learning task",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084473199&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning can lead to rapid changes in performance. Computational accounts of reinforcement learning align with classic learning theory, as reported by Sutton and Barto (1998, 2018) and suggest that trial-to-trial changes in performance follow rapid but decelerating learning curves. Although there is some support for a link between changes in behavioural and neural data, evidence has been inconclusive. Here, we had a computational model and human participants learn a novel language through trial-and-error while recording electroencephalographic data. By conducting linear mixed-effects models of trial-to-trial analyses, we sought to determine whether neural signals were indicative of a learning process and whether they were related to changes in behaviour. We found that neural measures did diminish with trial-to-trial changes in performance and that they were predictive of behavioural adaptations in both simulated and empirical data. These neural signals are theorised as reward prediction errors\u2014the computational difference between expectations and outcomes\u2014and here we provide compelling evidence that they reflect an underlying learning process that parallels behavioural adaptation.",
      "authors": [
        "Williams, Chad C.",
        "Hassall, Cameron D.",
        "Lindenbach, Talise",
        "Krigolson, Olave E."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s42113-019-00069-4",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "189-199",
      "publication": {
        "category": "Journal",
        "cite_score": 4.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2522-087X",
        "publisher": "Springer Nature",
        "sjr": 0.968,
        "snip": 0.919,
        "subject_areas": [
          "Developmental and Educational Psychology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Computational Brain and Behavior"
      },
      "publication_date": "2020-06-01",
      "selected": null,
      "title": "Reward Prediction Errors Reflect an Underlying Learning Process That Parallels Behavioural Adaptations: A Trial-to-Trial Analysis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091313941&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s42113-019-00069-4.pdf"
      ]
    },
    {
      "abstract": "The rising demand for electricity and its essential nature in today's world\ncalls for intelligent home energy management (HEM) systems that can reduce\nenergy usage. This involves scheduling of loads from peak hours of the day when\nenergy consumption is at its highest to leaner off-peak periods of the day when\nenergy consumption is relatively lower thereby reducing the system's peak load\ndemand, which would consequently result in lesser energy bills, and improved\nload demand profile. This work introduces a novel way to develop a learning\nsystem that can learn from experience to shift loads from one time instance to\nanother and achieve the goal of minimizing the aggregate peak load. This paper\nproposes a Deep Reinforcement Learning (DRL) model for demand response where\nthe virtual agent learns the task like humans do. The agent gets feedback for\nevery action it takes in the environment; these feedbacks will drive the agent\nto learn about the environment and take much smarter steps later in its\nlearning stages. Our method outperformed the state of the art mixed integer\nlinear programming (MILP) for load peak reduction. The authors have also\ndesigned an agent to learn to minimize both consumers' electricity bills and\nutilities' system peak load demand simultaneously. The proposed model was\nanalyzed with loads from five different residential consumers; the proposed\nmethod increases the monthly savings of each consumer by reducing their\nelectricity bill drastically along with minimizing the peak load on the system\nwhen time shiftable loads are handled by the proposed method.",
      "authors": [
        "Alwyn Mathew",
        "Abhijit Roy",
        "Jimson Mathew"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": "10.1109/JSYST.2020.2996547",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-05-28",
      "selected": null,
      "title": "Intelligent Residential Energy Management System using Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2005.14259v1",
        "http://arxiv.org/pdf/2005.14259v1",
        "http://dx.doi.org/10.1109/JSYST.2020.2996547"
      ]
    },
    {
      "abstract": "The human masticatory system is a complex functional unit characterized by a multitude of skeletal components, muscles, soft tissues, and teeth. Muscle activation dynamics cannot be directly measured on live human subjects due to ethical, safety, and accessibility limitations. Therefore, estimation of muscle activations and their resultant forces is a longstanding and active area of research. Reinforcement learning (RL) is an adaptive learning strategy which is inspired by the behavioral psychology and enables an agent to learn the dynamics of an unknown system via policy-driven explorations. The RL framework is a well-formulated closed-loop system where high capacity neural networks are trained with the feedback mechanism of rewards to learn relatively complex actuation patterns. In this work, we are building on a deep RL algorithm, known as the Soft Actor-Critic, to learn the inverse dynamics of a simulated masticatory system, i.e., learn the activation patterns that drive the jaw to its desired location. The outcome of the proposed training procedure is a parametric neural model which acts as the brain of the biomechanical system. We demonstrate the model\u2019s ability to navigate the feasible three-dimensional (3D) envelope of motion with sub-millimeter accuracies. We also introduce a performance analysis platform consisting of a set of quantitative metrics to assess the functionalities of a given simulated masticatory system. This platform assesses the range of motion, metabolic efficiency, the agility of motion, the symmetry of activations, and the accuracy of reaching the desired target positions. We demonstrate how the model learns more metabolically efficient policies by integrating a force regularization term in the RL reward. We also demonstrate the inverse correlation between the metabolic efficiency of the models and their agility and range of motion. The presented masticatory model and the proposed RL training mechanism are valuable tools for the analysis of mastication and other biomechanical systems. We see this framework\u2019s potential in facilitating the functional analyses aspects of surgical treatment planning and predicting the rehabilitation performance in post-operative subjects.",
      "authors": [
        "Abdi, Amir H.",
        "Sagl, Benedikt",
        "Srungarapu, Venkata P.",
        "Stavness, Ian",
        "Prisman, Eitan",
        "Abolmaesumi, Purang",
        "Fels, Sidney"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2020.00188",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2020-05-26",
      "selected": null,
      "title": "Characterizing Motor Control of Mastication With Soft Actor-Critic",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086151237&origin=inward"
      ]
    },
    {
      "abstract": "When making decisions, people often overlook critical information or are overly swayed by irrelevant information. A common approach to mitigate these biases is to provide decision-makers, especially professionals such as medical doctors, with decision aids, such as decision trees and flowcharts. Designing effective decision aids is a difficult problem. We propose that recently developed reinforcement learning methods for discovering clever heuristics for good decision-making can be partially leveraged to assist human experts in this design process. One of the biggest remaining obstacles to leveraging the aforementioned methods for improving human decision-making is that the policies they learn are opaque to people. To solve this problem, we introduce AI-Interpret: a general method for transforming idiosyncratic policies into simple and interpretable descriptions. Our algorithm combines recent advances in imitation learning and program induction with a new clustering method for identifying a large subset of demonstrations that can be accurately described by a simple, high-performing decision rule. We evaluate our new AI-Interpret algorithm and employ it to translate information-acquisition policies discovered through metalevel reinforcement learning. The results of three large behavioral experiments showed that providing the decision rules generated by AI-Interpret as flowcharts significantly improved people\u2019s planning strategies and decisions across three different classes of sequential decision problems. Moreover, our fourth experiment revealed that this approach is significantly more effective at improving human decision-making than training people by giving them performance feedback. Finally, a series of ablation studies confirmed that our AI-Interpret algorithm was critical to the discovery of interpretable decision rules and that it is ready to be applied to other reinforcement learning problems. We conclude that the methods and findings presented in this article are an important step towards leveraging automatic strategy discovery to improve human decision-making. The code for our algorithm and the experiments is available at https://github.com/RationalityEnhancement/InterpretableStrategyDiscovery .",
      "authors": [
        "Skirzy\u0144ski, Julian",
        "Becker, Frederic",
        "Lieder, Falk"
      ],
      "categories": null,
      "citations": 7,
      "comments": "Submitted to the Special Issue on Reinforcement Learning for Real\n  Life in Machine Learning Journal (2021). Code available at\n  https://github.com/RationalityEnhancement/InterpretableStrategyDiscovery",
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1007/s10994-021-05963-2",
      "keywords": [
        "Program induction",
        "Automatic strategy discovery",
        "Interpretability",
        "Rationality enhancement",
        "Reinforcement learning",
        "Imitation learning",
        "Decision support"
      ],
      "number_of_pages": 43,
      "pages": "2641-2683",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0885-6125",
        "publisher": "Kluwer Academic Publishers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Machine Learning"
      },
      "publication_date": "2020-05-24",
      "selected": null,
      "title": "Automatic discovery of interpretable planning strategies",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10994-021-05963-2.pdf",
        "https://dl.acm.org/doi/10.1007/s10994-021-05963-2",
        "http://arxiv.org/pdf/2005.11730v3",
        "http://arxiv.org/abs/2005.11730v3",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85104157271&origin=inward",
        "http://dx.doi.org/10.1007/s10994-021-05963-2"
      ]
    },
    {
      "abstract": "Reinforcement learning has been successful in training autonomous agents to accomplish goals in complex environments. Although this has been adapted to multiple settings, including robotics and computer games, human players often find it easier to obtain higher rewards in some environments than reinforcement learning algorithms. This is especially true of high-dimensional state spaces where the reward obtained by the agent is sparse or extremely delayed. In this paper, we seek to effectively integrate feedback signals supplied by a human operator with deep reinforcement learning algorithms in high-dimensional state spaces. We call this FRESH (Feedback-based REward SHaping). During training, a human operator is presented with trajectories from a replay buffer and then provides feedback on states and actions in the trajectory. In or- der to generalize feedback signals provided by the human operator to previously unseen states and actions at test-time, we use a feed-back neural network. We use an ensemble of neural networks with a shared network architecture to represent model uncertainty and the confidence of the neural network in its output. The output of the feedback neural network is converted to a shaping reward that is augmented to the reward provided by the environment. We evaluate our approach on the Bowling and Skiing Atari games in the arcade learning environment. Although human experts have achieved high scores in these environments, state-of-the-art deep learning algorithms perform poorly. We observe that FRESH achieves much higher scores than state-of-the-art deep learning algorithms in both environments. FRESH also achieves a 21.4% higher score than a human expert in Bowling and does as well as an expert in Skiing.",
      "authors": [
        "Baicen Xiao",
        "Qifan Lu",
        "Bhaskar Ramasubramanian",
        "Andrew Clark",
        "Linda Bushnell",
        "Radha Poovendran"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3398761.3398935",
      "keywords": [
        "human feedback",
        "feedback-based reward shaping",
        "deep reinforcement learning",
        "ensemble of neural networks"
      ],
      "number_of_pages": 9,
      "pages": "1512-1520",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450375184",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems"
      },
      "publication_date": "2020-05-13",
      "selected": null,
      "title": "FRESH: Interactive Reward Shaping in High-Dimensional State Spaces using Human Feedback",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3398761.3398935"
      ]
    },
    {
      "abstract": "Autonomous agents promise users of a personalized future, allowing them to direct their attention to tasks most meaningful to them. However, the demands of personalization stand unfulfilled by current agent training paradigms such as machine learning, which require many orders of data to train agents on a single task. In sequential decision making domains, Reinforcement Learning (RL) enables this need, when a priori training of desired behaviors is intractable. Prior work has leveraged user input to train agents by mapping them to numerical reward signals. However, recent approaches have identified inconsistent human feedback as a bottleneck to achieving best-case performance. In this work, we present empirical evidence to show that human perception affected by contrast effects distorts their feedback to Reinforcement Learning agents. Through a set of studies involving 900 participants from Amazon Mechanical Turk who were asked to give feedback to RL agents, we show that participants significantly underrate an agent's actions after being exposed to an agent of higher competence on the same task. To understand the significance of this effect on agent performance during training, we then simulate trainers that underrate actions of an agent based on past performance - creating a systematically skewed feedback signal - integrated into an actor-critic framework. Our results show that agent performance is reduced by up to 98% in the presence of systematic skews in human feedback in Atari environments. Our work provides a conceptual understanding of a source of inconsistency in human feedback, thus informing the design of human-agent interactions.",
      "authors": [
        "Divya Ramesh",
        "Anthony Z. Liu",
        "Andres J. Echeverria",
        "Jean Y. Song",
        "Nicholas R. Waytowich",
        "Walter S. Lasecki"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3398761.3398888",
      "keywords": [
        "human-agent interaction",
        "reinforcement learning",
        "contrast effects",
        "human-in-the-loop"
      ],
      "number_of_pages": 8,
      "pages": "1090-1097",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450375184",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems"
      },
      "publication_date": "2020-05-13",
      "selected": null,
      "title": "Yesterday's Reward is Today's Punishment: Contrast Effects in Human Feedback to Reinforcement Learning Agents",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3398761.3398888"
      ]
    },
    {
      "abstract": "The high request for autonomous and flexible HRI implies the necessity of deploying Machine Learning (ML) mechanisms in the robot control. Indeed, the use of ML techniques, such as Reinforcement Learning (RL), makes the robot behaviour, during the learning process, not transparent to the observing user. In this work, we proposed an emotional model to improve the transparency in RL tasks for human-robot collaborative scenarios. The architecture we propose supports the RL algorithm with an emotional model able to both receive human feedback and exhibit emotional responses based on the learning process. The model is entirely based on the Temporal Difference (TD) error. The architecture was tested in an isolated laboratory with a simple setup. The results highlight that showing its internal state through an emotional response is enough to make a robot transparent to its human teacher. People also prefer to interact with a responsive robot because they are used to understand their intentions via emotions and social signals.",
      "authors": [
        "Matarese, Marco",
        "Rossi, Silvia",
        "Sciutti, Alessandra",
        "Rea, Francesco"
      ],
      "categories": null,
      "citations": null,
      "comments": "Presented at the 2020 Workshop on Assessing, Explaining, and\n  Conveying Robot Proficiency for Human-Robot Teaming",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-05-12",
      "selected": null,
      "title": "Towards Transparency of TD-RL Robotic Systems with a Human Teacher",
      "urls": [
        "http://arxiv.org/pdf/2005.05926v1",
        "http://arxiv.org/pdf/2005.05926.pdf",
        "http://arxiv.org/abs/2005.05926v1"
      ]
    },
    {
      "abstract": "We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18-39%. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.",
      "authors": [
        "Gao, Yang",
        "Zhao, Wei",
        "Eger, Steffen"
      ],
      "categories": null,
      "citations": null,
      "comments": "ACL 2020",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-05-07",
      "selected": null,
      "title": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization",
      "urls": [
        "http://arxiv.org/pdf/2005.03724v1",
        "http://arxiv.org/pdf/2005.03724.pdf",
        "http://arxiv.org/abs/2005.03724v1"
      ]
    },
    {
      "abstract": "An important current challenge in Human-Robot Interaction (HRI) is to enable robots to learn on-the-fly from human feedback. However, humans show a great variability in the way they reward robots. We propose to address this issue by enabling the robot to combine different learning strategies, namely model-based (MB) and model-free (MF) reinforcement learning. We simulate two HRI scenarios: a simple task where the human congratulates the robot for putting the right cubes in the right boxes, and a more complicated version of this task where cubes have to be placed in a specific order. We show that our existing MB-MF coordination algorithm previously tested in robot navigation works well here without retuning parameters. It leads to the maximal performance while producing the same minimal computational cost as MF alone. Moreover, the algorithm gives a robust performance no matter the variability of the simulated human feedback, while each strategy alone is impacted by this variability. Overall, the results suggest a promising way to promote robot learning flexibility when facing variable human feedback.",
      "authors": [
        "Dromnelle, R\u00e9mi",
        "Girard, Beno\u00eet",
        "Renaudo, Erwan",
        "Chatila, Raja",
        "Khamassi, Mehdi"
      ],
      "categories": null,
      "citations": null,
      "comments": "6 pages, 5 figures, written for the RO-MAN 2020 conference. arXiv\n  admin note: text overlap with arXiv:2004.14698",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-05-06",
      "selected": null,
      "title": "Coping with the variability in humans reward during simulated human-robot interactions through the coordination of multiple learning strategies",
      "urls": [
        "http://arxiv.org/pdf/2005.03987.pdf",
        "http://arxiv.org/abs/2005.03987v1",
        "http://arxiv.org/pdf/2005.03987v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sch\u00fcller T."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cortex.2019.12.027",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "39-48",
      "publication": {
        "category": "Journal",
        "cite_score": 7.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00109452",
        "publisher": "Masson SpA",
        "sjr": 1.303,
        "snip": 1.241,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Cortex"
      },
      "publication_date": "2020-05-01",
      "selected": null,
      "title": "Decreased transfer of value to action in Tourette syndrome",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079182503&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hackel L."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jesp.2019.103948",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00221031",
        "publisher": "Academic Press Inc.",
        "sjr": 1.596,
        "snip": 1.636,
        "subject_areas": [
          "Sociology and Political Science",
          "Social Psychology"
        ],
        "title": "Journal of Experimental Social Psychology"
      },
      "publication_date": "2020-05-01",
      "selected": null,
      "title": "Reinforcement learning in social interaction: The distinguishing role of trait inference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077660128&origin=inward"
      ]
    },
    {
      "abstract": "<p>Hormonal transitions across the menstrual cycle may modulate human reward processing and reinforcement learning, but previous results were contradictory. Studies assessed relatively small samples (<italic>n</italic> &lt; 30) and exclusively used within-subject designs to compare women in hormonally distinct menstrual cycle phases. This increased the risk of sporadic findings and results may have been disproportionally affected by expectancy effects. Also, replication studies are widely missing, which currently precludes any reliable inferences. The present study was intended as a conceptual replication of a previous study [(<xref ref-type=\"bibr\" rid=\"B1\">1</xref>), Neuropsychologia 84; <italic>n</italic> = 15]. There, we had observed a reduction in avoidance learning capacity when women were in the high estradiol state of the late follicular phase as compared to the mid luteal phase with enhanced progesterone influence. These results conformed to the idea that estradiol and progesterone may antagonistically modulate dopaminergic transmission as a dopamine agonist and antagonist, respectively. Heightened progesterone in the luteal phase thereby supported the ability to learn from the negative outcomes of one's actions, while the follicular rise in estradiol interfered with this capacity. Here, we re-examined the above described within-subject difference between the follicular and the luteal phase in a between-subjects design. Seventy-five women were tested once with a probabilistic feedback learning task, while being either in the follicular (36 women) or luteal phase (39 women), and were compared for phase-related differences in behavior. Secondly, we combined the new data with data from three previous studies from our laboratory that used the same task and menstrual cycle phases. This meta-analysis included only data from the first test day, free of any biasing expectancy effects. Both analyses demonstrated the consistency of the decline in avoidance learning in the follicular relative to the luteal phase. We also showed that this decline reliably occurred in all of the included samples. Altogether, these results provide evidence for the consistency of a behavioral difference and its apparent association with a transient change in hormonal state that occurs in the natural menstrual cycle. Our findings may also open new avenues for the development of reliable between-subjects test protocols in menstrual cycle research.</p>",
      "authors": [
        "Diekhof, Esther K.",
        "Korf, Sina",
        "Ott, Franziska",
        "Sch\u00e4dlich, Carolin",
        "Holtfrerich, Sarah K. C."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fendo.2020.00231",
      "keywords": [
        "Meta - analysis",
        "reinforcement learning (RL)",
        "estrogen (17b-estradiol)",
        "Replication crisis in psychology",
        "Menstrual Cycle",
        "Progesterone & estradiol",
        "reward processing"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-2392",
        "publisher": "Frontiers Media SA",
        "sjr": 1.278,
        "snip": 1.426,
        "subject_areas": [
          "Endocrinology, Diabetes and Metabolism"
        ],
        "title": "Frontiers in Endocrinology"
      },
      "publication_date": "2020-04-24",
      "selected": null,
      "title": "Avoidance Learning Across the Menstrual Cycle: A Conceptual Replication",
      "urls": [
        "https://www.frontiersin.org/journals/endocrinology/articles/10.3389/fendo.2020.00231/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084365550&origin=inward"
      ]
    },
    {
      "abstract": "Sequence-to-sequence learning involves a trade-off between signal strength and annotation cost of training data. For example, machine translation data range from costly expert-generated translations that enable supervised learning, to weak quality-judgment feedback that facilitate reinforcement learning. We present the first user study on annotation cost and machine learnability for the less popular annotation mode of error markings. We show that error markings for translations of TED talks from English to German allow precise credit assignment while requiring significantly less human effort than correcting/post-editing, and that error-marked data can be used successfully to fine-tune neural machine translation models.",
      "authors": [
        "Kreutzer, Julia",
        "Berger, Nathaniel",
        "Riezler, Stefan"
      ],
      "categories": null,
      "citations": 5,
      "comments": "To appear at EAMT 2020 (Research Track)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "135-144",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789893305898",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, EAMT 2020"
      },
      "publication_date": "2020-04-23",
      "selected": null,
      "title": "Correct Me If You Can: Learning from Error Corrections and Markings",
      "urls": [
        "http://arxiv.org/pdf/2004.11222.pdf",
        "http://arxiv.org/abs/2004.11222v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101616161&origin=inward",
        "http://arxiv.org/pdf/2004.11222v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bassen J."
      ],
      "categories": null,
      "citations": 35,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3313831.3376518",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450367080",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference on Human Factors in Computing Systems - Proceedings"
      },
      "publication_date": "2020-04-21",
      "selected": null,
      "title": "Reinforcement Learning for the Adaptive Scheduling of Educational Activities",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087425585&origin=inward"
      ]
    },
    {
      "abstract": "Prior work on generating explanations in a planning and decision-making context has focused on providing the rationale behind an AI agent's decision making. While these methods provide the right explanations from the explainer's perspective, they fail to heed the cognitive requirement of understanding an explanation from the explainee's (the human's) perspective. In this work, we set out to address this issue by first considering the influence of information order in an explanation, or the progressiveness of explanations. Intuitively, progression builds later concepts on previous ones and is known to contribute to better learning. In this work, we aim to investigate similar effects during explanation generation when an explanation is broken into multiple parts that are communicated sequentially. The challenge here lies in modeling the humans' preferences for information order in receiving such explanations to assist understanding. Given this sequential process, a formulation based on goal-based MDP for generating progressive explanations is presented. The reward function of this MDP is learned via inverse reinforcement learning based on explanations that are retrieved via human subject studies. We first evaluated our approach on a scavenger-hunt domain to demonstrate its effectively in capturing the humans' preferences. Upon analyzing the results, it revealed something more fundamental: the preferences arise strongly from both domain dependent and independence features. The correlation with domain independent features pushed us to verify this result further in an escape room domain. Results confirmed our hypothesis that the process of understanding an explanation was a dynamic process. The human preference that reflected this aspect corresponded exactly to the progression for knowledge assimilation hidden deeper in our cognitive process.",
      "authors": [
        "Zakershahrak, Mehrdad",
        "Marpally, Shashank Rao",
        "Sharma, Akshay",
        "Gong, Ze",
        "Zhang, Yu"
      ],
      "categories": null,
      "citations": null,
      "comments": "arXiv admin note: text overlap with arXiv:1902.00604",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-04-16",
      "selected": null,
      "title": "Order Matters: Generating Progressive Explanations for Planning Tasks in Human-Robot Teaming",
      "urls": [
        "http://arxiv.org/pdf/2004.07822v2",
        "http://arxiv.org/abs/2004.07822v2",
        "http://arxiv.org/pdf/2004.07822.pdf"
      ]
    },
    {
      "abstract": "<p>Adaptive coding of stimuli is well documented in perception, where it supports efficient encoding over a broad range of possible percepts. Recently, a similar neural mechanism has been reported also in value-based decision, where it allows optimal encoding of vast ranges of values in PFC: neuronal response to value depends on the choice context (relative coding), rather than being invariant across contexts (absolute coding). Additionally, value learning is sensitive to the amount of feedback information: providing complete feedback (both obtained and forgone outcomes) instead of partial feedback (only obtained outcome) improves learning. However, it is unclear whether relative coding occurs in all PFC regions and how it is affected by feedback information. We systematically investigated univariate and multivariate feedback encoding in various mPFC regions and compared three modes of neural coding: absolute, partially-adaptive and fully-adaptive.</p><p>Twenty-eight human participants (both sexes) performed a learning task while undergoing fMRI scanning. On each trial, they chose between two symbols associated with a certain outcome. Then, the decision outcome was revealed. Notably, in one-half of the trials participants received partial feedback, whereas in the other half they got complete feedback. We used univariate and multivariate analysis to explore value encoding in different feedback conditions.</p><p>We found that both obtained and forgone outcomes were encoded in mPFC, but with opposite sign in its ventral and dorsal subdivisions. Moreover, we showed that increasing feedback information induced a switch from absolute to relative coding. Our results suggest that complete feedback information enhances context-dependent outcome encoding.</p><p><b>SIGNIFICANCE STATEMENT</b> This study offers a systematic investigation of the effect of the amount of feedback information (partial vs complete) on univariate and multivariate outcome value encoding, within multiple regions in mPFC and cingulate cortex that are critical for value-based decisions and behavioral adaptation. Moreover, we provide the first comparison of three possible models of neural coding (i.e., absolute, partially-adaptive, and fully-adaptive coding) of value signal in these regions, by using commensurable measures of prediction accuracy. Taken together, our results help build a more comprehensive picture of how the human brain encodes and processes outcome value. In particular, our results suggest that simultaneous presentation of obtained and foregone outcomes promotes relative value representation.</p>",
      "authors": [
        "Doris Pischedda",
        "Stefano Palminteri",
        "Giorgio Coricelli"
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1712-19.2020",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "3268-3277",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2020-04-15",
      "selected": null,
      "title": "The Effect of Counterfactual Information on Outcome Value Coding in Medial Prefrontal and Cingulate Cortex: From an Absolute to a Relative Neural Code",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083409491&origin=inward"
      ]
    },
    {
      "abstract": "In older age, learning and feedback processing are usually impaired. This is thought to be due to impairments in the dopaminergic system and the anterior cingulate cortex. By contrast, processing of affective information seems to remain relatively intact. Recent research has also demonstrated that cognitive functioning can be influenced by affective materials or contexts and lead to an enhancement in diverse cognitive tasks. Hence, the aim of the present study was to explore, whether emotional feedback would counteract age-related learning deficits and strengthen early and later phases of feedback processing as reflected in the feedback-related negativity (FRN) and P3b of the event-related potential (ERP). Younger and older participants conducted a probabilistic reinforcement learning task in which the accurate responses had to be learned via feedback. In emotional trials, feedback stimuli consisted of faces with smiling and disgusted expressions, and in a non-emotional condition, positive and negative feedback was indicated by the background color of faces with neutral expressions. Our main results were that older adults showed better learning performance in the emotional feedback condition and a larger P3b after emotional than non-emotional feedback indexing heightened working memory updating after task relevant events.",
      "authors": [
        "Nicola K. Ferdinand",
        "Melanie Hilz"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0231964",
      "keywords": [
        "Electroencephalography",
        "Emotions",
        "Learning",
        "Event-related potentials",
        "Elderly",
        "Reaction time",
        "Facial expressions",
        "Face"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Emotional feedback ameliorates older adults\u2019 feedback-induced learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084276788&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0231964&type=printable"
      ]
    },
    {
      "abstract": "This paper presents a hierarchical deep reinforcement learning (DRL) method for the scheduling of energy consumptions of smart home appliances and distributed energy resources (DERs) including an energy storage system (ESS) and an electric vehicle (EV). Compared to Q-learning algorithms based on a discrete action space, the novelty of the proposed approach is that the energy consumptions of home appliances and DERs are scheduled in a continuous action space using an actor\u2013critic-based DRL method. To this end, a two-level DRL framework is proposed where home appliances are scheduled at the first level according to the consumer\u2019s preferred appliance scheduling and comfort level, while the charging and discharging schedules of ESS and EV are calculated at the second level using the optimal solution from the first level along with the consumer environmental characteristics. A simulation study is performed in a single home with an air conditioner, a washing machine, a rooftop solar photovoltaic system, an ESS, and an EV under a time-of-use pricing. Numerical examples under different weather conditions, weekday/weekend, and driving patterns of the EV confirm the effectiveness of the proposed approach in terms of total cost of electricity, state of energy of the ESS and EV, and consumer preference.",
      "authors": [
        "Lee, Sangyoon",
        "Choi, Dae-Hyun"
      ],
      "categories": null,
      "citations": 73,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/s20072157",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14248220",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.764,
        "snip": 1.317,
        "subject_areas": [
          "Atomic and Molecular Physics, and Optics",
          "Information Systems",
          "Instrumentation",
          "Biochemistry",
          "Analytical Chemistry",
          "Electrical and Electronic Engineering"
        ],
        "title": "Sensors (Switzerland)"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Energy Management of Smart Home with Home Appliances, Energy Storage System and Electric Vehicle: A Hierarchical Deep Reinforcement Learning Approach",
      "urls": [
        "https://www.mdpi.com/1424-8220/20/7/2157/pdf?version=1586839424",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083408386&origin=inward"
      ]
    },
    {
      "abstract": "The neural mechanisms of cognitive dysfunctions in neurological diseases remain poorly understood. Here, we conjecture that this unsatisfying state-of-the-art is in part due to the non-specificity of the typical behavioral indicators for cognitive dysfunctions. Our study addresses the topic by advancing the assessment of cognitive dysfunctions through computational modeling. We investigate bradyphrenia in Parkinson\u2019s disease (PD) as an exemplary case of cognitive dysfunctions in neurological diseases. Our computational model conceptualizes trial-by-trial behavioral data as resulting from parallel cognitive and sensorimotor reinforcement learning. We assessed PD patients \u2018on\u2019 and \u2018off\u2019 their dopaminergic medication and matched healthy control (HC) participants on a computerized version of the Wisconsin Card Sorting Test. PD patients showed increased retention of learned cognitive information and decreased retention of learned sensorimotor information from previous trials in comparison to HC participants. Systemic dopamine replacement therapy did not remedy these cognitive dysfunctions in PD patients but incurred non-desirable side effects such as decreasing cognitive learning from positive feedback. Our results reveal novel insights into facets of bradyphrenia that are indiscernible by observable behavioral indicators of cognitive dysfunctions. We discuss how computational modeling may contribute to the advancement of future research on brain\u2013behavior relationships and neuropsychological assessment.",
      "authors": [
        "Steinke, Alexander",
        "Lange, Florian",
        "Seer, Caroline",
        "Hendel, Merle K.",
        "Kopp, Bruno"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/jcm9041158",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2077-0383",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.935,
        "snip": 1.179,
        "subject_areas": [
          "Medicine (all)"
        ],
        "title": "Journal of Clinical Medicine"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Computational Modeling for Neuropsychological Assessment of Bradyphrenia in Parkinson\u2019s Disease",
      "urls": [
        "https://www.mdpi.com/2077-0383/9/4/1158/pdf?version=1587996503",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091279364&origin=inward"
      ]
    },
    {
      "abstract": "This study investigated individual differences of conflict monitoring (N2 component), feedback processing (feedback negativity component), and reinforcement learning in a discrimination learning task using a mock (fictitious) forensic scenario to set participants in a semantic task context. We investigated individual differences of anxiety-related, impulsivity-related traits and reasoning ability during trial-and-error learning of mock suspect and nonsuspect faces. Thereby, we asked how the differential investment of cognitive-motivational processes facilitates learning in a mock forensic context. As learning can be studied by means of time-on-task effects (i.e., variations of cognitive processes across task blocks), we investigated the differential investment of cognitive-motivational processes block-wise in N = 100 participants. By performing structural equation modeling, we demonstrate that conflict monitoring decreased across task blocks, whereas the percentage of correct responses increased across task blocks. Individuals with higher reasoning scores and higher impulsivity-related traits relied rather on feedback processing (i.e., external indicators) during reinforcement learning. Individuals with higher anxiety-related traits intensified their conflict monitoring throughout the task to learn successfully. Observation by relevant others intensified conflict monitoring more than nonobservation. Our data highlight that individual differences and social context modulate the intensity of information processing in a discrimination learning task using a mock forensic task scenario. We discuss our data with regard to recent cognitive-motivational approaches and in terms of reinforcement learning.",
      "authors": [
        "Leue, Anja",
        "Nieden, Katharina",
        "Scheuble, Vera",
        "Beauducel, Andr\u00e9"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-020-00776-7",
      "keywords": [],
      "number_of_pages": 19,
      "pages": "408-426",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Individual differences of conflict monitoring and feedback processing during reinforcement learning in a mock forensic context",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-020-00776-7.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079447167&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhang X."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.13531",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Acute stress impairs reward positivity effect in probabilistic learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078282863&origin=inward"
      ]
    },
    {
      "abstract": "Increasing evidence shows that the generation and regulation of affective responses is associated with activity of large brain networks that also include phylogenetically older regions in the brainstem. Mesencephalic regions not only control autonomic responses but also participate in the modulation of autonomic, emotional, and motivational responses. The specific contribution of the midbrain to emotion regulation in humans remains elusive. Neuroimaging studies grounding on appraisal models of emotion emphasize a major role of prefrontal cortex in modulating emotion-related cortical and subcortical regions but usually neglect the contribution of the midbrain and other brainstem regions. Here, the role of mesolimbic and mesocortical networks in core affect generation and regulation was explored during emotion regulation guided by real-time fMRI feedback of the anterior insula activity. The fMRI and functional connectivity analysis revealed that the upper midbrain significantly contributes to emotion regulation in humans. Moreover, differential functional interactions between the dopaminergic mesocorticolimbic system and frontoparietal networks mediate up and down emotion regulatory processes. Finally, these findings further indicate the potential of real-time fMRI feedback approach in guiding core affect regulation.",
      "authors": [
        "Caria, Andrea"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/brainsci10040223",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2076-3425",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.752,
        "snip": 0.938,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Brain Sciences"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Mesocorticolimbic Interactions Mediate fMRI-Guided Regulation of Self-Generated Affective States",
      "urls": [
        "https://www.mdpi.com/2076-3425/10/4/223/pdf?version=1586345708",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083957506&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu C."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2020.01.004",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "11-19",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "A tradeoff relationship between internal monitoring and external feedback during the dynamic process of reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078503177&origin=inward"
      ]
    },
    {
      "abstract": "Feedback-based learning relies on a procedural learning system driven by reward prediction errors (RPEs). The processing of temporally delayed feedback is supported by brain structures associated with declarative memory processes, but it is still unknown how delayed feedback processing and memory encoding interact. In this study, a subsequent memory paradigm was employed to investigate how the incidental encoding of feedback pictures presented with a short (SD, 500 ms) or long (LD, 6500 ms) delay in a probabilistic learning task affects the event-related potential (ERP) correlate of RPEs (i.e., the feedback-related negativity; FRN). In an ensuing test phase, a surprise recognition memory test for the feedback pictures was conducted. FRN amplitudes measured in the feedback-locked ERPs recorded during the learning phase (FRNpeak) and in the negative minus positive feedback difference wave (FRNdiff) were compared for subsequently remembered and forgotten feedback pictures. Feedback processing as reflected in the FRNpeak was diminished for remembered LD feedback pictures, indicating that delayed feedback processing and memory encoding competed for similar neural processing resources. As evidenced by large FRNdiff amplitudes in the SD condition, the evaluation of shortly delayed feedback strongly relied on the procedural learning system. A complementary model-based single trial analysis was conducted to validate models of the functional significance of the FRN. Consistent with previous studies, feedback-locked N170 and P300 amplitudes were sensitive to feedback delay. In the test phase, memory for LD feedback pictures was better than for SD pictures and accompanied by a late old\u2013new effect, presumably reflecting extended recollective processing.",
      "authors": [
        "H\u00f6ltje, Gerrit",
        "Mecklinger, Axel"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-019-00765-5",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "250-264",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Feedback timing modulates interactions between feedback processing and memory encoding: Evidence from event-related potentials",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077639794&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-019-00765-5.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Tejedor M."
      ],
      "categories": null,
      "citations": 71,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.artmed.2020.101836",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 14.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09333657",
        "publisher": "Elsevier B.V.",
        "sjr": 1.443,
        "snip": 2.22,
        "subject_areas": [
          "Artificial Intelligence",
          "Medicine (miscellaneous)"
        ],
        "title": "Artificial Intelligence in Medicine"
      },
      "publication_date": "2020-04-01",
      "selected": null,
      "title": "Reinforcement learning application in diabetes blood glucose control: A systematic review",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080919667&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Punt M."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jbiomech.2019.109513",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00219290",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.699,
        "snip": 1.187,
        "subject_areas": [
          "Biophysics",
          "Biomedical Engineering",
          "Orthopedics and Sports Medicine",
          "Rehabilitation"
        ],
        "title": "Journal of Biomechanics"
      },
      "publication_date": "2020-03-26",
      "selected": null,
      "title": "Real-time feedback to reduce low-back load in lifting and lowering",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075972424&origin=inward"
      ]
    },
    {
      "abstract": "This paper addresses the problem of training a robot to carry out temporal tasks of arbitrary complexity via evaluative human feedback that can be inaccurate. A key idea explored in our work is a kind of curriculum learning\u2014training the robot to master simple tasks and then building up to more complex tasks. We show how a training procedure, using knowledge of the formal task representation, can decompose and train any task efficiently in the size of its representation. We further provide a set of experiments that support the claim that non-expert human trainers can decompose tasks in a way that is consistent with our theoretical results, with more than half of participants successfully training all of our experimental missions. We compared our algorithm with existing approaches and our experimental results suggest that our method outperforms alternatives, especially when feedback contains mistakes.CCS CONCEPTS \u2022 Theory of computation \u2192 Reinforcement learning; Modal and temporal logics; \u2022 Computing methodologies \u2192 Learning from critiques; \u2022 Human-centered computing \u2192 Interaction paradigms. ACM Reference Format: Guan Wang, Carl Trimbach, Jun Ki Lee, Mark K. Ho, and Michael L. Littman. 2020. Teaching a Robot Tasks of Arbitrary Complexity via Human Feedback. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (HRI\u201920), March 23\u201326, 2020, Cambridge, United Kingdom. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3319502.3374824",
      "authors": [
        "Guan Wang",
        "Carl Trimbach",
        "Jun Ki Lee",
        "Mark K. Ho",
        "Michael L. Littman"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": null,
      "keywords": [
        "linear temporal logic",
        "human-robot interaction",
        "reinforcement learning",
        "learning from human feedback"
      ],
      "number_of_pages": 9,
      "pages": "649-657",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-8556-3",
        "issn": "2167-2121",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2020-03-23",
      "selected": null,
      "title": "Teaching a Robot Tasks of Arbitrary Complexity via Human Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9484303"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Del Duchetto F."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3371382.3377430",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "561-563",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450370578",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2020-03-23",
      "selected": null,
      "title": "Automatic assessment and learning of robot social abilities",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083166976&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement Learning (RL) in various decision-making tasks of machine learning provides effective results with an agent learning from a stand-alone reward function. However, it presents unique challenges with large amounts of environment states and action spaces, as well as in the determination of rewards. This complexity, coming from high dimensionality and continuousness of the environments considered herein, calls for a large number of learning trials to learn about the environment through Reinforcement Learning. Imitation Learning (IL) offers a promising solution for those challenges using a teacher. In IL, the learning process can take advantage of human-sourced assistance and/or control over the agent and environment. A human teacher and an agent learner are considered in this study. The teacher takes part in the agent training towards dealing with the environment, tackling a specific objective, and achieving a predefined goal. Within that paradigm, however, existing IL approaches have the drawback of expecting extensive demonstration information in long-horizon problems. This paper proposes a novel approach combining IL with different types of RL methods, namely state action reward state action (SARSA) and asynchronous advantage actor-critic (A3C) agents, to overcome the problems of both stand-alone systems. It is addressed how to effectively leverage the teacher feedback, be it direct binary or indirect detailed for the agent learner to learn sequential decision-making policies. The results of this study on various OpenAI Gym environments show that this algorithmic method can be incorporated with different combinations, significantly decreases both human endeavor and tedious exploration process.",
      "authors": [
        "Navidi, Neda"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-03-09",
      "selected": null,
      "title": "Human AI interaction loop training: New approach for interactive reinforcement learning",
      "urls": [
        "http://arxiv.org/pdf/2003.04203.pdf",
        "http://arxiv.org/abs/2003.04203v1",
        "http://arxiv.org/pdf/2003.04203v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang G."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3319502.3374824",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "649-657",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450367462",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2020-03-09",
      "selected": null,
      "title": "Teaching a robot tasks of arbitrary complexity via human feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082021557&origin=inward"
      ]
    },
    {
      "abstract": "Precision landing on large and small planetary bodies is a technology of\nutmost importance for future human and robotic exploration of the solar system.\nIn this context, the Zero-Effort-Miss/Zero-Effort-Velocity (ZEM/ZEV) feedback\nguidance algorithm has been studied extensively and is still a field of active\nresearch. The algorithm, although powerful in terms of accuracy and ease of\nimplementation, has some limitations. Therefore with this paper we present an\nadaptive guidance algorithm based on classical ZEM/ZEV in which machine\nlearning is used to overcome its limitations and create a closed loop guidance\nalgorithm that is sufficiently lightweight to be implemented on board\nspacecraft and flexible enough to be able to adapt to the given constraint\nscenario. The adopted methodology is an actor-critic reinforcement learning\nalgorithm that learns the parameters of the above-mentioned guidance\narchitecture according to the given problem constraints.",
      "authors": [
        "Roberto Furfaro",
        "Andrea Scorsoglio",
        "Richard Linares",
        "Mauro Massari"
      ],
      "categories": null,
      "citations": 70,
      "comments": "46 pages, 14 figures, Acta Astronautica, Pre-proof, Available March\n  4, 2020",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.1016/j.actaastro.2020.02.051",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "156-171",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00945765",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.015,
        "snip": 1.591,
        "subject_areas": [
          "Aerospace Engineering"
        ],
        "title": "Acta Astronautica"
      },
      "publication_date": "2020-03-04",
      "selected": null,
      "title": "Adaptive Generalized ZEM-ZEV Feedback Guidance for Planetary Landing via a Deep Reinforcement Learning Approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081025426&origin=inward",
        "http://arxiv.org/abs/2003.02182v1",
        "http://dx.doi.org/10.1016/j.actaastro.2020.02.051",
        "http://arxiv.org/pdf/2003.02182v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gao T."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/17470919.2019.1668846",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "170-185",
      "publication": {
        "category": "Journal",
        "cite_score": 3.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17470919",
        "publisher": "Taylor and Francis Ltd.",
        "sjr": 0.674,
        "snip": 0.73,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Social Psychology",
          "Development"
        ],
        "title": "Social Neuroscience"
      },
      "publication_date": "2020-03-03",
      "selected": null,
      "title": "Neural mechanisms of reinforcement learning under mortality threat",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073969640&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Vich C."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cnsns.2019.105048",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10075704",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Communications in Nonlinear Science and Numerical Simulation"
      },
      "publication_date": "2020-03-01",
      "selected": null,
      "title": "Corticostriatal synaptic weight evolution in a two-alternative forced choice task: a computational study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074147588&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Colino F.L."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2020.107849",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2020-03-01",
      "selected": null,
      "title": "Electroencephalographic evidence for a reinforcement learning advantage during motor skill acquisition",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078223942&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang T."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/ima.22387",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "92-103",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08999457",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Imaging Systems and Technology"
      },
      "publication_date": "2020-03-01",
      "selected": null,
      "title": "Functional neural interactions during adaptive reward learning: An functional magnetic resonance imaging study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076408105&origin=inward"
      ]
    },
    {
      "abstract": "BackgroundAberrant sensitivity to social reward may be an important contributor to abnormal social behavior that is a core feature of schizophrenia. The neuropeptide oxytocin impacts the salience of social information across species, but its effect on social reward in schizophrenia is unknown.MethodsWe used a competitive economic game and computational modeling to examine behavioral dynamics and oxytocin effects on sensitivity to social reward among 39 men with schizophrenia and 54 matched healthy controls. In a randomized, double-blind study, participants received one dose of oxytocin (40 IU) or placebo and completed a 35-trial Auction Game that quantifies preferences for monetary v. social reward. We analyzed bidding behavior using multilevel linear mixed models and reinforcement learning models.ResultsBidding was motivated by preferences for both monetary and social reward in both groups, but bidding dynamics differed: patients initially overbid less compared to controls, and across trials, controls decreased their bids while patients did not. Oxytocin administration was associated with sustained overbidding across trials, particularly in patients. This drug effect was driven by a stronger preference for winning the auction, regardless of monetary consequences. Learning rate and response variability did not differ between groups or drug condition, suggesting that differences in bidding derive primarily from differences in the subjective value of social rewards.ConclusionsOur findings suggest that schizophrenia is associated with diminished motivation for social reward that may be increased by oxytocin administration.",
      "authors": [
        "Ellen R. Bradley",
        "Johanna Brustkern",
        "Lize De Coster",
        "Wouter van den Bos",
        "Samuel M. McClure",
        "Alison Seitz",
        "Joshua D. Woolley"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1017/S0033291719000552",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "674-682",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00332917",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychological Medicine"
      },
      "publication_date": "2020-03-01",
      "selected": null,
      "title": "Victory is its own reward: oxytocin increases costly competitive behavior in schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063909263&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Linke J.O."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.jad.2019.11.063",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "400-406",
      "publication": {
        "category": "Journal",
        "cite_score": 9.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01650327",
        "publisher": "Elsevier B.V.",
        "sjr": 1.988,
        "snip": 1.877,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Journal of Affective Disorders"
      },
      "publication_date": "2020-03-01",
      "selected": null,
      "title": "Aberrant probabilistic reinforcement learning in first-degree relatives of individuals with bipolar disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076227514&origin=inward"
      ]
    },
    {
      "abstract": "Meal assistant robots form a very important part of the assistive robotics sector since self-feeding is a priority activity of daily living (ADL) for people suffering from physical disabilities like tetraplegia. A quick survey of the current trends in this domain reveals that, while tremendous progress has been made in the development of assistive robots for the feeding of solid foods, the task of feeding liquids from a cup remains largely underdeveloped. Therefore, this paper describes an assistive robot that focuses specifically on the feeding of liquids from a cup using tactile feedback through force sensors with direct human\u2013robot interaction (HRI). The main focus of this paper is the application of reinforcement learning (RL) to learn what the best robotic actions are, based on the force applied by the user. A model of the application environment is developed based on the Markov decision process and a software training procedure is designed for quick development and testing. Five of the commonly used RL algorithms are investigated, with the intention of finding the best fit for training, and the system is tested in an experimental study. The preliminary results show a high degree of acceptance by the participants. Feedback from the users indicates that the assistive robot functions intuitively and effectively.",
      "authors": [
        "Kumar Shastha, Tejas",
        "Kyrarini, Maria",
        "Gr\u00e4ser, Axel"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/robotics9010001",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2218-6581",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.812,
        "snip": 1.612,
        "subject_areas": [
          "Artificial Intelligence",
          "Mechanical Engineering",
          "Control and Optimization"
        ],
        "title": "Robotics"
      },
      "publication_date": "2020-03-01",
      "selected": null,
      "title": "Application of Reinforcement Learning to a Robotic Drinking Assistant",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079825461&origin=inward",
        "https://www.mdpi.com/2218-6581/9/1/1/pdf?version=1577342788"
      ]
    },
    {
      "abstract": "Rehabilitation assessment is critical to determine an adequate intervention for a patient. However, the current practices of assessment mainly rely on therapist's experience, and assessment is infrequently executed due to the limited availability of a therapist. In this paper, we identified the needs of therapists to assess patient's functional abilities (e.g. alternative perspective on assessment with quantitative information on patient's exercise motions). As a result, we developed an intelligent decision support system that can identify salient features of assessment using reinforcement learning to assess the quality of motion and summarize patient specific analysis. We evaluated this system with seven therapists using the dataset from 15 patient performing three exercises. The evaluation demonstrates that our system is preferred over a traditional system without analysis while presenting more useful information and significantly increasing the agreement over therapists' evaluation from 0.6600 to 0.7108 F1-scores ($p <0.05$). We discuss the importance of presenting contextually relevant and salient information and adaptation to develop a human and machine collaborative decision making system.",
      "authors": [
        "Lee, Min Hun",
        "Siewiorek, Daniel P.",
        "Smailagic, Asim",
        "Bernardino, Alexandre",
        "Badia, Sergi Berm\u00fadez i"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-02-27",
      "selected": null,
      "title": "Opportunities of a Machine Learning-based Decision Support System for Stroke Rehabilitation Assessment",
      "urls": [
        "http://arxiv.org/abs/2002.12261v2",
        "http://arxiv.org/pdf/2002.12261.pdf",
        "http://arxiv.org/pdf/2002.12261v2"
      ]
    },
    {
      "abstract": "The ability to handle single molecules as effectively as macroscopic\nbuilding-blocks would enable the construction of complex supramolecular\nstructures inaccessible to self-assembly. The fundamental challenges\nobstructing this goal are the uncontrolled variability and poor observability\nof atomic-scale conformations. Here, we present a strategy to work around both\nobstacles, and demonstrate autonomous robotic nanofabrication by manipulating\nsingle molecules. Our approach employs reinforcement learning (RL), which finds\nsolution strategies even in the face of large uncertainty and sparse feedback.\nWe demonstrate the potential of our RL approach by removing molecules\nautonomously with a scanning probe microscope from a supramolecular structure\n-- an exemplary task of subtractive manufacturing at the nanoscale. Our RL\nagent reaches an excellent performance, enabling us to automate a task which\npreviously had to be performed by a human. We anticipate that our work opens\nthe way towards autonomous agents for the robotic construction of functional\nsupramolecular structures with speed, precision and perseverance beyond our\ncurrent capabilities.",
      "authors": [
        "Philipp Leinen",
        "Malte Esders",
        "Kristof T. Sch\u00fctt",
        "Christian Wagner",
        "Klaus-Robert M\u00fcller",
        "F. Stefan Tautz"
      ],
      "categories": null,
      "citations": 39,
      "comments": "3 figures",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.1126/sciadv.abb6987",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Robotics",
          "Artificial Intelligence",
          "Learning",
          "Mesoscale and Nanoscale Physics"
        ],
        "title": "Sci. Adv. 6, eabb6987 (2020)"
      },
      "publication_date": "2020-02-27",
      "selected": null,
      "title": "Autonomous robotic nanofabrication with reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090914153&origin=inward",
        "http://dx.doi.org/10.1126/sciadv.abb6987",
        "http://arxiv.org/pdf/2002.11952v2",
        "http://arxiv.org/abs/2002.11952v2"
      ]
    },
    {
      "abstract": "The development of autonomous robotic systems that can learn from human demonstrations to imitate a desired behavior - rather than being manually programmed - has huge technological potential. One major challenge in imitation learning is the correspondence problem: how to establish corresponding states and actions between expert and learner, when the embodiments of the agents are different (morphology, dynamics, degrees of freedom, etc.). Many existing approaches in imitation learning circumvent the correspondence problem, for example, kinesthetic teaching or teleoperation, which are performed on the robot. In this work we explicitly address the correspondence problem by introducing a distance measure between dissimilar embodiments. This measure is then used as a loss function for static pose imitation and as a feedback signal within a model-free deep reinforcement learning framework for dynamic movement imitation between two anthropomorphic robotic arms in simulation. We find that the measure is well suited for describing the similarity between embodiments and for learning imitation policies by distance minimization.",
      "authors": [
        "von Eschenbach, Marcus Ebner",
        "Manela, Binyamin",
        "Peters, Jan",
        "Biess, Armin"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 5 figures, submitted to IEEE Robotics and Automation\n  Letters/IROS 2020",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-02-25",
      "selected": null,
      "title": "Metric-Based Imitation Learning Between Two Dissimilar Anthropomorphic Robotic Arms",
      "urls": [
        "http://arxiv.org/abs/2003.02638v1",
        "http://arxiv.org/pdf/2003.02638v1",
        "http://arxiv.org/pdf/2003.02638.pdf"
      ]
    },
    {
      "abstract": "Humans are spectacular reinforcement learners, constantly learning from and adjusting to experience and feedback. Unfortunately, this doesn't necessarily mean humans are fast learners. When tasks are challenging, learning can become unacceptably slow. Fortunately, humans do not have to learn tabula rasa, and learning speed can be greatly increased with learning aids. In this work we validate a new type of learning aid -- reward shaping for humans via inverse reinforcement learning (IRL). The goal of this aid is to increase the speed with which humans can learn good policies for specific tasks. Furthermore this approach compliments alternative machine learning techniques such as safety features that try to prevent individuals from making poor decisions. To achieve our results we first extend a well known IRL algorithm via kernel methods. Afterwards we conduct two human subjects experiments using an online game where players have limited time to learn a good policy. We show with statistical significance that players who receive our learning aid are able to approach desired policies more quickly than the control group.",
      "authors": [
        "Rucker, Mark A.",
        "Watson, Layne T.",
        "Gerber, Matthew S.",
        "Barnes, Laura E."
      ],
      "categories": null,
      "citations": null,
      "comments": "This paper has been modified considerably for resubmission to Journal\n  of Machine Learning Research, for source code, see\n  https://github.com/mrucker/kpirl-kla",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-02-25",
      "selected": null,
      "title": "Reward Shaping for Human Learning via Inverse Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/2002.10904v3",
        "http://arxiv.org/pdf/2002.10904.pdf",
        "http://arxiv.org/pdf/2002.10904v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huang K."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neucom.2019.07.109",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "269-285",
      "publication": {
        "category": "Journal",
        "cite_score": 10.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09252312",
        "publisher": "Elsevier B.V.",
        "sjr": 1.481,
        "snip": 1.853,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neurocomputing"
      },
      "publication_date": "2020-02-15",
      "selected": null,
      "title": "A self-organizing developmental cognitive architecture with interactive reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074461342&origin=inward"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning provides a way for agents to learn to solve tasks from evaluative feedback provided by a human user. Previous research showed that humans give copious feedback early in training but very sparsely thereafter. In this article, we investigate the potential of agent learning from trainers\u2019 facial expressions via interpreting them as evaluative feedback. To do so, we implemented TAMER which is a popular interactive reinforcement learning method in a reinforcement-learning benchmark problem\u2014Infinite Mario, and conducted the first large-scale study of TAMER involving 561 participants. With designed CNN\u2013RNN model, our analysis shows that telling trainers to use facial expressions and competition can improve the accuracies for estimating positive and negative feedback using facial expressions. In addition, our results with a simulation experiment show that learning solely from predicted feedback based on facial expressions is possible and using strong/effective prediction models or a regression method, facial responses would significantly improve the performance of agents. Furthermore, our experiment supports previous studies demonstrating the importance of bi-directional feedback and competitive elements in the training interface.",
      "authors": [
        "Li, Guangliang",
        "Dibeklio\u011flu, Hamdi",
        "Whiteson, Shimon",
        "Hung, Hayley"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10458-020-09447-w",
      "keywords": [
        "Interactive reinforcement learning",
        "Facial expressions",
        "Reinforcement learning",
        "Human agent interaction"
      ],
      "number_of_pages": 29,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1387-2532",
        "publisher": "Kluwer Academic Publishers",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Autonomous Agents and Multi-Agent Systems"
      },
      "publication_date": "2020-02-12",
      "selected": null,
      "title": "Facial feedback for reinforcement learning: a case study and offline analysis using the TAMER framework",
      "urls": [
        "https://dl.acm.org/doi/10.1007/s10458-020-09447-w",
        "https://link.springer.com/content/pdf/10.1007/s10458-020-09447-w.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079570004&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Stoji\u0107 H."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.1911348117",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "3291-3300",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2020-02-11",
      "selected": null,
      "title": "Uncertainty in learning, choice, and visual fixation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079327354&origin=inward"
      ]
    },
    {
      "abstract": "This paper presents a deep reinforcement learning algorithm for online accompaniment generation, with potential for real-time interactive human-machine duet improvisation. Different from offline music generation and harmonization, online music accompaniment requires the algorithm to respond to human input and generate the machine counterpart in a sequential order. We cast this as a reinforcement learning problem, where the generation agent learns a policy to generate a musical note (action) based on previously generated context (state). The key of this algorithm is the well-functioning reward model. Instead of defining it using music composition rules, we learn this model from monophonic and polyphonic training data. This model considers the compatibility of the machine-generated note with both the machine-generated context and the human-generated context. Experiments show that this algorithm is able to respond to the human part and generate a melodic, harmonic and diverse machine part. Subjective evaluations on preferences show that the proposed algorithm generates music pieces of higher quality than the baseline method.",
      "authors": [
        "Jiang, Nan",
        "Jin, Sheng",
        "Duan, Zhiyao",
        "Zhang, Changshui"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-02-08",
      "selected": null,
      "title": "RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2002.03082.pdf",
        "http://arxiv.org/abs/2002.03082v1",
        "http://arxiv.org/pdf/2002.03082v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Baker T.E."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/ntr/nty136",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "164-171",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14622203",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Nicotine and Tobacco Research"
      },
      "publication_date": "2020-02-06",
      "selected": null,
      "title": "Smoking Decisions: Altered Reinforcement Learning Signals Induced by Nicotine State",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062968420&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lee M.H."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3368555.3384452",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "160-169",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450370462",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM CHIL 2020 - Proceedings of the 2020 ACM Conference on Health, Inference, and Learning"
      },
      "publication_date": "2020-02-04",
      "selected": null,
      "title": "Interactive hybrid approach to combine machine and human intelligence for personalized rehabilitation assessment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082750748&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Cox K.M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/ejn.14577",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "909-921",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0953816X",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.044,
        "snip": 0.891,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "European Journal of Neuroscience"
      },
      "publication_date": "2020-02-01",
      "selected": null,
      "title": "Abstract inference of unchosen option values",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074581034&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Master S.L."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.dcn.2019.100732",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 8.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18789293",
        "publisher": "Elsevier B.V.",
        "sjr": 1.792,
        "snip": 1.42,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Developmental Cognitive Neuroscience"
      },
      "publication_date": "2020-02-01",
      "selected": null,
      "title": "Distentangling the systems contributing to changes in learning during adolescence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075943508&origin=inward"
      ]
    },
    {
      "abstract": "Interactive reinforcement learning provides a way for agents to learn to solve tasks from evaluative feedback provided by a human user. Previous research showed that humans give copious feedback early in training but very sparsely thereafter. In this article, we investigate the potential of agent learning from trainers' facial expressions via interpreting them as evaluative feedback. To do so, we implemented TAMER which is a popular interactive reinforcement learning method in a reinforcement-learning benchmark problem --- Infinite Mario, and conducted the first large-scale study of TAMER involving 561 participants. With designed CNN-RNN model, our analysis shows that telling trainers to use facial expressions and competition can improve the accuracies for estimating positive and negative feedback using facial expressions. In addition, our results with a simulation experiment show that learning solely from predicted feedback based on facial expressions is possible and using strong/effective prediction models or a regression method, facial responses would significantly improve the performance of agents. Furthermore, our experiment supports previous studies demonstrating the importance of bi-directional feedback and competitive elements in the training interface.",
      "authors": [
        "Li, Guangliang",
        "Dibeklio\u011flu, Hamdi",
        "Whiteson, Shimon",
        "Hung, Hayley"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-01-23",
      "selected": null,
      "title": "Facial Feedback for Reinforcement Learning: A Case Study and Offline Analysis Using the TAMER Framework",
      "urls": [
        "http://arxiv.org/abs/2001.08703v1",
        "http://arxiv.org/pdf/2001.08703v1",
        "http://arxiv.org/pdf/2001.08703.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning has been successful in training autonomous agents to accomplish goals in complex environments. Although this has been adapted to multiple settings, including robotics and computer games, human players often find it easier to obtain higher rewards in some environments than reinforcement learning algorithms. This is especially true of high-dimensional state spaces where the reward obtained by the agent is sparse or extremely delayed. In this paper, we seek to effectively integrate feedback signals supplied by a human operator with deep reinforcement learning algorithms in high-dimensional state spaces. We call this FRESH (Feedback-based REward SHaping). During training, a human operator is presented with trajectories from a replay buffer and then provides feedback on states and actions in the trajectory. In order to generalize feedback signals provided by the human operator to previously unseen states and actions at test-time, we use a feedback neural network. We use an ensemble of neural networks with a shared network architecture to represent model uncertainty and the confidence of the neural network in its output. The output of the feedback neural network is converted to a shaping reward that is augmented to the reward provided by the environment. We evaluate our approach on the Bowling and Skiing Atari games in the arcade learning environment. Although human experts have been able to achieve high scores in these environments, state-of-the-art deep learning algorithms perform poorly. We observe that FRESH is able to achieve much higher scores than state-of-the-art deep learning algorithms in both environments. FRESH also achieves a 21.4% higher score than a human expert in Bowling and does as well as a human expert in Skiing.",
      "authors": [
        "Xiao, Baicen",
        "Lu, Qifan",
        "Ramasubramanian, Bhaskar",
        "Clark, Andrew",
        "Bushnell, Linda",
        "Poovendran, Radha"
      ],
      "categories": null,
      "citations": 4,
      "comments": "Accepted as Full Paper to International Conference on Autonomous\n  Agents and Multi-Agent Systems (AAMAS) 2020",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1512-1520",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2020-01-19",
      "selected": null,
      "title": "FRESH: Interactive Reward Shaping in High-Dimensional State Spaces using Human Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096709359&origin=inward",
        "http://arxiv.org/pdf/2001.06781.pdf",
        "http://arxiv.org/pdf/2001.06781v1",
        "http://arxiv.org/abs/2001.06781v1"
      ]
    },
    {
      "abstract": "Many real-world human behaviors can be modeled and characterized as sequential decision-making processes, such as a taxi driver\u00e2\u0080\u0099s choices of working regions and times. Each driver possesses unique preferences on the sequential choices over time and improves the driver\u00e2\u0080\u0099s working efficiency. Understanding the dynamics of such preferences helps accelerate the learning process of taxi drivers. Prior works on taxi operation management mostly focus on finding optimal driving strategies or routes, lacking in-depth analysis on what the drivers learned during the process and how they affect the performance of the driver. In this work, we make the first attempt to establish Dynamic Human Preference Analytics. We inversely learn the taxi drivers\u00e2\u0080\u0099 preferences from data and characterize the dynamics of such preferences over time. We extract two types of features (i.e., profile features and habit features) to model the decision space of drivers. Then through inverse reinforcement learning, we learn the preferences of drivers with respect to these features. The results illustrate that self-improving drivers tend to keep adjusting their preferences to habit features to increase their earning efficiency while keeping the preferences to profile features invariant. However, experienced drivers have stable preferences over time. The exploring drivers tend to randomly adjust the preferences over time.",
      "authors": [
        "Menghai Pan",
        "Weixiao Huang",
        "Yanhua Li",
        "Xun Zhou",
        "Zhenming Liu",
        "Rui Song",
        "Hui Lu",
        "Zhihong Tian",
        "Jun Luo"
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3360312",
      "keywords": [
        "Urban computing",
        "inverse reinforcement learning",
        "preference dynamics"
      ],
      "number_of_pages": 19,
      "pages": "1-19",
      "publication": {
        "category": "Journal",
        "cite_score": 14.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2157-6904",
        "publisher": "Association for Computing Machinery (ACM)",
        "sjr": 2.603,
        "snip": 3.308,
        "subject_areas": [
          "Theoretical Computer Science",
          "Artificial Intelligence"
        ],
        "title": "ACM Transactions on Intelligent Systems and Technology"
      },
      "publication_date": "2020-01-17",
      "selected": null,
      "title": "DHPA: Dynamic Human Preference Analytics Framework: A Case Study on Taxi Drivers\u2019 Learning Curve Analysis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078246990&origin=inward",
        "https://dl.acm.org/doi/10.1145/3360312"
      ]
    },
    {
      "abstract": "With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",
      "authors": [
        "Huang, Wenhui",
        "Braghin, Francesco",
        "Wang, Zhuo"
      ],
      "categories": null,
      "citations": null,
      "comments": "7 pages, 11 figures, conference",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2020-01-12",
      "selected": null,
      "title": "Learning to drive via Apprenticeship Learning and Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/2001.03864v1",
        "http://arxiv.org/abs/2001.03864v1",
        "http://arxiv.org/pdf/2001.03864.pdf"
      ]
    },
    {
      "abstract": "The researchers in this study have developed a novel approach using mutual reinforcement learning (MRL) where both the robot and human act as empathetic individuals who function as reinforcement learning agents for each other to achieve a particular task over continuous communication and feedback. This shared model not only has a collective impact but improves human cognition and helps in building a successful human-robot relationship. In our current work, we compared our learned reinforcement model with a baseline non-reinforcement and random approach in a robotics domain to identify the significance and impact of MRL. MRL contributed to improved skill transfer, and the robot was able successfully to predict which reinforcement behaviors would be most valuable to its human partners.",
      "authors": [
        "Sayanti Roy",
        "Emily Kieson",
        "Charles Abramson",
        "Christopher Crick"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3378680.3378793",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "572-573",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781538685556",
        "issn": null,
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2020-01-10",
      "selected": null,
      "title": "Mutual reinforcement learning with robot trainers",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3378680.3378793"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) has achieved tremendous success as a general\nframework for learning how to make decisions. However, this success relies on\nthe interactive hand-tuning of a reward function by RL experts. On the other\nhand, inverse reinforcement learning (IRL) seeks to learn a reward function\nfrom readily-obtained human demonstrations. Yet, IRL suffers from two major\nlimitations: 1) reward ambiguity - there are an infinite number of possible\nreward functions that could explain an expert's demonstration and 2)\nheterogeneity - human experts adopt varying strategies and preferences, which\nmakes learning from multiple demonstrators difficult due to the common\nassumption that demonstrators seeks to maximize the same reward. In this work,\nwe propose a method to jointly infer a task goal and humans' strategic\npreferences via network distillation. This approach enables us to distill a\nrobust task reward (addressing reward ambiguity) and to model each strategy's\nobjective (handling heterogeneity). We demonstrate our algorithm can better\nrecover task reward and strategy rewards and imitate the strategies in two\nsimulated tasks and a real-world table tennis task.",
      "authors": [
        "Letian Chen",
        "Rohan Paleja",
        "Muyleng Ghuy",
        "Matthew Gombolay"
      ],
      "categories": null,
      "citations": 17,
      "comments": "In Proceedings of the 2020 ACM/IEEE In-ternational Conference on\n  Human-Robot Interaction (HRI '20), March 23 to 26, 2020, Cambridge, United\n  Kingdom.ACM, New York, NY, USA, 10 pages",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.1145/3319502.3374791",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "659-668",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450367462",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2020-01-02",
      "selected": null,
      "title": "Joint Goal and Strategy Inference across Heterogeneous Demonstrators via Reward Network Distillation",
      "urls": [
        "http://arxiv.org/abs/2001.00503v3",
        "http://dx.doi.org/10.1145/3319502.3374791",
        "http://arxiv.org/pdf/2001.00503v3",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082017422&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Starita F."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/scan/nsz089",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "1119-1129",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17495016",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Social Cognitive and Affective Neuroscience"
      },
      "publication_date": "2020-01-02",
      "selected": null,
      "title": "Aberrant reward prediction error during Pavlovian appetitive learning in alexithymia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078576286&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Farmer H."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/scan/nsz076",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1061-1072",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17495016",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Social Cognitive and Affective Neuroscience"
      },
      "publication_date": "2020-01-02",
      "selected": null,
      "title": "The neural basis of shared preference learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078573952&origin=inward"
      ]
    },
    {
      "abstract": "As a branch of reinforcement learning, interactive reinforcement learning mainly studies the interaction process between humans and agents, allowing agents to learn from the intentions of human users and adapt to their preferences. In most of the current studies, human users need to intentionally provide explicit feedback via pressing keyboard buttons or mouse clicks. However, in our paper, we proposed an interactive reinforcement learning method that facilitates an agent to learn from human social signals \u2014 facial feedback via a ordinary camera and gestural feedback via a leap motion sensor. Our method provides a natural way for ordinary people to train agents how to perform a task according to their preferences. We tested our method in two reinforcement learning benchmarking domains \u2014 LoopMaze and Tetris, and compared to the state of the art \u2014 the TAMER framework. Our experimental results show that when learning from facial feedback the recognition of which is very low, the TAMER agent can get a similar performance to that of learning from keypress feedback with slightly more feedback. When learning from gestural feedback with a more accurate recognition, the TAMER agent can obtain a similar performance to that of learning from keypress feedback with much less feedback received. Moreover, our results indicate that the recognition error of facial feedback has a large effect on the agent performance in the beginning training process than in the later training stage. Finally, our results indicate that with enough recognition accuracy, human social signals can effectively improve the learning efficiency of agents with less human feedback.",
      "authors": [
        "Jinying Lin",
        "Qilei Zhang",
        "Randy Gomez",
        "Keisuke Nakamura",
        "Bo He",
        "Guangliang Li"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN47096.2020.9223516",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "706-712",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Human Social Feedback for Efficient Interactive Reinforcement Agent Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9223516",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095791094&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning agent learns how to perform a task by interacting with the environment. The use of reinforcement learning in real-life applications has been limited because of the sample efficiency problem. Interactive reinforcement learning has been developed to speed up the agent's learning and facilitate to learn from ordinary people by allowing them to provide social feedback, e.g, evaluative feedback, advice or instruction. Inspired by real-life biological learning scenarios, there could be many ways to provide feedback for agent learning, such as via hardware delivered, natural interaction like facial expressions, speech or gestures. The agent can even learn from feedback via unimodal or multimodal sensory input. This paper reviews methods for interactive reinforcement learning agent to learn from human social feedback and the ways of delivering feedback. Finally, we discuss some open problems and possible future research directions.",
      "authors": [
        "Jinying Lin",
        "Zhen Ma",
        "Randy Gomez",
        "Keisuke Nakamura",
        "Bo He",
        "Guangliang Li"
      ],
      "categories": null,
      "citations": 36,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2020.3006254",
      "keywords": [
        "Human agent/robot interaction",
        "social interaction",
        "interactive reinforcement learning",
        "interactive shaping"
      ],
      "number_of_pages": 9,
      "pages": "120757-120765",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "A Review on Interactive Reinforcement Learning From Human Social Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088301053&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9131765"
      ]
    },
    {
      "abstract": "One main challenge to robot decision making in human-robot teams involves predicting the intents of a human team member through observations of the human\u2019s behavior. Inverse Reinforcement Learning (IRL) is one approach to predicting human intent, however, such approaches typically assume that the human\u2019s intent is stationary. Furthermore, there are few approaches that identify when the human\u2019s intent changes during observations. Modeling human decision making as a Markov decision process, we address these two limitations by maintaining a belief over the reward parameters of the model (representing the human\u2019s preference for tasks or goals), and updating the parameters using IRL estimates from short windows of observations. We posit that a human\u2019s preferences can change with time, due to gradual drift of preference and/or discrete, step-wise changes of intent. Our approach maintains an estimate of the human\u2019s preferences under such conditions, and is able to identify changes of intent based on the divergence between subsequent belief updates. We demonstrate that our approach can effectively track dynamic reward parameters and identify changes of intent in a simulated environment, and that this approach can be leveraged by a robot team member to improve team performance.",
      "authors": [
        "Dana Hughes",
        "Akshat Agarwal",
        "Yue Guo",
        "Katia Sycara"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN47096.2020.9223520",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1178-1185",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Inferring Non-Stationary Human Preferences for Human-Agent Teams",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9223520",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095723870&origin=inward"
      ]
    },
    {
      "abstract": "Flexible adaptation of learning strategy depending on online changes of the user's current intents have a high relevance in human-robot collaboration. In our previous study, we proposed an intrinsic interactive reinforcement learning approach for human-robot interaction, in which a robot learns his/her action strategy based on intrinsic human feedback that is generated in the human's brain as neural signature of the human's implicit evaluation of the robot's actions. Our approach has an inherent property that allows robots to adapt their behavior depending on online changes of the human's current intents. Such flexible adaptation is possible, since robot learning is updated in real time by human's online feedback. In this paper, the adaptivity of robot learning is tested on eight subjects who change their current control strategy by adding a new gesture to the previous used gestures. This paper evaluates the learning progress by analyzing learning phases (before and after adding a new gesture for control). The results show that the robot can adapt the previously learned policy depending on online changes of the user's intents. Especially, learning progress is interrelated with the classification performance of electroencephalograms (EEGs), which are used to measure the human's implicit evaluation of the robot's actions.",
      "authors": [
        "Su Kyoung Kim",
        "Elsa Andrea Kirchner",
        "Frank Kirchner"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRA40945.2020.9197538",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "4885-4891",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7396-2",
        "issn": "1050-4729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Flexible online adaptation of learning strategy using EEG-based reinforcement signals in real-world robotic applications",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9197538",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092715733&origin=inward"
      ]
    },
    {
      "abstract": "An important current challenge in Human-Robot Interaction (HRI) is to enable robots to learn on-the-fly from human feedback. However, humans show a great variability in the way they reward robots. We propose to address this issue by enabling the robot to combine different learning strategies, namely model-based (MB) and model-free (MF) reinforcement learning. We simulate two HRI scenarios: a simple task where the human congratulates the robot for putting the right cubes in the right boxes, and a more complicated version of this task where cubes have to be placed in a specific order. We show that our existing MB-MF coordination algorithm previously tested in robot navigation works well here without retuning parameters. It leads to the maximal performance while producing the same minimal computational cost as MF alone. Moreover, the algorithm gives a robust performance no matter the variability of the simulated human feedback, while each strategy alone is impacted by this variability. Overall, the results suggest a promising way to promote robot learning flexibility when facing variable human feedback.",
      "authors": [
        "R\u00e9mi Dromnelle",
        "Beno\u00eet Girard",
        "Erwan Renaudo",
        "Raja Chatila",
        "Mehdi Khamassi"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN47096.2020.9223451",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "612-617",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Coping with the variability in humans reward during simulated human-robot interactions through the coordination of multiple learning strategies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095773376&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9223451"
      ]
    },
    {
      "abstract": "The recent research trends for achieving ultra-reliable and low-latency communication networks are largely driven by smart manufacturing and industrial Internet-of-Things applications. Such applications are being realized through Tactile Internet that allows users to control remote things and involve the bidirectional transmission of video, audio, and haptic data. However, the end-to-end propagation latency presents a stubborn bottleneck, which can be alleviated by using various artificial intelligence-based application layer and network layer prediction algorithms, e.g., forecasting and preempting haptic feedback transmission. In this paper, we study the experimental data on traffic characteristics of control signals and haptic feedback samples obtained through virtual reality-based human-to-machine teleoperation. Moreover, we propose the installation of edge-intelligence servers between master and slave devices to implement the preemption of haptic feedback from control signals. Harnessing virtual reality-based teleoperation experiments, we further propose a two-stage artificial intelligence-based module for forecasting haptic feedback samples. The first-stage unit is a supervised binary classifier that detects if haptic sample forecasting is necessary and the second-stage unit is a reinforcement learning unit that ensures haptic feedback samples are forecasted accurately when different types of material are present. Furthermore, by evaluating analytical expressions, we show the feasibility of deploying remote human-to-machine teleoperation over fiber backhaul by using our proposed artificial intelligence-based module, even under heavy traffic intensity.",
      "authors": [
        "Sourav Mondal",
        "Lihua Ruan",
        "Martin Maier",
        "David Larrabeiti",
        "Goutam Das",
        "Elaine Wong"
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/OJCOMS.2020.3009023",
      "keywords": [
        "supervised learning",
        "Human-to-machine applications",
        "reinforcement learning",
        "ultra-low latency communication"
      ],
      "number_of_pages": 11,
      "pages": "889-899",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2644-125X",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Open Journal of the Communications Society"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Enabling Remote Human-to-Machine Applications With AI-Enhanced Servers Over Access Networks",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9139304"
      ]
    },
    {
      "abstract": "In reinforcement learning (RL), sparse rewards are a natural way to specify the task to be learned. However, most RL algorithms struggle to learn in this setting since the learning signal is mostly zeros. In contrast, humans are good at assessing and predicting the future consequences of actions and can serve as good reward/policy shapers to accelerate the robot learning process. Previous works have shown that the human brain generates an error-related signal, measurable using electroencephelography (EEG), when the human perceives the task being done erroneously. In this work, we propose a method that uses evaluative feedback obtained from human brain signals measured via scalp EEG to accelerate RL for robotic agents in sparse reward settings. As the robot learns the task, the EEG of a human observer watching the robot attempts is recorded and decoded into noisy error feedback signal. From this feedback, we use supervised learning to obtain a policy that subsequently augments the behavior policy and guides exploration in the early stages of RL. This bootstraps the RL learning process to enable learning from sparse reward. Using a simple robotic navigation task as a test bed, we show that our method achieves a stable obstacle-avoidance policy with high success rate, outperforming learning from sparse rewards only that struggles to achieve obstacle avoidance behavior or fails to advance to the goal.",
      "authors": [
        "Iretiayo Akinola",
        "Zizhao Wang",
        "Junyao Shi",
        "Xiaomin He",
        "Pawan Lapborisuth",
        "Jingxi Xu",
        "David Watkins-Valls",
        "Paul Sajda",
        "Peter Allen"
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRA40945.2020.9196566",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "3799-3805",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7396-2",
        "issn": "1050-4729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Accelerated Robot Learning via Human Brain Signals",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9196566",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092724869&origin=inward"
      ]
    },
    {
      "abstract": "An important aspect of Human-Robot-cooperation is that the robot is capable of clearly communicating its intentions to its human collaborator. This communication of intentions often requires the generation of legible motion trajectories. The concept of legible motion is usually not studied together with machine learning. Studying these fields together is an important step towards better Human-Robot cooperation. In this paper, we investigate interactive robot learning approaches with the aim of developing models that are able to generate legible motions by taking observer feedback into account. We explore how to integrate the observer feedback into a Reinforcement Learning (RL) framework. We do this by proposing three different observer algorithms as observer strategies in an interactive RL scheme and compare with one non-interactive RL algorithm as baseline. For the observer strategies we vary the method how the observer estimates how likely the agent is going for the target goal. We evaluate our approach on five environments and calculate the legibility of the learned trajectories. The results show that the legibility of the learned trajectories is significantly higher while integrating the feedback from the observer compared with a standard Q-Learning algorithm not using the observer feedback.",
      "authors": [
        "Manuel Bied",
        "Mohamed Chetouani"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN47096.2020.9223338",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "760-767",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Integrating an Observer in Interactive Reinforcement Learning to Learn Legible Trajectories",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9223338",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095764483&origin=inward"
      ]
    },
    {
      "abstract": "As a massive number of the Internet of Things (IoT) devices are deployed, the security and privacy issues in IoT arouse more and more attention. The IoT attacks are causing tremendous loss to the IoT networks and even threatening human safety. Compared to traditional networks, IoT networks have unique characteristics, which make the attack detection more challenging. First, the heterogeneity of platforms, protocols, software, and hardware exposes various vulnerabilities. Second, in addition to the traditional high-rate attacks, the low-rate attacks are also extensively used by IoT attackers to obfuscate the legitimate and malicious traffic. These low-rate attacks are challenging to detect and can persist in the networks. Last, the attackers are evolving to be more intelligent and can dynamically change their attack strategies based on the environment feedback to avoid being detected, making it more challenging for the defender to discover a consistent pattern to identify the attack. In order to adapt to the new characteristics in IoT attacks, we propose a reinforcement learning-based attack detection model that can automatically learn and recognize the transformation of the attack pattern. Therefore, we can continuously detect IoT attacks with less human intervention. In this paper, we explore the crucial features of IoT traffics and utilize the entropy-based metrics to detect both the high-rate and low-rate IoT attacks. Afterward, we leverage the reinforcement learning technique to continuously adjust the attack detection threshold based on the detection feedback, which optimizes the detection and the false alarm rate. We conduct extensive experiments over a real IoT attack data set and demonstrate the effectiveness of our IoT attack detection framework.",
      "authors": [
        "Tianbo Gu",
        "Allaukik Abhishek",
        "Hao Fu",
        "Huanle Zhang",
        "Debraj Basu",
        "Prasant Mohapatra"
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/WoWMoM49955.2020.00029",
      "keywords": [
        "Artificial Intelligence",
        "IoT security",
        "Anomaly detection",
        "Intrusion detection",
        "Internet of Things",
        "Reinforcement learning",
        "Entropy",
        "Wireless traffic"
      ],
      "number_of_pages": 10,
      "pages": "88-97",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7375-7",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 21st IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks, WoWMoM 2020"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Towards Learning-automation IoT Attack Detection through Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096557094&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9217785"
      ]
    },
    {
      "abstract": "As energy demand continues to increase, demand response (DR) programs in the electricity distribution grid are gaining momentum and their adoption is set to grow gradually over the years ahead. Demand response schemes seek to incentivise consumers to use green energy and reduce their electricity usage during peak periods which helps support grid balancing of supply-demand and generate revenue by selling surplus of energy back to the grid. This paper proposes an effective energy management system for residential demand response using Reinforcement Learning (RL) and Fuzzy Reasoning (FR). RL is considered as a model-free control strategy which learns from the interaction with its environment by performing actions and evaluating the results. The proposed algorithm considers human preference by directly integrating user feedback into its control logic using fuzzy reasoning as reward functions. Q-learning, a RL strategy based on a reward mechanism, is used to make optimal decisions to schedule the operation of smart home appliances by shifting controllable appliances from peak periods, when electricity prices are high, to off-peak hours, when electricity prices are lower without affecting the customer's preferences. The proposed approach works with a single agent to control 14 household appliances and uses a reduced number of state-action pairs and fuzzy logic for rewards functions to evaluate an action taken for a certain state. The simulation results show that the proposed appliances scheduling approach can smooth the power consumption profile and minimise the electricity cost while considering user's preferences, user's feedbacks on each action taken and his/her preference settings. A user-interface is developed in MATLAB/Simulink for the Home Energy Management System (HEMS) to demonstrate the proposed DR scheme. The simulation tool includes features such as smart appliances, electricity pricing signals, smart meters, solar photovoltaic generation, battery energy storage, electric vehicle and grid supply.",
      "authors": [
        "Fayiz Alfaverh",
        "M. Dena\u00ef",
        "Yichuang Sun"
      ],
      "categories": null,
      "citations": 86,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2020.2974286",
      "keywords": [
        "Q-learning",
        "home energy management system",
        "reinforcement learning",
        "smart appliances",
        "fuzzy reasoning",
        "smart home",
        "Demand response"
      ],
      "number_of_pages": 12,
      "pages": "39310-39321",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Demand Response Strategy Based on Reinforcement Learning and Fuzzy Reasoning for Home Energy Management",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9000577",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081669247&origin=inward"
      ]
    },
    {
      "abstract": "Interactive Reinforcement Learning (RL) enables agents to learn from two sources: rewards taken from observations of the environment, and feedback or advice from a secondary critic source, such as human teachers or sensor feedback. The addition of information from a critic during the learning process allows the agents to learn more quickly than non-interactive RL. There are many methods that allow policy feedback or advice to be combined with RL. However, critics can often give imperfect information. In this work, we introduce a framework for characterizing Interactive RL methods with imperfect teachers and propose an algorithm, Revision Estimation from Partially Incorrect Resources (REPaIR), which can estimate corrections to imperfect feedback over time. We run experiments both in simulations and demonstrate performance on a physical robot, and find that when baseline algorithms do not have prior information on the exact quality of a feedback source, using REPaIR matches or improves the expected performance of these algorithms.",
      "authors": [
        "Taylor A. Kessler Faulkner",
        "Elaine Schaertl Short",
        "Andrea L. Thomaz"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRA40945.2020.9197219",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "7498-7504",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7396-2",
        "issn": "1050-4729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Interactive Reinforcement Learning with Inaccurate Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9197219",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092743370&origin=inward"
      ]
    },
    {
      "abstract": "Integrating human-provided location priors into video object segmentation has been shown to be an effective strategy to enhance performance, but their application at large scale is unfeasible. Gamification can help reduce the annotation burden, but it still requires user involvement. We propose a video object segmentation framework that leverages the combined advantages of user feedback for segmentation and gamification strategy by simulating multiple game players through a reinforcement learning (RL) model that reproduces human ability to pinpoint moving objects and using the simulated feedback to drive the decisions of a fully convolutional deep segmentation network. Experimental results on the DAVIS-17 benchmark show that: 1) including user-provided prior, even if not precise, yields high performance; 2) our RL agent replicates satisfactorily the same variability of humans in identifying spatiotemporal salient objects; and 3) employing artificially generated priors in an unsupervised video object segmentation model reaches state-of-the-art performance.",
      "authors": [
        "Giuseppe Vecchio",
        "Simone Palazzo",
        "Daniela Giordano",
        "Francesco Rundo",
        "Concetto Spampinato"
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2019.2963282",
      "keywords": [
        "machine intelligence",
        "learning systems",
        "Computer vision"
      ],
      "number_of_pages": 13,
      "pages": "5103-5115",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162-2388",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "MASK-RL: Multiagent Video Object Segmentation Framework Through Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079452882&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8967004"
      ]
    },
    {
      "abstract": "Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.",
      "authors": [
        "Jinyoung Choi",
        "Christopher Dance",
        "Jung-eun Kim",
        "Kyung-sik Park",
        "Jaehun Han",
        "Joonho Seo",
        "Minsu Kim"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRA40945.2020.9197159",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "3363-3370",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7396-2",
        "issn": "1050-4729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092712969&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9197159"
      ]
    },
    {
      "abstract": "Recognizing the destination of a maneuvering agent is important to create intelligent AI players in Real Time Strategy (RTS) games. Among different ways of problem formulation, goal recognition can be solved as a model-based planning problem using off-the-shelf planners. However, the common problem in these frameworks is that they usually lack of the modeling of the action duration as in real-world scenarios the agent may take several steps to transit between grids. To solve this problem, a semi-Markov decision model (SMDM), which explicitly models the duration of an action, is proposed in this paper. Besides, most of the current works do not establish a behavioral model of the identified person, and there is almost no work modeling individual behavioral preference, which limits the accuracy of the recognition results. In this paper, the Inverse Reinforcement Learning (IRL) method is adopted in opponent behavior learning for the destination recognition problem. To adapt to the dynamic environment, the Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) method is transformed by defining a Fitness index to measure the effect of weight and use the Nelder-Mead polyhedron search to find the optimal weight. In experiments, we build the game scenario in the Unreal Engine 4 environment and collect the moving trajectories from the human players in several different tasks for evaluating the performance of our methods. The results show that the recognizer using IRL can recognize the destination effectively even if the intention changes during the midway, and it performs better than other models in terms of several most frequently-used metrics.",
      "authors": [
        "Yunxiu Zeng",
        "Kai Xu",
        "Long Qin",
        "Quanjun Yin"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/ACCESS.2020.2967642",
      "keywords": [
        "inverse reinforcement learning",
        "goal recognition",
        "Real time strategy games"
      ],
      "number_of_pages": 18,
      "pages": "15392-15409",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "A Semi-Markov Decision Model With Inverse Reinforcement Learning for Recognizing the Destination of a Maneuvering Agent in Real Time Strategy Games",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8962086"
      ]
    },
    {
      "abstract": "Telerobotic systems must adapt to new environmental conditions and deal with high uncertainty caused by long-time delays. As one of the best alternatives to human-level intelligence, Reinforcement Learning (RL) may offer a solution to cope with these issues. This paper proposes to integrate RL with the Model Mediated Teleoperation (MMT) concept. The teleoperator interacts with a simulated virtual environment, which provides instant feedback. Whereas feedback from the real environment is delayed, feedback from the model is instantaneous, leading to high transparency. The MMT is realized in combination with an intelligent system with two layers. The first layer utilizes Dynamic Movement Primitives (DMP) which accounts for certain changes in the avatar environment. And, the second layer addresses the problems caused by uncertainty in the model using RL methods. Augmented reality was also provided to fuse the avatar device and virtual environment models for the teleoperator. Implemented on DLR\u2019s Exodex Adam hand-arm haptic exoskeleton, the results show RL methods are able to find different solutions when changes are applied to the object position after the demonstration. The results also show DMPs to be effective at adapting to new conditions where there is no uncertainty involved.",
      "authors": [
        "Hadi Beik-Mohammadi",
        "Matthias Kerzel",
        "Benedikt Pleintinger",
        "Thomas Hulin",
        "Philipp Reisich",
        "Annika Schmidt",
        "Aaron Pereira",
        "Stefan Wermter",
        "Neal Y. Lii"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN47096.2020.9223477",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "713-720",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time Delays Using Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9223477",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095730060&origin=inward"
      ]
    },
    {
      "abstract": "Despite growing progresses in recent years, cross-scenario person re-identification remains challenging, mainly due to the pedestrians commonly surrounded by highly-complex environment contexts. In reality, the human perception mechanism could adaptively find proper contextualized spatial-temporal clues towards pedestrian recognition. However, conventional methods fall short in adaptively leveraging the long-term spatial-temporal information due to ever-increasing computational cost. Moreover, CNN-based deep learning methods are hard to conduct optimization due to the non-differentiable property of the built-in context search operation. To ameliorate, this paper proposes a novel Context-Interactive CNN (CI-CNN) to dynamically find both spatial and temporal contexts by embedding multi-task Reinforcement Learning (MTRL). The CI-CNN streamlines the multi-task reinforcement learning by using an actor-critic agent to capture the temporal-spatial context simultaneously, which comprises a context-policy network and a context-critic network. The former network learns policies to determine the optimal spatial context region and temporal sequence range. Based on the inferred temporal-spatial cues, the latter one focuses on the identification task and provides feedback for the policy network. Thus, CI-CNN can simultaneously zoom in/out the perception field in spatial and temporal domain for the context interaction with the environment. By fostering the collaborative interaction between the person and context, our method could achieve outstanding performance on various public benchmarks, which confirms the rationality of our hypothesis, and verifies the effectiveness of our CI-CNN framework.",
      "authors": [
        "Wenfeng Song",
        "Shuai Li",
        "Tao Chang",
        "Aimin Hao",
        "Qinping Zhao",
        "Hong Qin"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TIP.2019.2953587",
      "keywords": [
        "Person re-identification",
        "actor-critic agent",
        "context-critic network",
        "multi-task reinforcement learning",
        "context interaction"
      ],
      "number_of_pages": 15,
      "pages": "2860-2874",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1941-0042",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Image Processing"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Context-Interactive CNN for Person Re-Identification",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8907836",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078539716&origin=inward"
      ]
    },
    {
      "abstract": "We investigated the application of haptic feedback control and deep reinforcement learning (DRL) to robot-assisted dressing. Our method uses DRL to simultaneously train human and robot control policies as separate neural networks using physics simulations. In addition, we modeled variations in human impairments relevant to dressing, including unilateral muscle weakness, involuntary arm motion, and limited range of motion. Our approach resulted in control policies that successfully collaborate in a variety of simulated dressing tasks involving a hospital gown and a T-shirt. In addition, our approach resulted in policies trained in simulation that enabled a real PR2 robot to dress the arm of a humanoid robot with a hospital gown. We found that training policies for specific impairments dramatically improved performance; that controller execution speed could be scaled after training to reduce the robot's speed without steep reductions in performance; that curriculum learning could be used to lower applied forces; and that multi-modal sensing, including a simulated capacitive sensor, improved performance.",
      "authors": [
        "Alexander Clegg",
        "Zackory Erickson",
        "Patrick Grady",
        "Greg Turk",
        "Charles C. Kemp",
        "C. Karen Liu"
      ],
      "categories": null,
      "citations": 25,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/LRA.2020.2972852",
      "keywords": [
        "Simulation and animation",
        "physically assistive devices",
        "deep learning in robotics and automation"
      ],
      "number_of_pages": 8,
      "pages": "2746-2753",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2377-3774",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Learning to Collaborate From Simulation for Robot-Assisted Dressing",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8988245",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080937370&origin=inward"
      ]
    },
    {
      "abstract": "In this article, we present recEnergy, a recommender system for reducing energy consumption in commercial buildings with human-in-the-loop. We formulate the building energy optimization problem as a Markov decision process, show how deep reinforcement learning can be used to learn energy-saving recommendations, and effectively engage occupants in energy-saving actions. recEnergy is a recommender system that learns actions with high-energy-saving potential, actively distributes recommendations to occupants in a commercial building, and utilizes feedback from the occupants to learn better energy-saving recommendations. Over a four-week user study, four different types of energy-saving recommendations were trained and learned. recEnergy improves building energy reduction from a baseline saving (passive-only strategy) of 19%-26%.",
      "authors": [
        "Peter Wei",
        "Stephen Xia",
        "Runfeng Chen",
        "Jingyi Qian",
        "Chong Li",
        "Xiaofan Jiang"
      ],
      "categories": null,
      "citations": 46,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/JIOT.2020.2974848",
      "keywords": [
        "recommender system",
        "energy savings",
        "deep reinforcement learning",
        "Building energy optimization"
      ],
      "number_of_pages": 12,
      "pages": "6402-6413",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2372-2541",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Internet of Things Journal"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "A Deep-Reinforcement-Learning-Based Recommender System for Occupant-Driven Energy Optimization in Commercial Buildings",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089304878&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9001078"
      ]
    },
    {
      "abstract": "In robotic grasping, objects are often occluded in ungraspable configurations such that no feasible grasp pose can be found, e.g. large flat boxes on the table that can only be grasped once lifted. Inspired by human bimanual manipulation, e.g. one hand to lift up things and the other to grasp, we address this type of problems by introducing pregrasp manipulation \u2013 push and lift actions. We propose a model-free Deep Reinforcement Learning framework to train feedback control policies that utilize visual information and proprioceptive states of the robot to autonomously discover robust pregrasp manipulation. The robot arm learns to push the object first towards a support surface and then lift up one side of the object, creating an object-table clearance for possible grasping solutions. Furthermore, we show the robustness of the proposed learning framework in training pregrasp policies that can be directly transferred to a real robot. Lastly, we evaluate the effectiveness and generalization ability of the learned policy in real-world experiments, and demonstrate pregrasp manipulation of objects with various sizes, shapes, weights, and surface friction.",
      "authors": [
        "Zhaole Sun",
        "Kai Yuan",
        "Wenbin Hu",
        "Chuanyu Yang",
        "Zhibin Li"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICRA40945.2020.9196982",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "9917-9923",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7396-2",
        "issn": "1050-4729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Learning Pregrasp Manipulation of Objects from Ungraspable Poses",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9196982",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85092726083&origin=inward"
      ]
    },
    {
      "abstract": "Bots in video games has been gaining the interest of industry as well as academia as a problem that has been enabled by the recent advances in deep learning and reinforcement learning. In turn several studies have attempted to establish bot detectors in various video games. In this article, we introduce a bot detection model that can implemented in real-time and provide feedback on whether a player that is being observed is a bot or human. The model uses a limited feature set and amount of time of observation in order to be small and generalize easily to other domains. We trained and tested our model in a series of replays for Starcraft: Brood War and have yielded a higher accuracy than past studies and a fraction of detection time.",
      "authors": [
        "Michail Tsikerdekis",
        "Sean Barret",
        "Raleigh Hansen",
        "Matthew Klein",
        "Josh Orritt",
        "Jason Whitmore"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2020.3033725",
      "keywords": [
        "deep learning",
        "bot",
        "detection",
        "Video games"
      ],
      "number_of_pages": 9,
      "pages": "195763-195771",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Efficient Deep Learning Bot Detection in Games Using Time Windows and Long Short-Term Memory (LSTM)",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9239256",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101524349&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "21st International Conference on Artificial Intelligence in Education, AIED 2020",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089503148&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rosenbloom P.S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 6,
      "pages": "198-203",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780998508238",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of ICCM 2019 - 17th International Conference on Cognitive Modeling"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "An architectural integration of temporal motivation theory for decision making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085490049&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358350",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAAI 2020 - 34th AAAI Conference on Artificial Intelligence"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "AAAI 2020 - 34th AAAI Conference on Artificial Intelligence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099926607&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Seo P.H."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "2693-2700",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358350",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAAI 2020 - 34th AAAI Conference on Artificial Intelligence"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Reinforcing an image caption generator using off-line human feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85106425659&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sanfilippo F."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "881-890",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780998133133",
        "issn": "15301605",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Hawaii International Conference on System Sciences"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "A comparison between a two-feedback control loop and a reinforcement learning algorithm for compliant low-cost series elastic actuators",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076784551&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789898704207",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 14th IADIS International Conference Interfaces and Human Computer Interaction 2020, IHCI 2020 and Proceedings of the 13th IADIS International Conference Game and Entertainment Technologies 2020, GET 2020 - Part of the 14th Multi Conference on Computer Science and Information Systems, MCCSIS 2020"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Proceedings of the 14th IADIS International Conference Interfaces and Human Computer Interaction 2020, IHCI 2020 and Proceedings of the 13th IADIS International Conference Game and Entertainment Technologies 2020, GET 2020 - Part of the 14th Multi Conference on Computer Science and Information Systems, MCCSIS 2020",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85101122645&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zeng Y."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/aCCESS.2020.2967642",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "15392-15409",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "a Semi-Markov Decision Model with Inverse Reinforcement Learning for Recognizing the Destination of a Maneuvering agent in Real Time Strategy Games",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079766287&origin=inward"
      ]
    },
    {
      "abstract": "\nDeep Reinforcement Learning (DRL) has become a powerful strategy to solve complex decision making problems based on Deep Neural Networks (DNNs). However, it is highly data demanding, so unfeasible in physical systems for most applications. In this work, we approach...",
      "authors": [
        "P\u00e9rez-Dattari, Rodrigo",
        "Celemin, Carlos",
        "Ruiz-del-Solar, Javier",
        "Kober, Jens"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-33950-0_31",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "353-363",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 1.8,
        "is_potentially_predatory": false,
        "isbn": "9783031210891",
        "issn": "25111256",
        "publisher": "Springer, Cham",
        "sjr": 1.679,
        "snip": 2.74,
        "subject_areas": [
          "Mechanical Engineering",
          "Artificial Intelligence",
          "Computer Science Applications",
          "Engineering (miscellaneous)",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering",
          "Applied Mathematics"
        ],
        "title": "International Symposium on Experimental Robotics"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Interactive Learning with Corrective Feedback for Policies Based on Deep Neural Networks",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-030-33950-0_31.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85107075873&origin=inward"
      ]
    },
    {
      "abstract": "Feedback processing contributes to efficient learning, decision making, and social interaction. Studies using event-related brain potentials (ERPs) reveal that feedback processing is associated with transient ERP components over the medial frontal and posterior regions of the scalp that distinguish between positive and negative feedback. There is some evidence indicating that aging has differential effects on the ERP correlates of feedback processing in a gambling task, and the current study was designed to extend these findings to a reinforcement learning paradigm. Younger and older adults performed the probabilistic selection task while ERPs elicited by feedback cues indicating a correct or incorrect choice were recorded during the learning phase. The ERPs revealed that the amplitude of the feedback negativity and frontal P3 were attenuated in older adults relative to younger adults. The amplitude of a temporal positivity was also attenuated in older adults; in contrast, the amplitude of an occipital negativity was insensitive to the effects of aging. These findings indicate that aging may be associated with the disruption of both local activity and long-range connectivity between neural structures related to feedback processing.",
      "authors": [
        "West, Robert",
        "Huet, AnnMarie"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/brainsci10010040",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 3.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2076-3425",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.752,
        "snip": 0.938,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Brain Sciences"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "The Effect of Aging on the ERP Correlates of Feedback Processing in the Probabilistic Selection Task",
      "urls": [
        "https://www.mdpi.com/2076-3425/10/1/40/pdf?version=1579666144",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078254037&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dyson B.J."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2019.107778",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Behavioural and neural limits in competitive decision making: The roles of outcome, opponency and observation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074655941&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ramesh D."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1090-1097",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Yesterday's reward is today's punishment: Contrast effects in human feedback to reinforcement learning agents",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096644624&origin=inward"
      ]
    },
    {
      "abstract": "Anticipating human behavior requires a model of the rationale how humans acquire knowledge while solving a problem. The rational aspects of decision making needs to be taken into consideration for improving computational models that currently fail to fully explain...",
      "authors": [
        "Lommerzheim, Marcel",
        "Prezenski, Sabine",
        "Russwinkel, Nele",
        "Brechmann, Andr\u00e9"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-39512-4_25",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "159-164",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789811599521",
        "issn": "21945357",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent Human Systems Integration"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Category Learning as a Use Case for Anticipating Individual Human Decision Making by Intelligent Systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081895713&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-030-39512-4_25.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wirth C."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/1741-2552/ab53fe",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17412560",
        "publisher": "IOP Publishing Ltd.",
        "sjr": 1.135,
        "snip": 1.25,
        "subject_areas": [
          "Biomedical Engineering",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Journal of Neural Engineering"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Towards error categorisation in BCI: Single-trial EEG classification between different errors",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076446524&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ghosh L."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-981-15-3290-0_21",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "273-285",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789811599521",
        "issn": "21945357",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent Human Systems Integration"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "EEG-Induced Error Correction in Path Planning by a Mobile Robot Using Learning Automata",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084660383&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang H."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 11,
      "pages": "6355-6365",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781959429869",
        "issn": "0736587X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Learning efficient dialogue policy from demonstrations through shaping",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85099899242&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Palidis D.J."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/JN.00370.2020",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "610-622",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Control of movement EEG correlates of physical effort and reward processing during reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089611844&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lohse K.R."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2019.107775",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Dissociating the contributions of reward-prediction errors to trial-level adaptation and long-term learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072823775&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 1.0,
        "is_potentially_predatory": false,
        "isbn": "9789819927883",
        "issn": "18650929",
        "publisher": "Springer Science and Business Media Deutschland GmbH",
        "sjr": 0.194,
        "snip": 0.241,
        "subject_areas": [
          "Computer Science (all)",
          "Mathematics (all)"
        ],
        "title": "International Conference on Neural Information Processing"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "22nd International Conference on Human-Computer Interaction, HCII 2020",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089245699&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Weber K."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1476-1484",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Predicting persuasive effectiveness for multimodal behavior adaptation using bipolar weighted argument graphs",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096567609&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780998508245",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of ICCM 2020 - 18th International Conference on Cognitive Modelling"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Proceedings of ICCM 2020 - 18th International Conference on Cognitive Modelling",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85180757406&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ceballos J.M."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/tops.12488",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "402-416",
      "publication": {
        "category": "Journal",
        "cite_score": 8.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17568757",
        "publisher": "Wiley-Blackwell",
        "sjr": 1.242,
        "snip": 1.748,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Artificial Intelligence",
          "Cognitive Neuroscience",
          "Linguistics and Language",
          "Human-Computer Interaction"
        ],
        "title": "Topics in Cognitive Science"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "The Role of Basal Ganglia Reinforcement Learning in Lexical Ambiguity Resolution",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078965675&origin=inward"
      ]
    },
    {
      "abstract": "Programming by demonstrations is one of the most efficient methods for knowledge transfer to develop advanced learning systems, provided that teachers deliver abundant and correct demonstrations, and learners correctly perceive them. Nevertheless, demonstrations are <i>sparse</i> and <i>inaccurate</i> in almost all real-world problems. Complementary information is needed to compensate these shortcomings of demonstrations. In this paper, we target programming by a combination of <i>nonoptimal</i> and <i>sparse</i> demonstrations and a limited number of binary evaluative feedbacks, where the learner uses its own evaluated experiences as new demonstrations in an extended inverse reinforcement learning method. This provides the learner with a broader generalization and less regret as well as robustness in face of sparsity and nonoptimality in demonstrations and feedbacks. Our method alleviates the unrealistic burden on teachers to provide optimal and abundant demonstrations. Employing an evaluative feedback, which is easy for teachers to deliver, provides the opportunity to correct the learner\u2019s behavior in an interactive social setting without requiring teachers to know and use their own accurate reward function. Here, we enhance the inverse reinforcement learning (<span class=\"nowrap\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.1802902pt\" id=\"M1\" height=\"8.70527pt\" version=\"1.1\" viewBox=\"-0.0498162 -8.52498 19.6728 8.70527\" width=\"19.6728pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><path id=\"g190-74\" d=\"M303 0V28C221 34 213 39 213 125V525C213 610 221 616 303 622V650H38V622C120 616 128 610 128 525V125C128 40 120 34 38 28V0H303Z\"/></g><g transform=\"matrix(.013,0,0,-0.013,4.433,0)\"><path id=\"g190-83\" d=\"M631 18C609 24 585 35 559 65C534 91 514 117 478 169C448 214 406 281 389 313C462 346 516 399 516 485C516 545 490 590 449 616C412 641 363 650 290 650H42V622C120 615 128 612 128 527V125C128 40 120 34 38 28V0H300V28C221 34 212 40 212 125V284H244C295 284 312 272 329 244C359 195 395 133 430 84C475 19 516 -3 592 -7C603 -8 615 -8 627 -8L631 18ZM212 316V563C212 591 215 602 223 607C231 613 248 617 277 617C352 617 423 577 423 469C423 415 407 375 368 345C343 324 310 316 260 316H212Z\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.506,0)\"><path id=\"g190-77\" d=\"M495 163C480 117 462 85 444 65C421 39 387 34 332 34C290 34 256 36 236 47C218 57 213 77 213 131V526C213 612 222 616 301 622V650H40V622C122 616 128 611 128 526V126C128 41 120 34 36 28V0H489C498 31 519 126 525 157L495 163Z\"/></g></svg>)</span> to estimate the reward function using a mixture of nonoptimal and sparse demonstrations and evaluative feedbacks. Our method, called <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.1802902pt\" id=\"M2\" height=\"8.70527pt\" version=\"1.1\" viewBox=\"-0.0498162 -8.52498 19.6728 8.70527\" width=\"19.6728pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><use xlink:href=\"#g190-74\"/></g><g transform=\"matrix(.013,0,0,-0.013,4.433,0)\"><use xlink:href=\"#g190-83\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.506,0)\"><use xlink:href=\"#g190-77\"/></g></svg> from demonstration and human\u2019s critique (<span class=\"nowrap\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.2324905pt\" id=\"M3\" height=\"8.98583pt\" version=\"1.1\" viewBox=\"-0.0498162 -8.75334 38.1759 8.98583\" width=\"38.1759pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><use xlink:href=\"#g190-74\"/></g><g transform=\"matrix(.013,0,0,-0.013,4.433,0)\"><use xlink:href=\"#g190-83\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.506,0)\"><use xlink:href=\"#g190-77\"/></g><g transform=\"matrix(.013,0,0,-0.013,19.5,0)\"><path id=\"g190-69\" d=\"M43 650V622C120 616 128 612 128 526V124C128 39 120 33 34 27V0H270C392 0 492 25 567 83C643 141 690 230 690 350C690 444 655 517 605 565C543 625 450 650 323 650H43ZM213 547C213 587 217 598 226 604C236 612 262 617 304 617C371 617 429 604 474 576C554 529 592 439 592 336C592 176 505 36 319 36C246 36 213 55 213 131V547Z\"/></g><g transform=\"matrix(.013,0,0,-0.013,29.29,0)\"><path id=\"g190-68\" d=\"M614 175C564 76 510 21 408 21C256 21 146 149 146 336C146 488 235 629 402 629C510 629 570 586 597 480L626 488C620 541 614 582 606 638C578 643 510 665 429 665C206 665 44 527 44 316C44 157 153 -15 402 -15C474 -15 558 5 586 11C604 45 629 119 643 165L614 175Z\"/></g></svg>),</span> has two phases. The teacher first provides some demonstrations for the learner to initialize its policy. Next, the learner interacts with the environment and the teacher provides binary evaluative feedbacks. Taking into account possible inconsistencies and mistakes in issuing and receiving feedbacks, the learner revises the estimated reward function by solving a single optimization problem. The <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.2324905pt\" id=\"M4\" height=\"8.98583pt\" version=\"1.1\" viewBox=\"-0.0498162 -8.75334 38.1759 8.98583\" width=\"38.1759pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><use xlink:href=\"#g190-74\"/></g><g transform=\"matrix(.013,0,0,-0.013,4.433,0)\"><use xlink:href=\"#g190-83\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.506,0)\"><use xlink:href=\"#g190-77\"/></g><g transform=\"matrix(.013,0,0,-0.013,19.5,0)\"><use xlink:href=\"#g190-69\"/></g><g transform=\"matrix(.013,0,0,-0.013,29.29,0)\"><use xlink:href=\"#g190-68\"/></g></svg> is devised to handle errors and sparsities in demonstrations and feedbacks and can generalize different combinations of these two sources expertise. We apply our method to three domains: a simulated navigation task, a simulated car driving problem with human interactions, and a navigation experiment of a mobile robot. The results indicate that the <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.2324905pt\" id=\"M5\" height=\"8.98583pt\" version=\"1.1\" viewBox=\"-0.0498162 -8.75334 38.1759 8.98583\" width=\"38.1759pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><use xlink:href=\"#g190-74\"/></g><g transform=\"matrix(.013,0,0,-0.013,4.433,0)\"><use xlink:href=\"#g190-83\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.506,0)\"><use xlink:href=\"#g190-77\"/></g><g transform=\"matrix(.013,0,0,-0.013,19.5,0)\"><use xlink:href=\"#g190-69\"/></g><g transform=\"matrix(.013,0,0,-0.013,29.29,0)\"><use xlink:href=\"#g190-68\"/></g></svg> significantly enhances the learning process where the standard <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.1802902pt\" id=\"M6\" height=\"8.70527pt\" version=\"1.1\" viewBox=\"-0.0498162 -8.52498 19.6728 8.70527\" width=\"19.6728pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><use xlink:href=\"#g190-74\"/></g><g transform=\"matrix(.013,0,0,-0.013,4.433,0)\"><use xlink:href=\"#g190-83\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.506,0)\"><use xlink:href=\"#g190-77\"/></g></svg> methods fail and learning from feedbacks (<span class=\"nowrap\"><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.04981995pt\" id=\"M7\" height=\"9.35772pt\" version=\"1.1\" viewBox=\"-0.0498162 -9.3079 19.0855 9.35772\" width=\"19.0855pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><use xlink:href=\"#g190-77\"/></g><g transform=\"matrix(.013,0,0,-0.013,7.072,0)\"><path id=\"g190-103\" d=\"M54 437L27 408L31 397H101V103C101 37 94 32 30 26V0H266V25C187 33 180 36 180 110V397H288C299 404 304 428 298 437H180V477C179 562 190 610 203 630C214 647 230 659 256 659C289 659 318 641 337 622C346 612 355 612 364 619C374 627 380 635 383 643C388 655 387 667 378 678C362 697 333 710 299 712C260 707 225 689 189 659C135 613 119 563 112 541S101 490 101 458V437H54Z\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.042,0)\"><path id=\"g190-71\" d=\"M493 503C489 551 484 614 483 650H43V622C120 616 128 611 128 525V126C128 40 120 34 40 28V0H312V28C221 34 213 40 213 126V307H316C407 307 412 296 424 227H453V420H424C412 355 407 346 316 346H213V584C213 613 216 616 246 616H322C398 616 419 607 436 579C449 559 455 539 464 499L493 503Z\"/></g></svg>)</span> methods has a high regret. Also, the <svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\" style=\"vertical-align:-0.2324905pt\" id=\"M8\" height=\"8.98583pt\" version=\"1.1\" viewBox=\"-0.0498162 -8.75334 38.1759 8.98583\" width=\"38.1759pt\"><g transform=\"matrix(.013,0,0,-0.013,0,0)\"><use xlink:href=\"#g190-74\"/></g><g transform=\"matrix(.013,0,0,-0.013,4.433,0)\"><use xlink:href=\"#g190-83\"/></g><g transform=\"matrix(.013,0,0,-0.013,12.506,0)\"><use xlink:href=\"#g190-77\"/></g><g transform=\"matrix(.013,0,0,-0.013,19.5,0)\"><use xlink:href=\"#g190-69\"/></g><g transform=\"matrix(.013,0,0,-0.013,29.29,0)\"><use xlink:href=\"#g190-68\"/></g></svg> works well at different levels of sparsity and optimality of the teacher\u2019s demonstrations and feedbacks, where other state-of-the-art methods fail.",
      "authors": [
        "Mourad, Nafee",
        "Ezzeddine, Ali",
        "Nadjar Araabi, Babak",
        "Nili Ahmadabadi, Majid"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "https://doi.org/10.1155/2020/3849309",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 3.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1687-9600",
        "publisher": "Hindawi Limited",
        "sjr": 0.424,
        "snip": 0.935,
        "subject_areas": [
          "Computer Science (all)",
          "Control and Systems Engineering"
        ],
        "title": "Journal of Robotics"
      },
      "publication_date": "2020-01-01",
      "selected": null,
      "title": "Learning from Demonstrations and Human Evaluative Feedbacks: Handling Sparsity and Imperfection Using Inverse Reinforcement Learning Approach",
      "urls": [
        "https://dl.acm.org/doi/10.1155/2020/3849309",
        "https://downloads.hindawi.com/journals/jr/2020/3849309.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078670070&origin=inward"
      ]
    },
    {
      "abstract": "We seek to align agent policy with human expert behavior in a reinforcement learning (RL) setting, without any prior knowledge about dynamics, reward function, and unsafe states. There is a human expert knowing the rewards and unsafe states based on his preference and objective, but querying that human expert is expensive. To address this challenge, we propose a new framework for imitation learning (IL) algorithm that actively and interactively learns a model of the user's reward function with efficient queries. We build an adversarial generative model of states and a successor feature (SR) model trained over transition experience collected by learning policy. Our method uses these models to select state-action pairs, asking the user to comment on the optimality or safety, and trains a adversarial neural network to predict the rewards. Different from previous papers, which are almost all based on uncertainty sampling, the key idea is to actively and efficiently select state-action pairs from both on-policy and off-policy experience, by discriminating the queried (expert) and unqueried (generated) data and maximizing the efficiency of value function learning. We call this method adversarial reward query with successor representation. We evaluate the proposed method with simulated human on a state-based 2D navigation task, robotic control tasks and the image-based video games, which have high-dimensional observation and complex state dynamics. The results show that the proposed method significantly outperforms uncertainty-based methods on learning reward models, achieving better query efficiency, where the adversarial discriminator can make the agent learn human behavior more efficiently and the SR can select states which have stronger impact on value function. Moreover, the proposed method can also learn to avoid unsafe states when training the reward model.",
      "authors": [
        "Hsu, Daniel"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-12-30",
      "selected": null,
      "title": "A New Framework for Query Efficient Active Imitation Learning",
      "urls": [
        "http://arxiv.org/pdf/1912.13037v1",
        "http://arxiv.org/abs/1912.13037v1",
        "http://arxiv.org/pdf/1912.13037.pdf"
      ]
    },
    {
      "abstract": "The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards find a sub-optimal solution. We introduce a general-purpose control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a particular subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment \u2013 namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.1",
      "authors": [
        "Benjamin Eysenbach",
        "Ruslan Salakhutdinov",
        "Sergey Levine"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3454287.3455653",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "15246-15257",
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 33rd International Conference on Neural Information Processing Systems"
      },
      "publication_date": "2019-12-08",
      "selected": null,
      "title": "Search on the replay buffer: bridging planning and reinforcement learning",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3454287.3455653"
      ]
    },
    {
      "abstract": "Rapid advancements in the Internet of Things (IoT) have facilitated more efficient deployment of smart environment solutions for specific user requirement. With the increase in the number of IoT devices, it has become difficult for the user to control or operate every individual smart device into achieving some desired goal like optimized power consumption, scheduled appliance running time, etc. Furthermore, existing solutions to automatically adapt the IoT devices are not capable enough to incorporate the user behavior. This paper presents a novel approach to accurately configure IoT devices while achieving the twin objectives of energy optimization along with conforming to user preferences. Our work comprises of unsupervised clustering of devices' data to find the states of operation for each device, followed by probabilistically analyzing user behavior to determine their preferred states. Eventually, we deploy an online reinforcement learning (RL) agent to find the best device settings automatically. Results for three different smart homes' data-sets show the effectiveness of our methodology. To the best of our knowledge, this is the first time that a practical approach has been adopted to achieve the above mentioned objectives without any human interaction within the system.",
      "authors": [
        "Verma, Mudit",
        "Bhambri, Siddhant",
        "Gupta, Saurabh",
        "Buduru, Arun Balaji"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-12-06",
      "selected": null,
      "title": "Making Smart Homes Smarter: Optimizing Energy Consumption with Human in the Loop",
      "urls": [
        "http://arxiv.org/abs/1912.03298v3",
        "http://arxiv.org/pdf/1912.03298.pdf",
        "http://arxiv.org/pdf/1912.03298v3"
      ]
    },
    {
      "abstract": "In this contribution, we develop a feedback controller for a wheeled inverted pendulum in the form of a neural network that is not only stabilizing the unstable system, but also allows the wheeled robot to drive to arbitrary positions within a certain radius and take a desired orientation, without the need to compute a feasible trajectory to the desired position online. While some techniques from the reinforcement learning community can be used to optimize the parameters of a general feedback controller, i.e. policy gradient methods, the method used in this work is an approach related to imitation learning or learning from demonstration. The demonstration data however does not result from e.g. a human demonstrator, but is a set of precomputed optimal trajectories. The neural network is trained to imitate the behavior of those optimal trajectories. We show that a good choice of initial states and a large number of training targets can be used to alleviate a problem of imitation learning, namely deviating from training trajectories, and we demonstrate results in simulation as well as on the physical system.",
      "authors": [
        "Christian Dengler",
        "Lohmann Boris"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICAR46387.2019.8981659",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "350-355",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-2468-1",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 19th International Conference on Advanced Robotics, ICAR 2019"
      },
      "publication_date": "2019-12-02",
      "selected": null,
      "title": "Neural network position and orientation control of an inverted pendulum on wheels",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8981659",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084284651&origin=inward"
      ]
    },
    {
      "abstract": "The assembly industry is shifting more towards customizable products, or requiring assembly of small batches. This requires a lot of reprogramming, which is expensive because a specialized engineer is required. It would be an improvement if untrained workers could help a cobot to learn an assembly sequence by giving advice. Learning an assembly sequence is a hard task for a cobot, because the solution space increases drastically when the complexity of the task increases. This work introduces a novel method where human knowledge is used to reduce this solution space, and as a result increases the learning speed. The method proposed is the IRL-PBRS method, which uses Interactive Reinforcement Learning (IRL) to learn from human advice in an interactive way, and uses Potential Based Reward Shaping (PBRS), in a simulated environment, to focus learning on a smaller part of the solution space. The method was compared in simulation to two other feedback strategies. The results show that IRL-PBRS converges more quickly to a valid assembly sequence policy and does this with the fewest human interactions. Finally, a use case is presented where participants were asked to program an assembly task. Here, the results show that IRL-PBRS learns quickly enough to keep up with advice given by a user, and is able to adapt online to a changing knowledge base.",
      "authors": [
        "De Winter, Joris",
        "De Beir, Albert",
        "El Makrini, Ilias",
        "Van de Perre, Greet",
        "Now\u00e9, Ann",
        "Vanderborght, Bram"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3390/robotics8040104",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2218-6581",
        "publisher": "Multidisciplinary Digital Publishing Institute (MDPI)",
        "sjr": 0.812,
        "snip": 1.612,
        "subject_areas": [
          "Artificial Intelligence",
          "Mechanical Engineering",
          "Control and Optimization"
        ],
        "title": "Robotics"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "Accelerating Interactive Reinforcement Learning by Human Advice for an Assembly Task by a Cobot",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079805787&origin=inward",
        "https://www.mdpi.com/2218-6581/8/4/104/pdf?version=1576659596"
      ]
    },
    {
      "abstract": "Schizophrenia is often associated with distinctive or odd social behaviours. Previous work suggests this could be due to a general reduction in conformity; however, this work only assessed the tendency to publicly agree with others, which may involve a number of different mechanisms. In this study, we specifically investigated whether patients display a reduced tendency to adopt other people\u2019s opinions (socially learned attitude change). We administered a computerized conformity task, assumed to rely on reinforcement learning circuits, to 32 patients with schizophrenia or schizo-affective disorder and 39 matched controls. Each participant rated 153 faces for trustworthiness. After each rating, they were immediately shown the opinion of a group. After approximately 1\u2009hour, participants were unexpectedly asked to rate all the faces again. We compared the degree of attitude change towards group opinion in patients and controls. Patients presented equal or more social influence on attitudes than controls. This effect may have been medication induced, as increased conformity was seen with higher antipsychotic dose. The results suggest that there is not a general decline in conformity in medicated patients with schizophrenia and that previous findings of reduced conformity are likely related to mechanisms other than reinforcement based social influence on attitudes.",
      "authors": [
        "Simonsen, Arndis",
        "Fusaroli, Riccardo",
        "Skewes, Joshua Charles",
        "Roepstorff, Andreas",
        "Mors, Ole",
        "Bliksted, Vibeke",
        "Campbell-Meiklejohn, Daniel"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-018-37250-x",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "Socially Learned Attitude Change is not reduced in Medicated Patients with Schizophrenia",
      "urls": [
        "https://www.nature.com/articles/s41598-018-37250-x.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060942246&origin=inward"
      ]
    },
    {
      "abstract": "The ability to learn from feedback, especially under social scrutiny, is an essential prerequisite for successful interaction with the environment. Patients suffering from social anxiety disorder (SAD) have been proposed to show altered processing of and learning from feedback, especially depending on social context. However, the neural basis and behavioral consequences of altered reinforcement learning in SAD are not clear yet. In the present event-related potentials (ERPs) study, 34 SAD patients and 30 healthy control subjects (HC) performed an adapted version of a probabilistic feedback learning task in two distinct social conditions. In the observation condition, participants were observed by a confederate; in the control condition, they performed the task without being observed. Patients as compared to healthy controls experienced more subjective discomfort under social observation. Moreover, they showed better learning from negative feedback in the control condition, but reduced learning from negative feedback in the observation condition. This effect correlated with reduced differentiation of positive and negative feedback in the time range of the feedback-related negativity (FRN) under high action-feedback contingency. In addition, SAD patients demonstrated increased FRN amplitudes in the first half of the observation condition, in particular to positive feedback. The present results demonstrate that processing of and learning from feedback are altered in SAD, especially under social scrutiny. In particular, it appears that SAD patients do not process positive information adequately on the neural level, which may impair their ability to differentiate between negative and positive outcomes.",
      "authors": [
        "Voegler, Rolf",
        "Peterburs, Jutta",
        "Bellebaum, Christian",
        "Straube, Thomas"
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-019-41268-0",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "Modulation of feedback processing by social context in social anxiety disorder (SAD)\u00e2\u0080\u0093an event-related potentials (ERPs) study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063049751&origin=inward",
        "https://www.nature.com/articles/s41598-019-41268-0.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Feng G."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TIE.2019.2903751",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "9734-9744",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02780046",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Industrial Electronics"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "Data-efficient reinforcement learning for energy optimization of power-assisted wheelchairs",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85070465281&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lin C.Y."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/SmartCloud.2019.000-2",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "175-180",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728155050",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 4th IEEE International Conference on Smart Cloud, SmartCloud 2019 and 3rd International Symposium on Reinforcement Learning, ISRL 2019"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "A Mechanism for Solving the Elderly Posture Problem",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085488613&origin=inward"
      ]
    },
    {
      "abstract": "It is widely known that reinforcement learning systems in the brain contribute to learning via interactions with the environment. These systems are capable of solving multidimensional problems, in which some dimensions are relevant to a reward, while others are not. To solve these problems, computational models use Bayesian learning, a strategy supported by behavioral and neural evidence in human. Bayesian learning takes into account beliefs, which represent a learner\u2019s confidence in a particular dimension being relevant to the reward. Beliefs are given as a posterior probability of the state-transition (reward) function that maps the optimal actions to the states in each dimension. However, when it comes to implementing this learning strategy, the order in which beliefs and state-transition functions update remains unclear. The present study investigates this update order using a trial-by-trial analysis of human behavior and electroencephalography signals during a task in which learners have to identify the reward-relevant dimension. Our behavioral and neural results reveal a cooperative update\u2014within 300 ms after the outcome feedback, the state-transition functions are updated, followed by the beliefs for each dimension.",
      "authors": [
        "Higashi, Hiroshi",
        "Minami, Tetsuto",
        "Nakauchi, Shigeki"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-019-53600-9",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "Cooperative update of beliefs and state-transition functions in human reinforcement learning",
      "urls": [
        "https://www.nature.com/articles/s41598-019-53600-9.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075743138&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Groman, Stephanie M."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41386-019-0412-x",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "2149-2150",
      "publication": {
        "category": "Journal",
        "cite_score": 14.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0893133X",
        "publisher": "Nature Publishing Group",
        "sjr": 2.385,
        "snip": 1.864,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Pharmacology"
        ],
        "title": "Neuropsychopharmacology"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "Investigating the computational underpinnings of addiction",
      "urls": [
        "https://www.nature.com/articles/s41386-019-0412-x.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065781548&origin=inward"
      ]
    },
    {
      "abstract": "Converging evidence suggests that reinforcement learning (RL) signals exist within the human brain and that they play a role in the modification of behaviour. According to RL theory, prediction errors are used to update values associated with actions and/or predictive cues, thus facilitate decision-making. For example, the reward positivity\u2014a feedback-sensitive component of the event-related brain potential (ERP)\u2014is thought to index an RL prediction error. An unresolved question, however, is whether or not action is required to elicit a reward positivity. Reinforcement learning theory would predict that the reward positivity should diminish or disappear in the absence of action, but evidence for this claim is conflicting. To investigate the impact of cue, choice, and action on the amplitude of the reward positivity, we altered a two-armed bandit task by systematically removing these factors. The reward positivity was greatly reduced or absent in the altered versions of the task. This result highlights the key role of agency in producing learning signals, such as the reward positivity.",
      "authors": [
        "Hassall, Cameron D.",
        "Hajcak, Greg",
        "Krigolson, Olave E."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-019-00730-2",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1458-1466",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "The importance of agency in human reward processing",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-019-00730-2.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067381539&origin=inward"
      ]
    },
    {
      "abstract": "Robot learning problems are limited by physical constraints, which make learning successful policies for complex motor skills on real systems unfeasible. Some reinforcement learning methods, like Policy Search, offer stable convergence toward locally optimal solutions, whereas interactive machine learning or learning-from-demonstration methods allow fast transfer of human knowledge to the agents. However, most methods require expert demonstrations. In this work, we propose the use of human corrective advice in the actions domain for learning motor trajectories. Additionally, we combine this human feedback with reward functions in a Policy Search learning scheme. The use of both sources of information speeds up the learning process, since the intuitive knowledge of the human teacher can be easily transferred to the agent, while the Policy Search method with the cost/reward function take over for supervising the process and reducing the influence of occasional wrong human corrections. This interactive approach has been validated for learning movement primitives with simulated arms with several degrees of freedom in reaching via-point movements, and also using real robots in such tasks as \u00e2\u0080\u009cwriting characters\u00e2\u0080\u009d and the ball-in-a-cup game. Compared with standard reinforcement learning without human advice, the results show that the proposed method not only converges to higher rewards when learning movement primitives, but also that the learning is sped up by a factor of 4\u00e2\u0080\u009340 times, depending on the task.",
      "authors": [
        "Carlos Celemin",
        "Guilherme Maeda",
        "Javier Ruiz-del-Solar",
        "Jan Peters",
        "Jens Kober"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1177/0278364919871998",
      "keywords": [
        "policy search",
        "motor skills",
        "learning from demonstrations",
        "interactive machine learning",
        "Reinforcement learning",
        "movement primitives"
      ],
      "number_of_pages": 21,
      "pages": "1560-1580",
      "publication": {
        "category": "Journal",
        "cite_score": 16.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0278-3649",
        "publisher": "SAGE Publications Inc.",
        "sjr": 2.729,
        "snip": 3.657,
        "subject_areas": [
          "Mechanical Engineering",
          "Artificial Intelligence",
          "Software",
          "Modeling and Simulation",
          "Electrical and Electronic Engineering",
          "Applied Mathematics"
        ],
        "title": "International Journal of Robotics Research"
      },
      "publication_date": "2019-12-01",
      "selected": null,
      "title": "Reinforcement learning of motor skills using Policy Search and human corrective advice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073830851&origin=inward",
        "https://dl.acm.org/doi/10.1177/0278364919871998"
      ]
    },
    {
      "abstract": "Socially assistive robots (SAR) have shown great potential to augment the social and educational development of children with autism spectrum disorders (ASD). As SAR continues to substantiate itself as an effective enhancement to human intervention, researchers have sought to study its longitudinal impacts in real-world environments, including the home. Computational personalization stands out as a central computational challenge as it is necessary to enable SAR systems to adapt to each child's unique and changing needs. Toward that end, we formalized personalization as a hierarchical human robot learning framework (hHRL) consisting of five controllers (disclosure, promise, instruction, feedback, and inquiry) mediated by a meta-controller that utilized reinforcement learning to personalize instruction challenge levels and robot feedback based on each user's unique learning patterns. We instantiated and evaluated the approach in a study with 17 children with ASD, aged 3 to 7 years old, over month-long interventions in their homes. Our findings demonstrate that the fully autonomous SAR system was able to personalize its instruction and feedback over time to each child's proficiency. As a result, every child participant showed improvements in targeted skills and long-term retention of intervention content. Moreover, all child users were engaged for a majority of the intervention, and their families reported the SAR system to be useful and adaptable. In summary, our results show that autonomous, personalized SAR interventions are both feasible and effective in providing long-term in-home developmental support for children with diverse learning needs.",
      "authors": [
        "Clabaugh, Caitlyn",
        "Mahajan, Kartik",
        "Jain, Shomik",
        "Pakkar, Roxanna",
        "Becerra, David",
        "Shi, Zhonghao",
        "Deng, Eric",
        "Lee, Rhianna",
        "Ragusa, Gisele",
        "Matari\u0107, Maja"
      ],
      "categories": null,
      "citations": 63,
      "comments": "30 pages, 10 figures, Frontiers in Robotics and AI journal",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.3389/frobt.2019.00110",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.9,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "2296-9144",
        "publisher": "Frontiers Media SA",
        "sjr": 0.726,
        "snip": 1.254,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Robotics"
        ],
        "title": "Frontiers in Robotics and AI6, 110 (2019)"
      },
      "publication_date": "2019-11-18",
      "selected": null,
      "title": "Long-Term Personalization of an In-Home Socially Assistive Robot for Children With Autism Spectrum Disorders",
      "urls": [
        "http://dx.doi.org/10.3389/frobt.2019.00110",
        "http://arxiv.org/abs/1911.07992v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083086191&origin=inward",
        "http://arxiv.org/pdf/1911.07992v1"
      ]
    },
    {
      "abstract": "Reinforcement Learning (RL) - a research branch of artificial intelligence - exploits the concept of reward/penalty from human learning so that the feedback information of the environment can be used for self-learning without previous knowledge. Although RL has been applied to many research fields, there are difficulties when implementing it in some real world applications. One difficulty is the tradeoff between exploration and exploitation for the agent of RL in choosing a proper action. An improper action may lead to a learning failure or an increase in learning cost. Another difficulty is that the learning agent of RL needs to interact with the environment to attain a real-time reward/penalty; however, the learning time spent in the interaction process may be too long. In order to overcome the aforementioned difficulties, this paper proposes an approach that employs a personalized recommendation system to provide a feedforward candidate action for RL to implement self-adaptive learning through teaching. A real visual tracking experiment using a pan-tilt camera system is conducted to assess the performance of the proposed approach. Experimental results show that the proposed personalized recommendation system-based approach is able to improve the effectiveness and practicality of RL.",
      "authors": [
        "Chia-Ling Chiang",
        "Ming-Yang Cheng",
        "Ting-Yu Ye",
        "Ya-Ling Chen",
        "Pin-Hsuan Huang"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/CACS47674.2019.9024742",
      "keywords": [
        "Visual Tracking",
        "Personalized Recommendation System",
        "Reinforcement Learning",
        "Q-learning"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-3847-3",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 International Automatic Control Conference, CACS 2019"
      },
      "publication_date": "2019-11-13",
      "selected": null,
      "title": "Convergence Improvement of Q-learning Based on a Personalized Recommendation System",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082986739&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9024742"
      ]
    },
    {
      "abstract": "Artificially intelligent assistive agents are playing an increased role in our work and homes. In contrast with currently predominant conversational agents, whose intelligence derives from dialogue trees and external modules, a fully autonomous domestic or workplace robot must carry out more complex reasoning. Such a robot must make good decisions as soon as possible, learn from experience, respond to feedback, and rely on feedback only as much as necessary. In this research, we narrow the focus of a hypothetical robot assistant to a room-tidying task in a simulated domestic environment. Given an item, the robot chooses where to put it among many destinations, then optionally receives feedback from a human operator. We frame the problem as a contextual bandit, a reinforcement learning approach frequently used in Web recommendation systems. We evaluate epsilon-greedy and LinUCB action selection methods under a variety of infrequent feedback scenarios, with several methods for managing the lack of feedback. Our empirical results show that, while early-episode performance and overall accuracy of epsilon-greedy action selection can be improved through learning from no-response feedback and careful management of remembered training episodes, a baseline LinUCB approach outperforms epsilon-greedy action selection in early-episode performance, overall accuracy, and simplicity.",
      "authors": [
        "Matthew McNeill",
        "Damian Lyons"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICTAI.2019.00025",
      "keywords": [
        "contextual bandits",
        "sparse feedback",
        "robotics",
        "human in the loop reinforcement learning"
      ],
      "number_of_pages": 8,
      "pages": "117-124",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-3799-5",
        "issn": "1082-3409",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)"
      },
      "publication_date": "2019-11-04",
      "selected": null,
      "title": "A Comparison of Contextual Bandit Approaches to Human-in-the-Loop Robot Task Completion with Infrequent Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8995186",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081092042&origin=inward"
      ]
    },
    {
      "abstract": "Textile pattern design is a challenging task that can be hardly resolved by a single deep neural network, due to the requirements on high resolution, periodic tiling, copyright protection and aesthetic preference of designers. In this paper, we present our NAD system which can automatically produce high-quality textile patterns for printing industry. Our NAD system splits the work into three steps: layout design, image filtering and pattern style transfer. In the first and last step, we employ different neural models to learn the process of artwork creation by human designers. Specifically, a reinforcement learning model is first developed for layout adjustment, followed by a CNN-based model for style transfer. We have employed our NAD system in an online production system with real customers and the results are very impressive and promising. The NAD system not only frees human designers from the labor intensive design process, but also results in a 2%-5% daily purchase rate.",
      "authors": [
        "Zhifei Pang",
        "Sai Wu",
        "Dongxiang Zhang",
        "Yunjun Gao",
        "Gang Chen"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3357384.3358103",
      "keywords": [
        "deep learning",
        "neural networks",
        "textile pattern design"
      ],
      "number_of_pages": 4,
      "pages": "2081-2084",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450369763",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management"
      },
      "publication_date": "2019-11-03",
      "selected": null,
      "title": "NAD: Neural Network Aided Design for Textile Pattern Generation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075415327&origin=inward",
        "https://dl.acm.org/doi/10.1145/3357384.3358103"
      ]
    },
    {
      "abstract": "Numerous cognitive studies have demonstrated experience-induced plasticity in the primary sensory cortex, indicating that repeated decisions could modulate sensory processing. In this context, we investigated whether an auditory version of the monetary incentive delay (MID) task could change the neural processing of the incentive cues that code expected monetary outcomes. To study sensory plasticity, we presented the incentive cues as deviants during oddball sessions recorded before and after training in the two MID task sessions. We found that after two days of training in the MID task, incentive cues evoked a larger P3a (compared with the baseline condition), indicating there was an enhancement of the involuntary attention to the stimuli that predict rewards. At the individual level, the training-induced change of mismatch-related negativity was correlated with the amplitude of the feedback-related negativity recorded during the first MID task session. Our results show that the MID task evokes plasticity changes in the auditory system associated with better passive discrimination of incentive cues and with enhanced involuntary attention switching towards these cues. Thus, the sensory processing of incentive cues is dynamically modulated by previous outcomes. ",
      "authors": [
        "Krugliakova, Elena",
        "Gorin, Alexey",
        "Fedele, Tommaso",
        "Shtyrov, Yury",
        "Moiseeva, Victoria",
        "Klucharev, Vasily",
        "Shestakova, Anna"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2019.00382",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2019-11-01",
      "selected": null,
      "title": "The Monetary Incentive Delay (MID) Task Induces Changes in Sensory Processing: ERP Evidence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075386706&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Yamazaki T."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neures.2019.03.001",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01680102",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Neuroscience Research"
      },
      "publication_date": "2019-11-01",
      "selected": null,
      "title": "Revisiting a theory of cerebellar cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063689288&origin=inward"
      ]
    },
    {
      "abstract": "Visual paragraph generation aims to automatically describe a given image from different perspectives and organize sentences in a coherent way. In this paper, we address three critical challenges for this task in a reinforcement learning setting: the mode collapse, the delayed feedback, and the time-consuming warm-up for policy networks. Generally, we propose a novel Curiosity-driven Reinforcement Learning (CRL) framework to jointly enhance the diversity and accuracy of the generated paragraphs. First, by modeling the paragraph captioning as a long-term decision-making process and measuring the prediction uncertainty of state transitions as intrinsic rewards, the model is incentivized to memorize precise but rarely spotted descriptions to context, rather than being biased towards frequent fragments and generic patterns. Second, since the extrinsic reward from evaluation is only available until the complete paragraph is generated, we estimate its expected value at each time step with temporal-difference learning, by considering the correlations between successive actions. Then the estimated extrinsic rewards are complemented by dense intrinsic rewards produced from the derived curiosity module, in order to encourage the policy to fully explore action space and find a global optimum. Third, discounted imitation learning is integrated for learning from human demonstrations, without separately performing the time-consuming warm-up in advance. Extensive experiments conducted on the Standford image-paragraph dataset demonstrate the effectiveness and efficiency of the proposed method, improving the performance by 38.4% compared with state-of-the-art.",
      "authors": [
        "Yadan Luo",
        "Zi Huang",
        "Zheng Zhang",
        "Ziwei Wang",
        "Jingjing Li",
        "Yang Yang"
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3343031.3350961",
      "keywords": [
        "visual paragraph generation",
        "reinforcement learning"
      ],
      "number_of_pages": 10,
      "pages": "2341-2350",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450368896",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia"
      },
      "publication_date": "2019-10-15",
      "selected": null,
      "title": "Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074866245&origin=inward",
        "https://dl.acm.org/doi/10.1145/3343031.3350961"
      ]
    },
    {
      "abstract": "It is expected that many human drivers will still prefer to drive themselves even if the self-driving technologies are ready. Therefore, human-driven vehicles and autonomous vehicles (AVs) will coexist in a mixed traffic for a long time. To enable AVs to safely and efficiently maneuver in this mixed traffic, it is critical that the AVs can understand how humans cope with risks and make driving-related decisions. On the other hand, the driving environment is highly dynamic and ever-changing, and it is thus difficult to enumerate all the scenarios and hard-code the controllers. To face up these challenges, in this work, we incorporate a human decision-making model in reinforcement learning to control AVs for safe and efficient operations. Specifically, we adapt regret theory to describe a human driver's lane-changing behavior, and fit the personalized models to individual drivers for predicting their lane-changing decisions. The predicted decisions are incorporated in the safety constraints for reinforcement learning in training and in implementation. We then use an extended version of double deep Q-network (DDQN) to train our AV controller within the safety set. By doing so, the amount of collisions in training is reduced to zero, while the training accuracy is not impinged.",
      "authors": [
        "Chen, Dong",
        "Jiang, Longsheng",
        "Wang, Yue",
        "Li, Zhaojian"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 6 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-10-10",
      "selected": null,
      "title": "Autonomous Driving using Safe Reinforcement Learning by Incorporating a Regret-based Human Lane-Changing Decision Model",
      "urls": [
        "http://arxiv.org/pdf/1910.04803.pdf",
        "http://arxiv.org/abs/1910.04803v1",
        "http://arxiv.org/pdf/1910.04803v1"
      ]
    },
    {
      "abstract": "Autonomous robots have the potential to serve as versatile caregivers that improve quality of life for millions of people worldwide. Yet, conducting research in this area presents numerous challenges, including the risks of physical interaction between people and robots. Physics simulations have been used to optimize and train robots for physical assistance, but have typically focused on a single task. In this paper, we present Assistive Gym, an open source physics simulation framework for assistive robots that models multiple tasks. It includes six simulated environments in which a robotic manipulator can attempt to assist a person with activities of daily living (ADLs): itch scratching, drinking, feeding, body manipulation, dressing, and bathing. Assistive Gym models a person's physical capabilities and preferences for assistance, which are used to provide a reward function. We present baseline policies trained using reinforcement learning for four different commercial robots in the six environments. We demonstrate that modeling human motion results in better assistance and we compare the performance of different robots. Overall, we show that Assistive Gym is a promising tool for assistive robotics research.",
      "authors": [
        "Erickson, Zackory",
        "Gangaram, Vamsee",
        "Kapusta, Ariel",
        "Liu, C. Karen",
        "Kemp, Charles C."
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 5 figures, 2 tables",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-10-10",
      "selected": null,
      "title": "Assistive Gym: A Physics Simulation Framework for Assistive Robotics",
      "urls": [
        "http://arxiv.org/pdf/1910.04700v1",
        "http://arxiv.org/abs/1910.04700v1",
        "http://arxiv.org/pdf/1910.04700.pdf"
      ]
    },
    {
      "abstract": "Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning (IML), wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice\u2014Feedback Arbitration, and Newtonian Action Advice\u2014under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. The training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",
      "authors": [
        "Spencer Frazier",
        "Mark Riedl"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3505425.3505446",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "146-152",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-819-0",
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Fifteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment"
      },
      "publication_date": "2019-10-08",
      "selected": null,
      "title": "Improving deep reinforcement learning in minecraft with action advice",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3505425.3505446"
      ]
    },
    {
      "abstract": "Reinforcement learning is one of the sub of machine learning. A machine learning agent learns from the feedback of the try-and-error in order to predict their next step. Machine learning can be use on various field and one of them is games. The challenge to win a game is that the player needs to come up with a good strategy. In order to produce good strategy, player need to play the game multiple time which are time, energy and money consuming. The objective of this research is to introduce a reinforcement learning agent in game that run the simulation of the game and produce improved results after each iteration. Then human can imitate the agent performance in order to improve their chance of winning the game. Reinforcement learning can be implemented in various method. This paper will focus more on Q-learning and State-Action-Reward-State-Action (SARSA) method. Both methods are chosen as both are almost similar except Q-learning is off-policy algorithm and SARSA is on-policy algorithm. The results of this paper is a list of results from previous research related to Q-learning and SARSA on different test field or setting. The second results are the proposed reinforcement learning methodology what will cover on understanding data, categorizing problem, finding the available algorithm and implementing the algorithm.",
      "authors": [
        "Mohd Azmin Samsuden",
        "Norizan Mat Diah",
        "Nurazzah Abdul Rahman"
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICSEngT.2019.8906400",
      "keywords": [
        "SARSA",
        "Q-learning",
        "reinforcement learning",
        "games",
        "machine learning",
        "DDPG",
        "DQN"
      ],
      "number_of_pages": 6,
      "pages": "258-263",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-0759-2",
        "issn": "2470-6396",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 IEEE 9th International Conference on System Engineering and Technology, ICSET 2019 - Proceeding"
      },
      "publication_date": "2019-10-07",
      "selected": null,
      "title": "A Review Paper on Implementing Reinforcement Learning Technique in Optimising Games Performance",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076416103&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8906400"
      ]
    },
    {
      "abstract": "In reinforcement learning (RL), sparse rewards are a natural way to specify the task to be learned. However, most RL algorithms struggle to learn in this setting since the learning signal is mostly zeros. In contrast, humans are good at assessing and predicting the future consequences of actions and can serve as good reward/policy shapers to accelerate the robot learning process. Previous works have shown that the human brain generates an error-related signal, measurable using electroencephelography (EEG), when the human perceives the task being done erroneously. In this work, we propose a method that uses evaluative feedback obtained from human brain signals measured via scalp EEG to accelerate RL for robotic agents in sparse reward settings. As the robot learns the task, the EEG of a human observer watching the robot attempts is recorded and decoded into noisy error feedback signal. From this feedback, we use supervised learning to obtain a policy that subsequently augments the behavior policy and guides exploration in the early stages of RL. This bootstraps the RL learning process to enable learning from sparse reward. Using a robotic navigation task as a test bed, we show that our method achieves a stable obstacle-avoidance policy with high success rate, outperforming learning from sparse rewards only that struggles to achieve obstacle avoidance behavior or fails to advance to the goal.",
      "authors": [
        "Akinola, Iretiayo",
        "Wang, Zizhao",
        "Shi, Junyao",
        "He, Xiaomin",
        "Lapborisuth, Pawan",
        "Xu, Jingxi",
        "Watkins-Valls, David",
        "Sajda, Paul",
        "Allen, Peter"
      ],
      "categories": null,
      "citations": null,
      "comments": "2020 IEEE International Conference on Robotics and Automation - ICRA\n  2020",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-10-01",
      "selected": null,
      "title": "Accelerated Robot Learning via Human Brain Signals",
      "urls": [
        "http://arxiv.org/abs/1910.00682v2",
        "http://arxiv.org/pdf/1910.00682v2",
        "http://arxiv.org/pdf/1910.00682.pdf"
      ]
    },
    {
      "abstract": "We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency (\"human-oriented\" quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a \"machine-oriented\" criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with machine-oriented translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.",
      "authors": [
        "Tebbifakhr, Amirhossein",
        "Bentivogli, Luisa",
        "Negri, Matteo",
        "Turchi, Marco"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "1368-1374",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781950737901",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference"
      },
      "publication_date": "2019-10-01",
      "selected": null,
      "title": "Machine Translation for Machines: the Sentiment Classification Use Case",
      "urls": [
        "http://arxiv.org/abs/1910.00478v1",
        "http://arxiv.org/pdf/1910.00478.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084324713&origin=inward",
        "http://arxiv.org/pdf/1910.00478v1"
      ]
    },
    {
      "abstract": "This article describes mechanistic links that exist in advanced brains between processes that regulate conscious attention, seeing, and knowing, and those that regulate looking and reaching. These mechanistic links arise from basic properties of brain design principles such as complementary computing, hierarchical resolution of uncertainty, and adaptive resonance. These principles require conscious states to mark perceptual and cognitive representations that are complete, context sensitive, and stable enough to control effective actions. Surface\u2013shroud resonances support conscious seeing and action, whereas feature\u2013category resonances support learning, recognition, and prediction of invariant object categories. Feedback interactions between cortical areas such as peristriate visual cortical areas V2, V3A, and V4, and the lateral intraparietal area (LIP) and inferior parietal sulcus (IPS) of the posterior parietal cortex (PPC) control sequences of saccadic eye movements that foveate salient features of attended objects and thereby drive invariant object category learning. Learned categories can, in turn, prime the objects and features that are attended and searched. These interactions coordinate processes of spatial and object attention, figure\u2013ground separation, predictive remapping, invariant object category learning, and visual search. They create a foundation for learning to control motor-equivalent arm movement sequences, and for storing these sequences in cognitive working memories that can trigger the learning of cognitive plans with which to read out skilled movement sequences. Cognitive\u2013emotional interactions that are regulated by reinforcement learning can then help to select the plans that control actions most likely to acquire valued goal objects in different situations. Many interdisciplinary psychological and neurobiological data about conscious and unconscious behaviors in normal individuals and clinical patients have been explained in terms of these concepts and mechanisms.",
      "authors": [
        "Grossberg, Stephen"
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13414-019-01789-2",
      "keywords": [],
      "number_of_pages": 28,
      "pages": "2237-2264",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "19433921",
        "publisher": "Springer US",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Attention, Perception, and Psychophysics"
      },
      "publication_date": "2019-10-01",
      "selected": null,
      "title": "The resonant brain: How attentive conscious seeing regulates action sequences that interact with attentive cognitive learning, recognition, and prediction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068195245&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13414-019-01789-2.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Weism\u00fcller B."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.13428",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2019-10-01",
      "selected": null,
      "title": "Effects of feedback delay and agency on feedback-locked beta and theta power during reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068134631&origin=inward"
      ]
    },
    {
      "abstract": "Humans generally teach their fellow collaborators to perform tasks through a small number of demonstrations, often followed by episodes of coaching that tune and refine the execution during practice. Adopting a similar framework for teaching robots through demonstrations makes teaching tasks highly intuitive and imitating the refinement of complex tasks through coaching improves the efficacy. Unlike traditional Learning from Demonstration (LfD) approaches which rely on multiple demonstrations to train a task, we present a novel one-shot learning from demonstration approach, augmented by coaching, to transfer the task from task expert to robot. The demonstration is automatically segmented into a sequence of a priori skills (the task policy) parametrized to match task goals. During practice, the robotic skills self-evaluate their performances and refine the task policy to locally optimize cumulative performance. Then, human coaching further refines the task policy to explore and globally optimize the net performance. Both the self-evaluation and coaching are implemented using reinforcement learning (RL) methods. The proposed approach is evaluated using the task of scooping and unscooping granular media. The self-evaluator of the scooping skill uses the realtime force signature and resistive force theory to minimize scooping resistance similar to how humans scoop. Coaching feedback focuses modifications to sub-domains of the action space, using RL to converge to desired performance. Thus, the proposed method provides a framework for learning tasks from one demonstration and generalizing it using human feedback through coaching achieving a success rate of &#x2248;90&#x0025;.",
      "authors": [
        "Mythra V. Balakuntala",
        "Vishnunandan L. N. Venkatesh",
        "Jyothsna Padmakumar Bindu",
        "Richard M. Voyles",
        "Juan Wachs"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/RO-MAN46459.2019.8956364",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "1-7",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 28th IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2019"
      },
      "publication_date": "2019-10-01",
      "selected": null,
      "title": "Extending Policy from One-Shot Learning through Coaching",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8956364",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078858892&origin=inward",
        "https://dl.acm.org/doi/10.1109/RO-MAN46459.2019.8956364"
      ]
    },
    {
      "abstract": "Flexible needle insertion with bevel tips is becoming a preferred method for approaching targets in the human body in the least invasive manner. However, to successfully implement needle insertion, surgeons require prolonged training processes and long-term experience to develop essential handling skills. This paper presents a new path planning approach with Deep Reinforcement Learning (DRL) to implement automatic needle insertion using a surgical robot. In this paper, Deep Q-Network (DQN) algorithm is utilized to learn the control policy for flexible needle steering with needle-tissue interaction. As the human body is composed of a complex environment such as tissues, blood vessels, bones, and muscles, the uncertainty of the needle-tissue interaction should be considered during insertion. To model this complex interaction in path planning, utilizing a neural network to approximate the action-value function is more efficient than using traditional array methods in terms of time and accuracy. In our simulation, the agent (needle) can be controlled with 2 degrees of freedom (bevel direction rotation and insertion) and received negative rewards when it collides with obstacles, goes out of range, or exceeds a predefined number of rotations. During the training, the agent demonstrates the accuracy and efficiency of the learned policy through feedback scores in every episode. In addition, this system incorporates the uncertainty within flexible needle-tissue interaction using a stochastic environment. Compared with other traditional methods for flexible needle path planning, we demonstrated that motion planning of bevel-tip flexible needles in complex human bodies using DRL has better efficiency and accuracy.",
      "authors": [
        "Yonggu Lee",
        "Xiaoyu Tan",
        "Chin-Boon Chng",
        "Chee-Kong Chui"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/SMC.2019.8914191",
      "keywords": [
        "Path Planning",
        "Deep Learning in Robotics and Automation",
        "Surgical Robotics: Deep Reinforcement Learning",
        "AI Based Methods",
        "Flexible Needle Insertion"
      ],
      "number_of_pages": 5,
      "pages": "342-346",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3703-7",
        "issn": "1062-922X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics"
      },
      "publication_date": "2019-10-01",
      "selected": null,
      "title": "Simulation of Robot-Assisted Flexible Needle Insertion Using Deep Q-Network",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8914191",
        "https://dl.acm.org/doi/10.1109/SMC.2019.8914191",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076730100&origin=inward"
      ]
    },
    {
      "abstract": "               Deep reinforcement learning (DRL) has emerged as the dominant approach to achieving successive advancements in the creation of human-wise agents. By leveraging neural networks as decision-making controllers, DRL supplements traditional reinforcement methods to address the curse of dimensionality in complicated tasks. However, agents in complicated environments are likely to get stuck in sub-optimal solutions. In such cases, the agent inadvertently turns into a \u201czombie\u201d owing to its short-term vision and harmful behaviors. In this study, we use human learning strategies to adjust agent behaviors in high-dimensional environments. Therefore, the agent behaves predictably and succeeds in attaining its designated goal. In summary, the contribution of this study is two-fold. First, we introduce a lightweight workflow that enables a nonexpert to preserve a certain level of safety in AI systems. Specifically, the workflow involves a novel concept of a target map and a multi-agent behavioral control system named Multi-Policy Control System (MPCS). MPCS successfully controls agent behaviors in real time without involving the burden of human feedback. Second, we develop a multi-agent game named Tank Battle that provides a configurable environment to examine agent behaviors and human-agent interactions in DRL. Finally, simulation results show that agents guided by MPCS outperform agents that do not use MPCS with respect to the mean of total rewards and human-like behaviors in complicated environments such as Seaquest and Tank Battle.",
      "authors": [
        "Ngoc Duy Nguyen",
        "Thanh Nguyen",
        "Saeid Nahavandi"
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.neucom.2019.05.062",
      "keywords": [
        "Robotics",
        "Autonomous system",
        "Multi-agent system",
        "Deep learning",
        "Neural network",
        "Human control",
        "Reinforcement learning",
        "Agent behavior",
        "Human-machine interaction"
      ],
      "number_of_pages": 11,
      "pages": "58-68",
      "publication": {
        "category": "Journal",
        "cite_score": 10.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0925-2312",
        "publisher": "Elsevier B.V.",
        "sjr": 1.481,
        "snip": 1.853,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neurocomputing"
      },
      "publication_date": "2019-09-24",
      "selected": null,
      "title": "Multi-agent behavioral control system using deep reinforcement learning",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.neucom.2019.05.062",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066428189&origin=inward"
      ]
    },
    {
      "abstract": "Recent successes of Reinforcement Learning (RL) allow an agent to learn policies that surpass human experts but suffers from being time-hungry and data-hungry. By contrast, human learning is significantly faster because prior and general knowledge and multiple information resources are utilized. In this paper, we propose a Planner-Actor-Critic architecture for huMAN-centered planning and learning (PACMAN), where an agent uses its prior, high-level, deterministic symbolic knowledge to plan for goal-directed actions, and also integrates the Actor-Critic algorithm of RL to fine-tune its behavior towards both environmental rewards and human feedback. This work is the first unified framework where knowledge-based planning, RL, and human teaching jointly contribute to the policy learning of an agent. Our experiments demonstrate that PACMAN leads to a significant jump-start at the early stage of learning, converges rapidly and with small variance, and is robust to inconsistent, infrequent, and misleading feedback.",
      "authors": [
        "Lyu, Daoming",
        "Yang, Fangkai",
        "Liu, Bo",
        "Gustafson, Steven"
      ],
      "categories": null,
      "citations": 2,
      "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note:\n  significant text overlap with arXiv:1906.07268",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.4204/EPTCS.306.23",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "182-195",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 1.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "20752180",
        "publisher": "Open Publishing Association",
        "sjr": 0.349,
        "snip": 0.556,
        "subject_areas": [
          "Artificial Intelligence",
          "Software",
          "Learning",
          "Human-Computer Interaction",
          "Logic in Computer Science"
        ],
        "title": "Electronic Proceedings in Theoretical Computer Science, EPTCS"
      },
      "publication_date": "2019-09-18",
      "selected": null,
      "title": "A Human-Centered Data-Driven Planner-Actor-Critic Architecture via Logic Programming",
      "urls": [
        "http://arxiv.org/abs/1909.09209v1",
        "http://arxiv.org/pdf/1909.09209v1",
        "http://arxiv.org/pdf/1909.09209.pdf",
        "http://dx.doi.org/10.4204/EPTCS.306.23",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074747357&origin=inward"
      ]
    },
    {
      "abstract": "We investigated the application of haptic feedback control and deep reinforcement learning (DRL) to robot-assisted dressing. Our method uses DRL to simultaneously train human and robot control policies as separate neural networks using physics simulations. In addition, we modeled variations in human impairments relevant to dressing, including unilateral muscle weakness, involuntary arm motion, and limited range of motion. Our approach resulted in control policies that successfully collaborate in a variety of simulated dressing tasks involving a hospital gown and a T-shirt. In addition, our approach resulted in policies trained in simulation that enabled a real PR2 robot to dress the arm of a humanoid robot with a hospital gown. We found that training policies for specific impairments dramatically improved performance; that controller execution speed could be scaled after training to reduce the robot's speed without steep reductions in performance; that curriculum learning could be used to lower applied forces; and that multi-modal sensing, including a simulated capacitive sensor, improved performance.",
      "authors": [
        "Clegg, Alexander",
        "Erickson, Zackory",
        "Grady, Patrick",
        "Turk, Greg",
        "Kemp, Charles C.",
        "Liu, C. Karen"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 8 figures, 3 tables; simulation to reality experiment added\n  to evaluation; authors added; modified: title, abstract, conclusion,\n  references; figure added",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-09-14",
      "selected": null,
      "title": "Learning to Collaborate from Simulation for Robot-Assisted Dressing",
      "urls": [
        "http://arxiv.org/pdf/1909.06682v2",
        "http://arxiv.org/pdf/1909.06682.pdf",
        "http://arxiv.org/abs/1909.06682v2"
      ]
    },
    {
      "abstract": "One typical assumption in inverse reinforcement learning (IRL) is that human experts act to optimize the expected utility of a stochastic cost with a fixed distribution. This assumption deviates from actual human behaviors under ambiguity. Risk-sensitive inverse reinforcement learning (RS-IRL) bridges such gap by assuming that humans act according to a random cost with respect to a set of subjectively distorted distributions instead of a fixed one. Such assumption provides the additional flexibility to model human's risk preferences, represented by a risk envelope, in safe-critical tasks. However, like other learning from demonstration techniques, RS-IRL could also suffer inefficient learning due to redundant demonstrations. Inspired by the concept of active learning, this research derives a probabilistic disturbance sampling scheme to enable an RS-IRL agent to query expert support that is likely to expose unrevealed boundaries of the expert's risk envelope. Experimental results confirm that our approach accelerates the convergence of RS-IRL algorithms with lower variance while still guaranteeing unbiased convergence.",
      "authors": [
        "Chen, Rui",
        "Wang, Wenshuo",
        "Zhao, Zirui",
        "Zhao, Ding"
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages without acknowledgment, 7 figures, submitted to RA-L and ICRA\n  2020 for the IEEE Robotics and Automation Letters (RA-L)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-09-14",
      "selected": null,
      "title": "Active Learning for Risk-Sensitive Inverse Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/1909.07843v2",
        "http://arxiv.org/pdf/1909.07843v2",
        "http://arxiv.org/pdf/1909.07843.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang D."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2019.04.026",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "1-7",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2019-09-01",
      "selected": null,
      "title": "Approximate neural optimal control with reinforcement learning for a torsional pendulum device",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065908159&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ni Z."
      ],
      "categories": null,
      "citations": 132,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2018.2885530",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "2684-2695",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE transactions on neural networks and learning systems"
      },
      "publication_date": "2019-09-01",
      "selected": null,
      "title": "A Multistage Game in Smart Grid Security: A Reinforcement Learning Solution",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071439725&origin=inward"
      ]
    },
    {
      "abstract": "In this paper we present a control synthesis framework for a multi-agent system under hard and soft constraints, which performs online re-planning to achieve collision avoidance and execution of the optimal path with respect to some human preference considering the type of the violation of the soft constraints. The human preference is indicated by a mixed initiative controller and the resulting change of trajectory is used by an inverse reinforcement learning based algorithm to improve the path which the affected agent tries to follow. A case study is presented to validate the result.",
      "authors": [
        "Sofie Ahlberg",
        "Dimos V. Dimarogonas"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/COASE.2019.8842954",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "788-793",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-0357-0",
        "issn": "2161-8070",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)"
      },
      "publication_date": "2019-08-22",
      "selected": null,
      "title": "Human-in-the-Loop Control Synthesis for Multi-Agent Systems under Hard and Soft Metric Interval Temporal Logic Specifications&lt;sup&gt;&amp;#x002A;&lt;/sup&gt;",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072955571&origin=inward",
        "https://dl.acm.org/doi/10.1109/COASE.2019.8842954",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8842954"
      ]
    },
    {
      "abstract": "Robotic Chinese calligraphy is an attempt to lead robots to learn mankind's culture and knowledge. The current research on robotic calligraphy ignores the usage of human preferences. This has restricted robots to produce writing results reflecting personalized styles. This paper proposes a robotic learning approach that introduces a inverse reinforcement learning algorithm with human preferences into a robotic writing system. Through selections of human users, the robot system learns to write Chinese character strokes according to the user's aesthetic preference. Thus, the paper first uses a generative network adopting from the Generative Adversarial Nets to produce a basic writing ability of Chinese strokes for a robot system. Then, the writing results of the robot are captured by the robot's visual device and then presented to the human users as images. Then, the human users give their preferences as the feedbacks of the images, the approach uses the marked images to train a reward predictive mechanism. In the end, the reward predictive mechanism aids the inverse reinforcement learning algorithm to enable the robot to automatically improve its writing ability of Chinese character strokes. Experimental results show that the proposed framework can successfully allow the robot to write Chinese characters strokes in accordance with the human user's preference. In addition, the robot demonstrates a fast learning speed with a small number of human selections. This gives a very promising solution to the robot's learning of more complex movements.",
      "authors": [
        "Fei Chao",
        "Jitu Lyu",
        "Ruiqi Wu",
        "Xingen Gao",
        "Changle Zhou",
        "Longzhi Yang",
        "Chih-Min Lin",
        "Changjing Shang"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00105",
      "keywords": [
        "inverse reinforcement learning",
        "human-robot interaction",
        "robotic calligraphy"
      ],
      "number_of_pages": 7,
      "pages": "360-366",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-4035-3",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)"
      },
      "publication_date": "2019-08-19",
      "selected": null,
      "title": "Robotic Chinese Calligraphy with Human Preference",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9060257",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083579949&origin=inward"
      ]
    },
    {
      "abstract": "Prior work on prosocial and self-serving behavior in human economic exchanges has shown that counterparts\u2019 high social reputations bias striatal reward signals and elicit cooperation, even when such cooperation is disadvantageous. This phenomenon suggests that the human striatum is modulated by the other\u2019s social value, which is insensitive to the individual\u2019s own choices to cooperate or defect. We tested an alternative hypothesis that, when people learn from their interactions with others, they encode prediction error updates with respect to their own policy. Under this policy update account striatal signals would reflect positive prediction errors when the individual\u2019s choices correctly anticipated not only the counterpart\u2019s cooperation but also defection. We examined behavior in three samples using reinforcement learning and model-free analyses and performed an fMRI study of striatal learning signals. In order to uncover the dynamics of goal-directed learning, we introduced reversals in the counterpart\u2019s behavior and provided counterfactual (would-be) feedback when the individual chose not to engage with the counterpart. Behavioral data and model-derived prediction error maps (in both whole-brain and a priori striatal region of interest analyses) supported the policy update model. Thus, as people continually adjust their rate of cooperation based on experience, their behavior and striatal learning signals reveal a self-centered instrumental process corresponding to reciprocal altruism.",
      "authors": [
        "Vanyukov, Polina M.",
        "Hallquist, Michael N.",
        "Delgado, Mauricio",
        "Szanto, Katalin",
        "Dombrovski, Alexandre Y."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-019-00697-0",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "985-997",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2019-08-15",
      "selected": null,
      "title": "Neurocomputational mechanisms of adaptive learning in social exchanges",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-019-00697-0.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061508340&origin=inward"
      ]
    },
    {
      "abstract": "Deep Reinforcement Learning (DRL) has become a powerful methodology to solve complex decision-making problems. However, DRL has several limitations when used in real-world problems (e.g., robotics applications). For instance, long training times are required and cannot be accelerated in contrast to simulated environments, and reward functions may be hard to specify/model and/or to compute. Moreover, the transfer of policies learned in a simulator to the real-world has limitations (reality gap). On the other hand, machine learning methods that rely on the transfer of human knowledge to an agent have shown to be time efficient for obtaining well performing policies and do not require a reward function. In this context, we analyze the use of human corrective feedback during task execution to learn policies with high-dimensional state spaces, by using the D-COACH framework, and we propose new variants of this framework. D-COACH is a Deep Learning based extension of COACH (COrrective Advice Communicated by Humans), where humans are able to shape policies through corrective advice. The enhanced version of D-COACH, which is proposed in this paper, largely reduces the time and effort of a human for training a policy. Experimental results validate the efficiency of the D-COACH framework in three different problems (simulated and with real robots), and show that its enhanced version reduces the human training effort considerably, and makes it feasible to learn policies within periods of time in which a DRL agent do not reach any improvement.",
      "authors": [
        "P\u00e9rez-Dattari, Rodrigo",
        "Celemin, Carlos",
        "Ruiz-del-Solar, Javier",
        "Kober, Jens"
      ],
      "categories": null,
      "citations": null,
      "comments": "7 pages, 8 figures, IEEE International Conference on Robotics and\n  Automation (ICRA 2019)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-08-14",
      "selected": null,
      "title": "Continuous Control for High-Dimensional State Spaces: An Interactive Learning Approach",
      "urls": [
        "http://arxiv.org/abs/1908.05256v1",
        "http://arxiv.org/pdf/1908.05256.pdf",
        "http://arxiv.org/pdf/1908.05256v1"
      ]
    },
    {
      "abstract": "Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",
      "authors": [
        "Frazier, Spencer",
        "Riedl, Mark"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "146-152",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358190",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 15th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2019"
      },
      "publication_date": "2019-08-02",
      "selected": null,
      "title": "Improving Deep Reinforcement Learning in Minecraft with Action Advice",
      "urls": [
        "http://arxiv.org/pdf/1908.01007v1",
        "http://arxiv.org/pdf/1908.01007.pdf",
        "http://arxiv.org/abs/1908.01007v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094393807&origin=inward"
      ]
    },
    {
      "abstract": "Visual paragraph generation aims to automatically describe a given image from different perspectives and organize sentences in a coherent way. In this paper, we address three critical challenges for this task in a reinforcement learning setting: the mode collapse, the delayed feedback, and the time-consuming warm-up for policy networks. Generally, we propose a novel Curiosity-driven Reinforcement Learning (CRL) framework to jointly enhance the diversity and accuracy of the generated paragraphs. First, by modeling the paragraph captioning as a long-term decision-making process and measuring the prediction uncertainty of state transitions as intrinsic rewards, the model is incentivized to memorize precise but rarely spotted descriptions to context, rather than being biased towards frequent fragments and generic patterns. Second, since the extrinsic reward from evaluation is only available until the complete paragraph is generated, we estimate its expected value at each time step with temporal-difference learning, by considering the correlations between successive actions. Then the estimated extrinsic rewards are complemented by dense intrinsic rewards produced from the derived curiosity module, in order to encourage the policy to fully explore action space and find a global optimum. Third, discounted imitation learning is integrated for learning from human demonstrations, without separately performing the time-consuming warm-up in advance. Extensive experiments conducted on the Standford image-paragraph dataset demonstrate the effectiveness and efficiency of the proposed method, improving the performance by 38.4% compared with state-of-the-art.",
      "authors": [
        "Luo, Yadan",
        "Huang, Zi",
        "Zhang, Zheng",
        "Wang, Ziwei",
        "Li, Jingjing",
        "Yang, Yang"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-08-01",
      "selected": null,
      "title": "Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation",
      "urls": [
        "http://arxiv.org/pdf/1908.00169v2",
        "http://arxiv.org/pdf/1908.00169.pdf",
        "http://arxiv.org/abs/1908.00169v2"
      ]
    },
    {
      "abstract": "Drug addiction has been suggested to develop through drug-induced changes in learning and memory processes. Whilst the initiation of drug use is typically goal-directed and hedonically motivated, over time, drug-taking may develop into a stimulus-driven habit, characterised by persistent use of the drug irrespective of the consequences. Converging lines of evidence suggest that stimulant drugs facilitate the transition of goal-directed into habitual drug-taking, but their contribution to goal-directed learning is less clear. Computational modelling may provide an elegant means for elucidating changes during instrumental learning that may explain enhanced habit formation. We used formal reinforcement learning algorithms to deconstruct the process of appetitive instrumental learning and to explore potential associations between goal-directed and habitual actions in patients with cocaine use disorder (CUD). We re-analysed appetitive instrumental learning data in 55 healthy control volunteers and 70 CUD patients by applying a reinforcement learning model within a hierarchical Bayesian framework. We used a regression model to determine the influence of learning parameters and variations in brain structure on subsequent habit formation. Poor instrumental learning performance in CUD patients was largely determined by difficulties with learning from feedback, as reflected by a significantly reduced learning rate. Subsequent formation of habitual response patterns was partly explained by group status and individual variation in reinforcement sensitivity. White matter integrity within goal-directed networks was only associated with performance parameters in controls but not in CUD patients. Our data indicate that impairments in reinforcement learning are insufficient to account for enhanced habitual responding in CUD.",
      "authors": [
        "Lim, T. V.",
        "Cardinal, R. N.",
        "Savulich, G.",
        "Jones, P. S.",
        "Moustafa, A. A.",
        "Robbins, T. W.",
        "Ersche, K. D."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00213-019-05330-z",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "2359-2371",
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00333158",
        "publisher": "Springer Verlag",
        "sjr": 1.05,
        "snip": 0.934,
        "subject_areas": [
          "Pharmacology"
        ],
        "title": "Psychopharmacology"
      },
      "publication_date": "2019-08-01",
      "selected": null,
      "title": "Impairments in reinforcement learning do not explain enhanced habit formation in cocaine use disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068837045&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s00213-019-05330-z.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ashley D.R."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/CIG.2019.8848114",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9798350322774",
        "issn": "23254270",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Conference on Computatonal Intelligence and Games, CIG"
      },
      "publication_date": "2019-08-01",
      "selected": null,
      "title": "Learning to select mates in evolving non-playable characters",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073105109&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Harmon Z."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cognition.2019.03.011",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "76-88",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00100277",
        "publisher": "Elsevier B.V.",
        "sjr": 1.691,
        "snip": 1.813,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Cognitive Neuroscience",
          "Language and Linguistics",
          "Linguistics and Language"
        ],
        "title": "Cognition"
      },
      "publication_date": "2019-08-01",
      "selected": null,
      "title": "Learning mechanisms in cue reweighting",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063505009&origin=inward"
      ]
    },
    {
      "abstract": "Pairing rewarding outcomes with audiovisual cues in simulated gambling games increases risky choice in both humans and rats. However, the cognitive mechanism through which this sensory enhancement biases decision-making is unknown. To assess the computational mechanisms that promote risky choice during gambling, we applied a series of reinforcement learning models to a large dataset of choices acquired from rats as they each performed one of two variants of a rat gambling task (rGT), in which rewards on \u201cwin\u201d trials were delivered either with or without salient audiovisual cues. We used a sampling technique based on Markov chain Monte Carlo to obtain posterior estimates of model parameters for a series of RL models of increasing complexity, in order to assess the relative contribution of learning about positive and negative outcomes to the latent valuation of each choice option on the cued and uncued rGT. Rats which develop a preference for the risky options on the rGT substantially down-weight the equivalent cost of the time-out punishments during these tasks. For each model tested, the reduction in learning from the negative time-outs correlated with the degree of risk preference in individual rats. We found no apparent relationship between risk preference and the parameters that govern learning from the positive rewards. The emergence of risk-preferring choice on the rGT derives from a relative insensitivity to the cost of the time-out punishments, as opposed to a relative hypersensitivity to rewards. This hyposensitivity to punishment is more likely to be induced in individual rats by the addition of salient audiovisual cues to rewards delivered on win trials.",
      "authors": [
        "Langdon, Angela J.",
        "Hathaway, Brett A.",
        "Zorowitz, Samuel",
        "Harris, Cailean B. W.",
        "Winstanley, Catharine A."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00213-019-05308-x",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "2543-2556",
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00333158",
        "publisher": "Springer Verlag",
        "sjr": 1.05,
        "snip": 0.934,
        "subject_areas": [
          "Pharmacology"
        ],
        "title": "Psychopharmacology"
      },
      "publication_date": "2019-08-01",
      "selected": null,
      "title": "Relative insensitivity to time-out punishments induced by win-paired cues in a rat gambling task",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00213-019-05308-x.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068311314&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Uehara S."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00390.2018",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "797-808",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2019-08-01",
      "selected": null,
      "title": "Interactions between motor exploration and reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071353863&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang J."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuropsychologia.2019.06.002",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "266-274",
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00283932",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.995,
        "snip": 1.03,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neuropsychologia"
      },
      "publication_date": "2019-08-01",
      "selected": null,
      "title": "Beta-gamma oscillation reveals learning from unexpected reward in learners versus non-learners",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067062898&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Queirazza F."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1126/sciadv.aav4962",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Science Advances"
      },
      "publication_date": "2019-07-31",
      "selected": null,
      "title": "Neural correlates of weighted reward prediction error during reinforcement learning classify response to cognitive behavioral therapy in depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072078453&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhao S."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neucom.2019.03.029",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "170-180",
      "publication": {
        "category": "Journal",
        "cite_score": 10.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09252312",
        "publisher": "Elsevier B.V.",
        "sjr": 1.481,
        "snip": 1.853,
        "subject_areas": [
          "Computer Science Applications",
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neurocomputing"
      },
      "publication_date": "2019-07-20",
      "selected": null,
      "title": "Observer-based adaptive neural optimal control for discrete-time systems in nonstrict-feedback form",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064656796&origin=inward"
      ]
    },
    {
      "abstract": "Recently, collaborative robots have begun to train humans to achieve complex tasks, and the mutual information exchange between them can lead to successful robot-human collaborations. In this paper we demonstrate the application and effectiveness of a new approach called mutual reinforcement learning (MRL), where both humans and autonomous agents act as reinforcement learners in a skill transfer scenario over continuous communication and feedback. An autonomous agent initially acts as an instructor who can teach a novice human participant complex skills using the MRL strategy. While teaching skills in a physical (block-building) ($n=34$) or simulated (Tetris) environment ($n=31$), the expert tries to identify appropriate reward channels preferred by each individual and adapts itself accordingly using an exploration-exploitation strategy. These reward channel preferences can identify important behaviors of the human participants, because they may well exercise the same behaviors in similar situations later. In this way, skill transfer takes place between an expert system and a novice human operator. We divided the subject population into three groups and observed the skill transfer phenomenon, analyzing it with Simpson\"s psychometric model. 5-point Likert scales were also used to identify the cognitive models of the human participants. We obtained a shared cognitive model which not only improves human cognition but enhances the robot's cognitive strategy to understand the mental model of its human partners while building a successful robot-human collaborative framework.",
      "authors": [
        "Roy, Sayanti",
        "Kieson, Emily",
        "Abramson, Charles",
        "Crick, Christopher"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-07-15",
      "selected": null,
      "title": "Mutual Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/1907.06725.pdf",
        "http://arxiv.org/abs/1907.06725v3",
        "http://arxiv.org/pdf/1907.06725v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Smith R.J."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3321707.3321866",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "179-187",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450361118",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "GECCO 2019 - Proceedings of the 2019 Genetic and Evolutionary Computation Conference"
      },
      "publication_date": "2019-07-13",
      "selected": null,
      "title": "Evolving dota 2 shadow fiend bots using genetic programming with external memory",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072319911&origin=inward"
      ]
    },
    {
      "abstract": "The performance of imitation learning is typically upper-bounded by the performance of the demonstrator. While recent empirical results demonstrate that ranked demonstrations allow for better-than-demonstrator performance, preferences over demonstrations may be difficult to obtain, and little is known theoretically about when such methods can be expected to successfully extrapolate beyond the performance of the demonstrator. To address these issues, we first contribute a sufficient condition for better-than-demonstrator imitation learning and provide theoretical results showing why preferences over demonstrations can better reduce reward function ambiguity when performing inverse reinforcement learning. Building on this theory, we introduce Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation learning method that injects noise into a policy learned through behavioral cloning to automatically generate ranked demonstrations. These ranked demonstrations are used to efficiently learn a reward function that can then be optimized using reinforcement learning. We empirically validate our approach on simulated robot and Atari imitation learning benchmarks and show that D-REX outperforms standard imitation learning approaches and can significantly surpass the performance of the demonstrator. D-REX is the first imitation learning approach to achieve significant extrapolation beyond the demonstrator's performance without additional side-information or supervision, such as rewards or human preferences. By generating rankings automatically, we show that preference-based inverse reinforcement learning can be applied in traditional imitation learning settings where only unlabeled demonstrations are available.",
      "authors": [
        "Brown, Daniel S.",
        "Goo, Wonjoon",
        "Niekum, Scott"
      ],
      "categories": null,
      "citations": 67,
      "comments": "In proceedings of 3rd Conference on Robot Learning (CoRL) 2019",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 30,
      "pages": "330-359",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2019-07-09",
      "selected": null,
      "title": "Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations",
      "urls": [
        "http://arxiv.org/pdf/1907.03976.pdf",
        "http://arxiv.org/pdf/1907.03976v3",
        "http://arxiv.org/abs/1907.03976v3",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85142702287&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Imani M."
      ],
      "categories": null,
      "citations": 42,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TCBB.2018.2830357",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1250-1261",
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15455963",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 0.794,
        "snip": 1.17,
        "subject_areas": [
          "Biotechnology",
          "Genetics",
          "Applied Mathematics"
        ],
        "title": "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
      },
      "publication_date": "2019-07-01",
      "selected": null,
      "title": "Control of gene regulatory networks using bayesian inverse reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046001548&origin=inward"
      ]
    },
    {
      "abstract": "This paper presents a hybrid model consisting of fuzzy ARTMAP (FAM) and reinforcement learning (RL) for tackling data classification problems. RL is used as a feedback mechanism to reward the prototype nodes of data samples established by FAM. Specifically, Q-learning is adopted to develop the hybrid model known as QFAM. A Q-value is assigned to each prototype node, which is updated incrementally based on the prediction accuracy of the node pertaining to each data sample. To evaluate the performance of the proposed QFAM model, a series of experiments with benchmark problems and a real-world case study, i.e., human motion recognition, are conducted. The bootstrap method is used to quantify the results with the 95% confidence interval estimates. The results are also compared with those from FAM as well as other models reported in the literature. The outcomes indicate the effectiveness of QFAM in tackling data classification tasks.",
      "authors": [
        "Pourpanah, Farhad",
        "Lim, Chee Peng",
        "Hao, Qi"
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s13042-018-0843-4",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "1643-1655",
      "publication": {
        "category": "Journal",
        "cite_score": 8.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18688071",
        "publisher": "Springer Science + Business Media",
        "sjr": 1.002,
        "snip": 1.37,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Computer Vision and Pattern Recognition"
        ],
        "title": "International Journal of Machine Learning and Cybernetics"
      },
      "publication_date": "2019-07-01",
      "selected": null,
      "title": "A reinforced fuzzy ARTMAP model for data classification",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s13042-018-0843-4.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067185340&origin=inward"
      ]
    },
    {
      "abstract": "The combination of first principles modelling (e.g. multiphysics) and machine learning (e.g. Reinforcement Learning) represents a new powerful tool for in-silico modelling of human physiology. Biological feedback loops occurring, for instance, in peristaltic or metachronal motion, which until now could not be accounted for in in-silico models, can be tackled by the proposed technique.",
      "authors": [
        "Alessio Alexiadis"
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.artmed.2019.06.005",
      "keywords": [
        "Discrete multiphysics",
        "Particle-based computational methods",
        "Coupling first-principles models with machine learning",
        "Reinforcement Learning"
      ],
      "number_of_pages": 8,
      "pages": "27-34",
      "publication": {
        "category": "Journal",
        "cite_score": 14.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0933-3657",
        "publisher": "Elsevier B.V.",
        "sjr": 1.443,
        "snip": 2.22,
        "subject_areas": [
          "Artificial Intelligence",
          "Medicine (miscellaneous)"
        ],
        "title": "Artificial Intelligence in Medicine"
      },
      "publication_date": "2019-07-01",
      "selected": null,
      "title": "Deep multiphysics: Coupling discrete multiphysics with machine learning to attain self-learning in-silico models replicating human physiology",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.artmed.2019.06.005",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85068525912&origin=inward"
      ]
    },
    {
      "abstract": "Some imitation learning approaches rely on Inverse Reinforcement Learning (IRL) methods, to decode and generalize implicit goals given by expert demonstrations. The study of IRL normally has the assumption of available expert demonstrations, which is not always possible. There are Machine Learning methods that allow non-expert teachers to guide robots to learn complex policies, which eventually fills the expert dependencies of IRL. This work introduces an approach for simultaneously teaching robot policies and objective functions from vague human corrective feedback. The main goal is to generalize the insights that a non-expert human teacher provides to the robot, to unseen conditions, without further need for human effort in the complementary training process. We present an experimental validation of the introduced approach for transfer learning of knowledge to scenarios not considered while the non-expert was teaching. Experimental results show that the learned reward functions obtain similar performance in RL processes compared to engineered reward functions used as baseline, both in simulated and real environments.",
      "authors": [
        "Carlos Celemin",
        "Jens Kober"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/AIM.2019.8868805",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "726-732",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-2494-0",
        "issn": "2159-6247",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)"
      },
      "publication_date": "2019-07-01",
      "selected": null,
      "title": "Simultaneous Learning of Objective Function and Policy from Interactive Teaching with Corrective Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8868805",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074267290&origin=inward",
        "https://dl.acm.org/doi/10.1109/AIM.2019.8868805"
      ]
    },
    {
      "abstract": "Increasingly, autonomous agents will be required to operate on long-term missions. This will create a demand for general intelligence because feedback from a human operator may be sparse and delayed, and because not all behaviours can be prescribed. Deep neural networks and reinforcement learning methods can be applied in such environments but their fixed updating routines imply an inductive bias in learning spatio-temporal patterns, meaning some environments will be unsolvable. To address this problem, this paper proposes active adaptive perception, the ability of an architecture to learn when and how to modify and selectively utilise its perception module. To achieve this, a generic architecture based on a self-modifying policy (SMP) is proposed, and implemented using Incremental Self-improvement with the Success Story Algorithm. The architecture contrasts to deep reinforcement learning systems which follow fixed training strategies and earlier SMP studies which for perception relied either entirely on the working memory or on untrainable active perception instructions. One computationally cheap and one more expensive implementation are presented and compared to DRQN, an off-policy deep reinforcement learner using experience replay and Incremental Self-improvement, an SMP, on various non-episodic partially observable mazes. The results show that the simple instruction set leads to emergent strategies to avoid detracting corridors and rooms, and that the expensive implementation allows selectively ignoring perception where it is inaccurate.",
      "authors": [
        "D.M. Bossens",
        "N.C. Townsend",
        "A.J. Sobey"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.neunet.2019.03.006",
      "keywords": [
        "Self-modifying policies",
        "Adaptive perception",
        "Partial observability",
        "Inductive bias",
        "Reinforcement learning"
      ],
      "number_of_pages": 20,
      "pages": "30-49",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0893-6080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2019-07-01",
      "selected": null,
      "title": "Learning to learn with active adaptive perception",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.neunet.2019.03.006",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064274085&origin=inward"
      ]
    },
    {
      "abstract": "The main goal of this article is to present COACH (COrrective Advice Communicated by Humans), a new learning framework that allows non-expert humans to advise an agent while it interacts with the environment in continuous action problems. The human feedback is given in the action domain as binary corrective signals (increase/decrease the current action magnitude), and COACH is able to adjust the amount of correction that a given action receives adaptively, taking state-dependent past feedback into consideration. COACH also manages the credit assignment problem that normally arises when actions in continuous time receive delayed corrections. The proposed framework is characterized and validated extensively using four well-known learning problems. The experimental analysis includes comparisons with other interactive learning frameworks, with classical reinforcement learning approaches, and with human teleoperators trying to solve the same learning problems by themselves. In all the reported experiments COACH outperforms the other methods in terms of learning speed and final performance. It is of interest to add that COACH has been applied successfully for addressing a complex real-world learning problem: the dribbling of the ball by humanoid soccer players.",
      "authors": [
        "Celemin, Carlos",
        "Ruiz-del-Solar, Javier"
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10846-018-0839-z",
      "keywords": [
        "Decision making systems",
        "Human teachers",
        "Human feedback",
        "Learning from demonstration",
        "Interactive machine learning"
      ],
      "number_of_pages": 21,
      "pages": "77-97",
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0921-0296",
        "publisher": "Springer Netherlands",
        "sjr": 0.861,
        "snip": 1.437,
        "subject_areas": [
          "Mechanical Engineering",
          "Artificial Intelligence",
          "Software",
          "Industrial and Manufacturing Engineering",
          "Control and Systems Engineering",
          "Electrical and Electronic Engineering"
        ],
        "title": "Journal of Intelligent and Robotic Systems: Theory and Applications"
      },
      "publication_date": "2019-07-01",
      "selected": null,
      "title": "An Interactive Framework for Learning Continuous Actions Policies Based on Corrective Feedback",
      "urls": [
        "https://dl.acm.org/doi/10.1007/s10846-018-0839-z",
        "https://link.springer.com/content/pdf/10.1007/s10846-018-0839-z.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046824032&origin=inward"
      ]
    },
    {
      "abstract": "Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. These are critical shortcomings for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment -- e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms, which are able to effectively learn offline, without exploring, from a fixed batch of human interaction data. We leverage models pre-trained on data as a strong prior, and use KL-control to penalize divergence from this prior during RL training. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. The algorithms are tested on the problem of open-domain dialog generation -- a challenging reinforcement learning problem with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we can extract multiple different reward functions post-hoc from collected human interaction data, and learn effectively from all of these. We test the real-world generalization of these systems by deploying them live to converse with humans in an open-domain setting, and demonstrate that our algorithm achieves significant improvements over prior methods in off-policy batch RL.",
      "authors": [
        "Jaques, Natasha",
        "Ghandeharioun, Asma",
        "Shen, Judy Hanwen",
        "Ferguson, Craig",
        "Lapedriza, Agata",
        "Jones, Noah",
        "Gu, Shixiang",
        "Picard, Rosalind"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-06-30",
      "selected": null,
      "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog",
      "urls": [
        "http://arxiv.org/pdf/1907.00456v2",
        "http://arxiv.org/pdf/1907.00456.pdf",
        "http://arxiv.org/abs/1907.00456v2"
      ]
    },
    {
      "abstract": "Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for a reinforcement learning problem, which extends the standard Q-learning approach to incorporate a two-stream framework of reward processing with biases biologically associated with several neurological and psychiatric conditions, including Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. For AI community, the development of agents that react differently to different types of rewards can enable us to understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions and user preferences in long-term recommendation systems.",
      "authors": [
        "Lin, Baihan",
        "Bouneffouf, Djallel",
        "Cecchi, Guillermo"
      ],
      "categories": null,
      "citations": null,
      "comments": "IJCAI 2019. This article supersedes our work arXiv:1706.02897 into RL\n  setting, with a different focus by applying Inverse Reinforcement Learning to\n  model human clinical behavioral bias. It also precedes our work\n  arXiv:1906.11286 which introduces extensive emphases in RL games",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-06-21",
      "selected": null,
      "title": "Split Q Learning: Reinforcement Learning with Two-Stream Rewards",
      "urls": [
        "http://arxiv.org/pdf/1906.12350v2",
        "http://arxiv.org/abs/1906.12350v2",
        "http://arxiv.org/pdf/1906.12350.pdf"
      ]
    },
    {
      "abstract": "Conventional reinforcement learning (RL) allows an agent to learn policies via environmental rewards only, with a long and slow learning curve, especially at the beginning stage. On the contrary, human learning is usually much faster because prior and general knowledge and multiple information resources are utilized. In this paper, we propose a \\textbf{P}lanner-\\textbf{A}ctor-\\textbf{C}ritic architecture for hu\\textbf{MAN}-centered planning and learning (\\textbf{PACMAN}), where an agent uses prior, high-level, deterministic symbolic knowledge to plan for goal-directed actions. PACMAN integrates Actor-Critic algorithm of RL to fine-tune its behavior towards both environmental rewards and human feedback. To the best our knowledge, This is the first unified framework where knowledge-based planning, RL, and human teaching jointly contribute to the policy learning of an agent. Our experiments demonstrate that PACMAN leads to a significant jump-start at the early stage of learning, converges rapidly and with small variance, and is robust to inconsistent, infrequent, and misleading feedback.",
      "authors": [
        "Lyu, Daoming",
        "Yang, Fangkai",
        "Liu, Bo",
        "Gustafson, Steven"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-06-17",
      "selected": null,
      "title": "A Joint Planning and Learning Framework for Human-Aided Decision-Making",
      "urls": [
        "http://arxiv.org/pdf/1906.07268.pdf",
        "http://arxiv.org/abs/1906.07268v3",
        "http://arxiv.org/pdf/1906.07268v3"
      ]
    },
    {
      "abstract": "The characterization of an operator by its eigenvectors and eigenvalues\nallows us to know its action over any quantum state. Here, we propose a\nprotocol to obtain an approximation of the eigenvectors of an arbitrary\nHermitian quantum operator. This protocol is based on measurement and feedback\nprocesses, which characterize a reinforcement learning protocol. Our proposal\nis composed of two systems, a black box named environment and a quantum state\nnamed agent. The role of the environment is to change any quantum state by a\nunitary matrix $\\hat{U}_E=e^{-i\\tau\\hat{\\mathcal{O}}_E}$ where\n$\\hat{\\mathcal{O}}_E$ is a Hermitian operator, and $\\tau$ is a real parameter.\nThe agent is a quantum state which adapts to some eigenvector of\n$\\hat{\\mathcal{O}}_E$ by repeated interactions with the environment, feedback\nprocess, and semi-random rotations. With this proposal, we can obtain an\napproximation of the eigenvectors of a random qubit operator with average\nfidelity over 90\\% in less than 10 iterations, and surpass 98\\% in less than\n300 iterations. Moreover, for the two-qubit cases, the four eigenvectors are\nobtained with fidelities above 89\\% in 8000 iterations for a random operator,\nand fidelities of $99\\%$ for an operator with the Bell states as eigenvectors.\nThis protocol can be useful to implement semi-autonomous quantum devices which\nshould be capable of extracting information and deciding with minimal resources\nand without human intervention.",
      "authors": [
        "F. Albarr\u00e1n-Arriagada",
        "J. C. Retamal",
        "E. Solano",
        "L. Lamata"
      ],
      "categories": null,
      "citations": 15,
      "comments": "15 pages, 6 figures",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.1088/2632-2153/ab43b4",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Quantum Physics"
        ],
        "title": "Mach. Learn.: Sci. Technol. 1 015002 (2020)"
      },
      "publication_date": "2019-06-16",
      "selected": null,
      "title": "Reinforcement learning for semi-autonomous approximate quantum eigensolver",
      "urls": [
        "http://arxiv.org/pdf/1906.06702v3",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85095000329&origin=inward",
        "http://arxiv.org/abs/1906.06702v3",
        "http://dx.doi.org/10.1088/2632-2153/ab43b4"
      ]
    },
    {
      "abstract": "Major depression is characterized by abnormal reward processing and reinforcement learning (RL). This impairment might stem from deficient motivation processes, in addition to reduced reward sensitivity. In this study, we recorded 64-channel EEG in a large cohort of major depressive disorder (MDD) patients and matched healthy controls (HC) while they performed a standard RL task. Participants were asked to discover, by trial and error, several hidden stimulus-response associations having different reward probabilities, as enforced using evaluative feedback. We extracted induced fronto-midline Theta (FMT) power time-locked to the response and feedback as neurophysiological index of RL. Furthermore, we assessed approach-related motivation by measuring frontal alpha asymmetry concurrently. At the behavioral level, MDD patients and HCs showed comparable RL. At the EEG level, FMT power systematically varied as a function of reward probability, with opposing effects found at the response and feedback levels. Although this global pattern was spared in MDD, at the feedback level these patients showed however a steep FMT power decrease across trials when reward probability was low. Moreover, they showed impaired approach-related motivation during task execution, as reflected by frontal Alpha asymmetry. These results suggest a dissociation between (globally spared) RL and (impaired) approach motivation in MDD.",
      "authors": [
        "Gheza, Davide",
        "Bakic, Jasmina",
        "Baeken, Chris",
        "De Raedt, Rudi",
        "Pourtois, Gilles"
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-019-00693-4",
      "keywords": [],
      "number_of_pages": 19,
      "pages": "759-777",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2019-06-15",
      "selected": null,
      "title": "Abnormal approach-related motivation but spared reinforcement learning in MDD: Evidence from fronto-midline Theta oscillations and frontal Alpha asymmetry",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-019-00693-4.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060670893&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) models describe how humans and animals learn by trial-and-error to select actions that maximize rewards and minimize punishments. Traditional RL models focus exclusively on choices, thereby ignoring the interactions between choice preference and response time (RT), or how these interactions are influenced by contextual factors. However, in the field of perceptual decision-making, such interactions have proven to be important to dissociate between different underlying cognitive processes. Here, we investigated such interactions to shed new light on overlooked differences between learning to seek rewards and learning to avoid losses. We leveraged behavioral data from four RL experiments, which feature manipulations of two factors: outcome valence (gains vs. losses) and feedback information (partial vs. complete feedback). A Bayesian meta-analysis revealed that these contextual factors differently affect RTs and accuracy: While valence only affects RTs, feedback information affects both RTs and accuracy. To dissociate between the latent cognitive processes, we jointly fitted choices and RTs across all experiments with a Bayesian, hierarchical diffusion decision model (DDM). We found that the feedback manipulation affected drift rate, threshold, and non-decision time, suggesting that it was not a mere difficulty effect. Moreover, valence affected non-decision time and threshold, suggesting a motor inhibition in punishing contexts. To better understand the learning dynamics, we finally fitted a combination of RL and DDM (RLDDM). We found that while the threshold was modulated by trial-specific decision conflict, the non-decision time was modulated by the learned context valence. Overall, our results illustrate the benefits of jointly modeling RTs and choice data during RL, to reveal subtle mechanistic differences underlying decisions in different learning contexts.",
      "authors": [
        "Fontanesi, Laura",
        "Palminteri, Stefano",
        "Lebreton, Ma\u00ebl"
      ],
      "categories": null,
      "citations": 33,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-019-00723-1",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "490-502",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2019-06-15",
      "selected": null,
      "title": "Decomposing the effects of context valence and feedback information on speed and accuracy during reinforcement learning: a meta-analytical approach using diffusion decision modeling",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-019-00723-1.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067313356&origin=inward"
      ]
    },
    {
      "abstract": "The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards finding a sub-optimal solution. We introduce a general control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment -- namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.",
      "authors": [
        "Eysenbach, Benjamin",
        "Salakhutdinov, Ruslan",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": 115,
      "comments": "Run our algorithm in your browser: http://bit.ly/rl_search",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2019-06-12",
      "selected": null,
      "title": "Search on the Replay Buffer: Bridging Planning and Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088536167&origin=inward",
        "http://arxiv.org/pdf/1906.05253v1",
        "http://arxiv.org/pdf/1906.05253.pdf",
        "http://arxiv.org/abs/1906.05253v1"
      ]
    },
    {
      "abstract": "Interactive NLP is a promising paradigm to close the gap between automatic NLP systems and the human upper bound. Preference-based interactive learning has been successfully applied, but the existing methods require several thousand interaction rounds even in simulations with perfect user feedback. In this paper, we study preference-based interactive summarisation. To reduce the number of interaction rounds, we propose the Active Preference-based ReInforcement Learning (APRIL) framework. APRIL uses Active Learning to query the user, Preference Learning to learn a summary ranking function from the preferences, and neural Reinforcement Learning to efficiently search for the (near-)optimal summary. Our results show that users can easily provide reliable preferences over summaries and that APRIL outperforms the state-of-the-art preference-based interactive method in both simulation and real-user experiments.",
      "authors": [
        "Gao, Yang",
        "Meyer, Christian M.",
        "Gurevych, Iryna"
      ],
      "categories": null,
      "citations": null,
      "comments": "Submitted to the special issue on \"Learning from User Interactions\",\n  Information Retrieval Journal",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-06-07",
      "selected": null,
      "title": "Preference-based Interactive Multi-Document Summarisation",
      "urls": [
        "http://arxiv.org/pdf/1906.02923v1",
        "http://arxiv.org/abs/1906.02923v1",
        "http://arxiv.org/pdf/1906.02923.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ritschel H."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3316782.3316791",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "247-255",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450362320",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM International Conference Proceeding Series"
      },
      "publication_date": "2019-06-05",
      "selected": null,
      "title": "Adaptive linguistic style for an assistive robotic health companion based on explicit human feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85069187562&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mas-Herrero E."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2019.02.052",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "67-74",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2019-06-01",
      "selected": null,
      "title": "The contribution of striatal pseudo-reward prediction errors to value-based decision-making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062819615&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement Learning agents can be supported by feedback from human teachers in the learning loop that guides the learning process. In this work we propose two hybrid strategies of Policy Search Reinforcement Learning and Interactive Machine Learning that benefit from both sources of information, the cost function and the human corrective feedback, for accelerating the convergence and improving the final performance of the learning process. Experiments with simulated and real systems of balancing tasks and a 3 DoF robot arm validate the advantages of the proposed learning strategies: (i) they speed up the convergence of the learning process between 3 and 30 times, saving considerable time during the agent adaptation, and (ii) they allow including non-expert feedback because they have low sensibility to erroneous human advice.",
      "authors": [
        "Celemin, Carlos",
        "Ruiz-del-Solar, Javier",
        "Kober, Jens"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10514-018-9786-6",
      "keywords": [
        "Policy search",
        "Learning from demonstration",
        "Interactive machine learning",
        "Reinforcement learning"
      ],
      "number_of_pages": 14,
      "pages": "1173-1186",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0929-5593",
        "publisher": "Springer Netherlands",
        "sjr": 1.165,
        "snip": 1.523,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Autonomous Robots"
      },
      "publication_date": "2019-06-01",
      "selected": null,
      "title": "A fast hybrid reinforcement learning framework with human corrective feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051668828&origin=inward",
        "https://dl.acm.org/doi/10.1007/s10514-018-9786-6",
        "https://link.springer.com/content/pdf/10.1007/s10514-018-9786-6.pdf"
      ]
    },
    {
      "abstract": "Recent reinforcement learning algorithms, though achieving impressive results in various fields, suffer from brittle training effects such as regression in results and high sensitivity to initialization and parameters. We claim that some of the brittleness stems from variance differences, i.e. when different environment areas - states and/or actions - have different rewards variance. This causes two problems: First, the \"Boring Areas Trap\" in algorithms such as Q-learning, where moving between areas depends on the current area variance, and getting out of a boring area is hard due to its low variance. Second, the \"Manipulative Consultant\" problem, when value-estimation functions used in DQN and Actor-Critic algorithms influence the agent to prefer boring areas, regardless of the mean rewards return, as they maximize estimation precision rather than rewards. This sheds a new light on how exploration contribute to training, as it helps with both challenges. Cognitive experiments in humans showed that noised reward signals may paradoxically improve performance. We explain this using the two mentioned problems, claiming that both humans and algorithms may share similar challenges. Inspired by this result, we propose the Adaptive Symmetric Reward Noising (ASRN), by which we mean adding Gaussian noise to rewards according to their states' estimated variance, thus avoiding the two problems while not affecting the environment's mean rewards behavior. We conduct our experiments in a Multi Armed Bandit problem with variance differences. We demonstrate that a Q-learning algorithm shows the brittleness effect in this problem, and that the ASRN scheme can dramatically improve the results. We show that ASRN helps a DQN algorithm training process reach better results in an end to end autonomous driving task using the AirSim driving simulator.",
      "authors": [
        "Vivanti, Refael",
        "Sohlberg-Baris, Talya D.",
        "Cohen, Shlomo",
        "Cohen, Orna"
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages, 7 figures, conference",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-05-24",
      "selected": null,
      "title": "Adaptive Symmetric Reward Noising for Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/1905.10144v1",
        "http://arxiv.org/pdf/1905.10144.pdf",
        "http://arxiv.org/abs/1905.10144v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "McDougle S.D."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cub.2019.04.011",
      "keywords": [],
      "number_of_pages": null,
      "pages": "1606-1613.e5",
      "publication": {
        "category": "Journal",
        "cite_score": 12.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09609822",
        "publisher": "Cell Press",
        "sjr": 2.806,
        "snip": 1.81,
        "subject_areas": [
          "Neuroscience (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Agricultural and Biological Sciences (all)"
        ],
        "title": "Current Biology"
      },
      "publication_date": "2019-05-20",
      "selected": null,
      "title": "Neural Signatures of Prediction Errors in a Decision-Making Task Are Modulated by Action Execution Failures",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065522881&origin=inward"
      ]
    },
    {
      "abstract": "Deep Reinforcement Learning (DRL) has become a powerful methodology to solve complex decision-making problems. However, DRL has several limitations when used in real-world problems (e.g., robotics applications). For instance, long training times are required and cannot be accelerated in contrast to simulated environments, and reward functions may be hard to specify/model and/or to compute. Moreover, the transfer of policies learned in a simulator to the real-world has limitations (reality gap). On the other hand, machine learning methods that rely on the transfer of human knowledge to an agent have shown to be time efficient for obtaining well performing policies and do not require a reward function. In this context, we analyze the use of human corrective feedback during task execution to learn policies with high-dimensional state spaces, by using the D-COACH framework, and we propose new variants of this framework. D-COACH is a Deep Learning based extension of COACH (COrrective Advice Communicated by Humans), where humans are able to shape policies through corrective advice. The enhanced version of DCOACH, which is proposed in this paper, largely reduces the time and effort of a human for training a policy. Experimental results validate the efficiency of the D-COACH framework in three different problems (simulated and with real robots), and show that its enhanced version reduces the human training effort considerably, and makes it feasible to learn policies within periods of time in which a DRL agent do not reach any improvement.",
      "authors": [
        "Rodrigo P\u00e9rez-Dattari",
        "Carlos Celemin",
        "Javier Ruiz-del-Solar",
        "Jens Kober"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA.2019.8793675",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "7611-7617",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-7396-2",
        "issn": "1050-4729",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2019-05-20",
      "selected": null,
      "title": "Continuous Control for High-Dimensional State Spaces: An Interactive Learning Approach",
      "urls": [
        "https://dl.acm.org/doi/10.1109/ICRA.2019.8793675",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071516358&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793675"
      ]
    },
    {
      "abstract": "Humans generally teach their fellow collaborators to perform tasks through a small number of demonstrations. The learnt task is corrected or extended to meet specific task goals by means of coaching. Adopting a similar framework for teaching robots through demonstrations and coaching makes teaching tasks highly intuitive. Unlike traditional Learning from Demonstration (LfD) approaches which require multiple demonstrations, we present a one-shot learning from demonstration approach to learn tasks. The learnt task is corrected and generalized using two layers of evaluation/modification. First, the robot self-evaluates its performance and corrects the performance to be closer to the demonstrated task. Then, coaching is used as a means to extend the policy learnt to be adaptable to varying task goals. Both the self-evaluation and coaching are implemented using reinforcement learning (RL) methods. Coaching is achieved through human feedback on desired goal and action modification to generalize to specified task goals. The proposed approach is evaluated with a scooping task, by presenting a single demonstration. The self-evaluation framework aims to reduce the resistance to scooping in the media. To reduce the search space for RL, we bootstrap the search using least resistance path obtained using resistive force theory. Coaching is used to generalize the learnt task policy to transfer the desired quantity of material. Thus, the proposed method provides a framework for learning tasks from one demonstration and generalizing it using human feedback through coaching.",
      "authors": [
        "Balakuntala, Mythra V.",
        "Venkatesh, Vishnunandan L. N.",
        "Bindu, Jyothsna Padmakumar",
        "Voyles, Richard M.",
        "Wachs, Juan"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-05-13",
      "selected": null,
      "title": "Extending Policy from One-Shot Learning through Coaching",
      "urls": [
        "http://arxiv.org/pdf/1905.04841v1",
        "http://arxiv.org/pdf/1905.04841.pdf",
        "http://arxiv.org/abs/1905.04841v1"
      ]
    },
    {
      "abstract": "In order perform a large variety of tasks and to achieve human-level performance in complex real-world environments, Artificial Intelligence (AI) Agents must be able to learn from their past experiences and gain both knowledge and an accurate representation of their environment from raw sensory inputs. Traditionally, AI agents have suffered from difficulties in using only sensory inputs to obtain a good representation of their environment and then mapping this representation to an efficient control policy. Deep reinforcement learning algorithms have provided a solution to this issue. In this study, the performance of different conventional and novel deep reinforcement learning algorithms was analysed. The proposed method utilises two types of algorithms, one trained with a variant of Q-learning (DQN) and another trained with SARSA learning (DSN) to assess the feasibility of using direct feedback alignment, a novel biologically plausible method for back-propagating the error. These novel agents, alongside two similar agents trained with the conventional backpropagation algorithm, were tested by using the OpenAI Gym toolkit on several classic control theory problems and Atari 2600 video games. The results of this investigation open the way into new, biologically-inspired deep reinforcement learning algorithms, and their implementation on neuromorphic hardware.",
      "authors": [
        "Roibu, Andrei Claudiu"
      ],
      "categories": null,
      "citations": null,
      "comments": "Dissertation submitted to the University of Sheffield in partial\n  fulfilment of the requirements for the degree of Master of Engineering. 98\n  pages, 21 Tables, 58 Figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-05-10",
      "selected": null,
      "title": "Design of Artificial Intelligence Agents for Games using Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/1905.04127.pdf",
        "http://arxiv.org/pdf/1905.04127v1",
        "http://arxiv.org/abs/1905.04127v1"
      ]
    },
    {
      "abstract": "We present the Active Attention-Modified Policy Shaping (Active AMPS) algorithm, which allows learning robots to request feedback from multi-tasking human teachers. Active AMPS uses Reinforcement Learning supplemented with feedback from teachers, while avoiding frequently interrupting the teacher. This algorithm does so by selectively asking for attention from teachers in low-information areas of the state space when there is uncertainty about the teacher's feedback. Active AMPS allows people to take breaks from teaching the robot to complete other tasks, and is forgiving to lapses in human attention if learning occurs over long periods of time. We test Active AMPS both in simulation and on a physical robot in a human study. In simulation, we find that Active AMPS outperforms Attention-Modified Policy Shaping (AMPS), achieving an 11.0% increase in area under its learning curve while receiving 89.9% less feedback. In the human study, we find statistically significant results showing that Active AMPS allows people to complete 77.5% more work than AMPS while the robot receives 48.5% less feedback, without decreasing performance.",
      "authors": [
        "Taylor Kessler Faulkner",
        "Reymundo A. Gutierrez",
        "Elaine Schaertl Short",
        "Guy Hoffman",
        "Andrea L. Thomaz"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3306127.3331762",
      "keywords": [
        "human-robot interaction",
        "reinforcement learning",
        "active learning"
      ],
      "number_of_pages": 9,
      "pages": "728-736",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450363099",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems"
      },
      "publication_date": "2019-05-08",
      "selected": null,
      "title": "Active Attention-Modified Policy Shaping: Socially Interactive Agents Track",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3306127.3331762"
      ]
    },
    {
      "abstract": "A goal of Interactive Machine Learning is to enable people without specialized training to teach agents how to perform tasks. Many of the existing algorithms that learn from human instructions are evaluated using simulated feedback and focus on how quickly the agent learns. While this is valuable information, it ignores important aspects of the human-agent interaction such as frustration. To correct this, we propose a method for the design and verification of interactive algorithms that includes a human-subject study that measures the human's experience working with the agent. In this paper, we present Newtonian Action Advice, a method of incorporating human verbal action advice with Reinforcement Learning in a way that improves the human-agent interaction. In addition to simulations, we validated the Newtonian Action Advice algorithm by conducting a human-subject experiment. The results show that Newtonian Action Advice can perform better than Policy Shaping, a state-of-the-art IML algorithm, both in terms of RL metrics like cumulative reward and human factors metrics like frustration.",
      "authors": [
        "Samantha Krening",
        "Karen M. Feigh"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3306127.3331761",
      "keywords": [
        "natural language interface",
        "reinforcement learning",
        "learning from human teachers",
        "interactive machine learning",
        "human-subject experiment",
        "verification"
      ],
      "number_of_pages": 8,
      "pages": "720-727",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450363099",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems"
      },
      "publication_date": "2019-05-08",
      "selected": null,
      "title": "Newtonian Action Advice: Integrating Human Verbal Instruction with Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3306127.3331761"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lee J."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/schbul/sby109",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "620-628",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "05867614",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Schizophrenia Bulletin"
      },
      "publication_date": "2019-05-01",
      "selected": null,
      "title": "Reduced neural sensitivity to social vs nonsocial reward in schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072057112&origin=inward"
      ]
    },
    {
      "abstract": "TAMER has proven to be a powerful interactive reinforcement learning method for allowing ordinary people to teach and personalize autonomous agents' behavior by providing evaluative feedback. However, a TAMER agent planning with UCT---a Monte Carlo Tree Search strategy, can only update states along its path and might induce high learning cost especially for a physical robot. In this paper, we propose to drive the agent's exploration along the optimal path and reduce the learning cost by initializing the agent's reward function via inverse reinforcement learning from demonstration. We test our proposed method in the RL benchmark domain---Grid World---with different discounts on human reward. Our results show that learning from demonstration can allow a TAMER agent to learn a roughly optimal policy up to the deepest search and encourage the agent to explore along the optimal path. In addition, we find that learning from demonstration can improve the learning efficiency by reducing total feedback, the number of incorrect actions and increasing the ratio of correct actions to obtain an optimal policy, allowing a TAMER agent to converge faster.",
      "authors": [
        "Li, Guangliang",
        "Gomez, Randy",
        "Nakamura, Keisuke",
        "Lin, Jinying",
        "Zhang, Qilei",
        "He, Bo"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-04-18",
      "selected": null,
      "title": "Improving Interactive Reinforcement Agent Planning with Human Demonstration",
      "urls": [
        "http://arxiv.org/abs/1904.08621v1",
        "http://arxiv.org/pdf/1904.08621.pdf",
        "http://arxiv.org/pdf/1904.08621v1"
      ]
    },
    {
      "abstract": "Focused attention meditation (FAM) practices are cognitive control exercises where meditators learn to maintain focus and attention in the face of distracting stimuli. Previous studies have shown that FAM is both activating and causing plastic changes to the mesolimbic dopamine system and some of its target structures, particularly the anterior cingulate cortex (ACC) and striatum. Feedback-based learning also depends on these systems and is known to be modulated by tonic dopamine levels. Capitalizing on previous findings that FAM practices seem to cause dopamine release, the present study shows that FAM experience predicts learning from negative feedback on a probabilistic selection task. Furthermore, meditators exhibited attenuated feedback-related negativity (FRN) as compared with nonmeditators and this effect scales with meditation experience. Given that reinforcement learning and FRN are modulated by dopamine levels, a possible explanation for our findings is that FAM practice causes persistent increases in tonic dopamine levels which scale with amount of practice, thus altering feedback processing.",
      "authors": [
        "Knytl, Paul",
        "Opitz, Bertram"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-018-00665-0",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "268-282",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2019-04-15",
      "selected": null,
      "title": "Meditation experience predicts negative reinforcement learning and is associated with attenuated FRN amplitude",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-018-00665-0.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056646255&origin=inward"
      ]
    },
    {
      "abstract": "The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster.",
      "authors": [
        "Xiong, Yunyang",
        "Mehta, Ronak",
        "Singh, Vikas"
      ],
      "categories": null,
      "citations": null,
      "comments": "ICCV 2019",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-04-08",
      "selected": null,
      "title": "Resource Constrained Neural Network Architecture Search: Will a Submodularity Assumption Help?",
      "urls": [
        "http://arxiv.org/pdf/1904.03786.pdf",
        "http://arxiv.org/pdf/1904.03786v2",
        "http://arxiv.org/abs/1904.03786v2"
      ]
    },
    {
      "abstract": "Policy gradient algorithms typically combine discounted future rewards with an estimated value function, to compute the direction and magnitude of parameter updates. However, for most Reinforcement Learning tasks, humans can provide additional insight to constrain the policy learning. We introduce a general method to incorporate multiple different feedback channels into a single policy gradient loss. In our formulation, the Multi-Preference Actor Critic (M-PAC), these different types of feedback are implemented as constraints on the policy. We use a Lagrangian relaxation to satisfy these constraints using gradient descent while learning a policy that maximizes rewards. Experiments in Atari and Pendulum verify that constraints are being respected and can accelerate the learning process.",
      "authors": [
        "Durugkar, Ishan",
        "Hausknecht, Matthew",
        "Swaminathan, Adith",
        "MacAlpine, Patrick"
      ],
      "categories": null,
      "citations": null,
      "comments": "NeurIPS Workshop on Deep RL, 2018",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-04-05",
      "selected": null,
      "title": "Multi-Preference Actor Critic",
      "urls": [
        "http://arxiv.org/abs/1904.03295v1",
        "http://arxiv.org/pdf/1904.03295v1",
        "http://arxiv.org/pdf/1904.03295.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ghosh A."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/SoutheastCon42311.2019.9020663",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728101378",
        "issn": "07347502",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Conference Proceedings - IEEE SOUTHEASTCON"
      },
      "publication_date": "2019-04-01",
      "selected": null,
      "title": "Cooperative Traffic Control where Autonomous Cars Meet Human Drivers",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082386850&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Frankenhuis W.E."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.beproc.2018.01.008",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "94-100",
      "publication": {
        "category": "Journal",
        "cite_score": 3.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03766357",
        "publisher": "Elsevier B.V.",
        "sjr": 0.531,
        "snip": 0.702,
        "subject_areas": [
          "Animal Science and Zoology",
          "Behavioral Neuroscience"
        ],
        "title": "Behavioural Processes"
      },
      "publication_date": "2019-04-01",
      "selected": null,
      "title": "Enriching behavioral ecology with reinforcement learning methods",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041920967&origin=inward"
      ]
    },
    {
      "abstract": "Motor adaptation to perturbations is provided by learning mechanisms operating in the cerebellum and basal ganglia. The cerebellum normally performs motor adaptation through supervised learning using information about movement error provided by visual feedback. However, if visual feedback is critically distorted, the system may disengage cerebellar error-based learning and switch to reinforcement learning mechanisms mediated by basal ganglia. Yet, the exact conditions and mechanisms of cerebellum and basal ganglia involvement in motor adaptation remain unknown. We use mathematical modeling to simulate control of planar reaching movements that relies on both error-based and non-error-based learning mechanisms. We show that for learning to be efficient only one of these mechanisms should be active at a time. We suggest that switching between the mechanisms is provided by a special circuit that effectively suppresses the learning process in one structure and enables it in the other. To do so, this circuit modulates learning rate in the cerebellum and dopamine release in basal ganglia depending on error-based learning efficiency. We use the model to explain and interpret experimental data on error- and non-error-based motor adaptation under different conditions.",
      "authors": [
        "Dmitrii I. Todorov",
        "Robert A. Capps",
        "William H. Barnett",
        "Elizaveta M. Latash",
        "Taegyo Kim",
        "Khaldoun C. Hamade",
        "Sergey N. Markin",
        "Ilya A. Rybak",
        "Yaroslav I. Molkov"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0214926",
      "keywords": [
        "Neurons",
        "Soil perturbation",
        "Learning",
        "Vision",
        "Cerebellum",
        "Basal ganglia",
        "Reflection",
        "Musculoskeletal mechanics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2019-04-01",
      "selected": null,
      "title": "The interplay between cerebellum and basal ganglia in motor adaptation: A modeling study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064332631&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0214926&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Pesce E."
      ],
      "categories": null,
      "citations": 66,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.media.2018.12.007",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "26-38",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13618415",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Medical Image Analysis"
      },
      "publication_date": "2019-04-01",
      "selected": null,
      "title": "Learning to detect chest radiographs containing pulmonary lesions using visual attention networks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060047284&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Tabrez A."
      ],
      "categories": null,
      "citations": 37,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/HRI.2019.8673104",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "249-257",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781538685556",
        "issn": null,
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2019-03-22",
      "selected": null,
      "title": "Explanation-Based Reward Coaching to Improve Human Performance via Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064001723&origin=inward"
      ]
    },
    {
      "abstract": "<p>Goal-directed behavior requires integrating action selection processes with learning systems that adapt control using environmental feedback. These functions are known to intersect at a common neural substrate with multiple known targets of plasticity (the cortico-basal ganglia-thalamic network), suggesting that feedback signals have a multifaceted impact on future decisions. Using a hybrid of accumulation-to-bound decision models and reinforcement learning, we modeled the performance of humans in a stop signal task where participants (N 75: 37 males, 38 females) learned the prior distribution of the timing of a stop signal through trial-and-error feedback. Changes in the drift rate of the action execution process were driven by errors in action timing, whereas adaptation in the boundary height served to increase caution following failed stops. These findings highlight two interactive learning mechanisms for adapting the control of goal-directed actions based on dissociable dimensions of feedback error.</p><p><b>SIGNIFICANCE STATEMENT</b> Many complex behavioral goals rely on the ability to regulate the timing of action execution while also maintaining enough control to cancel actions in response to \u201cStop\u201d cues in the environment. Here we examined how these fundamental components of behavior become tuned to the control demands of the environment by combining principles of reinforcement learning with accumulation-to-bound models. Model fits to behavioral data in an adaptive stop signal task revealed two adaptive mechanisms: (1) timing error-related changes in the rate of the execution signal; and (2) an increase in the execution boundary after failed stops. These findings demonstrate unique effects of timing and control errors on the underlying mechanisms of control, the rate and threshold of accumulating action signals.</p>",
      "authors": [
        "Kyle Dunovan",
        "Timothy Verstynen"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1924-18.2019",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "2251-2264",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2019-03-20",
      "selected": null,
      "title": "Errors in Action Timing and Inhibition Facilitate Learning by Tuning Distinct Mechanisms in the Underlying Decision Process",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063635096&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement Learning (RL) is used in the language sequence generation. The reason for the occurrence of neglected loss is that simulation representing the environment calculates rewards of environment without direct control rules and uses self-feedback strategies typically. This process is that the agent provides feedback to itself as an environment. To overcome drawback, we propose a goal-directed language sequence generation method for Natural Language Generation (NLG) tasks and a reliable implementation strategy. It helps the agent learn generation rules without human help and improve the accuracy of goal. The work demonstrates the way of using a preliminary generator as the environment simulator during the initialization phase to provide feedback for the training generator on behalf of the environment. When the training level meets the decision condition, the replacement strategy is selected for correction because the environment simulator is not enough to provide positive feedback. The discriminator provides the specific target reward, and the guide content is generated in the existing method based on RL and Generative Adversarial Net (GAN) method. Experimental results on the synthetic data and real-world tasks prove the advantages of goal-directed sequence generation with simulation feedback method over sequence generative adversarial nets models.",
      "authors": [
        "Xinyue Liu",
        "Wenbo Tian",
        "Wenxin Liang",
        "Hua Shen"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ITNEC.2019.8729439",
      "keywords": [
        "Goal-directed",
        "Generative Adversarial Net",
        "Natural Language Generation",
        "Reinforcement Learning"
      ],
      "number_of_pages": 8,
      "pages": "287-294",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-6244-1",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of 2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference, ITNEC 2019"
      },
      "publication_date": "2019-03-15",
      "selected": null,
      "title": "Goal-directed Sequence Generation with Simulation Feedback Method",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067883147&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8729439"
      ]
    },
    {
      "abstract": "Deep Reinforcement Learning has enabled the control of increasingly complex\nand high-dimensional problems. However, the need of vast amounts of data before\nreasonable performance is attained prevents its widespread application. We\nemploy binary corrective feedback as a general and intuitive manner to\nincorporate human intuition and domain knowledge in model-free machine\nlearning. The uncertainty in the policy and the corrective feedback is combined\ndirectly in the action space as probabilistic conditional exploration. As a\nresult, the greatest part of the otherwise ignorant learning process can be\navoided. We demonstrate the proposed method, Predictive Probabilistic Merging\nof Policies (PPMP), in combination with DDPG. In experiments on continuous\ncontrol problems of the OpenAI Gym, we achieve drastic improvements in sample\nefficiency, final performance, and robustness to erroneous feedback, both for\nhuman and synthetic feedback. Additionally, we show solutions beyond the\ndemonstrated knowledge.",
      "authors": [
        "Jan Scholten",
        "Daan Wout",
        "Carlos Celemin",
        "Jens Kober"
      ],
      "categories": null,
      "citations": 2,
      "comments": "6 pages",
      "databases": [
        "IEEE",
        "ACM",
        "arXiv",
        "Scopus"
      ],
      "doi": "10.1109/CDC40024.2019.9029503",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "803-808",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-1399-9",
        "issn": "0743-1546",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the IEEE Conference on Decision and Control"
      },
      "publication_date": "2019-03-14",
      "selected": null,
      "title": "Deep Reinforcement Learning with Feedback-based Exploration",
      "urls": [
        "http://dx.doi.org/10.1109/CDC40024.2019.9029503",
        "http://arxiv.org/abs/1903.06151v1",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9029503",
        "https://dl.acm.org/doi/10.1109/CDC40024.2019.9029503",
        "http://arxiv.org/pdf/1903.06151v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082485287&origin=inward"
      ]
    },
    {
      "abstract": "The researchers in this study have developed a novel approach using mutual reinforcement learning (MRL) where both the robot and human act as empathetic individuals who function as reinforcement learning agents for each other to achieve a particular task over continuous communication and feedback. This shared model not only has a collective impact but improves human cognition and helps in building a successful human-robot relationship. In our current work, we compared our learned reinforcement model with a baseline non-reinforcement and random approach in a robotics domain to identify the significance and impact of MRL. MRL contributed to improved skill transfer, and the robot was able successfully to predict which reinforcement behaviors would be most valuable to its human partners.",
      "authors": [
        "Sayanti Roy",
        "Emily Kieson",
        "Charles Abramson",
        "Christopher Crick"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/HRI.2019.8673284",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "572-573",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-8556-3",
        "issn": "2167-2121",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2019-03-11",
      "selected": null,
      "title": "Mutual Reinforcement Learning with Robot Trainers",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063999356&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8673284"
      ]
    },
    {
      "abstract": "We believe SSC can be a fundamental asset to allow the easy landing of data science in industrial domains.",
      "authors": [
        "Corrado Grappiolo",
        "Emile van Gerwen",
        "Jack Verhoosel",
        "Lou Somers"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3295750.3298915",
      "keywords": [
        "human-computer collaboration",
        "document classification",
        "semantic graph",
        "search engine",
        "natural language processing",
        "reinforcement learning"
      ],
      "number_of_pages": 5,
      "pages": "355-359",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450360258",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CHIIR 2019 - Proceedings of the 2019 Conference on Human Information Interaction and Retrieval"
      },
      "publication_date": "2019-03-08",
      "selected": null,
      "title": "The Semantic Snake Charmer Search Engine: A Tool to Facilitate Data Science in High-tech Industry Domains",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063150634&origin=inward",
        "https://dl.acm.org/doi/10.1145/3295750.3298915"
      ]
    },
    {
      "abstract": "Nonlinear optimal control problems are often solved with numerical methods that require knowledge of system's dynamics which may be difficult to infer, and that carry a large computational cost associated with iterative calculations. We present a novel neurobiologically inspired hierarchical learning framework, Reinforcement Learning Optimal Control, which operates on two levels of abstraction and utilises a reduced number of controllers to solve nonlinear systems with unknown dynamics in continuous state and action spaces. Our approach is inspired by research at two levels of abstraction: first, at the level of limb coordination human behaviour is explained by linear optimal feedback control theory. Second, in cognitive tasks involving learning symbolic level action selection, humans learn such problems using model-free and model-based reinforcement learning algorithms. We propose that combining these two levels of abstraction leads to a fast global solution of nonlinear control problems using reduced number of controllers. Our framework learns the local task dynamics from naive experience and forms locally optimal infinite horizon Linear Quadratic Regulators which produce continuous low-level control. A top-level reinforcement learner uses the controllers as actions and learns how to best combine them in state space while maximising a long-term reward. A single optimal control objective function drives high-level symbolic learning by providing training signals on desirability of each selected controller. We show that a small number of locally optimal linear controllers are able to solve global nonlinear control problems with unknown dynamics when combined with a reinforcement learner in this hierarchical framework. Our algorithm competes in terms of computational cost and solution quality with sophisticated control algorithms and we illustrate this with solutions to benchmark problems.",
      "authors": [
        "Abramova, Ekaterina",
        "Dickens, Luke",
        "Kuhn, Daniel",
        "Faisal, Aldo"
      ],
      "categories": null,
      "citations": null,
      "comments": "33 pages, 8 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-03-07",
      "selected": null,
      "title": "RLOC: Neurobiologically Inspired Hierarchical Reinforcement Learning Algorithm for Continuous Control of Nonlinear Dynamical Systems",
      "urls": [
        "http://arxiv.org/abs/1903.03064v1",
        "http://arxiv.org/pdf/1903.03064.pdf",
        "http://arxiv.org/pdf/1903.03064v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ho M.K."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/xge0000569",
      "keywords": [],
      "number_of_pages": 30,
      "pages": "520-549",
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00963445",
        "publisher": "American Psychological Association",
        "sjr": 2.023,
        "snip": 1.954,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Psychology (all)"
        ],
        "title": "Journal of Experimental Psychology: General"
      },
      "publication_date": "2019-03-01",
      "selected": null,
      "title": "People teach with rewards and punishments as communication, not reinforcements",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062056385&origin=inward"
      ]
    },
    {
      "abstract": "Reduced social motivation is a hallmark of individuals with autism spectrum disorders (ASDs). Although the exact neural mechanisms are unclear, oxytocin has been shown to enhance motivation and attention to social stimuli, suggesting a potential to augment social reinforcement learning as the central mechanism of behavioral interventions in ASD. We tested how reinforcement learning in social contexts and associated reward prediction error (RPE) signals in the nucleus accumbens (NAcc) were modulated by intranasal oxytocin. Male adults with a childhood diagnosis of ASD (n\u2009=\u200915) and healthy controls (n\u2009=\u200924; aged 18\u201326 years) performed a probabilistic reinforcement learning task during functional magnetic resonance imaging in a single-center (research center in Germany), randomized double-blind, placebo-controlled cross-over trial. The interventions were intranasal oxytocin (Syntocinon\u00ae, Novartis; 10 puffs\u2009=\u200920 international units (IUs) per treatment) and placebo spray. Using computational modeling of behavioral data, trial-by-trial RPE signals were assessed and related to brain activation in NAcc during reinforcing feedback in social and non-social contexts. The order of oxytocin/placebo was randomized for 60 participants. Twenty-one participants were excluded from analyses, leaving 39 for the final analysis. Behaviorally, individuals with ASD showed enhanced learning under oxytocin when the learning target as well as feedback was social as compared to non-social (social vs. non-social target: 87.09% vs. 71.29%, 95% confidence interval (CI): 7.28\u201324.33, p\u2009=\u2009.003; social vs. non-social feedback: 81.00% vs. 71.29%, 95% CI: 2.81\u201316.61, p\u2009=\u2009.027). Correspondingly, oxytocin enhanced the correlation of the RPE signal with NAcc activation during social (vs. non-social) feedback in ASD (3.48 vs. \u22121.12, respectively, 95% CI: 2.98\u20136.22, p\u2009=\u2009.000), whereas in controls, this effect was found in the placebo condition (2.90 vs. \u22121.14, respectively, 95% CI: 1.07\u20137.01, p\u2009=\u2009.010). In ASD, a similar pattern emerged when the learning target was social (3.00 vs. \u22120.64, respectively, 95% CI: \u22120.13 to 7.41, p\u2009=\u2009.057), whereas controls showed a reduced correlation for social learning targets under oxytocin (\u22120.70 vs. 2.72, respectively, 95% CI: \u22125.86 to 0.98, p\u2009=\u2009.008). The current data suggest that intranasal oxytocin has the potential to enhance social reinforcement learning in ASD. Future studies are warranted that investigate whether oxytocin can potentiate social learning when combined with behavioral therapies, resulting in greater treatment benefits than traditional behavior-only approaches.",
      "authors": [
        "Kruppa, Jana A.",
        "Gossen, Anna",
        "Oberwelland Wei\u00df, Eileen",
        "Kohls, Gregor",
        "Gro\u00dfheinrich, Nicola",
        "Cholemkery, Hannah",
        "Freitag, Christine M.",
        "Karges, Wolfram",
        "W\u00f6lfle, Elke",
        "Sinzig, Judith",
        "Fink, Gereon R.",
        "Herpertz-Dahlmann, Beate",
        "Konrad, Kerstin",
        "Schulte-R\u00fcther, Martin"
      ],
      "categories": null,
      "citations": 40,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41386-018-0258-7",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "749-756",
      "publication": {
        "category": "Journal",
        "cite_score": 14.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0893133X",
        "publisher": "Nature Publishing Group",
        "sjr": 2.385,
        "snip": 1.864,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Pharmacology"
        ],
        "title": "Neuropsychopharmacology"
      },
      "publication_date": "2019-03-01",
      "selected": null,
      "title": "Neural modulation of social reinforcement learning by intranasal oxytocin in male adults with high-functioning autism spectrum disorder: a randomized trial",
      "urls": [
        "https://www.nature.com/articles/s41386-018-0258-7.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056795983&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Padmanabhan R."
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.mbs.2019.01.012",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "131-142",
      "publication": {
        "category": "Journal",
        "cite_score": 6.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00255564",
        "publisher": "Elsevier Inc.",
        "sjr": 0.732,
        "snip": 1.081,
        "subject_areas": [
          "Applied Mathematics",
          "Agricultural and Biological Sciences (all)",
          "Modeling and Simulation",
          "Statistics and Probability",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Immunology and Microbiology (all)"
        ],
        "title": "Mathematical Biosciences"
      },
      "publication_date": "2019-03-01",
      "selected": null,
      "title": "Optimal adaptive control of drug dosing using integral reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061335797&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang J."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.concog.2019.01.016",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "103-112",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538100",
        "publisher": "Academic Press Inc.",
        "sjr": 0.933,
        "snip": 1.159,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology"
        ],
        "title": "Consciousness and Cognition"
      },
      "publication_date": "2019-03-01",
      "selected": null,
      "title": "Manipulating memory associations changes decision-making preferences in a preconditioning task",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061088742&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process. The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences. In this work we revisit the fundamentals of discounting in RL and bridge this disconnect by implementing an RL agent that acts via hyperbolic discounting. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL. Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learning value functions over multiple time-horizons is an effective auxiliary task which often improves over a strong value-based RL agent, Rainbow.",
      "authors": [
        "Fedus, William",
        "Gelada, Carles",
        "Bengio, Yoshua",
        "Bellemare, Marc G.",
        "Larochelle, Hugo"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-02-19",
      "selected": null,
      "title": "Hyperbolic Discounting and Learning over Multiple Horizons",
      "urls": [
        "http://arxiv.org/abs/1902.06865v3",
        "http://arxiv.org/pdf/1902.06865v3",
        "http://arxiv.org/pdf/1902.06865.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Murayama K."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 22,
      "pages": "141-162",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781316823279",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "The Cambridge Handbook of Motivation and Learning"
      },
      "publication_date": "2019-02-15",
      "selected": null,
      "title": "Neuroscientific and psychological approaches to incentives: Commonality and multifaceted views",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85070881866&origin=inward"
      ]
    },
    {
      "abstract": "To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.",
      "authors": [
        "Arumugam, Dilip",
        "Lee, Jun Ki",
        "Saskin, Sophie",
        "Littman, Michael L."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-02-12",
      "selected": null,
      "title": "Deep Reinforcement Learning from Policy-Dependent Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/1902.04257.pdf",
        "http://arxiv.org/abs/1902.04257v1",
        "http://arxiv.org/pdf/1902.04257v1"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",
      "authors": [
        "Shah, Rohin",
        "Krasheninnikov, Dmitrii",
        "Alexander, Jordan",
        "Abbeel, Pieter",
        "Dragan, Anca"
      ],
      "categories": null,
      "citations": null,
      "comments": "Published at ICLR 2019",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-02-12",
      "selected": null,
      "title": "Preferences Implicit in the State of the World",
      "urls": [
        "http://arxiv.org/abs/1902.04198v2",
        "http://arxiv.org/pdf/1902.04198v2",
        "http://arxiv.org/pdf/1902.04198.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) agents have traditionally been tasked with maximizing the value function of a Markov decision process (MDP), either in continuous settings, with fixed discount factor $\\gamma < 1$, or in episodic settings, with $\\gamma = 1$. While this has proven effective for specific tasks with well-defined objectives (e.g., games), it has never been established that fixed discounting is suitable for general purpose use (e.g., as a model of human preferences). This paper characterizes rationality in sequential decision making using a set of seven axioms and arrives at a form of discounting that generalizes traditional fixed discounting. In particular, our framework admits a state-action dependent \"discount\" factor that is not constrained to be less than 1, so long as there is eventual long run discounting. Although this broadens the range of possible preference structures in continuous settings, we show that there exists a unique \"optimizing MDP\" with fixed $\\gamma < 1$ whose optimal value function matches the true utility of the optimal policy, and we quantify the difference between value and utility for suboptimal policies. Our work can be seen as providing a normative justification for (a slight generalization of) Martha White's RL task formalism (2017) and other recent departures from the traditional RL, and is relevant to task specification in RL, inverse RL and preference-based RL.",
      "authors": [
        "Pitis, Silviu"
      ],
      "categories": null,
      "citations": 16,
      "comments": "8 pages + 1 page supplement. In proceedings of AAAI 2019. Slides,\n  poster and bibtex available at\n  https://silviupitis.com/#rethinking-the-discount-factor-in-reinforcement-learning-a-decision-theoretic-approach",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7949-7956",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358091",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019"
      },
      "publication_date": "2019-02-08",
      "selected": null,
      "title": "Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach",
      "urls": [
        "http://arxiv.org/pdf/1902.02893.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090801924&origin=inward",
        "http://arxiv.org/pdf/1902.02893v1",
        "http://arxiv.org/abs/1902.02893v1"
      ]
    },
    {
      "abstract": "Human personality is a combination of the behavior, emotion, motivation and thinking pattern and has great impact on a person's life, health, and other related preferences. Food preferences provide rich information for studying personality of a person. In this paper, we conduct an empirical study to predict human personality based on restaurant review on food and other related preferences. We choose the category to `judge/perceive' from the 4 categories of 16 personality traits. A data set is built from a survey of 100 people based on a questionnaire about their food related behavior along with standard personality traits. A classification algorithm is proposed to classify the participant's personality from his/her food preference and surrounding environment on a restaurant, using reinforcement learning that utilizes temporal difference, model based, and on policy techniques. We compare our proposed classification results with standard classification solutions for personality detection to determine the performance accuracy of our proposed model.",
      "authors": [
        "Tasfia Hoque",
        "Raqeebir Rab",
        "Khushnoor Rafsan Jani Alam",
        "Saif Hasan Khan",
        "M. A. Wadud Shuvro",
        "Umme Zakia"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ECACE.2019.8679214",
      "keywords": [
        "personality trait",
        "restaurant data",
        "reinforcement learning",
        "judge",
        "perceive surrounding environment",
        "machine learning"
      ],
      "number_of_pages": 6,
      "pages": "1-6",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-9112-0",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2nd International Conference on Electrical, Computer and Communication Engineering, ECCE 2019"
      },
      "publication_date": "2019-02-07",
      "selected": null,
      "title": "Empirical Study on Personality Trait Classification by Food Related Preferences",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8679214",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064635008&origin=inward"
      ]
    },
    {
      "abstract": "Multi-armed bandit(MAB) problem is a reinforcement learning framework where an agent tries to maximise her profit by proper selection of actions through absolute feedback for each action. The dueling bandits problem is a variation of MAB problem in which an agent chooses a pair of actions and receives relative feedback for the chosen action pair. The dueling bandits problem is well suited for modelling a setting in which it is not possible to provide quantitative feedback for each action, but qualitative feedback for each action is preferred as in the case of human feedback. The dueling bandits have been successfully applied in applications such as online rank elicitation, information retrieval, search engine improvement and clinical online recommendation. We propose a new method called Sup-KLUCB for K-armed dueling bandit problem specifically Copeland bandit problem by converting it into a standard MAB problem. Instead of using MAB algorithm independently for each action in a pair as in Sparring and in Self-Sparring algorithms, we combine a pair of action and use it as one action. Previous UCB algorithms such as Relative Upper Confidence Bound(RUCB) can be applied only in case of Condorcet dueling bandits, whereas this algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as a special case. Our empirical results outperform state of the art Double Thompson Sampling(DTS) in case of Copeland dueling bandits.",
      "authors": [
        "Agrawal, Nischal",
        "Chaporkar, Prasanna"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages, 2 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2019-02-07",
      "selected": null,
      "title": "KLUCB Approach to Copeland Bandits",
      "urls": [
        "http://arxiv.org/pdf/1902.02778.pdf",
        "http://arxiv.org/pdf/1902.02778v1",
        "http://arxiv.org/abs/1902.02778v1"
      ]
    },
    {
      "abstract": "In this paper, we propose a framework that enables a human teacher to shape a robot behaviour by interactively providing it with unlabeled instructions. We ground the meaning of instruction signals in the task-learning process, and use them simultaneously for guiding the latter. We implement our framework as a modular architecture, named TICS (Task-Instruction-Contingency-Shaping) that combines different information sources: a predefined reward function, human evaluative feedback and unlabeled instructions. This approach provides a novel perspective for robotic task learning that lies between Reinforcement Learning and Supervised Learning paradigms. We evaluate our framework both in simulation and with a real robot. The experimental results demonstrate the effectiveness of our framework in accelerating the task-learning process and in reducing the number of required teaching signals.",
      "authors": [
        "Najar, Anis",
        "Sigaud, Olivier",
        "Chetouani, Mohamed"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1007/s10458-020-09459-6",
      "keywords": [
        "Human\u2013robot interaction",
        "Shaping",
        "Reinforcement learning",
        "Interactive machine learning",
        "Unlabeled instructions"
      ],
      "number_of_pages": 35,
      "pages": "",
      "publication": {
        "category": "Journal",
        "cite_score": 5.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1387-2532",
        "publisher": "Springer Netherlands",
        "sjr": 0.927,
        "snip": 2.046,
        "subject_areas": [
          "Artificial Intelligence",
          "Learning",
          "Robotics",
          "Machine Learning"
        ],
        "title": "Auton Agent Multi-Agent Syst 34, 35 (2020)"
      },
      "publication_date": "2019-02-05",
      "selected": null,
      "title": "Interactively shaping robot behaviour with unlabeled human instructions",
      "urls": [
        "http://arxiv.org/pdf/1902.01670v2",
        "http://dx.doi.org/10.1007/s10458-020-09459-6",
        "https://link.springer.com/content/pdf/10.1007/s10458-020-09459-6.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084356488&origin=inward",
        "http://arxiv.org/abs/1902.01670v2",
        "https://dl.acm.org/doi/10.1007/s10458-020-09459-6"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "V\u00e1zquez-Canteli J.R."
      ],
      "categories": null,
      "citations": 427,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.apenergy.2018.11.002",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "1072-1089",
      "publication": {
        "category": "Journal",
        "cite_score": 21.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03062619",
        "publisher": "Elsevier B.V.",
        "sjr": 2.907,
        "snip": 2.758,
        "subject_areas": [
          "Energy (all)",
          "Mechanical Engineering",
          "Building and Construction",
          "Management, Monitoring, Policy and Law"
        ],
        "title": "Applied Energy"
      },
      "publication_date": "2019-02-01",
      "selected": null,
      "title": "Reinforcement learning for demand response: A review of algorithms and modeling techniques",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056655519&origin=inward"
      ]
    },
    {
      "abstract": "Novice programmers often struggle with the formal syntax of programming languages. In the traditional classroom setting, they can make progress with the help of real time feedback from their instructors which is often impossible to get in the massive open online course (MOOC) setting. Syntactic error repair techniques have huge potential to assist them at scale. Towards this, we design a novel programming language correction framework amenable to reinforcement learning. The framework allows an agent to mimic human actions for text navigation and editing. We demonstrate that the agent can be trained through self-exploration directly from the raw input, that is, program text itself, without either supervision or any prior knowledge of the formal syntax of the programming language. We evaluate our technique on a publicly available dataset containing 6975 erroneous C programs with typographic errors, written by students during an introductory programming course. Our technique fixes 1699 (24.4%) programs completely and 1310 (18.8%) program partially, outperforming DeepFix, a state-of-the-art syntactic error repair technique, which uses a fully supervised neural machine translation approach.",
      "authors": [
        "Rahul Gupta",
        "Aditya Kanade",
        "Shirish Shevade"
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1609/aaai.v33i01.3301930",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "930-937",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-809-1",
        "issn": "2374-3468",
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2019-01-27",
      "selected": null,
      "title": "Deep Reinforcement Learning for Syntactic Error Repair in Student Programs",
      "urls": [
        "https://ojs.aaai.org/index.php/AAAI/article/download/3882/3760",
        "https://dl.acm.org/doi/10.1609/aaai.v33i01.3301930"
      ]
    },
    {
      "abstract": "Feature selection is a crucial step in the conception of Machine Learning models, which is often performed via datadriven approaches that overlook the possibility of tapping into the human decision-making of the model\u2019s designers and users. We present a human-in-the-loop framework that interacts with domain experts by collecting their feedback regarding the variables (of few samples) they evaluate as the most relevant for the task at hand. Such information can be modeled via Reinforcement Learning to derive a per-example feature selection method that tries to minimize the model\u2019s loss function by focusing on the most pertinent variables from a human perspective. We report results on a proof-of-concept image classification dataset and on a real-world risk classification task in which the model successfully incorporated feedback from experts to improve its accuracy.",
      "authors": [
        "Alvaro H. C. Correia",
        "Freddy Lecue"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1609/aaai.v33i01.33012438",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "2438-2445",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-809-1",
        "issn": "2374-3468",
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2019-01-27",
      "selected": null,
      "title": "Human-in-the-Loop Feature Selection",
      "urls": [
        "https://dl.acm.org/doi/10.1609/aaai.v33i01.33012438",
        "https://ojs.aaai.org/index.php/AAAI/article/download/4088/3966"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) agents have traditionally been tasked with maximizing the value function of a Markov decision process (MDP), either in continuous settings, with fixed discount factor \u03b3 ",
      "authors": [
        "Silviu Pitis"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1609/aaai.v33i01.33017949",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7949-7956",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-809-1",
        "issn": "2374-3468",
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2019-01-27",
      "selected": null,
      "title": "Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach",
      "urls": [
        "https://ojs.aaai.org/index.php/AAAI/article/download/4795/4673",
        "https://dl.acm.org/doi/10.1609/aaai.v33i01.33017949"
      ]
    },
    {
      "abstract": "This paper presents a mirroring approach, inspired by the neuroscience discovery of the mirror neurons, to transfer demonstrated manipulation actions to robots. Designed to address the different embodiments between a human (demonstrator) and a robot, this approach extends the classic robot Learning from Demonstration (LfD) in the following aspects:i) It incorporates fine-grained hand forces collected by a tactile glove in demonstration to learn robot\u2019s fine manipulative actions; ii) Through model-free reinforcement learning and grammar induction, the demonstration is represented by a goal-oriented grammar consisting of goal states and the corresponding forces to reach the states, independent of robot embodiments; iii) A physics-based simulation engine is applied to emulate various robot actions and mirrors the actions that are functionally equivalent to the human\u2019s in the sense of causing the same state changes by exerting similar forces. Through this approach, a robot reasons about which forces to exert and what goals to achieve to generate actions (i.e., mirroring), rather than strictly mimicking demonstration (i.e., overimitation). Thus the embodiment difference between a human and a robot is naturally overcome. In the experiment, we demonstrate the proposed approach by teaching a real Baxter robot with a complex manipulation task involving haptic feedback\u2014opening medicine bottles.",
      "authors": [
        "Hangxin Liu",
        "Chi Zhang",
        "Yixin Zhu",
        "Chenfanfu Jiang",
        "Song-Chun Zhu"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1609/aaai.v33i01.33018025",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "8025-8033",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-809-1",
        "issn": "2374-3468",
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2019-01-27",
      "selected": null,
      "title": "Mirroring without Overimitation: Learning Functionally Equivalent Manipulation Actions",
      "urls": [
        "https://ojs.aaai.org/index.php/AAAI/article/download/4804/4682",
        "https://dl.acm.org/doi/10.1609/aaai.v33i01.33018025"
      ]
    },
    {
      "abstract": "Human-centered reinforcement learning (RL), in which an agent learns how to perform a task from evaluative feedback delivered by a human observer, has become more and more popular in recent years. The advantage of being able to learn from human feedback for a RL agent has led to increasing applicability to real-life problems. This paper describes the state-of-the-art human centered RL algorithms and aims to become a starting point for researchers who are initiating their endeavors in human-centered RL. Moreover, the objective of this paper is to present a comprehensive survey of the recent breakthroughs in this field and provide references to the most interesting and successful works. After starting with an introduction of the concepts of RL from environmental reward, this paper discusses the origins of human-centered RL and its difference from traditional RL. Then we describe different interpretations of human evaluative feedback, which have produced many human-centered RL algorithms in the past decade. In addition, we describe research on agents learning from both human evaluative feedback and environmental rewards as well as on improving the efficiency of human-centered RL. Finally, we conclude with an overview of application areas and a discussion of future work and open questions.",
      "authors": [
        "Guangliang Li",
        "Randy Gomez",
        "Keisuke Nakamura",
        "Bo He"
      ],
      "categories": null,
      "citations": 74,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/THMS.2019.2912447",
      "keywords": [
        "human reward",
        "policy shaping",
        "interactive reinforcement learning (RL)",
        "interactive shaping",
        "Human agent/robot interaction"
      ],
      "number_of_pages": 13,
      "pages": "337-349",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2168-2305",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Human-Machine Systems"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Human-Centered Reinforcement Learning: A Survey",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8708686",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065654889&origin=inward"
      ]
    },
    {
      "abstract": "Most existing person re-identification(Re-ID) approaches achieve superior results based on the assumption that a large amount of pre-labelled data is usually available and can be put into training phrase all at once. However, this assumption is not applicable to most real-world deployment of the Re-ID task. In this work, we propose an alternative reinforcement learning based human-in-the-loop model which releases the restriction of pre-labelling and keeps model upgrading with progressively collected data. The goal is to minimize human annotation efforts while maximizing Re-ID performance. It works in an iteratively updating framework by refining the RL policy and CNN parameters alternately. In particular, we formulate a Deep Reinforcement Active Learning (DRAL) method to guide an agent (a model in a reinforcement learning process) in selecting training samples on-the-fly by a human user/annotator. The reinforcement learning reward is the uncertainty value of each human selected sample. A binary feedback (positive or negative) labelled by the human annotator is used to select the samples of which are used to fine-tune a pre-trained CNN Re-ID model. Extensive experiments demonstrate the superiority of our DRAL method for deep reinforcement learning based human-in-the-loop person Re-ID when compared to existing unsupervised and transfer learning models as well as active learning models.",
      "authors": [
        "Zimo Liu",
        "Jingya Wang",
        "Shaogang Gong",
        "Dacheng Tao",
        "Huchuan Lu"
      ],
      "categories": null,
      "citations": 61,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICCV.2019.00622",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "6121-6130",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-4804-5",
        "issn": "1550-5499",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the IEEE International Conference on Computer Vision"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081908739&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9010038"
      ]
    },
    {
      "abstract": "Minimally invasive surgery (MIS) is increasingly becoming a vital method of reducing surgical trauma and significantly improving postoperative recovery. However, skillful handling of surgical instruments used in MIS, especially for laparoscopy, requires a long period of training and depends highly on the experience of surgeons. This letter presents a new robot-assisted surgical training system which is designed to improve the practical skills of surgeons through intrapractice feedback and demonstration from both human experts and reinforcement learning (RL) agents. This system utilizes proximal policy optimization to learn the control policy in simulation. Subsequently, a generative adversarial imitation learning agent is trained based on both expert demonstrations and learned policies in simulation. This agent then generates demonstration policies on the robot-assisted device for trainees and produces feedback scores during practice. To further acquire surgical tools coordinates and encourage self-oriented practice, a mask region-based convolution neural network is trained to perform the semantic segmentation of surgical tools and targets. To the best of our knowledge, this system is the first robot-assisted laparoscopy training system which utilizes actual surgical tools and leverages deep reinforcement learning to provide demonstration training from both human expert perspectives and RL criterion.",
      "authors": [
        "Xiaoyu Tan",
        "Chin-Boon Chng",
        "Ye Su",
        "Kah-Bin Lim",
        "Chee-Kong Chui"
      ],
      "categories": null,
      "citations": 34,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/LRA.2019.2891311",
      "keywords": [
        "AI-based methods",
        "learning from demonstration",
        "Surgical robotics",
        "laparoscopy",
        "deep learning in robotics and automation"
      ],
      "number_of_pages": 8,
      "pages": "485-492",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2377-3774",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Robotics and Automation Letters"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Robot-Assisted Training in Laparoscopy Using Deep Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8604070",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063310531&origin=inward"
      ]
    },
    {
      "abstract": "The sequential decision-making problem with large-scale state spaces is an important and challenging topic for multitask reinforcement learning (MTRL). Training near-optimality policies across tasks suffers from prior knowledge deficiency in discrete-time nonlinear environment, especially for continuous task variations, requiring scalability approaches to transfer prior knowledge among new tasks when considering large number of tasks. This paper proposes a multitask policy adversarial learning (MTPAL) method for learning a nonlinear feedback policy that generalizes across multiple tasks, making cognizance ability of robot much closer to human-level decision making. The key idea is to construct a parametrized policy model directly from large high-dimensional observations by deep function approximators, and then train optimal of sequential decision policy for each new task by an adversarial process, in which simultaneously two models are trained: a multitask policy generator transforms samples drawn from a prior distribution into samples from a complex data distribution with higher dimensionality, and a multitask policy discriminator decides whether the given sample is prior distribution from human-level empirically derived or from the generator. All the related human-level empirically derived are integrated into the sequential decision policy, transferring human-level policy at every layer in a deep policy network. Extensive experimental testing result of four different WeiChai Power manufacturing data sets shows that our approach can surpass human performance simultaneously from cart-pole to production assembly control.",
      "authors": [
        "Jun Ping Wang",
        "You Kang Shi",
        "Wen Sheng Zhang",
        "Ian Thomas",
        "Shi Hui Duan"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TII.2018.2881266",
      "keywords": [
        "flexible manufacturing",
        "sequential decision making (SDM)",
        "Deep multitask reinforcement learning",
        "industrial big data"
      ],
      "number_of_pages": 10,
      "pages": "2395-2404",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1941-0050",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Industrial Informatics"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Multitask Policy Adversarial Learning for Human-Level Control With Large State Spaces",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056593046&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8534402"
      ]
    },
    {
      "abstract": "The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster.",
      "authors": [
        "Yunyang Xiong",
        "Ronak Mehta",
        "Vikas Singh"
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICCV.2019.00199",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "1901-1910",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-4804-5",
        "issn": "1550-5499",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the IEEE International Conference on Computer Vision"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Resource Constrained Neural Network Architecture Search: Will a Submodularity Assumption Help?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081908706&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9009490"
      ]
    },
    {
      "abstract": "The next generation vehicular applications substantially shifting the paradigm of human activity have been projected to empower intelligent transportation systems. Targeting at supporting vehicle-to-everything connections, conventional mobile network architectures mandatorily requiring data routing through the core network, however, induce unacceptable costs both in end-to-end latency and backhaul resource consumption. The technical merit of moving computation and storage resources along with mobile vehicles consequently renders the mobile edge computing (MEC) a promising remedy to relieve the burden at the core network. To practice MEC, 3GPP has launched the normative works of a new paradigm known as the local area data network (LADN). Through performing in-network cache to store popular information at LADNs, a vehicle locating within the service area of an LADN is able to access particular location-based wireless application and information. Avoiding data routing through the core network, LADNs, however, encounter two critical challenges in downlink radio access to induce additional latency issues: 1) resource starvation at fronthaul links and 2) discrimination of quality-of-service requirements of vehicles with distinct capabilities. To tackle these challenges, through formulating the Lyapunov function, a stochastic optimization maximizing the utilization of fronthaul resources while stabilizing the queue (and thus latency) of each vehicle is proposed to address the resource starvation. Subsequently, a reinforcement learning-based multiarmed bandit algorithm is further proposed to achieve optimum harmonization of feedback-based and feedbackless transmissions, so as to strikes the tradeoff among energy efficiency, latency, and reliability. The performance evaluation results full demonstrate the effectiveness of the proposed design, to serve urgent needs in the deployment of LADNs.",
      "authors": [
        "Shao-Yu Lien",
        "Shao-Chou Hung",
        "Der-Jiunn Deng",
        "Chia-Lin Lai",
        "Hua-Lung Tsai"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/JIOT.2018.2874883",
      "keywords": [
        "mobile edge computing (MEC)",
        "multiarmed bandit (MAB) problem",
        "reinforcement learning",
        "vehicle-to-everything (V2X)",
        "stochastic optimization",
        "radio access",
        "Local area data network (LADN)"
      ],
      "number_of_pages": 13,
      "pages": "4867-4879",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2372-2541",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Internet of Things Journal"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Low Latency Radio Access in 3GPP Local Area Data Networks for V2X: Stochastic Optimization and Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054638472&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8486631"
      ]
    },
    {
      "abstract": "Dialog management plays an important role in the task-oriented dialog system. Most of the previous works divide dialog management into state tracker and action selector. The two parts are modeled separately and implemented in a pipelined way, which suffers from the problem of error accumulation, and the feedback signal from action selector cannot be propagated to state tracker and natural language understanding module. This paper proposes a word-based partially observable Markov decision processes' dialog management that integrates natural language understanding, state tracker, and action selector into an end-to-end architecture. Our proposed dialog management takes the words from user utterances as inputs and then produces optimal action as well as slot values of natural language understanding which are necessary for response generation. To this end, we propose a hybrid learning method, which integrates reinforcement learning and supervised learning, to optimize the action selector and slot filler jointly. In addition, we develop a high-return prioritized experience replay to speed up the convergence of the training process. The experimental results show that the proposed dialog management outperforms four strong baselines in a series of different dialog tasks. A human user's evaluation also shows the same results. The high-return prioritized experience replay accelerates the convergence effectively, especially in the scenario in which the proposed dialog management works on more complex tasks.",
      "authors": [
        "Shuyu Lei",
        "Xiaojie Wang",
        "Caixia Yuan"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2019.2903863",
      "keywords": [
        "multi-layer neural network",
        "supervised learning",
        "reinforcement learning",
        "task-oriented dialog system",
        "dialog management",
        "Recurrent neural networks",
        "partially observable Markov decision processes"
      ],
      "number_of_pages": 8,
      "pages": "39236-39243",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Word-Based POMDP Dialog Management via Hybrid Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8664102",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065122675&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Faulkner T.K."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "728-736",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Active attention-modified policy shaping socially interactive agents track",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85076990606&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "42nd German Conference on Artificial Intelligence, KI 2019",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072870966&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 1.0,
        "is_potentially_predatory": false,
        "isbn": "9789819927883",
        "issn": "18650929",
        "publisher": "Springer Science and Business Media Deutschland GmbH",
        "sjr": 0.194,
        "snip": 0.241,
        "subject_areas": [
          "Computer Science (all)",
          "Mathematics (all)"
        ],
        "title": "International Conference on Neural Information Processing"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "6th International Conference on Robot Intelligence Technology and Applications, RiTA 2018",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065082317&origin=inward"
      ]
    },
    {
      "abstract": "Redundant muscles in human-like musculoskeletal robotics bring extra dimensions to solution space, which makes the computation of muscle excitation become an open question. Conventional methods like dynamic optimization and reinforcement learning usually have a high computational cost or an unstable learning process when they are applied into a complex musculoskeletal system. In this paper, inspired by the learning process of human, we build a phased target learning framework which provides different targets to learners from different levels to guide their training process and avoid local optima. By introducing an extra layer of neurons with preference, we improved the Q-network method to generate continuous excitation. In addition, based on the process of information transmission in human nervous system, two kinds of biological noises are designed in our algorithm to enhance exploration ability of our method in solution space. Tracking experiments based on a simplified musculoskeletal arm model indicate that under the guiding of phased targets, our method avoids divergence of excitation and obtains a stable training process. Besides, with its enhanced ability of exploration, our method shows smaller motion errors during the motion. Importantly, the phased target learning framework can be expanded as a general reinforcement learning framework, and it is a preliminary interpretation for modeling human motion learning manners.",
      "authors": [
        "Zhou, Junjie",
        "Chen, Jiahao",
        "Deng, Hu",
        "Qiao, Hong"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2019.00061",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "From Rough to Precise: Human-Inspired Phased Target Learning Framework for Redundant Musculoskeletal Systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072053487&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "5th International Conference on Artificial Intelligence and Security, ICAIS 2019",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85070207136&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mill\u00e1n C."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 6,
      "pages": "661-666",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9782875870650",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ESANN 2019 - Proceedings, 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Human feedback in continuous actor-critic reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071323792&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) has been a promising approach in robotics and control because data-driven learning methods can reduce system reliance on human engineering knowledge. A model-based RL autonomously learns observed dynamics based on a general flexible nonparametric approach. Probabilistic Inference for Learning COntrol (PILCO) is one of the most data-efficient model-based RL frameworks. Since PILCO sets up a Bayesian estimator problem with a Gaussian process regression, it derives a fully deterministic approximate inference for policy evaluation, which makes it computationally efficient. However, PILCO requires a task-specific scenario. If an agent is given a new goal that is different than the original training goal, PILCO should relearn its model from scratch. This paper extends PILCO to tune a linear feedback controller with a quadratic cost function, where the quadratic cost function commonly used in control systems can adjust the trade-off relationship between control input consumption and convergence rate. The suggested method is not only able to maintain the analytic and deterministic approximate inference for policy evaluation, but is also able to interpret the controller design. The suggested RL framework is applied to the control of a small quadrotor unmanned aerial vehicle (UAV) with no given dynamics. The simulation results show the convergence of the learning control performance as a function of the number of RL iterations.",
      "authors": [
        "\u00ec\u009c\u00a0\u00ec\u009e\u00ac\u00ed\u0098\u0084"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.5302/J.ICROS.2019.18.0221",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "746-751",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "19765622",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Institute of Control, Robotics and Systems"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "\u00ea\u00b0\u0080\u00ec\u009a\u00b0\u00ec\u008b\u009c\u00ec\u0095\u0088 \u00ed\u0094\u0084\u00eb\u00a1\u009c\u00ec\u0084\u00b8\u00ec\u008a\u00a4 \u00eb\u00aa\u00a8\u00eb\u008d\u00b8 \u00ea\u00b8\u00b0\u00eb\u00b0\u0098 \u00ea\u00b0\u0095\u00ed\u0099\u0094 \u00ed\u0095\u0099\u00ec\u008a\u00b5 ",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073286856&origin=inward"
      ]
    },
    {
      "abstract": "There have been a great deal of researches based on deep neural networks for the network anomalies in the areas of network server workloads. We focus on deep neural networks to deal with huge server loads of network anomalies in a distributed MMOGs (massively...",
      "authors": [
        "Kim, Chayoung",
        "Park, Jung-min",
        "Kim, Hye-young"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-981-13-1056-0_64",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "653-661",
      "publication": {
        "category": "Book",
        "cite_score": 0.6,
        "is_potentially_predatory": false,
        "isbn": "9789819926527",
        "issn": "18761100",
        "publisher": "Springer Verlag",
        "sjr": 0.147,
        "snip": 0.158,
        "subject_areas": [
          "Industrial and Manufacturing Engineering"
        ],
        "title": "International Conference in Communications, Signal Processing, and Systems "
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "An Actor-Critic Algorithm for SVM Hyperparameters",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051064775&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-981-13-1056-0_64.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bezalel V."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2018.09.016",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "25-35",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Inhibitory and excitatory mechanisms in the human cingulate-cortex support reinforcement learning: A functional Proton Magnetic Resonance Spectroscopy study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053199781&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Palan M."
      ],
      "categories": null,
      "citations": 41,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.15607/RSS.2019.XV.023",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780992374754",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Robotics: Science and Systems"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Learning Reward Functions by Integrating Human Demonstrations and Preferences",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077608923&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Alkoby S."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1773-1775",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Teaching social behavior through human reinforcement for ad hoc teamwork-the star framework",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074999964&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang J."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2019.2927606",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "92465-92475",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Terrain Adaptive Walking of Biped Neuromuscular Virtual Human Using Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85073887400&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rai L.A."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.addbeh.2018.08.019",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "73-76",
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03064603",
        "publisher": "Elsevier Ltd.",
        "sjr": 1.328,
        "snip": 1.371,
        "subject_areas": [
          "Toxicology",
          "Medicine (miscellaneous)",
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Addictive Behaviors"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Individual differences in learning from probabilistic reward and punishment predicts smoking status",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052129640&origin=inward"
      ]
    },
    {
      "abstract": "This review is concerned with methods for assessing the processing of unrewarded responses in experimental animals and the mechanisms underlying performance of these tasks. A number of clinical populations, including Parkinson\u2019s disease, depression, compulsive disorders, and schizophrenia demonstrate either abnormal processing or learning from non-rewarded responses in laboratory-based reinforcement learning tasks. These effects are hypothesized to result from disturbances in modulatory neurotransmitter systems, including dopamine and serotonin. Parallel work in experimental animals has revealed consistent behavioral patterns associated with non-reward and, consistent with the human literature, modulatory roles for specific neurotransmitters. Classical tests involving an important reward omission component include appetitive extinction, ratio schedules of responding, reversal learning, and delay and probability discounting procedures. In addition, innovative behavioral tests have recently been developed leverage probabilistic feedback to specifically assay accommodation of, and learning from, non-rewarded responses. These procedures will be described and reviewed with discussion of the behavioral and neural determinants of performance. A final section focusses specifically on the benefits of trial-by-trial analysis of responding during such tasks, and the implications of such analyses for the translation of findings to clinical studies.",
      "authors": [
        "Phillips, Benjamin U.",
        "Lopez-Cruz, Laura",
        "Saksida, Lisa M.",
        "Bussey, Timothy J."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00213-018-5062-x",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "449-461",
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00333158",
        "publisher": "Springer Verlag",
        "sjr": 1.05,
        "snip": 0.934,
        "subject_areas": [
          "Pharmacology"
        ],
        "title": "Psychopharmacology"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Translational tests involving non-reward: methodological considerations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055324035&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s00213-018-5062-x.pdf"
      ]
    },
    {
      "abstract": "\nCollective intelligence is one major outcome of the digital revolution, but this outcome is hardly evaluated. By implementing a topological knowledge graph (KG) in the metaphor of a brain, the ViewpointS approach attempts to trace and assess the dynamics of...",
      "authors": [
        "Lemoisson, Philippe",
        "Rakotondrahaja, Clarel M. H.",
        "Andriamialison, Aroniaina Safidy Pr\u00e9cieux",
        "Sankar, Harish A.",
        "Cerri, Stefano A."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-28377-3_1",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "3-15",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "VWA: ViewpointS Web Application to Assess Collective Knowledge Building",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-030-28377-3_1.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85072872828&origin=inward"
      ]
    },
    {
      "abstract": "Artificial intelligence, control theory and neuroscience have a long history of interplay. An example is human motor control: optimal feedback control describes low-level motor functions and reinforcement learning explains high-level decision-making, but where the...",
      "authors": [
        "Moulton, Richard Hugh"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-18305-9_63",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "584-587",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Discrete-Event Systems for Modelling Decision-Making in Human Motor Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066150446&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-030-18305-9_63.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gomez-Andres A."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nicl.2019.102075",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "NeuroImage: Clinical"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Electrophysiological correlates of feedback processing in subarachnoid hemorrhage patients",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85074880524&origin=inward"
      ]
    },
    {
      "abstract": "Recent years have seen a growing focus on automated personalized services, with music recommendations a particularly prominent domain for such contributions. However, while most prior work on music recommender systems has focused on preferences for songs ",
      "authors": [
        "Liebman, Elad",
        "Saar-Tsechansky, Maytal",
        "Stone, Peter"
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.25300/MISQ/2019/14750",
      "keywords": [],
      "number_of_pages": 22,
      "pages": "765-786",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02767783",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "MIS Quarterly: Management Information Systems"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "The Right Music at the Right Time:  Adaptive Personalized Playlists Based on Sequence Modeling",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071864632&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Spektor M.S."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/rev0000122",
      "keywords": [],
      "number_of_pages": 37,
      "pages": "52-88",
      "publication": {
        "category": "Journal",
        "cite_score": 9.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0033295X",
        "publisher": "American Psychological Association",
        "sjr": 2.801,
        "snip": 3.144,
        "subject_areas": [
          "Psychology (all)"
        ],
        "title": "Psychological Review"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "How similarity between choice options affects decisions from experience: The accentuation-of-differences model",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059461866&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "McDuff D."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "7th International Conference on Learning Representations, ICLR 2019"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Visceral machines: Risk-aversion in reinforcement learning with intrinsic physiological rewards",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083950220&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gupta R."
      ],
      "categories": null,
      "citations": 38,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "930-937",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358091",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Deep reinforcement learning for syntactic error repair in student programs",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080360208&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Walsh J.J."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.physbeh.2018.10.005",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "18-26",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00319384",
        "publisher": "Elsevier Inc.",
        "sjr": 0.753,
        "snip": 1.02,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience"
        ],
        "title": "Physiology and Behavior"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "High-intensity interval exercise impairs neuroelectric indices of reinforcement-learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054440801&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu H."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "8025-8033",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358091",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Mirroring without overimitation: Learning functionally equivalent manipulation actions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075332897&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Csifcs\u00e1k G."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_01515",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "646-663",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Intermittent absence of control during reinforcement learning interferes with pavlovian bias in action selection",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081135873&origin=inward"
      ]
    },
    {
      "abstract": "<p>Deep brain stimulation has developed into an established treatment for movement disorders and is being actively investigated for numerous other neurological as well as psychiatric disorders. An accurate electrode placement in the target area and the effective programming of DBS devices are considered the most important factors for the individual outcome. Recent research in humans highlights the relevance of widespread networks connected to specific DBS targets. Improving the targeting of anatomical and functional networks involved in the generation of pathological neural activity will improve the clinical DBS effect and limit side-effects. Here, we offer a comprehensive overview over the latest research on target structures and targeting strategies in DBS. In addition, we provide a detailed synopsis of novel technologies that will support DBS programming and parameter selection in the future, with a particular focus on closed-loop stimulation and associated biofeedback signals.</p>",
      "authors": [
        "Hell, Franz",
        "Palleis, Carla",
        "Mehrkens, Jan H.",
        "Koeglsperger, Thomas",
        "B\u00f6tzel, Kai"
      ],
      "categories": null,
      "citations": 51,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fneur.2019.00314",
      "keywords": [
        "Feedback signals",
        "adaptive closed loop stimulation",
        "Deep Brain Stimulation",
        "DBS target",
        "machine   learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-2295",
        "publisher": "Frontiers Media SA",
        "sjr": 0.978,
        "snip": 1.151,
        "subject_areas": [
          "Neurology (clinical)",
          "Neurology"
        ],
        "title": "Frontiers in Neurology"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Deep Brain Stimulation Programming 2.0: Future Perspectives for Target Identification and Adaptive Closed Loop Stimulation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067840942&origin=inward",
        "https://www.frontiersin.org/journals/neurology/articles/10.3389/fneur.2019.00314/pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Burnside R."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.13389",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "The feedback-related negativity indexes prediction error in active but not observational learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85065287935&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gandhi S."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1970-1972",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Learning behaviors from a single video demonstration using human feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85077092561&origin=inward"
      ]
    },
    {
      "abstract": "The path planning task is defined as the process to compute the motion sequence allowing the robot to move from the start position to the final destination autonomously without human actions. The path planning is one of the popular tasks encountered by imprecision...",
      "authors": [
        "Cherroun, L.",
        "Boumehraz, M.",
        "Kouzou, A."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-981-13-2212-9_12",
      "keywords": [],
      "number_of_pages": 29,
      "pages": "255-283",
      "publication": {
        "category": "Book",
        "cite_score": 1.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21984182",
        "publisher": "Springer International Publishing AG",
        "sjr": 0.11,
        "snip": 0.0,
        "subject_areas": [
          "Decision Sciences (miscellaneous)",
          "Social Sciences (miscellaneous)",
          "Economics, Econometrics and Finance (miscellaneous)",
          "Computer Science (miscellaneous)",
          "Control and Optimization",
          "Control and Systems Engineering",
          "Automotive Engineering"
        ],
        "title": "Studies in Systems, Decision and Control"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Mobile Robot Path Planning Based on Optimized Fuzzy Logic Controllers",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-981-13-2212-9_12.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061078762&origin=inward"
      ]
    },
    {
      "abstract": "Digital Library",
      "authors": [
        "Marc Pouly",
        "Thomas Koller",
        "Ruedi Arnold"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.5220/0007745203980404",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "398-404",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789897583674",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CSEDU 2019 - Proceedings of the 11th International Conference on Computer Supported Education"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "A Game-centric Approach to Teaching Artificial Intelligence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85067123107&origin=inward"
      ]
    },
    {
      "abstract": "Interactive Machine Learning is concerned with creating systems that operate in environments alongside humans to achieve a task. A typical use is to extend or amplify the capabilities of a human in cognitive or physical ways, requiring the machine to adapt to the...",
      "authors": [
        "Alonso, Miguel"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-030-23563-5_32",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "403-418",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Learning User Preferences via Reinforcement Learning with Spatial Interface Valuing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85069704220&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-030-23563-5_32.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Krening S."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "720-727",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Newtonian action advice: Integrating human verbal instruction with reinforcement learning socially interactive agents track",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85070634290&origin=inward"
      ]
    },
    {
      "abstract": "Motor and reinforcement learning have been classically linked to functionally independent brain networks centered on the cerebellum and the basal ganglia, respectively. In a recent study published in eNeuro , Therrien et al. (2018) showed that increasing motor noise in healthy subjects disrupts reinforcement learning. However, this impairment remained well below that detected in cerebellar patients even when motor noise in healthy subjects was adjusted to match that observed in the patients. This suggests that impaired reinforcement learning following cerebellar damage cannot be solely accounted for by altered motor noise in these patients. Based on recent anatomic and functional evidence, we argue that the cerebellum may directly contribute to reinforcement learning, consistent with its tight connections with the basal ganglia.\n\nThe ability to adapt to changes occurring in the environment is a fundamental feature of human behavior, which relies on both sensory and reward feedbacks. On the one hand, the role of sensory feedback has been largely considered by studying how motor commands adapt to visual perturbations (e.g., a visuomotor rotation), a process called error-based learning (Shadmehr et al., 2010; Wolpert et al., 2011; Kim et al., 2018; Roemmich and Bastian, 2018). This type of motor learning involves the computation of sensory prediction errors (SPEs), namely, the difference between predicted and actual sensory outcome (Tseng et al., 2007; Schlerf and Ivry, 2012; Shadmehr, 2017, 2018). On the other hand, the role of reward feedback has been mostly investigated in tasks that require learning what action to select or not, by updating reward predictions based on previous experience, a process named reinforcement learning (Lee et al., 2012; Derosiere et al., 2017a,b; Gershman and Daw, 2017; O\u2019Doherty et al., 2017). A central \u2026",
      "authors": [
        "Pierre Vassiliadis",
        "Gerard Derosiere",
        "Julie Duque"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/ENEURO.0458-18.2019",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2373-2822",
        "publisher": "Society for Neuroscience",
        "sjr": 1.309,
        "snip": 0.893,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "eNeuro"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Beyond Motor Noise: Considering Other Causes of Impaired Reinforcement Learning in Cerebellar Patients",
      "urls": [
        "https://www.eneuro.org/content/eneuro/6/1/ENEURO.0458-18.2019.full.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063783640&origin=inward"
      ]
    },
    {
      "abstract": "This paper describes the application of hierarchical temporal memory (HTM) to the task of anomaly detection in human motions. A number of model experiments with well-known motion dataset of Carnegie Mellon University have been carried out. An extended version of HTM...",
      "authors": [
        "Daylidyonok, Ilya",
        "Frolenkova, Anastasiya",
        "Panov, Aleksandr I."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-319-99316-4_10",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "69-81",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789811599521",
        "issn": "21945357",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent Human Systems Integration"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Extended Hierarchical Temporal Memory for Motion Anomaly Detection",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053179132&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-319-99316-4_10.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "32nd Canadian Conference on Artificial Intelligence, Canadian AI 2019",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066142191&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ceballos J.M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "205-211",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "0991196775",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 41st Annual Meeting of the Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "The Role of Basal Ganglia Reinforcement Learning in Lexical Priming and Automatic Semantic Ambiguity Resolution",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85139388655&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Correia A.H.C."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "2438-2445",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358091",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Human-in-the-loop feature selection",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85090808021&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chawla K."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "833-842",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781950737727",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CoNLL 2019 - 23rd Conference on Computational Natural Language Learning, Proceedings of the Conference"
      },
      "publication_date": "2019-01-01",
      "selected": null,
      "title": "Generating formality-tuned summaries using input-dependent rewards",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85084340099&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wu G."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICDM.2018.00070",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "547-556",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728183169",
        "issn": "15504786",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Data Mining, ICDM"
      },
      "publication_date": "2018-12-27",
      "selected": null,
      "title": "Human-Centric Urban Transit Evaluation and Planning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061374967&origin=inward"
      ]
    },
    {
      "abstract": "Instructions have a powerful effect on learning and decision-making, biasing choice even in the face of disconfirming feedback. Detrimental biasing effects have been reported in a number of studies in which instruction was given prior to trial-and-error learning. Previous work has attributed individual differences in instructional bias to variation in prefrontal and striatal dopaminergic genes, suggesting a role for prefrontally-mediated cognitive control processes in biasing learning. The current study replicates and extends these findings. Human subjects performed a probabilistic reinforcement learning task after receiving inaccurate instructions about the quality of one of the options. In order to establish a causal relationship between prefrontal cortical mechanisms and instructional bias, we applied transcranial direct current stimulation over dorsolateral prefrontal cortex (anodal, cathodal, or sham) while subjects performed the task. We additionally genotyped subjects for the COMT Val158Met genetic polymorphism, which influences the breakdown of prefrontal dopamine, and for the DAT1/SLC6A3 variable number tandem repeat, which affects expression of striatal dopamine transporter. We replicated the finding that the COMT Met allele is associated with increased instructional bias and further demonstrated that variation in DAT1 has similar effects to variation in COMT, with 9-repeat carriers demonstrating increased bias relative to 10-repeat homozygotes. Consistent with increased top-down regulation of reinforcement learning, anodal subjects demonstrated greater bias relative to sham, though this effect was present only early in training. In contrast, there was no effect of cathodal stimulation. Finally, we fit computational models to subjects\u2019 data to better characterize the mechanisms underlying instruction bias. A novel choice bias model, in which instructions influence decision-making rather than learning, was found to best account for subjects\u2019 behavior. Overall, these data provide further evidence for the role of frontostriatal interactions in biasing instructed reinforcement learning, which adds to the growing literature documenting both costs and benefits of cognitive control.",
      "authors": [
        "Tardiff, Nathan",
        "Graves, Kathryn N.",
        "Thompson-Schill, Sharon L."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2018.00472",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2018-12-17",
      "selected": null,
      "title": "The Role of Frontostriatal Systems in Instructed Reinforcement Learning: Evidence From Genetic and Experimentally-Induced Variation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059017564&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "H\u00f6ltje G."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.brainres.2018.07.011",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "64-74",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00068993",
        "publisher": "Elsevier B.V.",
        "sjr": 0.854,
        "snip": 0.766,
        "subject_areas": [
          "Neuroscience (all)",
          "Molecular Biology",
          "Neurology (clinical)",
          "Developmental Biology"
        ],
        "title": "Brain Research"
      },
      "publication_date": "2018-12-15",
      "selected": null,
      "title": "Electrophysiological reward signals predict episodic memory for immediate and delayed positive feedback events",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050524839&origin=inward"
      ]
    },
    {
      "abstract": "To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",
      "authors": [
        "Borja Ibarz",
        "Jan Leike",
        "Tobias Pohlen",
        "Geoffrey Irving",
        "Shane Legg",
        "Dario Amodei"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3327757.3327897",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "8022-8034",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 32nd International Conference on Neural Information Processing Systems"
      },
      "publication_date": "2018-12-03",
      "selected": null,
      "title": "Reward learning from human preferences and demonstrations in Atari",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3327757.3327897"
      ]
    },
    {
      "abstract": "Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",
      "authors": [
        "Xiaoxiao Guo",
        "Hui Wu",
        "Yu Cheng",
        "Steven Rennie",
        "Gerald Tesauro",
        "Rogerio Schmidt Feris"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3326943.3327006",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "676-686",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 32nd International Conference on Neural Information Processing Systems"
      },
      "publication_date": "2018-12-03",
      "selected": null,
      "title": "Dialog-based interactive image retrieval",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3326943.3327006"
      ]
    },
    {
      "abstract": "In economics and perceptual decision-making contextual effects are well documented, where decision weights are adjusted as a function of the distribution of stimuli. Yet, in reinforcement learning literature whether and how contextual information pertaining to decision states is integrated in learning algorithms has received comparably little attention. Here, we investigate reinforcement learning behavior and its computational substrates in a task where we orthogonally manipulate outcome valence and magnitude, resulting in systematic variations in state-values. Model comparison indicates that subjects\u2019 behavior is best accounted for by an algorithm which includes both reference point-dependence and range-adaptation\u2014two crucial features of state-dependent valuation. In addition, we find that state-dependent outcome valuation progressively emerges, is favored by increasing outcome information and correlated with explicit understanding of the task structure. Finally, our data clearly show that, while being locally adaptive (for instance in negative valence and small magnitude contexts), state-dependent valuation comes at the cost of seemingly irrational choices, when options are extrapolated out from their original contexts. Humans often make sub-optimal decisions, choosing options that are less advantageous than available alternatives. Using computational modeling of behavior, the authors demonstrate that such irrational choices can arise from context dependence in reinforcement learning.",
      "authors": [
        "Bavard, Sophie",
        "Lebreton, Ma\u00ebl",
        "Khamassi, Mehdi",
        "Coricelli, Giorgio",
        "Palminteri, Stefano"
      ],
      "categories": null,
      "citations": 42,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41467-018-06781-2",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 24.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2041-1723",
        "publisher": "Nature Publishing Group",
        "sjr": 5.116,
        "snip": 3.268,
        "subject_areas": [
          "Chemistry (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Physics and Astronomy (all)"
        ],
        "title": "Nature Communications"
      },
      "publication_date": "2018-12-01",
      "selected": null,
      "title": "Reference-point centering and range-adaptation enhance human reinforcement learning at the cost of irrational preferences",
      "urls": [
        "https://www.nature.com/articles/s41467-018-06781-2.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055612212&origin=inward"
      ]
    },
    {
      "abstract": "We set out to investigate whether beta oscillations in the human basal ganglia are modulated during reinforcement learning. Based on previous research, we assumed that beta activity might either reflect the magnitudes of individuals\u2019 received reinforcements (reinforcement hypothesis), their reinforcement prediction errors (dopamine hypothesis) or their tendencies to repeat versus adapt responses based upon reinforcements (status-quo hypothesis). We tested these hypotheses by recording local field potentials (LFPs) from the subthalamic nuclei of 19 Parkinson\u2019s disease patients engaged in a reinforcement-learning paradigm. We then correlated patients\u2019 reinforcement magnitudes, reinforcement prediction errors and response repetition tendencies with task-related power changes in their LFP oscillations. During feedback presentation, activity in the frequency range of 14 to 27\u2009Hz (beta spectrum) correlated positively with reinforcement magnitudes. During responding, alpha and low beta activity (6 to 18\u2009Hz) was negatively correlated with previous reinforcement magnitudes. Reinforcement prediction errors and response repetition tendencies did not correlate significantly with LFP oscillations. These results suggest that alpha and beta oscillations during reinforcement learning reflect patients\u2019 observed reinforcement magnitudes, rather than their reinforcement prediction errors or their tendencies to repeat versus adapt their responses, arguing both against an involvement of phasic dopamine and against applicability of the status-quo theory.",
      "authors": [
        "Schroll, Henning",
        "Horn, Andreas",
        "Runge, Joachim",
        "Lipp, Axel",
        "Schneider, Gerd-Helge",
        "Krauss, Joachim K.",
        "Hamker, Fred H.",
        "K\u00fchn, Andrea A."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-018-26887-3",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2018-12-01",
      "selected": null,
      "title": "Reinforcement magnitudes modulate subthalamic beta band activity in patients with Parkinson\u00e2\u0080\u0099s disease",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048069440&origin=inward",
        "https://www.nature.com/articles/s41598-018-26887-3.pdf"
      ]
    },
    {
      "abstract": "How do people protect themselves in response to negative social feedback from others? How does such a self-protective system develop and affect social decisions? Here, using a novel reciprocal artwork evaluation task, we demonstrate that youths show self-protective bias based on current negative social evaluation, whereas into early adulthood, individuals show self-protective bias based on accumulated evidence of negative social evaluation. While the ventromedial prefrontal cortex (VMPFC) mediates self-defensive behavior based on both current and accumulated feedback, the rostromedial prefrontal cortex (RMPFC) exclusively mediates self-defensive behavior based on longer feedback history. Further analysis using a reinforcement learning model suggests that RMPFC extending into VMPFC, together with posterior parietal cortex&nbsp;(PPC), contribute to age-related increases in self-protection bias with deep feedback integration by computing the discrepancy between current feedback and previously estimated value of self-protection. These findings indicate that the development of RMPFC function is critical for sophisticated self-protective decisions. People insulate themselves against negative social feedback via self-protective behaviors. Here, the authors show that early adolescents react against immediate social feedback, but adults also consider accumulated past negative evaluations, a function mediated by the rostromedial prefrontal cortex (RMPFC).",
      "authors": [
        "Yoon, Leehyun",
        "Somerville, Leah H.",
        "Kim, Hackjin"
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41467-018-05553-2",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 24.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2041-1723",
        "publisher": "Nature Publishing Group",
        "sjr": 5.116,
        "snip": 3.268,
        "subject_areas": [
          "Chemistry (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Physics and Astronomy (all)"
        ],
        "title": "Nature Communications"
      },
      "publication_date": "2018-12-01",
      "selected": null,
      "title": "Development of MPFC function mediates shifts in self-protective behavior provoked by social feedback",
      "urls": [
        "https://www.nature.com/articles/s41467-018-05553-2.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051241487&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Shahnazian D."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2018.07.064",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "121-131",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2018-12-01",
      "selected": null,
      "title": "Electrophysiological responses of medial prefrontal cortex to feedback at different levels of hierarchy",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051404670&origin=inward"
      ]
    },
    {
      "abstract": "Learning the causal relation between actions and their outcomes (AO learning) is critical for goal-directed behavior when actions are guided by desire for the outcome. This can be contrasted with habits that are acquired by reinforcement and primed by prevailing stimuli, in which causal learning plays no part. Recently, we demonstrated that goal-directed actions are impaired in schizophrenia; however, whether this deficit exists alongside impairments in habit or reinforcement learning is unknown. The present study distinguished deficits in causal learning from reinforcement learning in schizophrenia. We tested people with schizophrenia (SZ, n\u2009=\u200925) and healthy adults (HA, n\u2009=\u200925) in a vending machine task. Participants learned two action\u2013outcome contingencies (e.g., push left to get a chocolate M&amp;M, push right to get a cracker), and they also learned one contingency was degraded by delivery of noncontingent outcomes (e.g., free M&amp;Ms), as well as changes in value by outcome devaluation. Both groups learned the best action to obtain rewards; however, SZ did not distinguish the more causal action when one AO contingency was degraded. Moreover, action selection in SZ was insensitive to changes in outcome value unless feedback was provided, and this was related to the deficit in AO learning. The failure to encode the causal relation between action and outcome in schizophrenia occurred without any apparent deficit in reinforcement learning. This implies that poor goal-directed behavior in schizophrenia cannot be explained by a more primary deficit in reward learning such as insensitivity to reward value or reward prediction errors.",
      "authors": [
        "Morris, Richard W.",
        "Cyrzon, Chad",
        "Green, Melissa J.",
        "Le Pelley, Mike E.",
        "Balleine, Bernard W."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41398-018-0103-0",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 10.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2158-3188",
        "publisher": "Nature Publishing Group",
        "sjr": 2.148,
        "snip": 1.549,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Translational Psychiatry"
      },
      "publication_date": "2018-12-01",
      "selected": null,
      "title": "Impairments in action\u00e2\u0080\u0093outcome learning in schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042859157&origin=inward",
        "https://www.nature.com/articles/s41398-018-0103-0.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sidarta A."
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00442.2018",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "3275-3286",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2018-12-01",
      "selected": null,
      "title": "Somatosensory working memory in human reinforcement-based motor learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058957375&origin=inward"
      ]
    },
    {
      "abstract": "To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",
      "authors": [
        "Ibarz, Borja",
        "Leike, Jan",
        "Pohlen, Tobias",
        "Irving, Geoffrey",
        "Legg, Shane",
        "Amodei, Dario"
      ],
      "categories": null,
      "citations": 62,
      "comments": "NIPS 2018",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 13,
      "pages": "8011-8023",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2018-11-15",
      "selected": null,
      "title": "Reward learning from human preferences and demonstrations in Atari",
      "urls": [
        "http://arxiv.org/abs/1811.06521v1",
        "http://arxiv.org/pdf/1811.06521.pdf",
        "http://arxiv.org/pdf/1811.06521v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064816289&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kumagai K."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ROMAN.2018.8525679",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "678-685",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781538679807",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "RO-MAN 2018 - 27th IEEE International Symposium on Robot and Human Interactive Communication"
      },
      "publication_date": "2018-11-06",
      "selected": null,
      "title": "Towards Individualized Affective Human-Machine Interaction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058075738&origin=inward"
      ]
    },
    {
      "abstract": "In order to operate in everyday human environment, humanoids robots will need to autonomously learn and adapt their actions, using among other reinforcement learning methods (RL). A common challenge in robotic RL is also the generation of appropriate reward functions. A vast body of literature investigates how active human feedback can be introduced into an interactive learning loop, with recent publications showing that user feedback can be used for the RL reward. However, increased complexity of robotic skills in everyday environment also increases their dimensionality, which can practically prevent use of user feedback for the reward, because too many trials are needed. In the paper we present the results of using discretized, user-assigned reward for RL of robotic throwing, with an emphasis on learning in the feature space, i. e., latent space of a deep autoencoder network. Statistical evaluation of a user study with 15 participants, who provided feedback for robotic throwing experiments, show that for certain tasks, RL with discrete user feedback can be effectively applied for robot learning.",
      "authors": [
        "Rok Pahi\u010d",
        "Zvezdan Lon\u010darevi\u0107",
        "Ale\u0161 Ude",
        "Bojan Nemec",
        "Andrej Gams"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/HUMANOIDS.2018.8624972",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "270-276",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-7284-6",
        "issn": "2164-0572",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids)"
      },
      "publication_date": "2018-11-06",
      "selected": null,
      "title": "User Feedback in Latent Space Robotic Skill Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1109/HUMANOIDS.2018.8624972",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062274009&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8624972"
      ]
    },
    {
      "abstract": "Thanks to the adoption of more sensors in the automotive industry, context-aware Advanced Driver Assistance Systems (ADAS) become possible. On one side, a common thread in ADAS applications is to focus entirely on the context of the vehicle and its surrounding vehicles leaving the human (driver) context out of consideration. On the other side, and due to the increasing sensing capabilities in mobile phones and wearable technologies, monitoring complex human context becomes feasible which paves the way to develop driver-in-the-loop context-aware ADAS that provide personalized driving experience. In this paper, we propose Sentio1; a Reinforcement Learning based algorithm to enhance the Forward Collision Warning (FCW) system leading to Driver-in-the-Loop FCW system. Since the human driving preference is unknown a priori, varies between different drivers, and moreover, varies across time for the same driver, the proposed Sentio algorithm needs to take into account all these variabilities which are not handled by the standard reinforcement learning algorithms. We verified the proposed algorithm against several human drivers. Our evaluation, across distracted human drivers, shows a significant enhancement in driver experience---compared to standard FCW systems---reflected by an increase in the driver safety by 94.28%, an improvement in the driving experience by 20.97%, a decrease in the false negatives from 55.90% down to 3.26%, while adding less than 130 ms runtime execution overhead.",
      "authors": [
        "Salma Elmalaki",
        "Huey-Ru Tsai",
        "Mani Srivastava"
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3274783.3274843",
      "keywords": [
        "Human-in-the-Loop",
        "Human Vehicular Interaction",
        "Autonomous Systems",
        "Reinforcement Learning",
        "Personalized ADAS"
      ],
      "number_of_pages": 13,
      "pages": "28-40",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450359528",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "SenSys 2018 - Proceedings of the 16th Conference on Embedded Networked Sensor Systems"
      },
      "publication_date": "2018-11-04",
      "selected": null,
      "title": "Sentio: Driver-in-the-Loop Forward Collision Warning Using Multisample Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3274783.3274843",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058235753&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "van de Vijver I."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2018.07.014",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "170-181",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2018-11-01",
      "selected": null,
      "title": "Interactions between frontal and posterior oscillatory dynamics support adjustment of stimulus processing during reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049611676&origin=inward"
      ]
    },
    {
      "abstract": "<h3>Abstract</h3>\n<p>We propose a model that includes interactions between the cortex, the basal ganglia (BG), and the thalamus based on a dual competition. We hypothesize that the striatum, the subthalamic nucleus (STN), the internal globus pallidus (GPi), the thalamus, and the cortex are involved in closed feedback loops through the hyperdirect and direct pathways. These loops support a competition process that results in the ability of BG to make a cognitive decision followed by a motor one. Considering lateral cortical interactions, another competition takes place inside the cortex allowing the latter to make a cognitive and a motor decision. We show how this dual competition endows the model with two regimes. One is driven by reinforcement learning and the other by Hebbian learning. The final decision is made according to a combination of these two mechanisms with a gradual transfer from the former to the latter. We confirmed these theoretical results on primates (<i>Macaca mulatta</i>) using a novel paradigm predicted by the model.</p>",
      "authors": [
        "Meropi Topalidou",
        "Daisuke Kase",
        "Thomas Boraud",
        "Nicolas P. Rougier"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/ENEURO.0339-17.2018",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2373-2822",
        "publisher": "Society for Neuroscience",
        "sjr": 1.309,
        "snip": 0.893,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "eNeuro"
      },
      "publication_date": "2018-11-01",
      "selected": null,
      "title": "A Computational Model of Dual Competition between the Basal Ganglia and the Cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85059795522&origin=inward",
        "https://www.eneuro.org/content/eneuro/5/6/ENEURO.0339-17.2018.full.pdf"
      ]
    },
    {
      "abstract": "Cognition can reveal itself in the pupil, as latent cognitive processes map onto specific pupil responses. For instance, the pupil dilates when we make decisions and these pupil size fluctuations reflect decision-making computations during and after a choice. Surprisingly little is known, however, about how pupil responses relate to decisions driven by the learned value of stimuli. This understanding is important, as most real-life decisions are guided by the outcomes of earlier choices. The goal of this study was to investigate which cognitive processes the pupil reflects during value-based decision-making. We used a reinforcement learning task to study pupil responses during value-based decisions and subsequent decision evaluations, employing computational modeling to quantitatively describe the underlying cognitive processes. We found that the pupil closely tracks reinforcement learning processes independently across participants and across trials. Prior to choice, the pupil dilated as a function of trial-by-trial fluctuations in value beliefs about the to-be chosen option and predicted an individual\u2019s tendency to exploit high value options. After feedback a biphasic pupil response was observed, the amplitude of which correlated with participants\u2019 learning rates. Furthermore, across trials, early feedback-related dilation scaled with value uncertainty, whereas later constriction scaled with signed reward prediction errors. These findings show that pupil size fluctuations can provide detailed information about the computations underlying value-based decisions and the subsequent updating of value beliefs. As these processes are affected in a host of psychiatric disorders, our results indicate that pupillometry can be used as an accessible tool to non-invasively study the processes underlying ongoing reinforcement learning in the clinic.",
      "authors": [
        "Joanne C. Van Slooten",
        "Sara Jahfari",
        "Tomas Knapen",
        "Jan Theeuwes"
      ],
      "categories": null,
      "citations": 35,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1006632",
      "keywords": [
        "Permutation",
        "Eye movements",
        "Dopamine",
        "Learning",
        "Behavior",
        "Decision making",
        "Attention",
        "Cognition"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2018-11-01",
      "selected": null,
      "title": "How pupil responses track value-based decision-making during and after reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058120805&origin=inward",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1006632&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ballard I."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/cercor/bhx259",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "3965-3975",
      "publication": {
        "category": "Journal",
        "cite_score": 8.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10473211",
        "publisher": "Oxford University Press",
        "sjr": 1.738,
        "snip": 1.159,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Cerebral Cortex"
      },
      "publication_date": "2018-11-01",
      "selected": null,
      "title": "Beyond Reward Prediction Errors: Human Striatum Updates Rule Values during Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052631397&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Li P."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2018.09.002",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "17-27",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2018-11-01",
      "selected": null,
      "title": "Disappearance of self-serving bias: Reward positivity reflects performance monitoring modulated by responsibility attribution in a two-person cooperative task",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053195742&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Uro\u0161evi\u0107 S."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/abn0000388",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "807-817",
      "publication": {
        "category": "Journal",
        "cite_score": 11.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0021843X",
        "publisher": "American Psychological Association",
        "sjr": 2.031,
        "snip": 2.532,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "Journal of Abnormal Psychology"
      },
      "publication_date": "2018-11-01",
      "selected": null,
      "title": "Probabilistic reinforcement learning abnormalities and their correlates in adolescent bipolar disorders.",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056629808&origin=inward"
      ]
    },
    {
      "abstract": "Trial-and-error learning is a universal strategy for establishing which actions are beneficial or harmful in new environments. However, learning stimulus-response associations solely via trial-and-error is often suboptimal, as in many settings dependencies among stimuli and responses can be exploited to increase learning efficiency. Previous studies have shown that in settings featuring such dependencies, humans typically engage high-level cognitive processes and employ advanced learning strategies to improve their learning efficiency. Here we analyze in detail the initial learning phase of a sample of human subjects (N = 85) performing a trial-and-error learning task with deterministic feedback and hidden stimulus-response dependencies. Using computational modeling, we find that the standard Q-learning model cannot sufficiently explain human learning strategies in this setting. Instead, newly introduced deterministic response models, which are theoretically optimal and transform stimulus sequences unambiguously into response sequences, provide the best explanation for 50.6% of the subjects. Most of the remaining subjects either show a tendency towards generic optimal learning (21.2%) or at least partially exploit stimulus-response dependencies (22.3%), while a few subjects (5.9%) show no clear preference for any of the employed models. After the initial learning phase, asymptotic learning performance during the subsequent practice phase is best explained by the standard Q-learning model. Our results show that human learning strategies in the presented trial-and-error learning task go beyond merely associating stimuli and responses via incremental reinforcement. Specifically during initial learning, high-level cognitive processes support sophisticated learning strategies that increase learning efficiency while keeping memory demands and computational efforts bounded. The good asymptotic fit of the Q-learning model indicates that these cognitive processes are successively replaced by the formation of stimulus-response associations over the course of learning.",
      "authors": [
        "Holger Mohr",
        "Katharina Zwosta",
        "Dimitrije Markovic",
        "Sebastian Bitzer",
        "Uta Wolfensteller",
        "Hannes Ruge"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1006621",
      "keywords": [
        "Phase determination",
        "Learning",
        "Human learning",
        "Learning curves",
        "Cognition",
        "Working memory",
        "Statistical models",
        "Control theory"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2018-11-01",
      "selected": null,
      "title": "Deterministic response strategies in a trial-and-error learning task",
      "urls": [
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1006621&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058097102&origin=inward"
      ]
    },
    {
      "abstract": "Exploration has been one of the greatest challenges in reinforcement learning (RL), which is a large obstacle in the application of RL to robotics. Even with state-of-the-art RL algorithms, building a well-learned agent often requires too many trials, mainly due to the difficulty of matching its actions with rewards in the distant future. A remedy for this is to train an agent with real-time feedback from a human observer who immediately gives rewards for some actions. This study tackles a series of challenges for introducing such a human-in-the-loop RL scheme. The first contribution of this work is our experiments with a precisely modeled human observer: binary, delay, stochasticity, unsustainability, and natural reaction. We also propose an RL method called DQN-TAMER, which efficiently uses both human feedback and distant rewards. We find that DQN-TAMER agents outperform their baselines in Maze and Taxi simulated environments. Furthermore, we demonstrate a real-world human-in-the-loop RL application where a camera automatically recognizes a user's facial expressions as feedback to the agent while the agent explores a maze.",
      "authors": [
        "Arakawa, Riku",
        "Kobayashi, Sosuke",
        "Unno, Yuya",
        "Tsuboi, Yuta",
        "Maeda, Shin-ichi"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-10-28",
      "selected": null,
      "title": "DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback",
      "urls": [
        "http://arxiv.org/pdf/1810.11748.pdf",
        "http://arxiv.org/pdf/1810.11748v1",
        "http://arxiv.org/abs/1810.11748v1"
      ]
    },
    {
      "abstract": "In this paper, we present a planning framework that uses a combination of\nimplicit (robot motion) and explicit (visual/audio/haptic feedback)\ncommunication during mobile robot navigation. First, we developed a model that\napproximates both continuous movements and discrete behavior modes in human\nnavigation, considering the effects of implicit and explicit communication on\nhuman decision making. The model approximates the human as an optimal agent,\nwith a reward function obtained through inverse reinforcement learning. Second,\na planner uses this model to generate communicative actions that maximize the\nrobot's transparency and efficiency. We implemented the planner on a mobile\nrobot, using a wearable haptic device for explicit communication. In a user\nstudy of an indoor human-robot pair of orthogonal crossing situation, the robot\nwas able to actively communicate its intent to users in order to avoid\ncollisions and facilitate efficient trajectories. Results showed that the\nplanner generated plans that were easier to understand, reduced users' effort,\nand increased users' trust of the robot, compared to simply performing\ncollision avoidance. The key contribution of this work is the integration and\nanalysis of explicit communication (together with implicit communication) for\nsocial navigation.",
      "authors": [
        "Yuhang Che",
        "Allison M. Okamura",
        "Dorsa Sadigh"
      ],
      "categories": null,
      "citations": 50,
      "comments": null,
      "databases": [
        "IEEE",
        "arXiv",
        "Scopus"
      ],
      "doi": "10.1109/TRO.2020.2964824",
      "keywords": [
        "motion and path planning",
        "social human\u2013robot interaction",
        "human-centered robotics",
        "Haptics and haptic interfaces"
      ],
      "number_of_pages": 16,
      "pages": "692-707",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1941-0468",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Robotics"
        ],
        "title": "IEEETransactionsonRobotics,pp(99):1-16,2020"
      },
      "publication_date": "2018-10-26",
      "selected": null,
      "title": "Efficient and Trustworthy Social Navigation Via Explicit and Implicit Robot-Human Communication",
      "urls": [
        "http://dx.doi.org/10.1109/TRO.2020.2964824",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087063126&origin=inward",
        "http://arxiv.org/pdf/1810.11556v2",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8967120",
        "http://arxiv.org/abs/1810.11556v2"
      ]
    },
    {
      "abstract": "We propose a biologically inspired model that enables a humanoid robot to learn how to track its end effector by integrating visual and proprioceptive cues as it interacts with the environment. A key novel feature of this model is the incorporation of sensorimotor prediction, where the robot predicts the sensory consequences of its current body motion as measured by proprioceptive feedback. The robot develops the ability to perform smooth pursuit-like eye movements to track its hand, both in the presence and absence of visual input, and to track exteroceptive visual motions. Our framework makes a number of advances over past work. First, our model does not require a fiducial marker to indicate the robot hand explicitly. Second, it does not require the forward kinematics of the robot arm to be known. Third, it does not depend upon pre-defined visual feature descriptors. These are learned during interaction with the environment. We demonstrate that the use of prediction in multisensory integration enables the agent to incorporate the information from proprioceptive and visual cues better. The proposed model has properties that are qualitatively similar to the characteristics of human eye-hand coordination.",
      "authors": [
        "Wijesinghe, Lakshitha P.",
        "Triesch, Jochen",
        "Shi, Bertram E."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2018.00066",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2018-10-16",
      "selected": null,
      "title": "Robot End Effector Tracking Using Predictive Multisensory Integration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055324247&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Yin H."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neulet.2018.08.039",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "179-184",
      "publication": {
        "category": "Journal",
        "cite_score": 5.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03043940",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 0.802,
        "snip": 0.777,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Neuroscience Letters"
      },
      "publication_date": "2018-10-15",
      "selected": null,
      "title": "Feedback delay impaired reinforcement learning: Principal components analysis of Reward Positivity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052651858&origin=inward"
      ]
    },
    {
      "abstract": "In assistive teleoperation, an autonomous agent uses a prediction about a human user's intent to attempt to align the behavior of a controlled system with the human's goal, even if the human's own inputs are not perfectly aligned to that goal. Haptic Assistance achieves this effect by influencing the human through forces/torques applied to the human's control interface. In this work, we describe our method for creating such haptic assistance via Inverse Reinforcement Learning applied to successful task demonstrations. We then use our assistance method to examine the role that haptic feedback plays in assistive teleoperation. Through our user study, we find that when the assistance incorrectly predicts a user's intent, aiding the user via haptic feedback on their control interface, rather than directly modifying their input signal, is preferable and provides the user with a significantly greater sense of control over the system.",
      "authors": [
        "Dexter R.R. Scobee",
        "Vicenc Rubies Royo",
        "Claire J. Tomlin",
        "S. Shankar Sastry"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/SMC.2018.00262",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1510-1517",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "979-8-3503-3703-7",
        "issn": "1062-922X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018"
      },
      "publication_date": "2018-10-07",
      "selected": null,
      "title": "Haptic Assistance via Inverse Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8616258",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062208365&origin=inward",
        "https://dl.acm.org/doi/10.1109/SMC.2018.00262"
      ]
    },
    {
      "abstract": "A shared sense of humor can result in positive feelings associated with amusement, laughter, and moments of bonding. If robotic companions could acquire their human counterparts' sense of humor in an unobtrusive manner, they could improve their skills of engagement. In order to explore this assumption, we have developed a dynamic user modeling approach based on Reinforcement Learning, which allows a robot to analyze a person's reaction while it tells jokes and continuously adapts its sense of humor. We evaluated our approach in a test scenario with a Reeti robot acting as an entertainer and telling different types of jokes. The exemplary adaptation process is accomplished only by using the audience's vocal laughs and visual smiles, but no other form of explicit feedback. We report on results of a user study with 24 participants, comparing our approach to a baseline condition (with a non-learning version of the robot) and conclude by providing limitations and implications of our approach in detail.",
      "authors": [
        "Klaus Weber",
        "Hannes Ritschel",
        "Ilhan Aslan",
        "Florian Lingenfelser",
        "Elisabeth Andr\u00e9"
      ],
      "categories": null,
      "citations": 55,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3242969.3242976",
      "keywords": [
        "socially-aware agents",
        "social adaptation",
        "human-robot-interaction"
      ],
      "number_of_pages": 9,
      "pages": "154-162",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450356923",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction"
      },
      "publication_date": "2018-10-02",
      "selected": null,
      "title": "How to Shape the Humor of a Robot - Social Behavior Adaptation Based on Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056634298&origin=inward",
        "https://dl.acm.org/doi/10.1145/3242969.3242976"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Krigolson O.E."
      ],
      "categories": null,
      "citations": 93,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2017.11.007",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "175-183",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Event-related brain potentials and the study of reward processing: Methodological considerations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85035110108&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Uehara S."
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/cercor/bhx214",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "3478-3490",
      "publication": {
        "category": "Journal",
        "cite_score": 8.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10473211",
        "publisher": "Oxford University Press",
        "sjr": 1.738,
        "snip": 1.159,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Cerebral Cortex"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Learning similar actions by reinforcement or sensory-prediction errors rely on distinct physiological mechanisms",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053892560&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Williams C.C."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2017.10.010",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "236-242",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "The application of reward learning in the real world: Changes in the reward positivity amplitude reflect learning in a medical education context",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85033678941&origin=inward"
      ]
    },
    {
      "abstract": "BackgroundThe significant proportion of schizophrenia patients refractory to treatment, primarily directed at the dopamine system, suggests that multiple mechanisms may underlie psychotic symptoms. Reinforcement learning tasks have been employed in schizophrenia to assess dopaminergic functioning and reward processing, but these have not directly compared groups of treatment-refractory and non-refractory patients.MethodsIn the current functional magnetic resonance imaging study, 21 patients with treatment-resistant schizophrenia (TRS), 21 patients with non-treatment-resistant schizophrenia (NTR), and 24 healthy controls (HC) performed a probabilistic reinforcement learning task, utilizing emotionally valenced face stimuli which elicit a social bias toward happy faces. Behavior was characterized with a reinforcement learning model. Trial-wise reward prediction error (RPE)-related neural activation and the differential impact of emotional bias on these reward signals were compared between groups.ResultsPatients showed impaired reinforcement learning relative to controls, while all groups demonstrated an emotional bias favoring happy faces. The pattern of RPE signaling was similar in the HC and TRS groups, whereas NTR patients showed significant attenuation of RPE-related activation in striatal, thalamic, precentral, parietal, and cerebellar regions. TRS patients, but not NTR patients, showed a positive relationship between emotional bias and RPE signal during negative feedback in bilateral thalamus and caudate.ConclusionTRS can be dissociated from NTR on the basis of a different neural mechanism underlying reinforcement learning. The data support the hypothesis that a favorable response to antipsychotic treatment is contingent on dopaminergic dysfunction, characterized by aberrant RPE signaling, whereas treatment resistance may be characterized by an abnormality of a non-dopaminergic mechanism \u2013 a glutamatergic mechanism would be a possible candidate.",
      "authors": [
        "Lucy D. Vanes",
        "Elias Mouchlianitis",
        "Tracy Collier",
        "Bruno B. Averbeck",
        "Sukhi S. Shergill"
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1017/S0033291718000041",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "2418-2427",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00332917",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychological Medicine"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Differential neural reward mechanisms in treatment-responsive and treatment-resistant schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052957593&origin=inward"
      ]
    },
    {
      "abstract": "Although a growing number of studies have investigated the neural mechanisms of reinforcement learning, it remains unclear how the brain responds to feedback that is unreliable. A recent theory proposes that the reward positivity (RewP) component of the event-related brain potential (ERP) and frontal midline theta (FMT) power reflect separate feedback-related processing functions of anterior cingulate cortex (ACC). In the present study, the electroencephalogram (EEG) was recorded from participants as they engaged in a time estimation task in which feedback reliability was manipulated across conditions. After each response, they received a cue that indicated that the following feedback stimulus was 100%, 75%, or 50% reliable. The results showed that participants\u2019 time estimates adjusted linearly according to the feedback reliability. Moreover, presentation of the cue indicating 100% reliability elicited a larger RewP-like ERP component than the other cues did, and feedback presentation elicited a RewP of approximately equal amplitude for all of the three reliability conditions. By contrast, FMT power elicited by negative feedback decreased linearly from the 100% condition to 75% and 50% condition, and only FMT power predicted behavioral adjustments on the following trials. In addition, an analysis of Beta power and cross-frequency coupling (CFC) of Beta power with FMT phase suggested that Beta-FMT communication modulated motor areas for the purpose of adjusting behavior. We interpreted these findings in terms of the hierarchical reinforcement learning account of ACC, in which the RewP and FMT are proposed to reflect reward processing and control functions of ACC, respectively.",
      "authors": [
        "Li, Peng",
        "Peng, Weiwei",
        "Li, Hong",
        "Holroyd, Clay B."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-018-0615-3",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "949-963",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Electrophysiological measures reveal the role of anterior cingulate cortex in learning from unreliable feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049683924&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-018-0615-3.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Cockburn J."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2017.11.017",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "243-251",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Feedback information and the reward positivity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85039434637&origin=inward"
      ]
    },
    {
      "abstract": "Robots deployed for long periods of time need to be able to explore and learn from their environment. One approach to this problem has been reinforcement learning (RL), in which robots receive rewards from the environment that allow them to choose optimal actions. To speed learning when human supervision is available, interactive reinforcement learning solicits feedback from a human teacher. However, this approach typically assumes that learning takes place under continuous supervision, which is unlikely to hold in long-term scenarios. We propose an extension to a method of interactive reinforcement learning, policy shaping, that takes into account human attention. Our approach enables better performance while unattended by favoring information-gathering actions when attended and actions that have received positive feedback when unattended. We test our approach in both simulation and on a robot, finding that our method learns faster than policy shaping and performs more safely than policy shaping while no one is paying attention to the robot.",
      "authors": [
        "Taylor Kessler Faulkner",
        "Elaine Schaertl Short",
        "Andrea Lockerd Thomaz"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/IROS.2018.8594312",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "842-847",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Policy Shaping with Supervisory Attention Driven Exploration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062980155&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594312",
        "https://dl.acm.org/doi/10.1109/IROS.2018.8594312"
      ]
    },
    {
      "abstract": "For a service robot, it is not adequate to let its navigational movement be based only on a single metric, such as minimum distance path. In the environment where the robot and humans are coexisting, the robot should always perform social navigation whenever it is moving. However, to perform social navigation, the robot needs to follow certain &#x201C;social norms&#x201D; of the environment. Recently, deep reinforcement learning (DRL) technique is popularly applied to the robotics field; yet, it is rarely used to solve the mentioned social navigation problem, generally deemed as a high dimension complex problem. In this paper, we propose the composite reinforcement learning (CRL) framework under which the robot learns appropriate social navigation with sensor input and reward update based on human feedback. For learning the aspect of human robot interaction (HRI), we provide a method to facilitate the training of DRL in real environment by incorporating prior knowledge to the system. It turns out that our CRL system not only can incrementally learn how to set its velocity and to perform HRI but also keep collecting human feedback to synchronize the reward functions to the current social norms. The experiments show that the proposed CRL system can safely learn how to navigate in the environment and show that our system is able to perform HRI for social navigation.",
      "authors": [
        "Pei-Huai Ciou",
        "Yu-Ting Hsiao",
        "Zong-Ze Wu",
        "Shih-Huan Tseng",
        "Li-Chen Fu"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/IROS.2018.8593410",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "2553-2558",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Composite Reinforcement Learning for Social Robot Navigation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062961710&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593410",
        "https://dl.acm.org/doi/10.1109/IROS.2018.8593410"
      ]
    },
    {
      "abstract": "Behavior trees (BT) are a popular control architecture in the computer game industry, and have been more recently applied in robotics. One open question is how can intelligent agents/robots autonomously acquire their behavior trees for task level control? In contrast with existing approaches that either refine an initially given BT, or directly build the BT based on human feedback/demonstration, we leverage reinforcement learning (RL) that allows robots to autonomously learn control policies by repeated task interaction, but often expressed in a language more difficult to interpret than BTs. The learned control policy is then converted to a behavior tree via our proposed decanonicalization algorithm. The feasibility of this idea is based on a proposed notion of canonical behavior trees (CBT). In particular, we show (1) CBTs are sufficiently expressive to capture RL control policies, and (2) that RL can be independent of an optimal behavior permutation, despite the BT convention of left-to-right priority, thus obviating the need for a combinatorial search. Two evaluation domains help illustrate our approach.",
      "authors": [
        "Bikramjit Banerjee"
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/IROS.2018.8594083",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "3460-3467",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-9191-4",
        "issn": "2153-0858",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2018-10-01",
      "selected": null,
      "title": "Autonomous Acquisition of Behavior Trees for Robot Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062968996&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594083",
        "https://dl.acm.org/doi/10.1109/IROS.2018.8594083"
      ]
    },
    {
      "abstract": "Deep Reinforcement Learning (DRL) has become a powerful strategy to solve complex decision making problems based on Deep Neural Networks (DNNs). However, it is highly data demanding, so unfeasible in physical systems for most applications. In this work, we approach an alternative Interactive Machine Learning (IML) strategy for training DNN policies based on human corrective feedback, with a method called Deep COACH (D-COACH). This approach not only takes advantage of the knowledge and insights of human teachers as well as the power of DNNs, but also has no need of a reward function (which sometimes implies the need of external perception for computing rewards). We combine Deep Learning with the COrrective Advice Communicated by Humans (COACH) framework, in which non-expert humans shape policies by correcting the agent's actions during execution. The D-COACH framework has the potential to solve complex problems without much data or time required. Experimental results validated the efficiency of the framework in three different problems (two simulated, one with a real robot), with state spaces of low and high dimensions, showing the capacity to successfully learn policies for continuous action spaces like in the Car Racing and Cart-Pole problems faster than with DRL.",
      "authors": [
        "P\u00e9rez-Dattari, Rodrigo",
        "Celemin, Carlos",
        "Ruiz-del-Solar, Javier",
        "Kober, Jens"
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages, 7 figures, 1 table, conference (ISER 2018)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-09-30",
      "selected": null,
      "title": "Interactive Learning with Corrective Feedback for Policies based on Deep Neural Networks",
      "urls": [
        "http://arxiv.org/pdf/1810.00466v1",
        "http://arxiv.org/abs/1810.00466v1",
        "http://arxiv.org/pdf/1810.00466.pdf"
      ]
    },
    {
      "abstract": "We present a robot eye-hand coordination learning method that can directly\nlearn visual task specification by watching human demonstrations. Task\nspecification is represented as a task function, which is learned using inverse\nreinforcement learning(IRL) by inferring differential rewards between state\nchanges. The learned task function is then used as continuous feedbacks in an\nuncalibrated visual servoing(UVS) controller designed for the execution phase.\nOur proposed method can directly learn from raw videos, which removes the need\nfor hand-engineered task specification. It can also provide task\ninterpretability by directly approximating the task function. Besides,\nbenefiting from the use of a traditional UVS controller, our training process\nis efficient and the learned policy is independent from a particular robot\nplatform. Various experiments were designed to show that, for a certain DOF\ntask, our method can adapt to task/environment variances in target positions,\nbackgrounds, illuminations, and occlusions without prior retraining.",
      "authors": [
        "Jun Jin",
        "Laura Petrich",
        "Masood Dehghan",
        "Zichen Zhang",
        "Martin Jagersand"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted in ICRA 2019",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1109/ICRA.2019.8793649",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-09-29",
      "selected": null,
      "title": "Robot eye-hand coordination learning by watching human demonstrations: a task function approximation approach",
      "urls": [
        "http://dx.doi.org/10.1109/ICRA.2019.8793649",
        "http://arxiv.org/abs/1810.00159v2",
        "http://arxiv.org/pdf/1810.00159v2"
      ]
    },
    {
      "abstract": "Decision-making is a crucial cognitive function for various animal species surviving in nature, and it is also a fundamental ability for intelligent agents. To make a step forward in the understanding of the computational mechanism of human-like decision-making, this paper proposes a brain-inspired decision-making spiking neural network (BDM-SNN) and applies it to decision-making tasks on intelligent agents. This paper makes the following contributions: (1) A spiking neural network (SNN) is used to model human decision-making neural circuit from both connectome and functional perspectives. (2) The proposed model combines dopamine and spike-timing-dependent plasticity (STDP) mechanisms to modulate the network learning process, which indicates more biological inspiration. (3) The model considers the effects of interactions among sub-areas in PFC on accelerating the learning process. (4) The proposed model can be easily applied to decision-making tasks in intelligent agents, such as an unmanned aerial vehicle (UAV) flying through a window and a UAV avoiding an obstacle. The experimental results support the effectiveness of the model. Compared with traditional reinforcement learning and existing biologically inspired methods, our method contains more biologically-inspired mechanistic principles, has greater accuracy and is faster.",
      "authors": [
        "Zhao, Feifei",
        "Zeng, Yi",
        "Xu, Bo"
      ],
      "categories": null,
      "citations": 25,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2018.00056",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2018-09-11",
      "selected": null,
      "title": "A Brain-Inspired Decision-Making Spiking Neural Network and Its Application in Unmanned Aerial Vehicle",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053857969&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sherman L.E."
      ],
      "categories": null,
      "citations": 88,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/scan/nsy051",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "699-707",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17495016",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Social Cognitive and Affective Neuroscience"
      },
      "publication_date": "2018-09-04",
      "selected": null,
      "title": "What the brain 'Likes': Neural correlates of providing feedback on social media",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054849839&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Anderson B.A."
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cognition.2018.05.005",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "26-36",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00100277",
        "publisher": "Elsevier B.V.",
        "sjr": 1.691,
        "snip": 1.813,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Cognitive Neuroscience",
          "Language and Linguistics",
          "Linguistics and Language"
        ],
        "title": "Cognition"
      },
      "publication_date": "2018-09-01",
      "selected": null,
      "title": "Mechanisms of value-learning in the guidance of spatial attention",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046999099&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sambrook T.D."
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2018.05.023",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "162-171",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2018-09-01",
      "selected": null,
      "title": "Model-free and model-based reward prediction errors in EEG",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85047245080&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "van Noordt S.J.R."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.brainres.2018.04.021",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "29-37",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00068993",
        "publisher": "Elsevier B.V.",
        "sjr": 0.854,
        "snip": 0.766,
        "subject_areas": [
          "Neuroscience (all)",
          "Molecular Biology",
          "Neurology (clinical)",
          "Developmental Biology"
        ],
        "title": "Brain Research"
      },
      "publication_date": "2018-09-01",
      "selected": null,
      "title": "Medial frontal theta dissociates unsuccessful from successful avoidance and is modulated by lack of perseverance",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046752036&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Tseng S."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TCDS.2017.2775621",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "701-711",
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "23798920",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.173,
        "snip": 1.584,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Cognitive and Developmental Systems"
      },
      "publication_date": "2018-09-01",
      "selected": null,
      "title": "Active Learning on Service Providing Model: Adjustment of Robot Behaviors Through Human Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85035799192&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Akaishi T."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.mehy.2018.06.030",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "107-113",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03069877",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Medical Hypotheses"
      },
      "publication_date": "2018-09-01",
      "selected": null,
      "title": "Unified neural structured model: A new diagnostic tool in primary care psychiatry",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049346908&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Narayanan V."
      ],
      "categories": null,
      "citations": 77,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TCYB.2017.2741342",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "2510-2519",
      "publication": {
        "category": "Journal",
        "cite_score": 22.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21682267",
        "publisher": "IEEE Advancing Technology for Humanity",
        "sjr": 5.365,
        "snip": 4.286,
        "subject_areas": [
          "Software",
          "Information Systems",
          "Computer Science Applications",
          "Human-Computer Interaction",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "IEEE Transactions on Cybernetics"
      },
      "publication_date": "2018-09-01",
      "selected": null,
      "title": "Event-Triggered Distributed Control of Nonlinear Interconnected Systems Using Online Reinforcement Learning with Exploration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029143654&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Keren H."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2018.05.039",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "266-276",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2018-09-01",
      "selected": null,
      "title": "Is the encoding of Reward Prediction Error reliable during development?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85047491773&origin=inward"
      ]
    },
    {
      "abstract": "<p>Over the past few decades, neuroscience research has illuminated the neural mechanisms supporting learning from reward feedback. Learning paradigms are increasingly being extended to study mood and psychiatric disorders as well as addiction. However, one potentially critical characteristic that this research ignores is the effect of time on learning: human feedback learning paradigms are usually conducted in a single rapidly paced session, whereas learning experiences in ecologically relevant circumstances and in animal research are almost always separated by longer periods of time. In our experiments, we examined reward learning in short condensed sessions distributed across weeks versus learning completed in a single \u201cmassed\u201d session in male and female participants. As expected, we found that after equal amounts of training, accuracy was matched between the spaced and massed conditions. However, in a 3-week follow-up, we found that participants exhibited significantly greater memory for the value of spaced-trained stimuli. Supporting a role for short-term memory in massed learning, we found a significant positive correlation between initial learning and working memory capacity. Neurally, we found that patterns of activity in the medial temporal lobe and prefrontal cortex showed stronger discrimination of spaced- versus massed-trained reward values. Further, patterns in the striatum discriminated between spaced- and massed-trained stimuli overall. Our results indicate that single-session learning tasks engage partially distinct learning mechanisms from distributed training. Our studies begin to address a large gap in our knowledge of human learning from reinforcement, with potential implications for our understanding of mood disorders and addiction.</p><p><b>SIGNIFICANCE STATEMENT</b> Humans and animals learn to associate predictive value with stimuli and actions, and these values then guide future behavior. Such reinforcement-based learning often happens over long time periods, in contrast to most studies of reward-based learning in humans. In experiments that tested the effect of spacing on learning, we found that associations learned in a single massed session were correlated with short-term memory and significantly decayed over time, whereas associations learned in short massed sessions over weeks were well maintained. Additionally, patterns of activity in the medial temporal lobe and prefrontal cortex discriminated the values of stimuli learned over weeks but not minutes. These results highlight the importance of studying learning over time, with potential applications to drug addiction and psychiatry.</p>",
      "authors": [
        "G. Elliott Wimmer",
        "Jamie K. Li",
        "Krzysztof J. Gorgolewski",
        "Russell A. Poldrack"
      ],
      "categories": null,
      "citations": 33,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.0075-18.2018",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "7649-7666",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2018-08-29",
      "selected": null,
      "title": "Reward Learning over Weeks Versus Minutes Increases the Neural Representation of Value in the Human Brain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052629769&origin=inward"
      ]
    },
    {
      "abstract": "Robots are capable of training humans to achieve complex tasks, and their helpful feedback can lead to useful human-robot collaborations. In this research we present a reinforcement learning model influenced by human cognition which is repurposed to enhance human learning, investigate a robot&#x0027;s ability to encourage and motivate humans and improve their performance. During teaching the robot trades off between exploration and exploitation to understand the human perception and develop a successful motivational approach. We compare our learned reinforcement model with a baseline nonreinforcement approach and with a random reinforcer, and achieve more effective teaching in the learned reinforcement condition. In addition, we discovered an extremely strong relationship (r &#x003D; 0.88) between the robot&#x0027;s regret, in a machine learning sense, and the performance of its human partner.",
      "authors": [
        "Sayanti Roy",
        "Christopher Crick",
        "Emily Kieson",
        "Charles Abramson"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ROMAN.2018.8525563",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "294-299",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2018-08-27",
      "selected": null,
      "title": "A Reinforcement Learning Model for Robots as Teachers&lt;sup&gt;&amp;#x002A;&lt;/sup&gt;",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058103425&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8525563",
        "https://dl.acm.org/doi/10.1109/ROMAN.2018.8525563"
      ]
    },
    {
      "abstract": "Programing robots to perform tasks is difficult in the real world because of its richness and uncertainty. For robots and agents to be more useful, they must be able to learn quickly from ordinary people via natural interactions. In this paper, we investigate how an agent can learn from demonstration and positive and negative evaluative feedback provided by a human teacher. Specifically, we proposed a model-based method-IRL-TAMER-by combining learning from demonstration via inverse reinforcement learning (IRL) and learning from human reward via the TAMER framework. We tested our method in the Grid World domain and compared with the TAMER framework using different discount factors on human reward. Our results suggest that although an agent learning via IRL can learn a useful value function indicating which states are good based on the demonstration, it cannot obtain an effective policy navigating to the goal state with one demonstration. However, learning from demonstration can reduce the number of human reward needed to obtain an optimal policy, especially the number of negative feedback. That is to say, learning from demonstration can be a jump-start for agent&#x0027;s learning from human reward and reduce the number of mistakes-incorrect actions. Furthermore, our results show that learning from demonstration can only be useful for agent&#x0027;s learning from human reward when the discount factor is small, i.e., learning from myopic human reward.",
      "authors": [
        "Guangliang Li",
        "Bo He",
        "Randy Gomez",
        "Keisuke Nakamura"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ROMAN.2018.8525837",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "1156-1162",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-7281-6076-4",
        "issn": "1944-9445",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2018-08-27",
      "selected": null,
      "title": "Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8525837",
        "https://dl.acm.org/doi/10.1109/ROMAN.2018.8525837",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058071354&origin=inward"
      ]
    },
    {
      "abstract": "Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.",
      "authors": [
        "Plisnier, H\u00e9l\u00e8ne",
        "Steckelmacher, Denis",
        "Brys, Tim",
        "Roijers, Diederik M.",
        "Now\u00e9, Ann"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted at the European Workshop on Reinforcement Learning 2018\n  (EWRL14)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-08-13",
      "selected": null,
      "title": "Directed Policy Gradient for Safe Reinforcement Learning with Human Advice",
      "urls": [
        "http://arxiv.org/pdf/1808.04096v1",
        "http://arxiv.org/pdf/1808.04096.pdf",
        "http://arxiv.org/abs/1808.04096v1"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450358194",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM SIGGRAPH 2018 Studio, SIGGRAPH 2018"
      },
      "publication_date": "2018-08-12",
      "selected": null,
      "title": "ACM SIGGRAPH 2018 Studio, SIGGRAPH 2018",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055414901&origin=inward"
      ]
    },
    {
      "abstract": "<p>Detecting and evaluating errors in action execution is essential for learning. Through complex interactions of the inverse and the forward model, the human motor system can predict and subsequently adjust ongoing or subsequent actions. Inputs to such a prediction are efferent and afferent signals from various sources. The aim of the current study was to examine the impact of visual as well as a combination of efferent and proprioceptive input signals to error prediction in a complex motor task. Predicting motor errors has been shown to be correlated with a neural signal known as the error-related negativity (Ne/ERN). Here, we tested how the Ne/ERN amplitude was modulated by the availability of different sensory signals in a semi-virtual throwing task where the action outcome (hit or miss of the target) was temporally delayed relative to movement execution allowing participants to form predictions about the outcome prior to the availability of knowledge of results. 19 participants practiced the task and electroencephalogram was recorded in two test conditions. In the <italic>Visual</italic> condition, participants received only visual input by passively observing the throwing movement. In the <italic>EffProp</italic> condition, participants actively executed the task while visual information about the real and the virtual effector was occluded. Hence, only efferent and proprioceptive signals were available. Results show a significant modulation of the Ne/ERN in the <italic>Visual</italic> condition while no effect could be observed in the <italic>EffProp</italic> condition. In addition, amplitudes of the feedback-related negativity in response to the actual outcome feedback were found to be inversely related to the Ne/ERN amplitudes. Our findings indicate that error prediction is modulated by the availability of input signals to the forward model. The observed amplitudes were found to be attenuated in comparison to previous studies, in which all efferent and sensory inputs were present. Furthermore, we assume that visual signals are weighted higher than proprioceptive signals, at least in goal-oriented tasks with visual targets.</p>",
      "authors": [
        "Joch, Michael",
        "Hegele, Mathias",
        "Maurer, Heiko",
        "M\u00fcller, Hermann",
        "Maurer, Lisa K."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyg.2018.01376",
      "keywords": [
        "EEG",
        "feedback related negativity",
        "error prediction",
        "reinforcement learning",
        "error negativity",
        "forward model",
        "Sensory signals"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-1078",
        "publisher": "Frontiers Media SA",
        "sjr": 0.891,
        "snip": 1.422,
        "subject_areas": [
          "Psychology (all)"
        ],
        "title": "Frontiers in Psychology"
      },
      "publication_date": "2018-08-07",
      "selected": null,
      "title": "Accuracy of Motor Error Predictions for Different Sensory Signals",
      "urls": [
        "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.01376/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052837728&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sojitra R."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neurobiolaging.2018.04.006",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "102-113",
      "publication": {
        "category": "Journal",
        "cite_score": 8.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01974580",
        "publisher": "Elsevier Inc.",
        "sjr": 1.521,
        "snip": 1.058,
        "subject_areas": [
          "Neuroscience (all)",
          "Geriatrics and Gerontology",
          "Developmental Biology",
          "Neurology (clinical)",
          "Aging"
        ],
        "title": "Neurobiology of Aging"
      },
      "publication_date": "2018-08-01",
      "selected": null,
      "title": "Age affects reinforcement learning through dopamine-based learning imbalance and high decision noise\u2014not through Parkinsonian mechanisms",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85047198105&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dixon-Gordon K.L."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1521/pedi_2017_31_299",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "432-446",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0885579X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Personality Disorders"
      },
      "publication_date": "2018-08-01",
      "selected": null,
      "title": "The influence of emotional state on learning from reward and punishment in borderline personality disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055292065&origin=inward"
      ]
    },
    {
      "abstract": "Emotions are intimately tied to motivation and the adaptation of behavior, and many animal species show evidence of emotions in their behavior. Therefore, emotions must be related to powerful mechanisms that aid survival, and, emotions must be evolutionary continuous phenomena. How and why did emotions evolve in nature, how do events get emotionally appraised, how do emotions relate to cognitive complexity, and, how do they impact behavior and learning? In this article I propose that all emotions are manifestations of reward processing, in particular Temporal Difference (TD) error assessment. Reinforcement Learning (RL) is a powerful computational model for the learning of goal oriented tasks by exploration and feedback. Evidence indicates that RL-like processes exist in many animal species. Key in the processing of feedback in RL is the notion of TD error, the assessment of how much better or worse a situation just became, compared to what was previously expected (or, the estimated gain or loss of utility - or well-being - resulting from new evidence). I propose a TDRL Theory of Emotion and discuss its ramifications for our understanding of emotions in humans, animals and machines, and present psychological, neurobiological and computational evidence in its support.",
      "authors": [
        "Broekens, Joost"
      ],
      "categories": null,
      "citations": null,
      "comments": "pre-print, don't cite verbatim",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-07-24",
      "selected": null,
      "title": "A Temporal Difference Reinforcement Learning Theory of Emotion: unifying emotion, cognition and adaptive behavior",
      "urls": [
        "http://arxiv.org/abs/1807.08941v1",
        "http://arxiv.org/pdf/1807.08941v1",
        "http://arxiv.org/pdf/1807.08941.pdf"
      ]
    },
    {
      "abstract": "Machine Learning models become increasingly proficient in complex tasks. However, even for experts in the field, it can be difficult to understand what the model learned. This hampers trust and acceptance, and it obstructs the possibility to correct the model. There is therefore a need for transparency of machine learning models. The development of transparent classification models has received much attention, but there are few developments for achieving transparent Reinforcement Learning (RL) models. In this study we propose a method that enables a RL agent to explain its behavior in terms of the expected consequences of state transitions and outcomes. First, we define a translation of states and actions to a description that is easier to understand for human users. Second, we developed a procedure that enables the agent to obtain the consequences of a single action, as well as its entire policy. The method calculates contrasts between the consequences of a policy derived from a user query, and of the learned policy of the agent. Third, a format for generating explanations was constructed. A pilot survey study was conducted to explore preferences of users for different explanation properties. Results indicate that human users tend to favor explanations about policy rather than about single actions.",
      "authors": [
        "van der Waa, Jasper",
        "van Diggelen, Jurriaan",
        "Bosch, Karel van den",
        "Neerincx, Mark"
      ],
      "categories": null,
      "citations": null,
      "comments": "XAI workshop on the IJCAI conference 2018, Stockholm, Sweden",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Learning",
          "Machine Learning"
        ],
        "title": "IJCAI-18 Workshop on Explainable AI (XAI). Vol. 37. 2018"
      },
      "publication_date": "2018-07-23",
      "selected": null,
      "title": "Contrastive Explanations for Reinforcement Learning in terms of Expected Consequences",
      "urls": [
        "http://arxiv.org/abs/1807.08706v1",
        "http://arxiv.org/pdf/1807.08706v1",
        "http://arxiv.org/pdf/1807.08706.pdf"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) has had many successes when learning autonomously. This paper and accompanying talk consider how to make use of a non-technical human participant, when available. In particular, we consider the case where a human could 1) provide demonstrations of good behavior, 2) provide online evaluative feedback, or 3) define a curriculum of tasks for the agent to learn on. In all cases, our work has shown such information can be effectively leveraged. After giving a high-level overview of this work, we will highlight a set of open questions and suggest where future work could be usefully focused.",
      "authors": [
        "Matthew E. Taylor"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3304652.3304833",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "5724-5728",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780999241127",
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 27th International Joint Conference on Artificial Intelligence"
      },
      "publication_date": "2018-07-13",
      "selected": null,
      "title": "Improving reinforcement learning with human input",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3304652.3304833"
      ]
    },
    {
      "abstract": "Humor is an essential element of human-human communication. Consequently, robots in the role of companions should exploit its potential as well to make interactions more enjoyable. Using a robot as an entertainer requires finding out what kind of humor its audience prefers. However, it is a challenging task for a social robot to learn what users prefer without bothering them by repeatedly asking questions. In this paper, we present an approach based on Reinforcement Learning that enables a robot to continuously adapt to the users' humor preferences without requiring them to explicitly provide feedback. Instead, we designed the robot to analyze the user's ideomotor social cues. We evaluated our approach in a scenario involving a Reeti robot acting as an entertainer. In this role, it is telling different types of jokes, (possibly) underlining its performance with grimaces and sounds. The adaptation process is accomplished only by using the audience's vocal laughs and visual smiles, but no other form of explicit feedback.",
      "authors": [
        "Klaus Weber",
        "Hannes Ritschel",
        "Florian Lingenfelser",
        "Elisabeth Andr\u00e9"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3237383.3238141",
      "keywords": [
        "socially-aware-agents",
        "social adaptation",
        "human-robot-interaction"
      ],
      "number_of_pages": 3,
      "pages": "2259-2261",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems"
      },
      "publication_date": "2018-07-09",
      "selected": null,
      "title": "Real-Time Adaptation of a Robotic Joke Teller Based on Human Social Signals",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3237383.3238141"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Saravanan R."
      ],
      "categories": null,
      "citations": 129,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICCONS.2018.8663155",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "945-949",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781538628423",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2nd International Conference on Intelligent Computing and Control Systems, ICICCS 2018"
      },
      "publication_date": "2018-07-02",
      "selected": null,
      "title": "A State of Art Techniques on Machine Learning Algorithms: A Perspective of Supervised Learning Approaches in Data Classification",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063788773&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ide J.S."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2018.02.035",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "35-43",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2018-07-01",
      "selected": null,
      "title": "Oxytocin attenuates trust as a subset of more general reinforcement learning, with altered reward circuit functional connectivity in males",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043489518&origin=inward"
      ]
    },
    {
      "abstract": "Recent advances in multi-stage algorithms have shown great promise, but two important problems still remain. First of all, at inference time, information can't feed back from downstream to upstream. Second, at training time, end-to-end training is not possible if the overall pipeline involves non-differentiable functions, and so different stages can't be jointly optimized. In this paper, we propose a novel environment upgrade reinforcement learning framework to solve the feedback and joint optimization problems. Our framework re-links the downstream stage to the upstream stage by a reinforcement learning agent. While training the agent to improve final performance by refining the upstream stage's output, we also upgrade the downstream stage (environment) according to the agent's policy. In this way, agent policy and environment are jointly optimized. We propose a training algorithm for this framework to address the different training demands of agent and environment. Experiments on instance segmentation and human pose estimation demonstrate the effectiveness of the proposed framework.",
      "authors": [
        "Shuqin Xie",
        "Zitian Chen",
        "Chao Xu",
        "Cewu Lu"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/CVPR.2018.00401",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "3810-3819",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 40.7,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-6421-6",
        "issn": "1063-6919",
        "publisher": "IEEE Computer Society",
        "sjr": 5.952,
        "snip": 8.489,
        "subject_areas": [
          "Software",
          "Computer Vision and Pattern Recognition"
        ],
        "title": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
      },
      "publication_date": "2018-06-18",
      "selected": null,
      "title": "Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062873049&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578499"
      ]
    },
    {
      "abstract": "The autonomous landing of an unmanned aerial vehicle (UAV) is still an open problem. Previous work focused on the use of hand-crafted geometric features and sensor-data fusion for identifying a fiducial marker and guide the UAV toward it. In this article we propose a method based on deep reinforcement learning that only requires low-resolution images coming from a down looking camera in order to drive the vehicle. The proposed approach is based on a hierarchy of Deep Q-Networks (DQNs) that are used as high-end control policy for the navigation in different phases. We implemented various technical solutions, such as the combination of vanilla and double DQNs trained using a form of prioritized buffer replay that separates experiences in multiple containers. The optimal control policy is learned without any human supervision, providing the agent with a sparse reward feedback indicating the success or failure of the landing. The results show that the quadrotor can autonomously land on a large variety of simulated environments and with relevant noise, proving that the underline DQNs are able to generalise effectively on unseen scenarios. Furthermore, it was proved that in some conditions the network outperformed human pilots.",
      "authors": [
        "Riccardo Polvara",
        "Massimiliano Patacchiola",
        "Sanjay Sharma",
        "Jian Wan",
        "Andrew Manning",
        "Robert Sutton",
        "Angelo Cangelosi"
      ],
      "categories": null,
      "citations": 58,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICUAS.2018.8453449",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "115-123",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-1355-9",
        "issn": "2575-7296",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2018 International Conference on Unmanned Aircraft Systems, ICUAS 2018"
      },
      "publication_date": "2018-06-12",
      "selected": null,
      "title": "Toward End-to-End Control for UAV Autonomous Landing via Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053921728&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8453449"
      ]
    },
    {
      "abstract": "In order to improve the intelligent level of UCAV in one-to-one air combat, an autonomous maneuvering decision algorithm based on deep reinforcement learning is proposed. UCAV learns strategies by sensing the environment, performing maneuvering actions, and getting feedback. In this way, we can avoid the limitations of existing theories and human operations. Firstly an environment is modeled to simulate the real-time situation of air combat. Then a situation assessment method based on Energy-Maneuverability theory is utilized to design the reward functions. Finally model based on deep reinforcement learning is created for UCAV to learn strategies to gain the advantage for the opponent.",
      "authors": [
        "Yesheng Zhang",
        "Wei Zu",
        "Yang Gao",
        "Hongxing Chang"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/CCDC.2018.8407136",
      "keywords": [
        "Autonomous Maneuvering Decision",
        "Deep Reinforcement Learning",
        "Air Combat"
      ],
      "number_of_pages": 6,
      "pages": "230-235",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-1245-3",
        "issn": "1948-9447",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 30th Chinese Control and Decision Conference, CCDC 2018"
      },
      "publication_date": "2018-06-09",
      "selected": null,
      "title": "Research on autonomous maneuvering decision of UCAV based on deep reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050856538&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8407136"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Holland P."
      ],
      "categories": null,
      "citations": 52,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00901.2017",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "2241-2255",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2018-06-07",
      "selected": null,
      "title": "Contribution of explicit processes to reinforcement-based motor learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048501481&origin=inward"
      ]
    },
    {
      "abstract": "Anhedonia (hyposensitivity to rewards) and negative bias (hypersensitivity to punishments) are core features of major depressive disorder (MDD), which could stem from abnormal reinforcement learning. Emerging evidence highlights blunted reward learning and reward prediction error (RPE) signaling in the striatum in MDD, although inconsistencies exist. Preclinical studies have clarified that ventral tegmental area (VTA) neurons encode RPE and habenular neurons encode punishment prediction error (PPE), which are then transmitted to the striatum and cortex to guide goal-directed behavior. However, few studies have probed striatal activation, and functional connectivity between VTA-striatum and VTA-habenula during reward and punishment learning respectively, in unmedicated MDD. To fill this gap, we acquired fMRI data from 25 unmedicated MDD and 26 healthy individuals during a monetary instrumental learning task and utilized a computational modeling approach to characterize underlying neural correlates of RPE and PPE. Relative to controls, MDD individuals showed impaired reward learning, blunted RPE signal in the striatum and overall reduced VTA-striatal connectivity to feedback. Critically, striatal RPE signal was increasingly blunted with more major depressive episodes (MDEs). No group differences emerged in PPE signals in the habenula and VTA or in connectivity between these regions. However, PPE signals in the habenula correlated positively with number of MDEs. These results highlight impaired reward learning, disrupted RPE signaling in the striatum (particularly among individuals with more lifetime MDEs) as well as reduced VTA-striatal connectivity in MDD. Collectively, these findings highlight reward-related learning deficits in MDD and their underlying pathophysiology.",
      "authors": [
        "Kumar, Poornima",
        "Goer, Franziska",
        "Murray, Laura",
        "Dillon, Daniel G.",
        "Beltzer, Miranda L.",
        "Cohen, Andrew L.",
        "Brooks, Nancy H.",
        "Pizzagalli, Diego A."
      ],
      "categories": null,
      "citations": 115,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41386-018-0032-x",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1581-1588",
      "publication": {
        "category": "Journal",
        "cite_score": 14.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0893133X",
        "publisher": "Nature Publishing Group",
        "sjr": 2.385,
        "snip": 1.864,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Pharmacology"
        ],
        "title": "Neuropsychopharmacology"
      },
      "publication_date": "2018-06-01",
      "selected": null,
      "title": "Impaired reward prediction error encoding and striatal-midbrain connectivity in depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043687515&origin=inward",
        "https://www.nature.com/articles/s41386-018-0032-x.pdf"
      ]
    },
    {
      "abstract": "Adaptive decision making relies on learning from feedback. Because feedback sometimes can be misleading, optimal learning requires that knowledge about the feedback\u2019s reliability be utilized to adjust feedback processing. Although previous research has shown that feedback reliability indeed influences feedback processing, the underlying mechanisms through which this is accomplished remain unclear. Here we propose that feedback processing is adjusted by the adaptive, top-down valuation of feedback. We assume that unreliable feedback is devalued relative to reliable feedback, thus reducing the reward prediction errors that underlie feedback-related brain activity and learning. A crucial prediction of this account is that the effects of feedback reliability are susceptible to contrast effects. That is, the effects of feedback reliability should be enhanced when both reliable and unreliable feedback are experienced within the same context, as compared to when only one level of feedback reliability is experienced. To evaluate this prediction, we measured the event-related potentials elicited by feedback in two experiments in which feedback reliability was varied either within or between blocks. We found that the fronto-central valence effect, a correlate of reward prediction errors during reinforcement learning, was reduced for unreliable feedback. But this result was obtained only when feedback reliability was varied within blocks, thus indicating a contrast effect. This suggests that the adaptive valuation of feedback is one mechanism underlying the effects of feedback reliability on feedback processing.",
      "authors": [
        "Ernst, Benjamin",
        "Steinhauser, Marco"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-018-0591-7",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "596-608",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2018-06-01",
      "selected": null,
      "title": "Effects of feedback reliability on feedback-related brain activity: A feedback valuation account",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-018-0591-7.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045046450&origin=inward"
      ]
    },
    {
      "abstract": "Test data generation is a tedious and laborious process. Search-based Software Testing (SBST) automatically generates test data optimising structural test criteria using metaheuristic algorithms. In essence, metaheuristic algorithms are systematic trial-and-error based on the feedback of fitness function. This is similar to an agent of reinforcement learning which iteratively decides an action based on the current state to maximise the cumulative reward. Inspired by this analogy, this paper investigates the feasibility of employing reinforcement learning in SBST to replace human designed metaheuristic algorithms. We reformulate the software under test (SUT) as an environment of reinforcement learning. At the same time, we present GunPowder, a novel framework for SBST which extends SUT to the environment. We train a Double Deep Q-Networks (DDQN) agent with deep neural network and evaluate the effectiveness of our approach by conducting a small empirical study. Finally, we find that agents can learn metaheuristic algorithms for SBST, achieving 100% branch coverage for training functions. Our study sheds light on the future integration of deep neural network and SBST.",
      "authors": [
        "Junhwi Kim",
        "Minhyuk Kwon",
        "Shin Yoo"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": null,
      "keywords": [
        "sbst",
        "reinforcement learning",
        "test data generation"
      ],
      "number_of_pages": 8,
      "pages": "51-58",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-6267-0",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2018 IEEE/ACM 11th International Workshop on Search-Based Software Testing (SBST)"
      },
      "publication_date": "2018-05-28",
      "selected": null,
      "title": "Generating Test Input with Deep Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8452812"
      ]
    },
    {
      "abstract": "Test data generation is a tedious and laborious process. Search-based Software Testing (SBST) automatically generates test data optimising structural test criteria using metaheuristic algorithms. In essence, metaheuristic algorithms are systematic trial-and-error based on the feedback of fitness function. This is similar to an agent of reinforcement learning which iteratively decides an action based on the current state to maximise the cumulative reward. Inspired by this analogy, this paper investigates the feasibility of employing reinforcement learning in SBST to replace human designed meta-heuristic algorithms. We reformulate the software under test (SUT) as an environment of reinforcement learning. At the same time, we present GunPowder, a novel framework for SBST which extends SUT to the environment. We train a Double Deep Q-Networks (DDQN) agent with deep neural network and evaluate the effectiveness of our approach by conducting a small empirical study. Finally, we find that agents can learn metaheuristic algorithms for SBST, achieving 100% branch coverage for training functions. Our study sheds light on the future integration of deep neural network and SBST.",
      "authors": [
        "Junhwi Kim",
        "Minhyuk Kwon",
        "Shin Yoo"
      ],
      "categories": null,
      "citations": 25,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/3194718.3194720",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "51-58",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 7.0,
        "is_potentially_predatory": false,
        "isbn": "9781450357418",
        "issn": "02705257",
        "publisher": "IEEE Computer Society",
        "sjr": 1.235,
        "snip": 1.945,
        "subject_areas": [
          "Software"
        ],
        "title": "Proceedings of the 11th International Workshop on Search-Based Software Testing"
      },
      "publication_date": "2018-05-28",
      "selected": null,
      "title": "Generating test input with deep reinforcement learning",
      "urls": [
        "https://dl.acm.org/doi/10.1145/3194718.3194720",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051164337&origin=inward"
      ]
    },
    {
      "abstract": "We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator $\\alpha$-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.",
      "authors": [
        "Kreutzer, Julia",
        "Uyheng, Joshua",
        "Riezler, Stefan"
      ],
      "categories": null,
      "citations": null,
      "comments": "ACL 2018",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-05-27",
      "selected": null,
      "title": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/1805.10627v3",
        "http://arxiv.org/abs/1805.10627v3",
        "http://arxiv.org/pdf/1805.10627.pdf"
      ]
    },
    {
      "abstract": "As people learn to navigate the world, autonomic nervous system (e.g., \"fight or flight\") responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger. We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.",
      "authors": [
        "McDuff, Daniel",
        "Kapoor, Ashish"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-05-25",
      "selected": null,
      "title": "Visceral Machines: Risk-Aversion in Reinforcement Learning with Intrinsic Physiological Rewards",
      "urls": [
        "http://arxiv.org/abs/1805.09975v2",
        "http://arxiv.org/pdf/1805.09975.pdf",
        "http://arxiv.org/pdf/1805.09975v2"
      ]
    },
    {
      "abstract": "In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent's actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.",
      "authors": [
        "Upadhyay, Utkarsh",
        "De, Abir",
        "Gomez-Rodriguez, Manuel"
      ],
      "categories": null,
      "citations": null,
      "comments": "To appear in Proceedings of the 32nd Conference on Neural Information\n  Processing Systems (NIPS 2018)",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-05-23",
      "selected": null,
      "title": "Deep Reinforcement Learning of Marked Temporal Point Processes",
      "urls": [
        "http://arxiv.org/abs/1805.09360v2",
        "http://arxiv.org/pdf/1805.09360v2",
        "http://arxiv.org/pdf/1805.09360.pdf"
      ]
    },
    {
      "abstract": "Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",
      "authors": [
        "Reddy, Siddharth",
        "Dragan, Anca D.",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted at Neural Information Processing Systems (NeurIPS) 2018",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-05-21",
      "selected": null,
      "title": "Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior",
      "urls": [
        "http://arxiv.org/abs/1805.08010v4",
        "http://arxiv.org/pdf/1805.08010v4",
        "http://arxiv.org/pdf/1805.08010.pdf"
      ]
    },
    {
      "abstract": "While reinforcement learning has led to promising results in robotics, defining an informative reward function is challenging. Prior work considered including the human in the loop to jointly learn the reward function and the optimal policy. Generating samples from a physical robot and requesting human feedback are both taxing efforts for which efficiency is critical. We propose to learn reward functions from both the robot and the human perspectives to improve on both efficiency metrics. Learning a reward function from the human perspective increases feedback efficiency by assuming that humans rank trajectories according to a low-dimensional outcome space. Learning a reward function from the robot perspective circumvents the need for a dynamics model while retaining the sample efficiency of model-based approaches. We provide an algorithm that incorporates bi-perspective reward learning into a general hierarchical reinforcement learning framework and demonstrate the merits of our approach on a toy task and a simulated robot grasping task.",
      "authors": [
        "Robert Pinsler",
        "Riad Akrour",
        "Takayuki Osa",
        "Jan Peters",
        "Gerhard Neumann"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA.2018.8460907",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "596-601",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-3082-2",
        "issn": "2577-087X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2018-05-21",
      "selected": null,
      "title": "Sample and Feedback Efficient Hierarchical Reinforcement Learning from Human Preferences",
      "urls": [
        "https://dl.acm.org/doi/10.1109/ICRA.2018.8460907",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460907",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063140889&origin=inward"
      ]
    },
    {
      "abstract": "This paper considers the motion control and task planning problem of mobile robots under complex high-level tasks and human initiatives. The assigned task is specified as Linear Temporal Logic (LTL) formulas that consist of hard and soft constraints. The human initiative influences the robot autonomy in two explicit ways: with additive terms in the continuous controller and with contingent task assignments. We propose an online coordination scheme that encapsulates (i) a mixed-initiative continuous controller that ensures all-time safety despite of possible human errors, (ii) a plan adaptation scheme that accommodates new features discovered in the workspace and short-term tasks assigned by the operator during run time, and (iii) an iterative inverse reinforcement learning (IRL) algorithm that allows the robot to asymptotically learn the human preference on the parameters during the plan synthesis. The results are demonstrated by both realistic human-in-the-loop simulations and experiments.",
      "authors": [
        "Meng Guo",
        "Sofie Andersson",
        "Dimos V. Dimarogonas"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA.2018.8460793",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "6395-6400",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-3082-2",
        "issn": "2577-087X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2018-05-21",
      "selected": null,
      "title": "Human-in-the-Loop Mixed-Initiative Control Under Temporal Tasks",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460793",
        "https://dl.acm.org/doi/10.1109/ICRA.2018.8460793",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063131458&origin=inward"
      ]
    },
    {
      "abstract": "Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach `learning to teach'. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).",
      "authors": [
        "Fan, Yang",
        "Tian, Fei",
        "Qin, Tao",
        "Li, Xiang-Yang",
        "Liu, Tie-Yan"
      ],
      "categories": null,
      "citations": 81,
      "comments": "ICLR 2018",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings"
      },
      "publication_date": "2018-05-09",
      "selected": null,
      "title": "Learning to Teach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083950453&origin=inward",
        "http://arxiv.org/abs/1805.03643v1",
        "http://arxiv.org/pdf/1805.03643v1",
        "http://arxiv.org/pdf/1805.03643.pdf"
      ]
    },
    {
      "abstract": "People apply what they have learned from past experiences to similar situations, a phenomenon known as generalization. For example, if eating a particular food caused illness, a person will likely avoid foods that look or smell similar in the future. Generalization can be helpful because it allows people to decide how to act in new situations. But over-generalizing after a bad experience could lead an individual to fear benign scenarios. This may lead to unnecessary anxiety. It can also create a negative cycle where people avoid certain situations or objects, which prevents them from learning that they are safe. Now, Norbury et al. show what happens in the brain when making decisions that involve generalization. In the experiments, volunteers were told seeing a particular flower design would lead to a painful electric shock, unless they pushed a button to \u2018avoid\u2019 that image. Individuals completed this task in a magnetic resonance imaging machine so Norbury et al. could observe their brain activity while they completed the task. A second group of individuals were asked to complete a similar task online, but instead of being shocked they lost money if they failed to hit a key when they saw the \u2018dangerous\u2019 flower. The online participants also filled out a survey about their experience of various psychological symptoms. Norbury et al. used computer modeling to reconstruct how people decided whether or not to avoid images that looked similar to the harm-associated images but were in fact safe (did not lead to pain or losing money). The experiments showed that different parts of the brain were involved in different parts of the generalization process. Areas of the brain that interpret vision, fear, and safety played distinct roles. People who generalized more from harmful outcomes were more likely to report feeling anxious and having intrusive negative thoughts in their everyday lives. A better understanding of the brain processes that cause these symptoms in different situations might help scientists develop better treatments for conditions like anxiety in the future.",
      "authors": [
        "Norbury A."
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7554/eLife.34779.001",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "eLife"
      },
      "publication_date": "2018-05-08",
      "selected": null,
      "title": "Value generalization in human avoidance learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85051962270&origin=inward"
      ]
    },
    {
      "abstract": "Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",
      "authors": [
        "Guo, Xiaoxiao",
        "Wu, Hui",
        "Cheng, Yu",
        "Rennie, Steven",
        "Tesauro, Gerald",
        "Feris, Rogerio Schmidt"
      ],
      "categories": null,
      "citations": 56,
      "comments": "accepted at NeurIPS 2018",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 11,
      "pages": "678-688",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2018-05-01",
      "selected": null,
      "title": "Dialog-based Interactive Image Retrieval",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064830499&origin=inward",
        "http://arxiv.org/pdf/1805.00145v3",
        "http://arxiv.org/abs/1805.00145v3",
        "http://arxiv.org/pdf/1805.00145.pdf"
      ]
    },
    {
      "abstract": "<h3>Abstract</h3>\n<p>Motor variability from exploration is crucial for reinforcement learning as it allows the nervous system to find new task solutions. However, motor variability from noise can be detrimental to learning and may underlie slowed reinforcement learning performance observed in individuals with cerebellar damage. Here we examine whether artificially increasing noise in healthy individuals slows reinforcement learning in a manner similar to that seen in patients with cerebellar damage. Participants used binary reinforcement to learn to rotate their reach angle in a series of directions. By comparing task performance between conditions with different levels of added noise, we show that adding a high level of noise\u2014matched to a group of patients with cerebellar damage\u2014slows learning. In additional experiments, we show that the detrimental effect of noise may lie in reinforcing incorrect behavior, rather than not reinforcing correct behavior. By comparing performance between healthy participants with added noise and a group of patients with cerebellar damage, we found that added noise does not slow the learning of the control group to the same degree observed in the patient group. Using a mechanistic model, we show that added noise in the present study matched patients\u2019 motor noise and total learning. However, increased exploration in the control group relative to the group with cerebellar damage supports faster learning. Our results suggest that motor noise slows reinforcement learning by impairing the mapping of reward to the correct action and that this may underlie deficits induced by cerebellar damage.</p>",
      "authors": [
        "Amanda S. Therrien",
        "Daniel M. Wolpert",
        "Amy J. Bastian"
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/ENEURO.0050-18.2018",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2373-2822",
        "publisher": "Society for Neuroscience",
        "sjr": 1.309,
        "snip": 0.893,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "eNeuro"
      },
      "publication_date": "2018-05-01",
      "selected": null,
      "title": "Increasing Motor Noise Impairs Reinforcement Learning in Healthy Individuals",
      "urls": [
        "https://www.eneuro.org/content/eneuro/5/3/ENEURO.0050-18.2018.full.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052105867&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hassall C.D."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2018.03.006",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "62-72",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2018-05-01",
      "selected": null,
      "title": "Learning what matters: A neural explanation for the sparsity bias",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85044150416&origin=inward"
      ]
    },
    {
      "abstract": "In this work, we present a hybrid learning method for training task-oriented dialogue systems through online user interactions. Popular methods for learning task-oriented dialogues include applying reinforcement learning with user feedback on supervised pre-training models. Efficiency of such learning method may suffer from the mismatch of dialogue state distribution between offline training and online interactive learning stages. To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learning from human teaching and feedback. We design a neural network based task-oriented dialogue agent that can be optimized end-to-end with the proposed learning method. Experimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching. Applying reinforcement learning with user feedback after the imitation learning stage further improves the agent's capability in successfully completing a task.",
      "authors": [
        "Liu, Bing",
        "Tur, Gokhan",
        "Hakkani-Tur, Dilek",
        "Shah, Pararth",
        "Heck, Larry"
      ],
      "categories": null,
      "citations": 76,
      "comments": "To appear in NAACL 2018 as a long paper",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "2060-2069",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781948087278",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference"
      },
      "publication_date": "2018-04-18",
      "selected": null,
      "title": "Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems",
      "urls": [
        "http://arxiv.org/pdf/1804.06512v1",
        "http://arxiv.org/pdf/1804.06512.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85062064411&origin=inward",
        "http://arxiv.org/abs/1804.06512v1"
      ]
    },
    {
      "abstract": "A goal of Interactive Machine Learning (IML) is to enable people without specialized training to teach agents how to perform tasks. Many of the existing machine learning algorithms that learn from human instructions are evaluated using simulated feedback and focus on how quickly the agent learns. While this is valuable information, it ignores important aspects of the human-agent interaction such as frustration. In this paper, we present the Newtonian Action Advice agent, a new method of incorporating human verbal action advice with Reinforcement Learning (RL) in a way that improves the human-agent interaction. In addition to simulations, we validated the Newtonian Action Advice algorithm by conducting a human-subject experiment. The results show that Newtonian Action Advice can perform better than Policy Shaping, a state-of-the-art IML algorithm, both in terms of RL metrics like cumulative reward and human factors metrics like frustration.",
      "authors": [
        "Krening, Samantha"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-04-16",
      "selected": null,
      "title": "Newtonian Action Advice: Integrating Human Verbal Instruction with Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/1804.05821.pdf",
        "http://arxiv.org/abs/1804.05821v1",
        "http://arxiv.org/pdf/1804.05821v1"
      ]
    },
    {
      "abstract": "Consider an assistive system that guides visually impaired users through speech and haptic feedback to their destination. Existing robotic and ubiquitous navigation technologies (e.g., portable, ground, or wearable systems) often operate in a generic, user-agnostic manner. However, to minimize confusion and navigation errors, our real-world analysis reveals a crucial need to adapt the instructional guidance across different end-users with diverse mobility skills. To address this practical issue in scalable system design, we propose a novel model-based reinforcement learning framework for personalizing the system-user interaction experience. When incrementally adapting the system to new users, we propose to use a weighted experts model for addressing data-efficiency limitations in transfer learning with deep models. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for (1) more accurate long-term human behavior prediction (up to 20 seconds into the future) through improved reasoning over personal mobility characteristics, interaction with surrounding obstacles, and the current navigation goal, and (2) quick adaptation at the onset of learning, when data is limited.",
      "authors": [
        "Ohn-Bar, Eshed",
        "Kitani, Kris",
        "Asakawa, Chieko"
      ],
      "categories": null,
      "citations": 8,
      "comments": "Oral Presentation in 2nd Conference on Robot Learning (CoRL, 2018)",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 24,
      "pages": "16-39",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of Machine Learning Research"
      },
      "publication_date": "2018-04-11",
      "selected": null,
      "title": "Personalized Dynamics Models for Adaptive Assistive Navigation Systems",
      "urls": [
        "http://arxiv.org/abs/1804.04118v2",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160313093&origin=inward",
        "http://arxiv.org/pdf/1804.04118.pdf",
        "http://arxiv.org/pdf/1804.04118v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "West R."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2018.02.011",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1-12",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2018-04-01",
      "selected": null,
      "title": "Transient and sustained ERP activity related to feedback processing in the probabilistic selection task",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042645851&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Min K."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/NECO_a_01063",
      "keywords": [],
      "number_of_pages": 28,
      "pages": "1104-1131",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08997667",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Neural Computation"
      },
      "publication_date": "2018-04-01",
      "selected": null,
      "title": "Muscle synergy-driven robust motion control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85044269780&origin=inward"
      ]
    },
    {
      "abstract": "Humans are able to robustly maintain desired motion and posture under dynamically changing circumstances, including novel conditions. To accomplish this, the brain needs to optimize the synergistic control between muscles against external dynamic factors. However, previous related studies have usually simplified the control of multiple muscles using two opposing muscles, which are minimum actuators to simulate linear feedback control. As a result, they have been unable to analyze how muscle synergy contributes to motion control robustness in a biological system. To address this issue, we considered a new muscle synergy concept used to optimize the synergy between muscle units against external dynamic conditions, including novel conditions. We propose that two main muscle control policies synergistically control muscle units to maintain the desired motion against external dynamic conditions. Our assumption is based on biological evidence regarding the control of multiple muscles via the corticospinal tract. One of the policies is the group control policy GCP, which is used to control muscle group units classified based on functional similarities in joint control. This policy is used to effectively resist external dynamic circumstances, such as disturbances. The individual control policy ICP assists the GCP in precisely controlling motion by controlling individual muscle units. To validate this hypothesis, we simulated the reinforcement of the synergistic actions of the two control policies during the reinforcement learning of feedback motion control. Using this learning paradigm, the two control policies were synergistically combined to result in robust feedback control under novel transient and sustained disturbances that did not involve learning. Further, by comparing our data to experimental data generated by human subjects under the same conditions as those of the simulation, we showed that the proposed synergy concept may be used to analyze muscle synergy-driven motion control robustness in humans.",
      "authors": [
        "Kyuengbo Min",
        "Masami Iwamoto",
        "Shinji Kakei",
        "Hideyuki Kimpara"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.1162/neco_a_01063",
      "keywords": [],
      "number_of_pages": 28,
      "pages": "1104-1131",
      "publication": {
        "category": "Journal",
        "cite_score": 9.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0899-7667",
        "publisher": "MIT Press Journals",
        "sjr": 1.544,
        "snip": 3.294,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Arts and Humanities (miscellaneous)"
        ],
        "title": "Neural Comput."
      },
      "publication_date": "2018-04-01",
      "selected": null,
      "title": "Muscle synergy-driven robust motion control",
      "urls": [
        "https://dl.acm.org/doi/10.1162/neco_a_01063"
      ]
    },
    {
      "abstract": "Groups of humans are often able to find ways to cooperate with one another in complex, temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently, multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However, this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many, but not all, human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma, via a profitable interaction with policy learnability. In particular, we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.",
      "authors": [
        "Hughes, Edward",
        "Leibo, Joel Z.",
        "Phillips, Matthew G.",
        "Tuyls, Karl",
        "Du\u00e9\u00f1ez-Guzm\u00e1n, Edgar A.",
        "Casta\u00f1eda, Antonio Garc\u00eda",
        "Dunning, Iain",
        "Zhu, Tina",
        "McKee, Kevin R.",
        "Koster, Raphael",
        "Roff, Heather",
        "Graepel, Thore"
      ],
      "categories": null,
      "citations": 90,
      "comments": "15 pages, 8 figures",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 11,
      "pages": "3326-3336",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2018-03-23",
      "selected": null,
      "title": "Inequity aversion improves cooperation in intertemporal social dilemmas",
      "urls": [
        "http://arxiv.org/pdf/1803.08884.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85064833401&origin=inward",
        "http://arxiv.org/abs/1803.08884v3",
        "http://arxiv.org/pdf/1803.08884v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Banovic N."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/oso/9780198799603.003.0015",
      "keywords": [],
      "number_of_pages": 22,
      "pages": "377-398",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780198799603",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Computational Interaction"
      },
      "publication_date": "2018-03-22",
      "selected": null,
      "title": "Computational model of human routine behaviours",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052962354&origin=inward"
      ]
    },
    {
      "abstract": "Rearranging objects on a tabletop surface by means of nonprehensile\nmanipulation is a task which requires skillful interaction with the physical\nworld. Usually, this is achieved by precisely modeling physical properties of\nthe objects, robot, and the environment for explicit planning. In contrast, as\nexplicitly modeling the physical environment is not always feasible and\ninvolves various uncertainties, we learn a nonprehensile rearrangement strategy\nwith deep reinforcement learning based on only visual feedback. For this, we\nmodel the task with rewards and train a deep Q-network. Our potential\nfield-based heuristic exploration strategy reduces the amount of collisions\nwhich lead to suboptimal outcomes and we actively balance the training set to\navoid bias towards poor examples. Our training process leads to quicker\nlearning and better performance on the task as compared to uniform exploration\nand standard experience replay. We demonstrate empirical evidence from\nsimulation that our method leads to a success rate of 85%, show that our system\ncan cope with sudden changes of the environment, and compare our performance\nwith human level performance.",
      "authors": [
        "Weihao Yuan",
        "Johannes A. Stork",
        "Danica Kragic",
        "Michael Y. Wang",
        "Kaiyu Hang"
      ],
      "categories": null,
      "citations": 41,
      "comments": "2018 International Conference on Robotics and Automation",
      "databases": [
        "IEEE",
        "ACM",
        "arXiv",
        "Scopus"
      ],
      "doi": "10.1109/ICRA.2018.8462863",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "270-277",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-3082-2",
        "issn": "2577-087X",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2018-03-15",
      "selected": null,
      "title": "Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.1109/ICRA.2018.8462863",
        "http://arxiv.org/abs/1803.05752v1",
        "http://arxiv.org/pdf/1803.05752v1",
        "http://dx.doi.org/10.1109/ICRA.2018.8462863",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85063133829&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462863"
      ]
    },
    {
      "abstract": "Machine learning employs dynamical algorithms that mimic the human capacity to learn, where the reinforcement learning ones are among the most similar to humans in this respect. On the other hand, adaptability is an essential aspect to perform any task efficiently in a changing environment, and it is fundamental for many purposes, such as natural selection. Here, we propose an algorithm based on successive measurements to adapt one quantum state to a reference unknown state, in the sense of achieving maximum overlap. The protocol naturally provides many identical copies of the reference state, such that in each measurement iteration more information about it is obtained. In our protocol, we consider a system composed of three parts, the ``environment'' system, which provides the reference state copies; the register, which is an auxiliary subsystem that interacts with the environment to acquire information from it; and the agent, which corresponds to the quantum state that is adapted by digital feedback with input corresponding to the outcome of the measurements on the register. With this proposal we can achieve an average fidelity between the environment and the agent of more than $90%$ with less than 30 iterations of the protocol. In addition, we extend the formalism to $d$-dimensional states, reaching an average fidelity of around $80%$ in less than 400 iterations for $d=11$, for a variety of genuinely quantum and semiclassical states. This work paves the way for the development of quantum reinforcement learning protocols using quantum data and for the future deployment of semiautonomous quantum systems.",
      "authors": [
        "F. Albarr\u00e1n-Arriagada",
        "J. C. Retamal",
        "E. Solano",
        "L. Lamata"
      ],
      "categories": null,
      "citations": 49,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.1103/PhysRevA.98.042315",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "24699926",
        "publisher": "American Physical Society",
        "sjr": 1.11,
        "snip": 0.925,
        "subject_areas": [
          "Atomic and Molecular Physics, and Optics",
          "Artificial Intelligence",
          "Quantum Physics",
          "Learning",
          "Machine Learning",
          "Mesoscale and Nanoscale Physics"
        ],
        "title": "Phys. Rev. A 98, 042315 (2018)"
      },
      "publication_date": "2018-03-14",
      "selected": null,
      "title": "Measurement-based adaptation protocol with quantum reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054803812&origin=inward",
        "http://arxiv.org/pdf/1803.05340v2",
        "http://arxiv.org/abs/1803.05340v2",
        "http://dx.doi.org/10.1103/PhysRevA.98.042315",
        "http://link.aps.org/pdf/10.1103/PhysRevA.98.042315"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Collins A."
      ],
      "categories": null,
      "citations": 70,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.1720963115",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "2502-2507",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2018-03-06",
      "selected": null,
      "title": "Within- and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042942199&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Massimo D."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3172944.3173151",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "675-676",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450349451",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent User Interfaces, Proceedings IUI"
      },
      "publication_date": "2018-03-05",
      "selected": null,
      "title": "User preference modeling and exploitation in IoT scenarios",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045153051&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Roy S."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3173386.3177074",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "225-226",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450356152",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2018-03-01",
      "selected": null,
      "title": "Using Human Reinforcement Learning Models to Improve Robots as Teachers",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85045282651&origin=inward"
      ]
    },
    {
      "abstract": "Gambling studies have described a \u201cnear-miss effect\u201d wherein the experience of almost winning increases gambling persistence. The near-miss has been proposed to inflate the value of preceding actions through its perceptual similarity to wins. We demonstrate here, however, that it acts as a conditioned stimulus to positively or negatively influence valuation, dependent on reward expectation and cognitive engagement. When subjects are asked to choose between two simulated slot machines, near-misses increase valuation of machines with a low payout rate, whereas they decrease valuation of high payout machines. This contextual effect impairs decisions and persists regardless of manipulations to outcome feedback or financial incentive provided for good performance. It is consistent with proposals that near-misses cause frustration when wins are expected, and we propose that it increases choice stochasticity and overrides avoidance of low-valued options. Intriguingly, the near-miss effect disappears when subjects are required to explicitly value machines by placing bets, rather than choosing between them. We propose that this task increases cognitive engagement and recruits participation of brain regions involved in cognitive processing, causing inhibition of otherwise dominant systems of decision-making. Our results reveal that only implicit, rather than explicit strategies of decision-making are affected by near-misses, and that the brain can fluidly shift between these strategies according to task demands.",
      "authors": [
        "Banks, Parker J.",
        "Tata, Matthew S.",
        "Bennett, Patrick J.",
        "Sekuler, Allison B.",
        "Gruber, Aaron J."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10899-017-9705-3",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "181-197",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10505350",
        "publisher": "Springer International Publishing AG",
        "sjr": 0.951,
        "snip": 1.188,
        "subject_areas": [
          "Sociology and Political Science",
          "Psychology (all)"
        ],
        "title": "Journal of Gambling Studies"
      },
      "publication_date": "2018-03-01",
      "selected": null,
      "title": "Implicit Valuation of the Near-Miss is Dependent on Outcome Context",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10899-017-9705-3.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042713670&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Waltz J.A."
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bpsc.2017.07.008",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "239-247",
      "publication": {
        "category": "Journal",
        "cite_score": 9.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "24519022",
        "publisher": "Elsevier Inc.",
        "sjr": 1.997,
        "snip": 1.337,
        "subject_areas": [
          "Neurology (clinical)",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
      },
      "publication_date": "2018-03-01",
      "selected": null,
      "title": "Motivational Deficits in Schizophrenia Are Associated With Reduced Differentiation Between Gain and Loss-Avoidance Feedback in the Striatum",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042469971&origin=inward"
      ]
    },
    {
      "abstract": "<p>Hormone by genotype interactions have been widely ignored by cognitive neuroscience. Yet, the dependence of cognitive performance on both baseline dopamine (DA) and current 17\u00df-estradiol (E2) level argues for their combined effect also in the context of reinforcement learning. Here, we assessed how the interaction between the natural rise of E2 in the late follicular phase (FP) and the 40 base-pair variable number tandem repeat polymorphism of the dopamine transporter (DAT1) affects reinforcement learning capacity. 30 women with a regular menstrual cycle performed a probabilistic feedback learning task twice during the early and late FP. In addition, 39 women, who took hormonal contraceptives (HC) to suppress natural ovulation, were tested during the \u201cpill break\u201d and the intake phase of HC. The present data show that DAT1-genotype may interact with transient hormonal state, but only in women with a natural menstrual cycle. We found that carriers of the 9-repeat allele (9RP) experienced a significant decrease in the ability to avoid punishment from early to late FP. Neither homozygote subjects of the 10RP allele, nor subjects from the HC group showed a change in behavior between phases. These data are consistent with neurobiological studies that found that rising E2 may reverse DA transporter function and could enhance DA efflux, which would in turn reduce punishment sensitivity particularly in subjects with a higher transporter density to begin with. Taken together, the present results, although based on a small sample, add to the growing understanding of the complex interplay between different physiological modulators of dopaminergic transmission. They may not only point out the necessity to control for hormonal state in behavioral genetic research, but may offer new starting points for studies in clinical settings.</p>",
      "authors": [
        "Jakob, Kristina",
        "Ehrentreich, Hanna",
        "Reimers, Luise",
        "Diekhof, Esther K."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fendo.2018.00060",
      "keywords": [
        "estrogen",
        "Hormonal contraception",
        "reinforcement learning",
        "steroid hormone",
        "dopamine transporter",
        "gender"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-2392",
        "publisher": "Frontiers Media SA",
        "sjr": 1.278,
        "snip": 1.426,
        "subject_areas": [
          "Endocrinology, Diabetes and Metabolism"
        ],
        "title": "Frontiers in Endocrinology"
      },
      "publication_date": "2018-02-28",
      "selected": null,
      "title": "DAT1-Genotype and Menstrual Cycle, but Not Hormonal Contraception, Modulate Reinforcement Learning: Preliminary Evidence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042704729&origin=inward",
        "https://www.frontiersin.org/journals/endocrinology/articles/10.3389/fendo.2018.00060/pdf"
      ]
    },
    {
      "abstract": "This paper considers the motion control and task planning problem of mobile robots under complex high-level tasks and human initiatives. The assigned task is specified as Linear Temporal Logic (LTL) formulas that consist of hard and soft constraints. The human initiative influences the robot autonomy in two explicit ways: with additive terms in the continuous controller and with contingent task assignments. We propose an online coordination scheme that encapsulates (i) a mixed-initiative continuous controller that ensures all-time safety despite of possible human errors, (ii) a plan adaptation scheme that accommodates new features discovered in the workspace and short-term tasks assigned by the operator during run time, and (iii) an iterative inverse reinforcement learning (IRL) algorithm that allows the robot to asymptotically learn the human preference on the parameters during the plan synthesis. The results are demonstrated by both realistic human-in-the-loop simulations and experiments.",
      "authors": [
        "Guo, Meng",
        "Andersson, Sofie",
        "Dimarogonas, Dimos V."
      ],
      "categories": null,
      "citations": null,
      "comments": "8 pages, 7 figures, IEEE International Conference on Robotics and\n  Automation",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-02-19",
      "selected": null,
      "title": "Human-in-the-Loop Mixed-Initiative Control under Temporal Tasks",
      "urls": [
        "http://arxiv.org/pdf/1802.06839.pdf",
        "http://arxiv.org/pdf/1802.06839v1",
        "http://arxiv.org/abs/1802.06839v1"
      ]
    },
    {
      "abstract": "What is the role of real-time control and learning in the formation of social conventions? To answer this question, we propose a computational model that matches human behavioral data in a social decision-making game that was analyzed both in discrete-time and continuous-time setups. Furthermore, unlike previous approaches, our model takes into account the role of sensorimotor control loops in embodied decision-making scenarios. For this purpose, we introduce the Control-based Reinforcement Learning (CRL) model. CRL is grounded in the Distributed Adaptive Control (DAC) theory of mind and brain, where low-level sensorimotor control is modulated through perceptual and behavioral learning in a layered structure. CRL follows these principles by implementing a feedback control loop handling the agent's reactive behaviors (pre-wired reflexes), along with an adaptive layer that uses reinforcement learning to maximize long-term reward. We test our model in a multi-agent game-theoretic task in which coordination must be achieved to find an optimal solution. We show that CRL is able to reach human-level performance on standard game-theoretic metrics such as efficiency in acquiring rewards and fairness in reward distribution.",
      "authors": [
        "Freire, Ismael T.",
        "Moulin-Frier, Clement",
        "Sanchez-Fibla, Marti",
        "Arsiwalla, Xerxes D.",
        "Verschure, Paul"
      ],
      "categories": null,
      "citations": null,
      "comments": "16 pages, 7 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-02-16",
      "selected": null,
      "title": "Modeling the Formation of Social Conventions from Embodied Real-Time Interactions",
      "urls": [
        "http://arxiv.org/pdf/1802.06108.pdf",
        "http://arxiv.org/abs/1802.06108v3",
        "http://arxiv.org/pdf/1802.06108v3"
      ]
    },
    {
      "abstract": "An artificial neural network can discover algorithms for quantum error correction without human guidance.",
      "authors": [
        "Thomas F\u00f6sel",
        "Petru Tighineanu",
        "Talitha Weiss",
        "Florian Marquardt"
      ],
      "categories": null,
      "citations": 181,
      "comments": "7 pages maintext + methods + supplementary, 6 maintext figures; for\n  related lectures, see: http://machine-learning-for-physicists.org",
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": "10.1103/PhysRevX.8.031084",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "American Physical Society",
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Quantum Physics"
        ],
        "title": "Phys. Rev. X 8, 031084 (2018)"
      },
      "publication_date": "2018-02-14",
      "selected": null,
      "title": "Reinforcement Learning with Neural Networks for Quantum Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054531362&origin=inward",
        "http://arxiv.org/pdf/1802.05267v3",
        "http://arxiv.org/abs/1802.05267v3",
        "http://dx.doi.org/10.1103/PhysRevX.8.031084",
        "http://link.aps.org/pdf/10.1103/PhysRevX.8.031084"
      ]
    },
    {
      "abstract": "In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user's policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user's actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user's input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) flying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user's private information through observations, but receives a reward signal and user input that both depend on the user's intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user's input. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.",
      "authors": [
        "Reddy, Siddharth",
        "Dragan, Anca D.",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": "Accepted to the Robotics: Science and Systems (RSS) 2018 conference",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2018-02-06",
      "selected": null,
      "title": "Shared Autonomy via Deep Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/1802.01744v2",
        "http://arxiv.org/pdf/1802.01744.pdf",
        "http://arxiv.org/abs/1802.01744v2"
      ]
    },
    {
      "abstract": "In anatomy education, a key hurdle to engaging in higher-level discussion in the classroom is recognizing and understanding the extensive terminology used to identify and describe anatomical structures. Given the time-limited classroom environment, seeking methods to impart this foundational knowledge to students in an efficient manner is essential. Just-in-Time Teaching (JiTT) methods incorporate pre-class exercises (typically online) meant to establish foundational knowledge in novice learners so subsequent instructor-led sessions can focus on deeper, more complex concepts. Determining how best do we design and assess pre-class exercises requires a detailed examination of learning and retention in an applied educational context. Here we used electroencephalography (EEG) as a quantitative dependent variable to track learning and examine the efficacy of JiTT activities to teach anatomy. Specifically, we examined changes in the amplitude of the N250 and reward positivity event-related brain potential (ERP) components alongside behavioural performance as novice students participated in a series of computerized reinforcement-based learning modules to teach neuroanatomical structures. We found that as students learned to identify anatomical structures, the amplitude of the N250 increased and reward positivity amplitude decreased in response to positive feedback. Both on a retention and transfer exercise when learners successfully remembered and translated their knowledge to novel images, the amplitude of the reward positivity remained decreased compared to early learning. Our findings suggest ERPs can be used as a tool to track learning, retention, and transfer of knowledge and that employing the reinforcement learning paradigm is an effective educational approach for developing anatomical expertise.",
      "authors": [
        "Anderson, Sarah J.",
        "Hecker, Kent G.",
        "Krigolson, Olave E.",
        "Jamniczky, Heather A."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2018.00038",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2018-02-06",
      "selected": null,
      "title": "A Reinforcement-Based Learning Paradigm Increases Anatomical Learning and Retention\u2014A Neuroeducation Study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043584910&origin=inward"
      ]
    },
    {
      "abstract": "User feedback can be an effective indicator to the success of the human-robot conversation. However, to avoid to interrupt the online real-time conversation process, explicit feedback is usually gained at the end of a conversation. Alternatively, users' responses usually contain their implicit feedback, such as stance, sentiment, emotion, etc., towards the conversation content or the interlocutors. Therefore, exploring the implicit feedback is a natural way to optimize the conversation generation process. In this paper, we propose a novel reward function which explores the implicit feedback to optimize the future reward of a reinforcement learning based neural conversation model. A simulation strategy is applied to explore the state-action space in training and test. Experimental results show that the proposed approach outperforms the Seq2Seq model and the state-of-the-art reinforcement learning model for conversation generation on automatic and human evaluations on the OpenSubtitles and Twitter datasets.",
      "authors": [
        "Wei-Nan Zhang",
        "Lingzhi Li",
        "Dongyan Cao",
        "Ting Liu"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3504035.3504103",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "547-554",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-800-8",
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2018-02-02",
      "selected": null,
      "title": "Exploring implicit feedback for open domain conversation generation",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3504035.3504103"
      ]
    },
    {
      "abstract": "While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose Deep TAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of BOWLING - a task that has proven difficult for even state-of-the-art reinforcement learning methods.",
      "authors": [
        "Garrett Warnell",
        "Nicholas Waytowich",
        "Vernon Lawhern",
        "Peter Stone"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3504035.3504224",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1545-1553",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-800-8",
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2018-02-02",
      "selected": null,
      "title": "Deep TAMER: interactive agent shaping in high-dimensional state spaces",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3504035.3504224"
      ]
    },
    {
      "abstract": "Impressive image captioning results (i.e., an objective description for an image) are achieved with plenty of training pairs. In this paper, we take one step further to investigate the creation of narrative paragraph for a photo stream. This task is even more challenging due to the difficulty in modeling an ordered photo sequence and in generating a relevant paragraph with expressive language style for storytelling. The difficulty can even be exacerbated by the limited training data, so that existing approaches almost focus on search-based solutions. To deal with these challenges, we propose a sequence-to-sequence modeling approach with reinforcement learning and adversarial training. First, to model the ordered photo stream, we propose a hierarchical recurrent neural network as story generator, which is optimized by reinforcement learning with rewards. Second, to generate relevant and story-style paragraphs, we design the rewards with two critic networks, including a multi-modal and a language-style discriminator. Third, we further consider the story generator and reward critics as adversaries. The generator aims to create indistinguishable paragraphs to human-level stories, whereas the critics aim at distinguishing them and further improving the generator by policy gradient. Experiments on three widely-used datasets show the effectiveness, against state-of-the-art methods with relative increase of 20.2% by METEOR. We also show the subjective preference for the proposed approach over the baselines through a user study with 30 human subjects.",
      "authors": [
        "Jing Wang",
        "Jianlong Fu",
        "Jinhui Tang",
        "Zechao Li",
        "Tao Mei"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3504035.3504941",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7396-7403",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-57735-800-8",
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence"
      },
      "publication_date": "2018-02-02",
      "selected": null,
      "title": "Show, reward and tell: automatic generation of narrative paragraph from photo stream by adversarial training",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3504035.3504941"
      ]
    },
    {
      "abstract": "Performance-related feedback plays an important role in improving human being\u2019s adaptive behavior. Using event-related potentials (ERPs), previous studies have associated a particular component, i.e., reward positivity (RewP), with outcome evaluation processing and found that this component was affected by waiting time before outcome evaluation. Prior research has also suggested that anxious individuals are more prone to detecting threats and susceptible to negative emotions, and show different patterns of brain activity in outcome evaluation. It is quite common that a decision-maker cannot receive feedback immediately; however, few studies have focused on the processing of delayed feedback, especially in subjects who exhibit trait anxiety. In this study, we recruited 2 groups of subjects with different trait anxiety levels and recorded ERPs when they conducted a time-estimation task with short (0.6 to 1 second) or long delayed (4 to 5 seconds) feedback. The ERP results during the cue phase showed that long waiting cues elicited more negative waves than short waiting cues in the high trait anxiety (HTA) group. More importantly, the 2 groups showed different patterns of ERP in the feedback condition. In the low trait anxiety (LTA) group, larger RewP was found in the short-delayed than in the long-delayed condition. In contrast, no difference was found in the HTA group. This pattern may reflect the hyperactivity of the reward systems of HTA individuals in uncertain environments (e.g., the long-delay condition) compared with LTA individuals. Our results provide a direction for future research on the neural mechanisms of reinforcement learning and anxiety.",
      "authors": [
        "Zhang, Xukai",
        "Lei, Yi",
        "Yin, Hang",
        "Li, Peng",
        "Li, Hong"
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2018.00020",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2018-01-25",
      "selected": null,
      "title": "Slow Is Also Fast: Feedback Delay Affects Anxiety and Outcome Evaluation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041349656&origin=inward"
      ]
    },
    {
      "abstract": "<p>As adolescents transition to the complex world of adults, optimizing predictions about others9 preferences becomes vital for successful social interactions. Mounting evidence suggests that these social learning processes are affected by ongoing brain development across adolescence. A mechanistic understanding of how adolescents optimize social predictions and how these learning strategies are implemented in the brain is lacking. To fill this gap, we combined computational modeling with functional neuroimaging. In a novel social learning task, male and female human adolescents and adults predicted the preferences of peers and could update their predictions based on trial-by-trial feedback about the peers9 actual preferences. Participants also rated their own preferences for the task items and similar additional items. To describe how participants optimize their inferences over time, we pitted simple reinforcement learning models against more specific \u201ccombination\u201d models, which describe inferences based on a combination of reinforcement learning from past feedback and participants9 own preferences. Formal model comparison revealed that, of the tested models, combination models best described how adults and adolescents update predictions of others. Parameter estimates of the best-fitting model differed between age groups, with adolescents showing more conservative updating. This developmental difference was accompanied by a shift in encoding predictions and the errors thereof within the medial prefrontal and fusiform cortices. In the adolescent group, encoding of own preferences and prediction errors scaled with parent-reported social traits, which provides additional external validity for our learning task and the winning computational model. Our findings thus help to specify adolescent-specific social learning processes.</p><p><b>SIGNIFICANCE STATEMENT</b> Adolescence is a unique developmental period of heightened awareness about other people. Here we probe the suitability of various computational models to describe how adolescents update their predictions of others9 preferences. Within the tested model space, predictions of adults and adolescents are best described by the same learning model, but adolescents show more conservative updating. Compared with adults, brain activity of adolescents is modulated less by predictions themselves and more by prediction errors per se, and this relationship scales with adolescents9 social traits. Our findings help specify social learning across adolescence and generate hypotheses about social dysfunctions in psychiatric populations.</p>",
      "authors": [
        "Gabriela Rosenblau",
        "Christoph W. Korn",
        "Kevin A. Pelphrey"
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1044-17.2017",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "974-988",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2018-01-24",
      "selected": null,
      "title": "A Computational Account of Optimizing Social Predictions Reveals That Adolescents Are Conservative Learners in Social Contexts",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040994017&origin=inward"
      ]
    },
    {
      "abstract": "Hand gesture recognition is one of the major research areas in the field of Human computer interaction (HCl). This paper proposes a deep reinforcement learning algorithm to recognize the human arm movement patterns using an IoT sensor device. Recent studies have explored supervised learning based methods, such as CNN and RNN to implement the HCl device. On the other hand, the deep reinforcement learning approach has also been investigated. Algorithms using this approach, learn the patterns from sensors using only the reward feedback with no class labels. This allows users to control the IoT device and produce the desired arm movement patterns without creating any labels. In this paper, the performance of convolutional neural network (CNN) with the DQN model was compared with that of long short-term memory (LSTM) models with DQN. Results show that the CNN based DQN model was more stable compared to the LSTM based model, and yielded a high classification accuracy of 98.33% to predict the arm movement patterns.",
      "authors": [
        "W. Seok",
        "Y. Kim",
        "C. Park"
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICOIN.2018.8343257",
      "keywords": [
        "Myo armband",
        "patterin recognition",
        "Human-Computer Interaction (HCl)",
        "deep reinforcemnet learning",
        "deep Q-Netn'rok"
      ],
      "number_of_pages": 3,
      "pages": "917-919",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-2291-9",
        "issn": "19767684",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2018 International Conference on Information Networking (ICOIN)"
      },
      "publication_date": "2018-01-10",
      "selected": null,
      "title": "Pattern recognition of human arm movement using deep reinforcement learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8343257",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85047009152&origin=inward"
      ]
    },
    {
      "abstract": "Energy consumption for hot water production is a major draw in high\nefficiency buildings. Optimizing this has typically been approached from a\nthermodynamics perspective, decoupled from occupant influence. Furthermore,\noptimization usually presupposes existence of a detailed dynamics model for the\nhot water system. These assumptions lead to suboptimal energy efficiency in the\nreal world. In this paper, we present a novel reinforcement learning based\nmethodology which optimizes hot water production. The proposed methodology is\ncompletely generalizable, and does not require an offline step or human domain\nknowledge to build a model for the hot water vessel or the heating element.\nOccupant preferences too are learnt on the fly. The proposed system is applied\nto a set of 32 houses in the Netherlands where it reduces energy consumption\nfor hot water production by roughly 20% with no loss of occupant comfort.\nExtrapolating, this translates to absolute savings of roughly 200 kWh for a\nsingle household on an annual basis. This performance can be replicated to any\ndomestic hot water system and optimization objective, given that the fairly\nminimal requirements on sensor data are met. With millions of hot water systems\noperational worldwide, the proposed framework has the potential to reduce\nenergy consumption in existing and new systems on a multi Gigawatt-hour scale\nin the years to come.",
      "authors": [
        "Hussain Kazmi",
        "Fahad Mehmood",
        "Stefan Lodeweyckx",
        "Johan Driesen"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": "10.1016/j.energy.2017.12.019",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Machine Learning",
          "Systems and Control",
          "Applications"
        ],
        "title": "Energy, Volume 144, 2018"
      },
      "publication_date": "2018-01-04",
      "selected": null,
      "title": "Deep Reinforcement Learning based Optimal Control of Hot Water Systems",
      "urls": [
        "http://dx.doi.org/10.1016/j.energy.2017.12.019",
        "http://arxiv.org/abs/1801.01467v1",
        "http://arxiv.org/pdf/1801.01467v1"
      ]
    },
    {
      "abstract": "It is important for humanoid-like mobile robots to learn the complex motion sequences in human-robot environment such that the robots can adapt such motions. This paper describes a reinforcement learning (RL) strategy for manipulation and grasping of a mobile manipulator, which reduces the complexity of the visual feedback and handle varying manipulation dynamics and uncertain external perturbations. Two hierarchies plannings have been considered in the proposed strategy: 1) high-level online redundancy resolution based on the neural-dynamic optimization algorithm in operational space; and 2) low-level RL in joint space. At this level, the dynamic movement primitives have been considered to model and learn the joint trajectories, and then the RL is employed to learn the trajectories with uncertainties. Experimental results on the developed humanoidlike mobile robot demonstrate that the presented approach can suppress the uncertain external perturbations.",
      "authors": [
        "Zhijun Li",
        "Ting Zhao",
        "Fei Chen",
        "Yingbai Hu",
        "Chun-Yi Su",
        "Toshio Fukuda"
      ],
      "categories": null,
      "citations": 150,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/TMECH.2017.2717461",
      "keywords": [
        "reinforcement learning (RL)",
        "Dynamic movement primitive (DMP)",
        "mobile manipulation",
        "redundancy resolution"
      ],
      "number_of_pages": 11,
      "pages": "121-131",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1941-014X",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE/ASME Transactions on Mechatronics"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Reinforcement Learning of Manipulation and Grasping Using Dynamical Movement Primitives for a Humanoidlike Mobile Manipulator",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021781497&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7953692"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Freire, Ismael T.",
        "Arsiwalla, Xerxes D.",
        "Puigb&#242;, Jordi-Ysard",
        "Verschure, Paul F.M.J."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/978-1-61499-918-8-297",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "297-301",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781643684369",
        "issn": "09226389",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Frontiers in Artificial Intelligence and Applications"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Limits of Multi-Agent Predictive Models in the Formation of Social Conventions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055291210&origin=inward",
        "https://ebooks.iospress.nl/pdf/doi/10.3233/978-1-61499-918-8-297"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Warnell G."
      ],
      "categories": null,
      "citations": 89,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1545-1553",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358008",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "32nd AAAI Conference on Artificial Intelligence, AAAI 2018"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Deep TAMER: Interactive agent shaping in high-dimensional state spaces",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060441431&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wang J."
      ],
      "categories": null,
      "citations": 46,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7396-7403",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358008",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "32nd AAAI Conference on Artificial Intelligence, AAAI 2018"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Show, reward and tell: Automatic generation of narrative paragraph from photo stream by adversarial training",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060479993&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kreutzer J."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.18653/v1/p18-1165",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1777-1788",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781948087322",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056312012&origin=inward"
      ]
    },
    {
      "abstract": "<h3>Abstract</h3>\n<p>Dopamine has been suggested to be crucially involved in effort-related choices. Key findings are that dopamine depletion (i) changed preference for a high-cost, large-reward option to a low-cost, small-reward option, (ii) but not when the large-reward option was also low-cost or the small-reward option gave no reward, (iii) while increasing the latency in all the cases but only transiently, and (iv) that antagonism of either dopamine D1 or D2 receptors also specifically impaired selection of the high-cost, large-reward option. The underlying neural circuit mechanisms remain unclear. Here we show that findings i\u2013iii can be explained by the dopaminergic representation of temporal-difference reward-prediction error (TD-RPE), whose mechanisms have now become clarified, if (1) the synaptic strengths storing the values of actions mildly decay in time and (2) the obtained-reward-representing excitatory input to dopamine neurons increases after dopamine depletion. The former is potentially caused by background neural activity\u2013induced weak synaptic plasticity, and the latter is assumed to occur through post-depletion increase of neural activity in the pedunculopontine nucleus, where neurons representing obtained reward exist and presumably send excitatory projections to dopamine neurons. We further show that finding iv, which is nontrivial given the suggested distinct functions of the D1 and D2 corticostriatal pathways, can also be explained if we additionally assume a proposed mechanism of TD-RPE calculation, in which the D1 and D2 pathways encode the values of actions with a temporal difference. These results suggest a possible circuit mechanism for the involvements of dopamine in effort-related choices and, simultaneously, provide implications for the mechanisms of TD-RPE calculation.</p>",
      "authors": [
        "Kenji Morita",
        "Ayaka Kato"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/ENEURO.0021-18.2018",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2373-2822",
        "publisher": "Society for Neuroscience",
        "sjr": 1.309,
        "snip": 0.893,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "eNeuro"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "A Neural Circuit Mechanism for the Involvements of Dopamine in Effort-Related Choices: Decay of Learned Values, Secondary Effects of Depletion, and Calculation of Temporal Difference Error",
      "urls": [
        "https://www.eneuro.org/content/eneuro/5/1/ENEURO.0021-18.2018.full.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043364692&origin=inward"
      ]
    },
    {
      "abstract": "In the practice of motor skills in general, errors in the execution of movements may go unnoticed when a human instructor is not available. In this case, a computer system or robotic device able to detect movement errors and propose corrections would be of great help. This paper addresses the problem of how to detect such execution errors and how to provide feedback to the human to correct his/her motor skill using a general, principled methodology based on imitation learning. The core idea is to compare the observed skill with a probabilistic model learned from expert demonstrations. The intensity of the feedback is regulated by the likelihood of the model given the observed skill. Based on demonstrations, our system can, for example, detect errors in the writing of Japanese characters with multiple strokes. Moreover, by using a haptic device, the Haption Virtuose 6D, we demonstrate a method to generate haptic feedback based on a distribution over trajectories, which could be used as an auxiliary means of communication between an instructor and an apprentice. Additionally, given a performance measurement, the haptic device can help the human discover and perform better movements to solve a given task. In this case, the human first tries a few times to solve the task without assistance. Our framework, in turn, uses a reinforcement learning algorithm to compute haptic feedback, which guides the human towards better solutions.",
      "authors": [
        "Ewerton, Marco",
        "Rother, David",
        "Weimar, Jakob",
        "Kollegger, Gerrit",
        "Wiemeyer, Josef",
        "Peters, Jan",
        "Maeda, Guilherme"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2018.00024",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Assisting Movement Training and Execution With Visual and Haptic Feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85071157188&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mattar M.G."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/netn_a_00021",
      "keywords": [],
      "number_of_pages": 22,
      "pages": "128-149",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Network Neuroscience"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "The network architecture of value learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85066099753&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Reddy S."
      ],
      "categories": null,
      "citations": 46,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.15607/RSS.2018.XIV.005",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780992374747",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Robotics: Science and Systems"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Shared Autonomy via Deep Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85127893599&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dridi S."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1086/694822",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "58-73",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00030147",
        "publisher": "University of Chicago",
        "sjr": 1.49,
        "snip": 1.188,
        "subject_areas": [
          "Ecology, Evolution, Behavior and Systematics"
        ],
        "title": "American Naturalist"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Learning to cooperate: The evolution of social rewards in repeated interactions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85038387101&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ritschel H."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1775-1777",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Socially-aware Reinforcement Learning for personalized Human-Robot Interaction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054729414&origin=inward"
      ]
    },
    {
      "abstract": "Learning from rewards generated by a human trainer observing an agent in action has been proven to be a powerful method for teaching autonomous agents to perform challenging tasks, especially for those non-technical users. Since the efficacy of this approach depends critically on the reward the trainer provides, we consider how the interaction between the trainer and the agent should be designed so as to increase the efficiency of the training process. This article investigates the influence of the agent\u2019s socio-competitive feedback on the human trainer\u2019s training behavior and the agent\u2019s learning. The results of our user study with 85 participants suggest that the agent\u2019s passive socio-competitive feedback\u2014showing performance and score of agents trained by trainers in a leaderboard\u2014substantially increases the engagement of the participants in the game task and improves the agents\u2019 performance, even though the participants do not directly play the game but instead train the agent to do so. Moreover, making this feedback active\u2014sending the trainer her agent\u2019s performance relative to others\u2014further induces more participants to train agents longer and improves the agent\u2019s learning. Our further analysis shows that agents trained by trainers affected by both the passive and active social feedback could obtain a higher performance under a score mechanism that could be optimized from the trainer\u2019s perspective and the agent\u2019s additional active social feedback can keep participants to further train agents to learn policies that can obtain a higher performance under such a score mechanism.",
      "authors": [
        "Li, Guangliang",
        "Whiteson, Shimon",
        "Knox, W. Bradley",
        "Hung, Hayley"
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10458-017-9374-8",
      "keywords": [],
      "number_of_pages": 25,
      "pages": "1-25",
      "publication": {
        "category": "Journal",
        "cite_score": 5.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13872532",
        "publisher": "Springer Netherlands",
        "sjr": 0.927,
        "snip": 2.046,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Autonomous Agents and Multi-Agent Systems"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Social interaction for efficient agent learning from human reward",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021808874&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10458-017-9374-8.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Schmid P."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12911",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Frontal cortical effects on feedback processing and reinforcement learning: Relation of EEG asymmetry with the feedback-related negativity and behavior",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021794986&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lam T.K."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "169-178",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9788409019014",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "EAMT 2018 - Proceedings of the 21st Annual Conference of the European Association for Machine Translation"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "A reinforcement learning approach to interactive-predictive neural machine translation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85057060207&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Weber K."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "2259-2261",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Real-time adaptation of a robotic joke teller based on human social signals",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85054768440&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Anderson B."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00489.2018",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "2654-2658",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "On the representational nature of value-driven spatial attentional biases",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061146375&origin=inward"
      ]
    },
    {
      "abstract": "Electronic proceedings of IJCAI 2018",
      "authors": [
        "Matthew E. Taylor"
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.24963/ijcai.2018/817",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "5724-5728",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781956792034",
        "issn": "10450823",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IJCAI International Joint Conference on Artificial Intelligence"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Improving Reinforcement Learning with Human Input",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055699975&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "38th SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence, AI 2018",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85058412336&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Scurto H."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "72-79",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789963697304",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 15th Sound and Music Computing Conference: Sonic Crossings, SMC 2018"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Perceiving agent collaborative sonic exploration in interactive reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85075110063&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Collins A.G.E."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-12-812098-9.00005-X",
      "keywords": [],
      "number_of_pages": 19,
      "pages": "105-123",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780128120989",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Goal-Directed Decision Making: Computations and Neural Circuits"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Learning structures through reinforcement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85081379776&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhang W.N."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "547-554",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577358008",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "32nd AAAI Conference on Artificial Intelligence, AAAI 2018"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Exploring implicit feedback for open domain conversation generation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060460252&origin=inward"
      ]
    },
    {
      "abstract": "Neural networks and reinforcement learning have successfully been applied to various games, such as Ms. Pacman and Go. We combine multilayer perceptrons and a class of reinforcement learning algorithms known as actor-critic to learn to play the arcade classic Donkey...",
      "authors": [
        "Ozkohen, Paul",
        "Visser, Jelle",
        "van Otterlo, Martijn",
        "Wiering, Marco"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-319-76892-2_11",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "145-160",
      "publication": {
        "category": "Book",
        "cite_score": 1.0,
        "is_potentially_predatory": false,
        "isbn": "9789819927883",
        "issn": "18650929",
        "publisher": "Springer Science and Business Media Deutschland GmbH",
        "sjr": 0.194,
        "snip": 0.241,
        "subject_areas": [
          "Computer Science (all)",
          "Mathematics (all)"
        ],
        "title": "International Conference on Neural Information Processing"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Learning to Play Donkey Kong Using Neural Networks and Reinforcement Learning",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-319-76892-2_11.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043256717&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Verdejo-Garcia A."
      ],
      "categories": null,
      "citations": 98,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.pbb.2017.02.003",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "99-105",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00913057",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Pharmacology Biochemistry and Behavior"
      },
      "publication_date": "2018-01-01",
      "selected": null,
      "title": "Stages of dysfunctional decision-making in addiction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014266239&origin=inward"
      ]
    },
    {
      "abstract": "Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints and angles, even in the presence of optical distortions. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we study how viewpoint-invariant visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. The problem that must be solved by this controller is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing system must use its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to most visual servoing methods, which either assume known dynamics or require a calibration phase. We show how we can learn this recurrent controller using simulated data and a reinforcement learning objective. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: https://fsadeghi.github.io/Sim2RealViewInvariantServo",
      "authors": [
        "Sadeghi, Fereshteh",
        "Toshev, Alexander",
        "Jang, Eric",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": "Supplementary video:\n  https://fsadeghi.github.io/Sim2RealViewInvariantServo",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-12-20",
      "selected": null,
      "title": "Sim2Real View Invariant Visual Servoing by Recurrent Control",
      "urls": [
        "http://arxiv.org/abs/1712.07642v1",
        "http://arxiv.org/pdf/1712.07642.pdf",
        "http://arxiv.org/pdf/1712.07642v1"
      ]
    },
    {
      "abstract": "Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.",
      "authors": [
        "Armstrong, Stuart",
        "Mindermann, S\u00f6ren"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-12-15",
      "selected": null,
      "title": "Occam's razor is insufficient to infer the preferences of irrational agents",
      "urls": [
        "http://arxiv.org/pdf/1712.05812v6",
        "http://arxiv.org/pdf/1712.05812.pdf",
        "http://arxiv.org/abs/1712.05812v6"
      ]
    },
    {
      "abstract": "Embedding multiple sensors \u2014 force/torque, vision, and distance \u2014 in the feedback loops of motion controllers has enabled new robot applications. For instance, safe human-robot interaction and many assembly tasks that could not be automated before. As important as these real-time control features is the ability to plan robot motions deterministically and in real-time. To enable spontaneous changes from sensor-guided robot motion control (e.g., force/torque or visual servo control) to trajectory-following motion control, an algorithmic framework is explained that lets us compute robot motions deterministically within less than one millisecond. The resulting class of on-line trajectory generation algorithms serves as an intermediate layer between low-level motion control and high-level sensor-based motion planning. Online motion generation from arbitrary states is an essential feature for autonomous hybrid switched motion control systems. Building upon this framework and with the goal of significantly reducing the amount of resources needed for programing industrial and service robots, reinforcement learning offers a yet unused potential that will be introduced as well. Samples and use-cases \u2014 including manipulation and human-robot interaction tasks \u2014 will accompany the talk in order to provide a comprehensible insight into these interesting and relevant fields of robotics.",
      "authors": [
        "Torsten Kroeger"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/SII.2017.8279182",
      "keywords": [],
      "number_of_pages": 1,
      "pages": "3-3",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-2264-3",
        "issn": "2474-2325",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2017 IEEE/SICE International Symposium on System Integration (SII)"
      },
      "publication_date": "2017-12-11",
      "selected": null,
      "title": "Sensor-based control, real-time motion planning, and reinforcement learning for industrial robots",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8279182"
      ]
    },
    {
      "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.",
      "authors": [
        "Paul F. Christiano",
        "Jan Leike",
        "Tom B. Brown",
        "Miljan Martic",
        "Shane Legg",
        "Dario Amodei"
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3294996.3295184",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "4302-4310",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781510860964",
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
      },
      "publication_date": "2017-12-04",
      "selected": null,
      "title": "Deep reinforcement learning from human preferences",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3294996.3295184"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wirth C."
      ],
      "categories": null,
      "citations": 136,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15324435",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Machine Learning Research"
      },
      "publication_date": "2017-12-01",
      "selected": null,
      "title": "A survey of preference-based reinforcement learning methods",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85040715600&origin=inward"
      ]
    },
    {
      "abstract": "The dopamine partial agonist aripiprazole is increasingly used to treat pathologies for which other antipsychotics are indicated because it displays fewer side effects, such as sedation and depression-like symptoms, than other dopamine receptor antagonists. Previously, we showed that aripiprazole may protect motivational function by preserving reinforcement-related signals used to sustain reward-maximization. However, the effect of aripiprazole on more cognitive facets of human reinforcement learning, such as learning from the forgone outcomes of alternative courses of action (i.e., counterfactual learning), is unknown. To test the influence of aripiprazole on counterfactual learning, we administered a reinforcement learning task that involves both direct learning from obtained outcomes and indirect learning from forgone outcomes to two groups of Gilles de la Tourette (GTS) patients, one consisting of patients who were completely unmedicated and the other consisting of patients who were receiving aripiprazole monotherapy, and to healthy subjects. We found that whereas learning performance improved in the presence of counterfactual feedback in both healthy controls and unmedicated GTS patients, this was not the case in aripiprazole-medicated GTS patients. Our results suggest that whereas aripiprazole preserves direct learning of action-outcome associations, it may impair more complex inferential processes, such as counterfactual learning from forgone outcomes, in GTS patients treated with this medication.",
      "authors": [
        "Salvador, Alexandre",
        "Worbe, Yulia",
        "Delorme, C\u00e9cile",
        "Coricelli, Giorgio",
        "Gaillard, Rapha\u00ebl",
        "Robbins, Trevor W.",
        "Hartmann, Andreas",
        "Palminteri, Stefano"
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-017-06547-8",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2017-12-01",
      "selected": null,
      "title": "Specific effect of a dopamine partial agonist on counterfactual learning: evidence from Gilles de la Tourette syndrome",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85025816494&origin=inward",
        "https://www.nature.com/articles/s41598-017-06547-8.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Baker T.E."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsych.2017.01.015",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "819-827",
      "publication": {
        "category": "Journal",
        "cite_score": 19.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00063223",
        "publisher": "Elsevier Inc.",
        "sjr": 3.768,
        "snip": 2.412,
        "subject_areas": [
          "Biological Psychiatry"
        ],
        "title": "Biological Psychiatry"
      },
      "publication_date": "2017-12-01",
      "selected": null,
      "title": "Reversing the Atypical Valuation of Drug and Nondrug Rewards in Smokers Using Multimodal Neuroimaging",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019667927&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gershman S.J."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_01170",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "2103-2113",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2017-12-01",
      "selected": null,
      "title": "Imaginative reinforcement learning: Computational principles and neural mechanisms",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85032722210&origin=inward"
      ]
    },
    {
      "abstract": "We can learn new tasks by listening to a teacher, but we can also learn by trial-and-error. Here, we investigate the factors that determine how participants learn new stimulus-response mappings by trial-and-error. Does learning in human observers comply with reinforcement learning theories, which describe how subjects learn from rewards and punishments? If yes, what is the influence of selective attention in the learning process? We developed a novel redundant-relevant learning paradigm to examine the conjoint influence of attention and reward feedback. We found that subjects only learned stimulus-response mappings for attended shapes, even when unattended shapes were equally informative. Reward magnitude also influenced learning, an effect that was stronger for attended than for non-attended shapes and that carried over to a subsequent visual search task. Our results provide insights into how attention and reward jointly determine how we learn. They support the powerful learning rules that capitalize on the conjoint influence of these two factors on neuronal plasticity.",
      "authors": [
        "Vartak, Devavrat",
        "Jeurissen, Danique",
        "Self, Matthew W.",
        "Roelfsema, Pieter R."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-017-08200-w",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2017-12-01",
      "selected": null,
      "title": "The influence of attention and reward on the learning of stimulus-response associations",
      "urls": [
        "https://www.nature.com/articles/s41598-017-08200-w.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027976283&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Carlisi C.O."
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/cercor/bhx265",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "5804-5816",
      "publication": {
        "category": "Journal",
        "cite_score": 8.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10473211",
        "publisher": "Oxford University Press",
        "sjr": 1.738,
        "snip": 1.159,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Cerebral Cortex"
      },
      "publication_date": "2017-12-01",
      "selected": null,
      "title": "Shared and disorder-specific neurocomputational mechanisms of decision-making in autism spectrum disorder and obsessive-compulsive disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85049383587&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) enables robots to learn its optimal behavioral strategy in dynamic environments based on feedback. Explicit human feedback during robot RL is advantageous, since an explicit reward function can be easily adapted. However, it is very demanding and tiresome for a human to continuously and explicitly generate feedback. Therefore, the development of implicit approaches is of high relevance. In this paper, we used an error-related potential (ErrP), an event-related activity in the human electroencephalogram (EEG), as an intrinsically generated implicit feedback (rewards) for RL. Initially we validated our approach with seven subjects in a simulated robot learning scenario. ErrPs were detected online in single trial with a balanced accuracy (bACC) of 91%, which was sufficient to learn to recognize gestures and the correct mapping between human gestures and robot actions in parallel. Finally, we validated our approach in a real robot scenario, in which seven subjects freely chose gestures and the real robot correctly learned the mapping between gestures and actions (ErrP detection (90% bACC)). In this paper, we demonstrated that intrinsically generated EEG-based human feedback in RL can successfully be used to implicitly improve gesture-based robot control during human-robot interaction. We call our approach intrinsic interactive RL.",
      "authors": [
        "Kim, Su Kyoung",
        "Kirchner, Elsa Andrea",
        "Stefes, Arne",
        "Kirchner, Frank"
      ],
      "categories": null,
      "citations": 79,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/s41598-017-17682-7",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2017-12-01",
      "selected": null,
      "title": "Intrinsic interactive reinforcement learning \u00e2\u0080\u0093 Using error-related potentials for real world human-robot interaction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85038834682&origin=inward",
        "https://www.nature.com/articles/s41598-017-17682-7.pdf"
      ]
    },
    {
      "abstract": "The literature on Inverse Reinforcement Learning (IRL) typically assumes that humans take actions in order to minimize the expected value of a cost function, i.e., that humans are risk neutral. Yet, in practice, humans are often far from being risk neutral. To fill this gap, the objective of this paper is to devise a framework for risk-sensitive IRL in order to explicitly account for a human's risk sensitivity. To this end, we propose a flexible class of models based on coherent risk measures, which allow us to capture an entire spectrum of risk preferences from risk-neutral to worst-case. We propose efficient non-parametric algorithms based on linear programming and semi-parametric algorithms based on maximum likelihood for inferring a human's underlying risk measure and cost function for a rich class of static and dynamic decision-making settings. The resulting approach is demonstrated on a simulated driving game with ten human participants. Our method is able to infer and mimic a wide range of qualitatively different driving styles from highly risk-averse to risk-neutral in a data-efficient manner. Moreover, comparisons of the Risk-Sensitive (RS) IRL approach with a risk-neutral model show that the RS-IRL framework more accurately captures observed participant behavior both qualitatively and quantitatively, especially in scenarios where catastrophic outcomes such as collisions can occur.",
      "authors": [
        "Singh, Sumeet",
        "Lacotte, Jonathan",
        "Majumdar, Anirudha",
        "Pavone, Marco"
      ],
      "categories": null,
      "citations": null,
      "comments": "Submitted to International Journal of Robotics Research; Revision 1:\n  (i) Clarified minor technical points; (ii) Revised proof for Theorem 3 to\n  hold under weaker assumptions; (iii) Added additional figures and expanded\n  discussions to improve readability",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-11-28",
      "selected": null,
      "title": "Risk-sensitive Inverse Reinforcement Learning via Semi- and Non-Parametric Methods",
      "urls": [
        "http://arxiv.org/abs/1711.10055v2",
        "http://arxiv.org/pdf/1711.10055.pdf",
        "http://arxiv.org/pdf/1711.10055v2"
      ]
    },
    {
      "abstract": "In this paper, we propose a fast adaptive learning method called supervised deep reinforcement learning to realize path following with high-dimensional input for autonomous driving task. We combine traditional feedback control method with deep reinforcement learning, the former providing a basic steering manipulation technique and the latter further improving the performance, which is similar with human's learning process. We validate our approach on computer simulating driving task. Experiments show that the fusion method steadily improves the performance based on the result of feedback control.",
      "authors": [
        "Wen-Yi Gu",
        "Xin Xu",
        "Jian Yang"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACPR.2017.30",
      "keywords": [
        "supervised actor-critic",
        "deep reinforcement learning",
        "path following"
      ],
      "number_of_pages": 5,
      "pages": "448-452",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-3355-7",
        "issn": "2327-0977",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 4th Asian Conference on Pattern Recognition, ACPR 2017"
      },
      "publication_date": "2017-11-26",
      "selected": null,
      "title": "Path Following with Supervised Deep Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8575865",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85060534244&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Pascucci D."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2017.08.067",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "56-64",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2017-11-15",
      "selected": null,
      "title": "Independent circuits in basal ganglia and cortex for the processing of reward and precision feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028707044&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Schulze C."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/xlm0000407",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "1752-1767",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02787393",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Experimental Psychology: Learning Memory and Cognition"
      },
      "publication_date": "2017-11-01",
      "selected": null,
      "title": "Hold it! the influence of lingering rewards on choice diversification and persistence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016811539&origin=inward"
      ]
    },
    {
      "abstract": "When a robot is learning it needs to explore its environment and how its environment responds on its actions. When the environment is large and there are a large number of possible actions the robot can take, this exploration phase can take prohibitively long. However, exploration can often be optimised by letting a human expert guide the robot during its learning. Interactive machine learning, in which a human user interactively guides the robot as it learns, has been shown to be an effective way to teach a robot. It requires an intuitive control mechanism to allow the human expert to provide feedback on the robots progress. This paper presents a novel method which combines Reinforcement Learning and Supervised Progressively Autonomous Robot Competencies (SPARC). By allowing the user to fully control the robot and by treating rewards as implicit, SPARC aims to learn an action policy while maintaining human supervisory oversight of the robots behaviour. This method is evaluated and compared to Interactive Reinforcement Learning in a robot teaching task. Qualitative and quantitative results indicate that SPARC allows for safer and faster learning by the robot, whilst not placing a high workload on the human teacher.",
      "authors": [
        "Emmanuel Senft",
        "Paul Baxter",
        "James Kennedy",
        "Sverin Lemaignan",
        "Tony Belpaeme"
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.patrec.2017.03.015",
      "keywords": [
        "Robotics",
        "Supervised autonomy",
        "Progressive Autonomy",
        "Reinforcement learning",
        "Human-Robot interaction",
        "Interactive machine learning"
      ],
      "number_of_pages": 10,
      "pages": "77-86",
      "publication": {
        "category": "Journal",
        "cite_score": 11.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0167-8655",
        "publisher": "Elsevier B.V.",
        "sjr": 1.302,
        "snip": 1.807,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Signal Processing",
          "Computer Vision and Pattern Recognition"
        ],
        "title": "Pattern Recognition Letters"
      },
      "publication_date": "2017-11-01",
      "selected": null,
      "title": "Supervised autonomy for online learning in human-robot interaction",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.patrec.2017.03.015",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016436619&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Collette S."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7554/eLife.29718",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "eLife"
      },
      "publication_date": "2017-10-30",
      "selected": null,
      "title": "Neural computations underlying inverse reinforcement learning in the human brain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85032946914&origin=inward"
      ]
    },
    {
      "abstract": "<p>Successful learning hinges on the evaluation of positive and negative feedback. We assessed differential learning from reward and punishment in a monetary reinforcement learning paradigm, together with cardiac concomitants of positive and negative feedback processing. On the behavioral level, learning from reward resulted in more advantageous behavior than learning from punishment, suggesting a differential impact of reward and punishment on successful feedback-based learning. On the autonomic level, learning and feedback processing were closely mirrored by phasic cardiac responses on a trial-by-trial basis: (1) Negative feedback was accompanied by faster and prolonged heart rate deceleration compared to positive feedback. (2) Cardiac responses shifted from feedback presentation at the beginning of learning to stimulus presentation later on. (3) Most importantly, the strength of phasic cardiac responses to the presentation of feedback correlated with the strength of prediction error signals that alert the learner to the necessity for behavioral adaptation. Considering participants' weight status and gender revealed obesity-related deficits in learning to avoid negative consequences and less consistent behavioral adaptation in women compared to men. In sum, our results provide strong new evidence for the notion that during learning phasic cardiac responses reflect an internal value and feedback monitoring system that is sensitive to the violation of performance-based expectations. Moreover, inter-individual differences in weight status and gender may affect both behavioral and autonomic responses in reinforcement-based learning.</p>",
      "authors": [
        "Kastner, Lucas",
        "Kube, Jana",
        "Villringer, Arno",
        "Neumann, Jane"
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2017.00598",
      "keywords": [
        "Obesity",
        "Punishment",
        "Reward",
        "Heart Rate",
        "reinforcement learning",
        "gender",
        "Prediction error"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2017-10-30",
      "selected": null,
      "title": "Cardiac Concomitants of Feedback and Prediction Error Processing in Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85033555645&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00598/pdf"
      ]
    },
    {
      "abstract": "Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect. Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the \u201cground-truth\u201d captions, while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity - two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",
      "authors": [
        "Bo Dai",
        "Sanja Fidler",
        "Raquel Urtasun",
        "Dahua Lin"
      ],
      "categories": null,
      "citations": 309,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICCV.2017.323",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "2989-2998",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-1033-6",
        "issn": "2380-7504",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the IEEE International Conference on Computer Vision"
      },
      "publication_date": "2017-10-22",
      "selected": null,
      "title": "Towards Diverse and Natural Image Descriptions via a Conditional GAN",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8237585",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85041897597&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Van Die\u00ebn J.H."
      ],
      "categories": null,
      "citations": 95,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1249/JES.0000000000000121",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "223-229",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00916331",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Exercise and Sport Sciences Reviews"
      },
      "publication_date": "2017-10-01",
      "selected": null,
      "title": "Low-Back Pain Patients Learn to Adapt Motor Behavior With Adverse Secondary Consequences",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85023775415&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "H\u00fcgelsch\u00e4fer S."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/bdm.2008",
      "keywords": [],
      "number_of_pages": 20,
      "pages": "913-932",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08943257",
        "publisher": "John Wiley and Sons Ltd",
        "sjr": 0.822,
        "snip": 1.167,
        "subject_areas": [
          "Applied Psychology",
          "Decision Sciences (all)",
          "Strategy and Management",
          "Sociology and Political Science",
          "Arts and Humanities (miscellaneous)"
        ],
        "title": "Journal of Behavioral Decision Making"
      },
      "publication_date": "2017-10-01",
      "selected": null,
      "title": "Reinforcement, Rationality, and Intentions: How Robust Is Automatic Reinforcement Learning in Economic Decision Making?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018778478&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Williams C.C."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2017.09.007",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "265-272",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2017-10-01",
      "selected": null,
      "title": "When theory and biology differ: The relationship between reward prediction errors and expectancy",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029620639&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Duverne S."
      ],
      "categories": null,
      "citations": 48,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/cercor/bhx210",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "5024-5039",
      "publication": {
        "category": "Journal",
        "cite_score": 8.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10473211",
        "publisher": "Oxford University Press",
        "sjr": 1.738,
        "snip": 1.159,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Cerebral Cortex"
      },
      "publication_date": "2017-10-01",
      "selected": null,
      "title": "Rewards and Cognitive Control in the Human Prefrontal Cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030760987&origin=inward"
      ]
    },
    {
      "abstract": "Background: Regular physical activity is known to be beneficial for people with type 2 diabetes. Nevertheless, most of the people who have diabetes lead a sedentary lifestyle. Smartphones create new possibilities for helping people to adhere to their physical activity goals through continuous monitoring and communication, coupled with personalized feedback. Objective: The aim of this study was to help type 2 diabetes patients increase the level of their physical activity. Methods: We provided 27 sedentary type 2 diabetes patients with a smartphone-based pedometer and a personal plan for physical activity. Patients were sent short message service messages to encourage physical activity between once a day and once per week. Messages were personalized through a Reinforcement Learning algorithm so as to improve each participant\u2019s compliance with the activity regimen. The algorithm was compared with a static policy for sending messages and weekly reminders. Results: Our results show that participants who received messages generated by the learning algorithm increased the amount of activity and pace of walking, whereas the control group patients did not. Patients assigned to the learning algorithm group experienced a superior reduction in blood glucose levels (glycated hemoglobin [HbA1c]) compared with control policies, and longer participation caused greater reductions in blood glucose levels. The learning algorithm improved gradually in predicting which messages would lead participants to exercise. Conclusions: Mobile phone apps coupled with a learning algorithm can improve adherence to exercise in diabetic patients. This algorithm can be used in large populations of diabetic patients to improve health and glycemic control. Our results can be expanded to other areas where computer-led health coaching of humans may have a positive impact. Summary of a part of this manuscript has been previously published as a letter in Diabetes Care, 2016. ",
      "authors": [
        "Yom-Tov E."
      ],
      "categories": null,
      "citations": 97,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.2196/jmir.7994",
      "keywords": [
        "physical activity",
        "reinforcement learning",
        "diabetes type 2"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMIR Publications Inc., Toronto, Canada",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Medical Internet Research"
      },
      "publication_date": "2017-10-01",
      "selected": null,
      "title": "Encouraging Physical Activity in Patients With Diabetes: Intervention Using a Reinforcement Learning System",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042765009&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lau B."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.conb.2017.08.015",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "241-247",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09594388",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Current Opinion in Neurobiology"
      },
      "publication_date": "2017-10-01",
      "selected": null,
      "title": "The many worlds hypothesis of dopamine prediction error: implications of a parallel circuit architecture in the basal ganglia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030319290&origin=inward"
      ]
    },
    {
      "abstract": "While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose Deep TAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.",
      "authors": [
        "Warnell, Garrett",
        "Waytowich, Nicholas",
        "Lawhern, Vernon",
        "Stone, Peter"
      ],
      "categories": null,
      "citations": null,
      "comments": "9 pages, 6 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-09-28",
      "selected": null,
      "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces",
      "urls": [
        "http://arxiv.org/abs/1709.10163v2",
        "http://arxiv.org/pdf/1709.10163.pdf",
        "http://arxiv.org/pdf/1709.10163v2"
      ]
    },
    {
      "abstract": "The following topics are dealt with: MAV; Android robot; augmented reality for kilobots; ARK; quadrotor; path planning; bipedal robots; adaptive depth control; micro diving agent; shape control; modular active-cell robots; mobile mixed-reality interaction; multi-robot systems; prestressed soft gripper; target-tracking game; time-delayed control; uncertain Euler-Lagrange systems; 7! Robots; recyclable robots; robot-integrated microfluidic chip; 3d-printed tactile gripper; humanoid robot; reinforcement learning; bioinspired continuum manipulator; musculoskeletal humanoids; feature-based matching; multimodal robotic skin; serial-link robots; feedback control; legged robots; ZMP constraints; state estimators; switching control; torque-controlled series-elastic actuators; RGB-D SLAM; reachability maps; co-safe temporal logic specifications; end effector; acceleration control; human-robot collaborative minimally invasive surgery; online learning; hexapod Giacometti robot; position control; stiffness control; gimbal systems; distal proprioceptive sensor; manufacturing; virtual reality; momentum control; model predictive control; mobile robot localization; magnetic hammer actuation; millirobot; and exoskeleton.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/IROS.2017.8202129",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "1-17",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-2683-2",
        "issn": "2153-0866",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
      },
      "publication_date": "2017-09-24",
      "selected": null,
      "title": "Index of papers presented at IROS 2017 and published in the IEEE Robotics and Automation Letters",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8202129"
      ]
    },
    {
      "abstract": "This paper presents a hybrid system of applying homeostatic drives theory and stimuli from environment or human on a robot, aiming to solve the increasing need of elder caring in modern aging society. Through reinforcement learning with human feedback, the robot is enabled to make appropriate decisions by reviewing its own motivation, environment stimuli and human inputs, allowing the robot to finish vague tasks, i.e. to serve, help or have a conversation with human on different situations. In our experiment, the human satisfaction rate could reach 94% and the robot could as well maintain a stable condition to perform long-term service. If the robot is to be deployed in a nursing home, it can further serve and meet the requirements of elders, while simultaneously maintaining its own system, such as battery level, in regular daily lives.",
      "authors": [
        "Chiao-Yu Yang",
        "Ming-Jen Lu",
        "Shih-Huan Tseng",
        "Li-Chen Fu"
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.23919/SICE.2017.8105748",
      "keywords": [
        "Elder care",
        "Pepper",
        "autonomy",
        "robot service",
        "homeostasis",
        "dementia"
      ],
      "number_of_pages": 6,
      "pages": "1401-1406",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-0718-3",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan, SICE 2017"
      },
      "publication_date": "2017-09-19",
      "selected": null,
      "title": "A companion robot for daily care of elders based on homeostasis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85044225938&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8105748"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Collins A.G.E."
      ],
      "categories": null,
      "citations": 69,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsych.2017.05.017",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "431-439",
      "publication": {
        "category": "Journal",
        "cite_score": 19.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00063223",
        "publisher": "Elsevier Inc.",
        "sjr": 3.768,
        "snip": 2.412,
        "subject_areas": [
          "Biological Psychiatry"
        ],
        "title": "Biological Psychiatry"
      },
      "publication_date": "2017-09-15",
      "selected": null,
      "title": "Interactions Among Working Memory, Reinforcement Learning, and Effort in Value-Based Choice: A New Paradigm and Selective Deficits in Schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021226480&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement Learning AI commonly uses reward/penalty signals that are objective and explicit in an environment -- e.g. game score, completion time, etc. -- in order to learn the optimal strategy for task performance. However, Human-AI interaction for such AI agents should include additional reinforcement that is implicit and subjective -- e.g. human preferences for certain AI behavior -- in order to adapt the AI behavior to idiosyncratic human preferences. Such adaptations would mirror naturally occurring processes that increase trust and comfort during social interactions. Here, we show how a hybrid brain-computer-interface (hBCI), which detects an individual's level of interest in objects/events in a virtual environment, can be used to adapt the behavior of a Deep Reinforcement Learning AI agent that is controlling a virtual autonomous vehicle. Specifically, we show that the AI learns a driving strategy that maintains a safe distance from a lead vehicle, and most novelly, preferentially slows the vehicle when the human passengers of the vehicle encounter objects of interest. This adaptation affords an additional 20\\% viewing time for subjectively interesting objects. This is the first demonstration of how an hBCI can be used to provide implicit reinforcement to an AI agent in a way that incorporates user preferences into the control system.",
      "authors": [
        "Shih, Victor",
        "Jangraw, David C",
        "Sajda, Paul",
        "Saproo, Sameer"
      ],
      "categories": null,
      "citations": null,
      "comments": "11 pages, 9 figures, 1 table, Submitted to IEEE Trans. on Neural\n  Networks and Learning Systems",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-09-14",
      "selected": null,
      "title": "Towards personalized human AI interaction - adapting the behavior of AI agents using neural signatures of subjective interest",
      "urls": [
        "http://arxiv.org/abs/1709.04574v1",
        "http://arxiv.org/pdf/1709.04574v1",
        "http://arxiv.org/pdf/1709.04574.pdf"
      ]
    },
    {
      "abstract": "We describe a method to use discrete human feedback to enhance the performance of deep learning agents in virtual three-dimensional environments by extending deep-reinforcement learning to model the confidence and consistency of human feedback. This enables deep reinforcement learning algorithms to determine the most appropriate time to listen to the human feedback, exploit the current policy model, or explore the agent's environment. Managing the trade-off between these three strategies allows DRL agents to be robust to inconsistent or intermittent human feedback. Through experimentation using a synthetic oracle, we show that our technique improves the training speed and overall performance of deep reinforcement learning in navigating three-dimensional environments using Minecraft. We further show that our technique is robust to highly innacurate human feedback and can also operate when no human feedback is given.",
      "authors": [
        "Lin, Zhiyu",
        "Harrison, Brent",
        "Keech, Aaron",
        "Riedl, Mark O."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-09-12",
      "selected": null,
      "title": "Explore, Exploit or Listen: Combining Human Feedback and Policy Model to Speed up Deep Reinforcement Learning in 3D Worlds",
      "urls": [
        "http://arxiv.org/pdf/1709.03969.pdf",
        "http://arxiv.org/abs/1709.03969v2",
        "http://arxiv.org/pdf/1709.03969v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "DePasque S."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nlm.2017.04.009",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "1-7",
      "publication": {
        "category": "Journal",
        "cite_score": 5.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10747427",
        "publisher": "Academic Press Inc.",
        "sjr": 0.986,
        "snip": 0.765,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neurobiology of Learning and Memory"
      },
      "publication_date": "2017-09-01",
      "selected": null,
      "title": "Frontostriatal development and probabilistic reinforcement learning during adolescence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019628162&origin=inward"
      ]
    },
    {
      "abstract": "In the physical world, people have dynamic preferences, e.g., the same\nsituation can lead to satisfaction for some humans and to frustration for\nothers. Personalization is called for. The same observation holds for online\nbehavior with interactive systems. It is natural to represent the behavior of\nusers who are engaging with interactive systems such as a search engine or a\nrecommender system, as a sequence of actions where each next action depends on\nthe current situation and the user reward of taking a particular action. By and\nlarge, current online evaluation metrics for interactive systems such as search\nengines or recommender systems, are static and do not reflect differences in\nuser behavior. They rarely capture or model the reward experienced by a user\nwhile interacting with an interactive system. We argue that knowing a user's\nreward function is essential for an interactive system as both for learning and\nevaluation. We propose to learn users' reward functions directly from observed\ninteraction traces. In particular, we present how users' reward functions can\nbe uncovered directly using inverse reinforcement learning techniques. We also\nshow how to incorporate user features into the learning process. Our main\ncontribution is a novel and dynamic approach to restore a user's reward\nfunction. We present an analytic approach to this problem and complement it\nwith initial experiments using the interaction logs of a cultural heritage\ninstitution that demonstrate the feasibility of the approach by uncovering\ndifferent reward functions for different user groups.",
      "authors": [
        "Ziming Li",
        "Julia Kiseleva",
        "Maarten de Rijke",
        "Artem Grotov"
      ],
      "categories": null,
      "citations": null,
      "comments": "5 pages",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1145/3121050.3121098",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-08-15",
      "selected": null,
      "title": "Towards Learning Reward Functions from User Interactions",
      "urls": [
        "http://arxiv.org/abs/1708.04378v1",
        "http://arxiv.org/pdf/1708.04378v1",
        "http://dx.doi.org/10.1145/3121050.3121098"
      ]
    },
    {
      "abstract": "<p>Theories of reinforcement learning and approach behavior suggest that reward can increase the perceptual salience of environmental stimuli, ensuring that potential predictors of outcome are noticed in the future. However, outcome commonly follows visual processing of the environment, occurring even when potential reward cues have long disappeared. How can reward feedback retroactively cause now-absent stimuli to become attention-drawing in the future? One possibility is that reward and attention interact to prime lingering visual representations of attended stimuli that sustain through the interval separating stimulus and outcome. Here, we test this idea using multivariate pattern analysis of fMRI data collected from male and female humans. While in the scanner, participants searched for examples of target categories in briefly presented pictures of cityscapes and landscapes. Correct task performance was followed by reward feedback that could randomly have either high or low magnitude. Analysis showed that high-magnitude reward feedback boosted the lingering representation of target categories while reducing the representation of nontarget categories. The magnitude of this effect in each participant predicted the behavioral impact of reward on search performance in subsequent trials. Other analyses show that sensitivity to reward\u2014as expressed in a personality questionnaire and in reactivity to reward feedback in the dopaminergic midbrain\u2014predicted reward-elicited variance in lingering target and nontarget representations. Credit for rewarding outcome thus appears to be assigned to the target representation, causing the visual system to become sensitized for similar objects in the future.</p><p><b>SIGNIFICANCE STATEMENT</b> How do reward-predictive visual stimuli become salient and attention-drawing? In the real world, reward cues precede outcome and reward is commonly received long after potential predictors have disappeared. How can the representation of environmental stimuli be affected by outcome that occurs later in time? Here, we show that reward acts on lingering representations of environmental stimuli that sustain through the interval between stimulus and outcome. Using naturalistic scene stimuli and multivariate pattern analysis of fMRI data, we show that reward boosts the representation of attended objects and reduces the representation of unattended objects. This interaction of attention and reward processing acts to prime vision for stimuli that may serve to predict outcome.</p>",
      "authors": [
        "Clayton Hickey",
        "Marius V. Peelen"
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.0684-17.2017",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "7297-7304",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2017-08-02",
      "selected": null,
      "title": "Reward Selectively Modulates the Lingering Neural Representation of Recently Attended Objects in Natural Scenes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85026853425&origin=inward"
      ]
    },
    {
      "abstract": "A broadly used computational framework posits that two learning systems operate in parallel during the learning of choice preferences\u2014namely, the model-free and model-based reinforcement-learning systems. In this study, we examined another possibility, through which model-free learning is the basic system and model-based information is its modulator. Accordingly, we proposed several modified versions of a temporal-difference learning model to explain the choice-learning process. Using the two-stage decision task developed by Daw, Gershman, Seymour, Dayan, and Dolan (2011), we compared their original computational model, which assumes a parallel learning process, and our proposed models, which assume a sequential learning process. Choice data from 23 participants showed a better fit with the proposed models. More specifically, the proposed eligibility adjustment model, which assumes that the environmental model can weight the degree of the eligibility trace, can explain choices better under both model-free and model-based controls and has a simpler computational algorithm than the original model. In addition, the forgetting learning model and its variation, which assume changes in the values of unchosen actions, substantially improved the fits to the data. Overall, we show that a hybrid computational model best fits the data. The parameters used in this model succeed in capturing individual tendencies with respect to both model use in learning and exploration behavior. This computational model provides novel insights into learning with interacting model-free and model-based components.",
      "authors": [
        "Toyama, Asako",
        "Katahira, Kentaro",
        "Ohira, Hideki"
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-017-0511-2",
      "keywords": [],
      "number_of_pages": 20,
      "pages": "764-783",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2017-08-01",
      "selected": null,
      "title": "A simple computational algorithm of model-based choice preference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85020128334&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-017-0511-2.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu H."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12870",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "1163-1179",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2017-08-01",
      "selected": null,
      "title": "Misfortune may be a blessing in disguise: Fairness perception and emotion modulate decision making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018561574&origin=inward"
      ]
    },
    {
      "abstract": "Previous studies suggest that factual learning, that is, learning from obtained outcomes, is biased, such that participants preferentially take into account positive, as compared to negative, prediction errors. However, whether or not the prediction error valence also affects counterfactual learning, that is, learning from forgone outcomes, is unknown. To address this question, we analysed the performance of two groups of participants on reinforcement learning tasks using a computational model that was adapted to test if prediction error valence influences learning. We carried out two experiments: in the factual learning experiment, participants learned from partial feedback (i.e., the outcome of the chosen option only); in the counterfactual learning experiment, participants learned from complete feedback information (i.e., the outcomes of both the chosen and unchosen option were displayed). In the factual learning experiment, we replicated previous findings of a valence-induced bias, whereby participants learned preferentially from positive, relative to negative, prediction errors. In contrast, for counterfactual learning, we found the opposite valence-induced bias: negative prediction errors were preferentially taken into account, relative to positive ones. When considering valence-induced bias in the context of both factual and counterfactual learning, it appears that people tend to preferentially take into account information that confirms their current choice.",
      "authors": [
        "Stefano Palminteri",
        "Germain Lefebvre",
        "Emma J. Kilford",
        "Sarah-Jayne Blakemore"
      ],
      "categories": null,
      "citations": 85,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1005684",
      "keywords": [
        "Learning",
        "Human learning",
        "Decision making",
        "Simulation and modeling",
        "Analysis of variance",
        "Economic history",
        "Learning curves",
        "Optimization"
      ],
      "number_of_pages": null,
      "pages": "e1005684",
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553-7358",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS computational biology"
      },
      "publication_date": "2017-08-01",
      "selected": null,
      "title": "Confirmation bias in human reinforcement learning: Evidence from counterfactual feedback processing",
      "urls": [
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005684&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85030614150&origin=inward"
      ]
    },
    {
      "abstract": "Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.",
      "authors": [
        "Nguyen, Khanh",
        "Daum\u00e9 III, Hal",
        "Boyd-Graber, Jordan"
      ],
      "categories": null,
      "citations": null,
      "comments": "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural\n  Language Processing (EMNLP) 2017",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-07-24",
      "selected": null,
      "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback",
      "urls": [
        "http://arxiv.org/pdf/1707.07402.pdf",
        "http://arxiv.org/abs/1707.07402v4",
        "http://arxiv.org/pdf/1707.07402v4"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Grogan J.P."
      ],
      "categories": null,
      "citations": 40,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7554/eLife.26801",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "eLife"
      },
      "publication_date": "2017-07-10",
      "selected": null,
      "title": "Effects of dopamine on reinforcement learning and consolidation in parkinson\u2019s disease",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027060052&origin=inward"
      ]
    },
    {
      "abstract": "Reaching movements are comprised of the coordinated action across multiple joints. The human skeleton is redundant for this task because different joint configurations can lead to the same endpoint in space. How do people learn to use combinations of joints that maximize success in goal-directed motor tasks? To answer this question, we used a 3-degree-of-freedom manipulandum to measure shoulder, elbow and wrist joint movements during reaching in a plane. We tested whether a shift in the relative contribution of the wrist and elbow joints to a reaching movement could be learned by an implicit reinforcement regime. Unknown to the participants, we decreased the task success for certain joint configurations (wrist flexion or extension, respectively) by adding random variability to the endpoint feedback. In return, the opposite wrist postures were rewarded in the two experimental groups (flexion and extension group). We found that the joint configuration slowly shifted towards movements that provided more control over the endpoint and hence higher task success. While the overall learning was significant, only the group that was guided to extend the wrist joint more during the movement showed substantial learning. Importantly, all changes in movement pattern occurred independent of conscious awareness of the experimental manipulation. These findings suggest that the motor system is generally sensitive to its output variability and can optimize joint-space solutions that minimize task-relevant output variability. We discuss biomechanical biases (e.g. joint\u2019s range of movement) that could impose hurdles to the learning process.",
      "authors": [
        "David Marc Anton Mehler",
        "Alexandra Reichenbach",
        "Julius Klein",
        "J\u00f6rn Diedrichsen"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0180803",
      "keywords": [
        "Robotics",
        "Motor system",
        "Wrist",
        "Learning",
        "Shoulders",
        "Skeletal joints",
        "Elbow",
        "Biomechanics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2017-07-01",
      "selected": null,
      "title": "Minimizing endpoint variability through reinforcement learning during reaching movements involving shoulder, elbow and wrist",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85024478857&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0180803&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ren X."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12859",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "969-981",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2017-07-01",
      "selected": null,
      "title": "Changes in the stimulus-preceding negativity and lateralized readiness potential during reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85017461521&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Umemoto A."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.clinph.2017.03.049",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "1184-1196",
      "publication": {
        "category": "Journal",
        "cite_score": 7.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13882457",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 1.212,
        "snip": 1.482,
        "subject_areas": [
          "Neurology (clinical)",
          "Physiology (medical)",
          "Sensory Systems",
          "Neurology"
        ],
        "title": "Clinical Neurophysiology"
      },
      "publication_date": "2017-07-01",
      "selected": null,
      "title": "Neural mechanisms of reward processing associated with depression-related personality traits",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018936483&origin=inward"
      ]
    },
    {
      "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",
      "authors": [
        "Christiano, Paul",
        "Leike, Jan",
        "Brown, Tom B.",
        "Martic, Miljan",
        "Legg, Shane",
        "Amodei, Dario"
      ],
      "categories": null,
      "citations": 474,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "4300-4308",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2017-06-12",
      "selected": null,
      "title": "Deep reinforcement learning from human preferences",
      "urls": [
        "http://arxiv.org/pdf/1706.03741.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046999643&origin=inward",
        "http://arxiv.org/pdf/1706.03741v4",
        "http://arxiv.org/abs/1706.03741v4"
      ]
    },
    {
      "abstract": "Rationale: Advances in neurocomputational modeling suggest that valuation systems for goal-directed (deliberative) on one side, and habitual (automatic) decision-making on the other side may rely on distinct computational strategies for reinforcement learning, namely model-free versus model-based learning. As a key theoretical difference, the model-based system strongly demands cognitive functions to plan actions prospectively based on an internal cognitive model of the environment, whereas valuation in the model-free system relies on rather simple learning rules from operant conditioning to retrospectively associate actions with their outcomes and is thus cognitively less demanding. Acute stress reactivity is known to impair model-based but not model-free choice behavior, with higher working memory capacity protecting the model-based system from acute stress. However, it is not clear which impact accumulated real life stress has on model-free and model-based decision systems and how this influence interacts with cognitive abilities. Methods: We used a sequential decision-making task distinguishing relative contributions of both learning strategies to choice behavior, the Social Readjustment Rating Scale questionnaire to assess accumulated real life stress, and the Digit Symbol Substitution Test to test cognitive speed in 95 healthy subjects. Results: Individuals reporting high stress exposure who had low cognitive speed showed reduced model-based but increased model-free behavioral control. In contrast, subjects exposed to accumulated real life stress with high cognitive speed displayed increased model-based performance but reduced model-free control. Conclusions: These findings suggest that accumulated real life stress exposure can enhance reliance on cognitive speed for model-based computations, which may ultimately protect the model-based system from the detrimental influences of accumulated real life stress. The combination of accumulated real life stress exposure and slower information processing capacities, however, might favor model-free strategies. Thus, the valence and preference of either system strongly depends on stressful experiences and individual cognitive capacities.",
      "authors": [
        "Friedel, Eva",
        "Sebold, Miriam",
        "Kuitunen-Paul, S\u00f6ren",
        "Nebe, Stephan",
        "Veer, Ilya M.",
        "Zimmermann, Ulrich S.",
        "Schlagenhauf, Florian",
        "Smolka, Michael N.",
        "Rapp, Michael",
        "Walter, Henrik",
        "Heinz, Andreas"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2017.00302",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2017-06-08",
      "selected": null,
      "title": "How Accumulated Real Life Stress Experience and Cognitive Speed Interact on Decision-Making Processes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021680836&origin=inward"
      ]
    },
    {
      "abstract": "Most experimental studies of depressive symptom effects on decision-making have examined situations in which a single correct answer exists based on external circumstances (externally guided decision-making, e.g., gambling task). In addition to such decision-making, for decision-making of other types, no correct answer exists based on external circumstances (internally guided decision-making, e.g., preference judgment). For internally guided decision-making, a phenomenon is known by which preference for the chosen item increases and preference for the rejected item is decreased after choosing between two equally preferred items which is designated as choice-induced preference change. Recent reports suggest that this phenomenon is explainable by reinforcement learning theory just as it is with externally guided decision-making. Although many earlier studies have revealed the effects of depression in externally guided decision-making, the relation between depressive symptoms and choice-induced preference change remains unclear. This study investigated the relation between depressive symptoms and choice-induced preference change using the blind choice paradigm. Results show that depressive symptoms are correlated with change in preference of rejected items (Spearman\u2019s r = .28, p = .04): depressed individuals tend to show less decreased preference of rejected items. These results indicate that individual differences of depressive symptoms affect choice-induced preference change. We discuss the mechanisms underlying the relation between depression and choice-induced preference change.",
      "authors": [
        "Madoka Miyagi",
        "Makoto Miyatani",
        "Takashi Nakao"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0180041",
      "keywords": [
        "Electroencephalography",
        "Learning",
        "Undergraduates",
        "Human learning",
        "Decision making",
        "Decision theory",
        "Deception",
        "Depression"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2017-06-01",
      "selected": null,
      "title": "Relation between choice-induced preference change and depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021675556&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0180041&type=printable"
      ]
    },
    {
      "abstract": "A human\u2019s, or lower insects\u2019, behavior is dominated by its nervous system. Each stable behavior has its own inner steps and control rules, and is regulated by a neural circuit. Understanding how the brain influences perception, thought, and behavior is a central mandate of neuroscience. The phototactic flight of insects is a widely observed deterministic behavior. Since its movement is not stochastic, the behavior should be dominated by a neural circuit. Based on the basic firing characteristics of biological neurons and the neural circuit\u2019s constitution, we designed a plausible neural circuit for this phototactic behavior from logic perspective. The circuit\u2019s output layer, which generates a stable spike firing rate to encode flight commands, controls the insect\u2019s angular velocity when flying. The firing pattern and connection type of excitatory and inhibitory neurons are considered in this computational model. We simulated the circuit\u2019s information processing using a distributed PC array, and used the real-time average firing rate of output neuron clusters to drive a flying behavior simulation. In this paper, we also explored how a correct neural decision circuit is generated from network flow view through a bee\u2019s behavior experiment based on the reward and punishment feedback mechanism. The significance of this study: firstly, we designed a neural circuit to achieve the behavioral logic rules by strictly following the electrophysiological characteristics of biological neurons and anatomical facts. Secondly, our circuit\u2019s generality permits the design and implementation of behavioral logic rules based on the most general information processing and activity mode of biological neurons. Thirdly, through computer simulation, we achieved new understanding about the cooperative condition upon which multi-neurons achieve some behavioral control. Fourthly, this study aims in understanding the information encoding mechanism and how neural circuits achieve behavior control. Finally, this study also helps establish a transitional bridge between the microscopic activity of the nervous system and macroscopic animal behavior.",
      "authors": [
        "Wei, Hui",
        "Dai, Dawei",
        "Bu, Yijie"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s11571-017-9426-4",
      "keywords": [],
      "number_of_pages": 23,
      "pages": "259-281",
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18714080",
        "publisher": "Springer Netherlands",
        "sjr": 0.675,
        "snip": 1.279,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive Neurodynamics"
      },
      "publication_date": "2017-06-01",
      "selected": null,
      "title": "A plausible neural circuit for decision making and its formation based on reinforcement learning",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s11571-017-9426-4.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85013073162&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Khodadadi A."
      ],
      "categories": null,
      "citations": 9,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cogpsych.2017.03.002",
      "keywords": [],
      "number_of_pages": 33,
      "pages": "17-49",
      "publication": {
        "category": "Journal",
        "cite_score": 6.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00100285",
        "publisher": "Academic Press Inc.",
        "sjr": 1.56,
        "snip": 1.796,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Artificial Intelligence",
          "Linguistics and Language",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Cognitive Psychology"
      },
      "publication_date": "2017-06-01",
      "selected": null,
      "title": "Learning to allocate limited time to decisions with different expected outcomes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85017639637&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Liu Q."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.11897/SP.J.1016.2017.01353",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "1353-1366",
      "publication": {
        "category": "Journal",
        "cite_score": 2.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02544164",
        "publisher": "Science Press",
        "sjr": 0.29,
        "snip": 0.741,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Hardware and Architecture",
          "Computer Graphics and Computer-Aided Design"
        ],
        "title": "Jisuanji Xuebao/Chinese Journal of Computers"
      },
      "publication_date": "2017-06-01",
      "selected": null,
      "title": "A Deep Recurrent Q-Network Based on Visual Attention Mechanism",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85029552019&origin=inward"
      ]
    },
    {
      "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. We propose an algorithm to automatically learn learning rates using neural network based actor-critic methods from deep reinforcement learning (RL).In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. The introduction of auxiliary actor and critic networks helps the main network achieve better performance. Experiments on different datasets and network architectures show that our approach leads to better convergence of SGD than human-designed competitors.",
      "authors": [
        "Xu, Chang",
        "Qin, Tao",
        "Wang, Gang",
        "Liu, Tie-Yan"
      ],
      "categories": null,
      "citations": null,
      "comments": "7 pages, 9 figures",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-05-31",
      "selected": null,
      "title": "Reinforcement Learning for Learning Rate Control",
      "urls": [
        "http://arxiv.org/abs/1705.11159v1",
        "http://arxiv.org/pdf/1705.11159.pdf",
        "http://arxiv.org/pdf/1705.11159v1"
      ]
    },
    {
      "abstract": "A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation &#x2014; pushing objects &#x2014; and can handle novel objects not seen during training.",
      "authors": [
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "categories": null,
      "citations": 414,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA.2017.7989324",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "2786-2793",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-4634-8",
        "issn": "10504729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2017-05-29",
      "selected": null,
      "title": "Deep visual foresight for planning robot motion",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989324",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027963592&origin=inward",
        "https://dl.acm.org/doi/10.1109/ICRA.2017.7989324"
      ]
    },
    {
      "abstract": "Robot Reinforcement Learning (RL) algorithms return a policy that maximizes a global cumulative reward signal but typically do not create diverse behaviors. Hence, the policy will typically only capture a single solution of a task. However, many motor tasks have a large variety of solutions and the knowledge about these solutions can have several advantages. For example, in an adversarial setting such as robot table tennis, the lack of diversity renders the behavior predictable and hence easy to counter for the opponent. In an interactive setting such as learning from human feedback, an emphasis on diversity gives the human more opportunity for guiding the robot and to avoid the latter to be stuck in local optima of the task. In order to increase diversity of the learned behaviors, we leverage prior work on intrinsic motivation and empowerment. We derive a new intrinsic motivation signal by enriching the description of a task with an outcome space, representing interesting aspects of a sensorimotor stream. For example, in table tennis, the outcome space could be given by the return position and return ball speed. The intrinsic motivation is now given by the diversity of future outcomes, a concept also known as empowerment. We derive a new policy search algorithm that maximizes a trade-off between the extrinsic reward and this intrinsic motivation criterion. Experiments on a planar reaching task and simulated robot table tennis demonstrate that our algorithm can learn a diverse set of behaviors within the area of interest of the tasks.",
      "authors": [
        "Alexander Gabriel",
        "Riad Akrour",
        "Jan Peters",
        "Gerhard Neumann"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ICRA.2017.7989760",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "6435-6441",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-4634-8",
        "issn": "10504729",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.186,
        "snip": 1.404,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "Proceedings - IEEE International Conference on Robotics and Automation"
      },
      "publication_date": "2017-05-29",
      "selected": null,
      "title": "Empowered skills",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989760",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028013662&origin=inward",
        "https://dl.acm.org/doi/10.1109/ICRA.2017.7989760"
      ]
    },
    {
      "abstract": "Humans learn multisensory eye-hand coordination starting from infancy without supervision. For an example, they learn to track their hands by exploiting various sensory modalities, such as vision and proprioception. This integration occurs as they learn to perceive the world around them and their relationship to it. Most prior work has focused on the role of vision, as it is a primary sensory source for humans. However, it is interesting to study how vision and proprioception interact. We propose a system which combines visual and proprioceptive information to learn the eye-hand coordination skills that enable a robot to fixate its camera gaze on the end effector of its arm. In our model, visual cues are part of the feedback control loop, whereas proprioceptive cues are part of a feedforward control loop. Both controllers, as well as the sensory transform from raw visual information to a neural sensory representation are learned as the robot performs motor babbling movements. Visual information is encoded by sparse coding. The basis functions that emerge are similar to the receptive fields in the human visual cortex. An actor-critic reinforcement learning algorithm is used to drive eye motor neurons fusing visual and proprioceptive cues. We model and test the system in the iCub simulation environment. Our results suggest that these sensory modalities are capable of jointly learning model parameters to perform the tracking task. The evolved policy has characteristics that are qualitatively similar to the human oculomotor plant.",
      "authors": [
        "Lakshitha P. Wijesinghe",
        "Marco Antonelli",
        "Jochen Triesch",
        "Bertram E. Shi"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IJCNN.2017.7966380",
      "keywords": [
        "Developmental Robotics",
        "Artificial Neural Networks",
        "Sparse Coding",
        "Hand-eye Coordination",
        "Reinforcement Learning"
      ],
      "number_of_pages": 8,
      "pages": "4150-4157",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-6183-9",
        "issn": "2161-4407",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Neural Networks"
      },
      "publication_date": "2017-05-14",
      "selected": null,
      "title": "Learning multisensory neural controllers for robot arm tracking",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85031025997&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7966380"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chen W.J."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.1618161114",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "4637-4642",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2017-05-02",
      "selected": null,
      "title": "Computational modeling of epiphany learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018732498&origin=inward"
      ]
    },
    {
      "abstract": "We consider settings where owners of electric vehicles (EVs) participate in a market mechanism to charge their vehicles. Existing work on such mechanisms has typically assumed that participants are fully rational and can report their preferences accurately via some interface to the mechanism or to a software agent participating on their behalf. However, this may not be reasonable in settings with non-expert human end-users.Thus, our overarching aim in this paper is to determine experimentally if a fully expressive market interface that enables accurate preference reports is suitable for the EV charging domain, or, alternatively, if a simpler, restricted interface that reduces the space of possible options is preferable. In doing this, we measure the performance of an interface both in terms of how it helps participants maximise their utility and how it affects deliberation time. Our secondary objective is to contrast two different types of restricted interfaces that vary in how they restrict the space of preferences that can be reported. To enable this analysis, we develop a novel game that replicates key features of an abstract EV charging scenario. In two experiments with over 300 users, we show that restricting the users' preferences significantly reduces the time they spend deliberating (by up to half in some cases). An extensive usability survey confirms that this restriction is furthermore associated with a lower perceived cognitive burden on the users. More surprisingly, at the same time, using restricted interfaces leads to an increase in the users' performance compared to the fully expressive interface (by up to 70%). We also show that some restricted interfaces have the desirable effect of reducing the energy consumption of their users by up to 20% while achieving the same utility as other interfaces. Finally, we find that a reinforcement learning agent displays similar performance trends to human users, enabling a novel methodology for evaluating market interfaces.",
      "authors": [
        "Sebastian Stein",
        "Enrico H. Gerding",
        "Adrian Nedea",
        "Avi Rosenfeld",
        "Nicholas R. Jennings"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1613/jair.5387",
      "keywords": [],
      "number_of_pages": 53,
      "pages": "175-227",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1076-9757",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Artificial Intelligence Research"
      },
      "publication_date": "2017-05-01",
      "selected": null,
      "title": "Market Interfaces for Electric Vehicle Charging",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85031903788&origin=inward",
        "https://jair.org/index.php/jair/article/download/11065/26244"
      ]
    },
    {
      "abstract": "Reinforcement learning tasks are often used to assess participants\u2019 tendency to learn more from the positive or more from the negative consequences of one\u2019s action. However, this assessment often requires comparison in learning performance across different task conditions, which may differ in the relative salience or discriminability of the stimuli associated with more and less rewarding outcomes, respectively. To address this issue, in a first set of studies, participants were subjected to two versions of a common probabilistic learning task. The two versions differed with respect to the stimulus (Hiragana) characters associated with reward probability. The assignment of character to reward probability was fixed within version but reversed between versions. We found that performance was highly influenced by task version, which could be explained by the relative perceptual discriminability of characters assigned to high or low reward probabilities, as assessed by a separate discrimination experiment. Participants were more reliable in selecting rewarding characters that were more discriminable, leading to differences in learning curves and their sensitivity to reward probability. This difference in experienced reinforcement history was accompanied by performance biases in a test phase assessing ability to learn from positive vs. negative outcomes. In a subsequent large-scale web-based experiment, this impact of task version on learning and test measures was replicated and extended. Collectively, these findings imply a key role for perceptual factors in guiding reward learning and underscore the need to control stimulus discriminability when making inferences about individual differences in reinforcement learning.",
      "authors": [
        "Iris Schutte",
        "Heleen A. Slagter",
        "Anne G. E. Collins",
        "Michael J. Frank",
        "J. Leon Kenemans"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0176205",
      "keywords": [
        "Phase determination",
        "Dopamine",
        "Learning",
        "Decision making",
        "Motivation",
        "Reaction time",
        "Learning curves",
        "Sensory perception"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2017-05-01",
      "selected": null,
      "title": "Stimulus discriminability may bias value-based probabilistic learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019015694&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0176205&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Gu R."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/brb3.672",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Brain and Behavior"
      },
      "publication_date": "2017-05-01",
      "selected": null,
      "title": "Valence and magnitude ambiguity in feedback processing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85017343424&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mathar D."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cortex.2016.09.004",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "149-162",
      "publication": {
        "category": "Journal",
        "cite_score": 7.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00109452",
        "publisher": "Masson SpA",
        "sjr": 1.303,
        "snip": 1.241,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Cortex"
      },
      "publication_date": "2017-05-01",
      "selected": null,
      "title": "The role of dopamine in positive and negative prediction error utilization during incidental learning \u2013 Insights from Positron Emission Tomography, Parkinson's disease and Huntington's disease",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85001837236&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "van Noordt S."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.rasd.2017.01.011",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "1-10",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17509467",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Research in Autism Spectrum Disorders"
      },
      "publication_date": "2017-05-01",
      "selected": null,
      "title": "Inter-trial coherence of medial frontal theta oscillations linked to differential feedback processing in youth and young adults with autism",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85012993495&origin=inward"
      ]
    },
    {
      "abstract": "We consider settings where owners of electric vehicles (EVs) participate in a market mechanism to charge their vehicles. Existing work on such mechanisms has typically assumed that participants are fully rational and can report their preferences accurately via some interface to the mechanism or to a software agent participating on their behalf. However, this may not be reasonable in settings with non-expert human end-users. Thus, our overarching aim in this paper is to determine experimentally if a fully expressive market interface that enables accurate preference reports is suitable for the EV charging domain, or, alternatively, if a simpler, restricted interface that reduces the space of possible options is preferable. In doing this, we measure the performance of an interface both in terms of how it helps participants maximise their utility and how it affects deliberation time. Our secondary objective is to contrast two different types of restricted interfaces that vary in how they restrict the space of preferences that can be reported. To enable this analysis, we develop a novel game that replicates key features of an abstract EV charging scenario. In two experiments with over 300 users, we show that restricting the users' preferences significantly reduces the time they spend deliberating (by up to half in some cases). An extensive usability survey confirms that this restriction is furthermore associated with a lower perceived cognitive burden on the users. More surprisingly, at the same time, using restricted interfaces leads to an increase in the users' performance compared to the fully expressive interface (by up to 70%). We also show that some restricted interfaces have the desirable effect of reducing the energy consumption of their users by up to 20% while achieving the same utility as other interfaces. Finally, we find that a reinforcement learning agent displays similar performance trends to human users, enabling a novel methodology for evaluating market interfaces.",
      "authors": [
        "Sebastian Stein",
        "Enrico H. Gerding",
        "Adrian Nedea",
        "Avi Rosenfeld",
        "Nicholas R. Jennings"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3176788.3176793",
      "keywords": [],
      "number_of_pages": 53,
      "pages": "175-227",
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1076-9757",
        "publisher": "AI Access Foundation",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "J. Artif. Int. Res."
      },
      "publication_date": "2017-05-01",
      "selected": null,
      "title": "Market interfaces for electric vehicle charging",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3176788.3176793"
      ]
    },
    {
      "abstract": "Information systems experience an ever-growing volume of unstructured data, particularly in the form of textual materials. This represents a rich source of information from which one can create value for people, organizations and businesses. For instance, recommender systems can benefit from automatically understanding preferences based on user reviews or social media. However, it is difficult for computer programs to correctly infer meaning from narrative content. One major challenge is negations that invert the interpretation of words and sentences. As a remedy, this paper proposes a novel learning strategy to detect negations: we apply reinforcement learning to find a policy that replicates the human perception of negations based on an exogenous response, such as a user rating for reviews. Our method yields several benefits, as it eliminates the former need for expensive and subjective manual labeling in an intermediate stage. Moreover, the inferred policy can be used to derive statistical inferences and implications regarding how humans process and act on negations.",
      "authors": [
        "Pr\u00f6llochs, Nicolas",
        "Feuerriegel, Stefan",
        "Neumann, Dirk"
      ],
      "categories": null,
      "citations": null,
      "comments": "39 pages",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-04-18",
      "selected": null,
      "title": "Understanding Negations in Information Processing: Learning from Replicating Human Behavior",
      "urls": [
        "http://arxiv.org/pdf/1704.05356.pdf",
        "http://arxiv.org/abs/1704.05356v1",
        "http://arxiv.org/pdf/1704.05356v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Moustafa A.A."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/01677063.2017.1301939",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "17-22",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01677063",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neurogenetics"
      },
      "publication_date": "2017-04-03",
      "selected": null,
      "title": "Drift diffusion model of reward and punishment learning in rare alpha-synuclein gene carriers",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85015618156&origin=inward"
      ]
    },
    {
      "abstract": "Reward learning is known to influence the automatic capture of attention. This study examined how the rate of learning, after high- or low-value reward outcomes, can influence future transfers into value-driven attentional capture. Participants performed an instrumental learning task that was directly followed by an attentional capture task. A hierarchical Bayesian reinforcement model was used to infer individual differences in learning from high or low reward. Results showed a strong relationship between high-reward learning rates (or the weight that is put on learning after a high reward) and the magnitude of attentional capture with high-reward colors. Individual differences in learning from high or low rewards were further related to performance differences when high- or low-value distractors were present. These findings provide novel insight into the development of value-driven attentional capture by showing how information updating after desired or undesired outcomes can influence future deployments of automatic attention.",
      "authors": [
        "Jahfari, Sara",
        "Theeuwes, Jan"
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13423-016-1106-6",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "408-415",
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10699384",
        "publisher": "Springer New York",
        "sjr": 1.783,
        "snip": 2.034,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Arts and Humanities (miscellaneous)"
        ],
        "title": "Psychonomic Bulletin and Review"
      },
      "publication_date": "2017-04-01",
      "selected": null,
      "title": "Sensitivity to value-driven attention is predicted by how we learn from value",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13423-016-1106-6.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84976421822&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Spike M."
      ],
      "categories": null,
      "citations": 27,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/cogs.12351",
      "keywords": [],
      "number_of_pages": 36,
      "pages": "623-658",
      "publication": {
        "category": "Journal",
        "cite_score": 3.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03640213",
        "publisher": "Wiley-Blackwell",
        "sjr": 1.057,
        "snip": 1.257,
        "subject_areas": [
          "Artificial Intelligence",
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive Science"
      },
      "publication_date": "2017-04-01",
      "selected": null,
      "title": "Minimal Requirements for the Emergence of Learned Signaling",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84961275808&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Loram I."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNSRE.2016.2641024",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "357-369",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15344320",
        "publisher": "Institute of Electrical and Electronics Engineers Inc.",
        "sjr": 1.26,
        "snip": 1.675,
        "subject_areas": [
          "Neuroscience (all)",
          "Biomedical Engineering",
          "Internal Medicine",
          "Rehabilitation"
        ],
        "title": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
      },
      "publication_date": "2017-04-01",
      "selected": null,
      "title": "Proactive Selective Inhibition Targeted at the Neck Muscles: This Proximal Constraint Facilitates Learning and Regulates Global Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018465310&origin=inward"
      ]
    },
    {
      "abstract": "In this study, we investigated the interplay of habitual (model-free) and goal-directed (model-based) decision processes by using a two-stage Markov decision task in combination with event-related potentials (ERPs) and computational modeling. To manipulate the demands on model-based decision making, we applied two experimental conditions with different probabilities of transitioning from the first to the second stage of the task. As we expected, when the stage transitions were more predictable, participants showed greater model-based (planning) behavior. Consistent with this result, we found that stimulus-evoked parietal (P300) activity at the second stage of the task increased with the predictability of the state transitions. However, the parietal activity also reflected model-free information about the expected values of the stimuli, indicating that at this stage of the task both types of information are integrated to guide decision making. Outcome-related ERP components only reflected reward-related processes: Specifically, a medial prefrontal ERP component (the feedback-related negativity) was sensitive to negative outcomes, whereas a component that is elicited by reward (the feedback-related positivity) increased as a function of positive prediction errors. Taken together, our data indicate that stimulus-locked parietal activity reflects the integration of model-based and model-free information during decision making, whereas feedback-related medial prefrontal signals primarily reflect reward-related decision processes.",
      "authors": [
        "Eppinger, Ben",
        "Walter, Maik",
        "Li, Shu-Chen"
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-016-0487-3",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "406-421",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2017-04-01",
      "selected": null,
      "title": "Electrophysiological correlates reflect the integration of model-based and model-free decision information",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85008215939&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-016-0487-3.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Prins N.W."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/1741-2552/aa6317",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17412560",
        "publisher": "IOP Publishing Ltd.",
        "sjr": 1.135,
        "snip": 1.25,
        "subject_areas": [
          "Biomedical Engineering",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Journal of Neural Engineering"
      },
      "publication_date": "2017-03-30",
      "selected": null,
      "title": "Feedback for reinforcement learning based brain-machine interfaces using confidence metrics",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85020472961&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) is the behavioral process of learning to associate rewards with actions or objects. Conceptual and theoretical accounts of RL have focused on the striatum. However, recent data shows that the amygdala also plays an important role in RL. Reinforcement learning (RL) is the behavioral process of learning the values of actions and objects. Most models of RL assume that the dopaminergic prediction error signal drives plasticity in frontal\u2013striatal circuits. The striatum then encodes value representations that drive decision processes. However, the amygdala has also been shown to play an important role in forming Pavlovian stimulus\u2013outcome associations. These Pavlovian associations can drive motivated behavior via the amygdala projections to the ventral striatum or the ventral tegmental area. The amygdala may, therefore, play a central role in RL. Here we compare the contributions of the amygdala and the striatum to RL and show that both the amygdala and striatum learn and represent expected values in RL tasks. Furthermore, value representations in the striatum may be inherited, to some extent, from the amygdala. The striatum may, therefore, play less of a primary role in learning stimulus\u2013outcome associations in RL than previously suggested.",
      "authors": [
        "Averbeck, Bruno B",
        "Costa, Vincent D"
      ],
      "categories": null,
      "citations": 109,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/nn.4506",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "505-512",
      "publication": {
        "category": "Journal",
        "cite_score": 43.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10976256",
        "publisher": "Nature Publishing Group",
        "sjr": 12.124,
        "snip": 4.893,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Nature Neuroscience"
      },
      "publication_date": "2017-03-29",
      "selected": null,
      "title": "Motivational neural circuits underlying reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016760961&origin=inward",
        "https://www.nature.com/articles/nn.4506.pdf"
      ]
    },
    {
      "abstract": "Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the \"ground-truth\" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",
      "authors": [
        "Dai, Bo",
        "Fidler, Sanja",
        "Urtasun, Raquel",
        "Lin, Dahua"
      ],
      "categories": null,
      "citations": null,
      "comments": "accepted in ICCV2017 as an Oral paper",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-03-17",
      "selected": null,
      "title": "Towards Diverse and Natural Image Descriptions via a Conditional GAN",
      "urls": [
        "http://arxiv.org/abs/1703.06029v3",
        "http://arxiv.org/pdf/1703.06029v3",
        "http://arxiv.org/pdf/1703.06029.pdf"
      ]
    },
    {
      "abstract": "<p>Closed-loop neuroprosthetics aim to <italic>compensate</italic> for lost function, e.g., by controlling external devices such as prostheses or wheelchairs. Such assistive approaches seek to maximize speed and classification accuracy for high-dimensional control. More recent approaches use similar technology, but aim to restore lost motor function in the long term. To achieve this goal, <italic>restorative</italic> neuroprosthetics attempt to facilitate motor re-learning and to strengthen damaged and/or alternative neural connections on the basis of neurofeedback training within rehabilitative environments. Such a restorative approach requires reinforcement learning of self-modulated brain activity which is considered to be beneficial for functional rehabilitation, e.g., improvement of \u03b2-power modulation over sensorimotor areas for post-stroke movement restoration. Patients with motor impairments, however, may also have a compromised ability for motor task-related regulation of the targeted brain activity. This would affect the estimation of feature weights and hence the classification accuracy of the feedback device. This, in turn, can frustrate the patients and compromise their motor learning. Furthermore, the feedback training may even become erroneous when unconstrained classifier adaptation\u2014which is often used in assistive approaches\u2014is also applied in this rehabilitation context. In conclusion, the conceptual switch from assistance toward restoration necessitates a methodological paradigm shift from classification accuracy toward instructional efficiency. Furthermore, a constrained feature space, a priori regularized feature weights, and difficulty adaptation present key elements of restorative brain interfaces. These factors need, therefore, to be addressed within a therapeutic framework to facilitate reinforcement learning of brain self-regulation for restorative purposes.</p>",
      "authors": [
        "Bauer, Robert",
        "Gharabaghi, Alireza"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2017.00111",
      "keywords": [
        "Brain-computer interface",
        "Stroke",
        "Assistive Technology",
        "Neurorehabilitation",
        "brain-robot interface",
        "rehabilitation robotics",
        "brain-machine interface"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2017-03-13",
      "selected": null,
      "title": "Constraints and Adaptation of Closed-Loop Neuroprosthetics for Functional Restoration",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00111/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85017126758&origin=inward"
      ]
    },
    {
      "abstract": "<p>Motor behaviors are shaped not only by current sensory signals but also by the history of recent experiences. For instance, repeated movements toward a particular target bias the subsequent movements toward that target direction. This process, called use-dependent plasticity (UDP), is considered a basic and goal-independent way of forming motor memories. Most studies consider movement history as the critical component that leads to UDP (Classen et al., 1998; Verstynen and Sabes, 2011). However, the effects of learning (i.e., improved performance) on UDP during movement repetition have not been investigated. Here, we used transcranial magnetic stimulation in two experiments to assess plasticity changes occurring in the primary motor cortex after individuals repeated reinforced and nonreinforced actions. The first experiment assessed whether learning a skill task modulates UDP. We found that a group that successfully learned the skill task showed greater UDP than a group that did not accumulate learning, but made comparable repeated actions. The second experiment aimed to understand the role of reinforcement learning in UDP while controlling for reward magnitude and action kinematics. We found that providing subjects with a binary reward without visual feedback of the cursor led to increased UDP effects. Subjects in the group that received comparable reward not associated with their actions maintained the previously induced UDP. Our findings illustrate how reinforcing consistent actions strengthens use-dependent memories and provide insight into operant mechanisms that modulate plastic changes in the motor cortex.</p><p><b>SIGNIFICANCE STATEMENT</b> Performing consistent motor actions induces use-dependent plastic changes in the motor cortex. This plasticity reflects one of the basic forms of human motor learning. Past studies assumed that this form of learning is exclusively affected by repetition of actions. However, here we showed that success-based reinforcement signals could affect the human use-dependent plasticity (UDP) process. Our results indicate that learning augments and interacts with UDP. This effect is important to the understanding of the interplay between the different forms of motor learning and suggests that reinforcement is not only important to learning new behaviors, but can shape our subsequent behavior via its interaction with UDP.</p>",
      "authors": [
        "Firas Mawase",
        "Shintaro Uehara",
        "Amy J. Bastian",
        "Pablo Celnik"
      ],
      "categories": null,
      "citations": 72,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.3303-16.2017",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "2673-2685",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2017-03-08",
      "selected": null,
      "title": "Motor Learning Enhances Use-Dependent Plasticity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014786957&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Shih V."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/3029798.3038399",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "285-286",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450348850",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2017-03-06",
      "selected": null,
      "title": "Deep reinforcement learning using neurophysiological signatures of interest",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016422151&origin=inward"
      ]
    },
    {
      "abstract": "This paper contributes a first study into how different human users deliver simultaneous control and feedback signals during human-robot interaction. As part of this work, we formalize and present a general interactive learning framework for online cooperation between humans and reinforcement learning agents. In many human-machine interaction settings, there is a growing gap between the degrees-of-freedom of complex semi-autonomous systems and the number of human control channels. Simple human control and feedback mechanisms are required to close this gap and allow for better collaboration between humans and machines on complex tasks. To better inform the design of concurrent control and feedback interfaces, we present experimental results from a human-robot collaborative domain wherein the human must simultaneously deliver both control and feedback signals to interactively train an actor-critic reinforcement learning robot. We compare three experimental conditions: 1) human delivered control signals, 2) reward-shaping feedback signals, and 3) simultaneous control and feedback. Our results suggest that subjects provide less feedback when simultaneously delivering feedback and control signals and that control signal quality is not significantly diminished. Our data suggest that subjects may also modify when and how they provide feedback. Through algorithmic development and tuning informed by this study, we expect semi-autonomous actions of robotic agents can be better shaped by human feedback, allowing for seamless collaboration and improved performance in difficult interactive domains.",
      "authors": [
        "Mathewson, Kory W.",
        "Pilarski, Patrick M."
      ],
      "categories": null,
      "citations": null,
      "comments": "10 pages, 2 pages of references, 8 figures. Under review for the 34th\n  International Conference on Machine Learning, Sydney, Australia, 2017.\n  Copyright 2017 by the authors",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2017-03-03",
      "selected": null,
      "title": "Actor-Critic Reinforcement Learning with Simultaneous Human Control and Feedback",
      "urls": [
        "http://arxiv.org/pdf/1703.01274.pdf",
        "http://arxiv.org/abs/1703.01274v2",
        "http://arxiv.org/pdf/1703.01274v2"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ryu V."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/cns.12671",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "248-256",
      "publication": {
        "category": "Journal",
        "cite_score": 9.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17555930",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.381,
        "snip": 1.351,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Pharmacology (medical)",
          "Physiology (medical)",
          "Pharmacology"
        ],
        "title": "CNS Neuroscience and Therapeutics"
      },
      "publication_date": "2017-03-01",
      "selected": null,
      "title": "Behavioral and Electrophysiological Alterations for Reinforcement Learning in Manic and Euthymic Patients with Bipolar Disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85010366970&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bellucci G."
      ],
      "categories": null,
      "citations": 69,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/hbm.23451",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "1233-1248",
      "publication": {
        "category": "Journal",
        "cite_score": 9.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10659471",
        "publisher": "Wiley-Liss Inc.",
        "sjr": 1.688,
        "snip": 1.395,
        "subject_areas": [
          "Radiological and Ultrasound Technology",
          "Anatomy",
          "Neurology",
          "Neurology (clinical)",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Human Brain Mapping"
      },
      "publication_date": "2017-03-01",
      "selected": null,
      "title": "Neural signatures of trust in reciprocity: A coordinate-based meta-analysis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85003936027&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Osinsky R."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_01055",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "424-434",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2017-03-01",
      "selected": null,
      "title": "The feedback-related negativity reflects the combination of instantaneous and long-term values of decision outcomes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85011629888&origin=inward"
      ]
    },
    {
      "abstract": "Traditional Q-Learning algorithm has problems of data transmission lag and its environmental reward model is too simple. It cannot be well applied to the reinforcement learning of affective virtual human behaviour decision. Analogizing the thought of human' s self-reflection in this paper, a improved Q-learning algorithm is proposed, which can be easily applied in behavioural decision-making of affective virtual human. The Q-learning algorithm in this paper not only strengthens the behaviour strategy with better learning cycle and weakens the behaviour strategy with worse learning cycle by the way of self-reflection reward, but also picks up the speed of the effect of behavioural decision feedback to state-action pair in a learning cycle, thus improving the convergence rate of Q-learning algorithm in affective virtual human's behavioural decision-making. The algorithm aims at helping affective virtual human carry out path optimization in a two-dimensional grid environment in the simulation test. The results show that the improved Q-learning algorithm is significantly faster than the traditional Q-learning algorithm in achieving the optimal control strategy with an average of 43.7 learning cycles. The validity of the algorithm is verified.",
      "authors": [
        "Yiwei Zhang",
        "Tianhuang Chen"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.23919/ICACT.2017.7890121",
      "keywords": [
        "environmental reward model",
        "affective model",
        "Reinforcement learning",
        "Q-learning algorithm",
        "affective virtual human",
        "behavioural decision-making"
      ],
      "number_of_pages": 5,
      "pages": "403-407",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-4892-2",
        "issn": "17389445",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2017 19th International Conference on Advanced Communication Technology (ICACT)"
      },
      "publication_date": "2017-02-19",
      "selected": null,
      "title": "A Q-leaming algorithm applied to the behavioural decision-making of affective virtual human",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018461058&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7890121"
      ]
    },
    {
      "abstract": "Anticipation and delivery of rewards improves memory formation, but little effort has been made to disentangle their respective contributions to memory enhancement. Moreover, it has been suggested that the effects of reward on memory are mediated by dopaminergic influences on hippocampal plasticity. Yet, evidence linking memory improvements to actual reward computations reflected in the activity of the dopaminergic system, i.e. prediction errors and expected values, is scarce and inconclusive. For example, different previous studies reported that the magnitude of prediction errors during a reinforcement learning task was a positive, negative, or non-significant predictor of successfully encoding simultaneously presented images. Individual sensitivities to reward and punishment have been found to influence the activation of the dopaminergic reward system and could therefore help explain these seemingly discrepant results. Here, we used a novel associative memory task combined with computational modelling and showed independent effects of reward-delivery and reward-anticipation on memory. Strikingly, the computational approach revealed positive influences from both reward delivery, as mediated by prediction error magnitude, and reward anticipation, as mediated by magnitude of expected value, even in the absence of behavioral effects when analyzed using standard methods, i.e. by collapsing memory performance across trials within conditions. We additionally measured trait estimates of reward and punishment sensitivity and found that individuals with increased reward (vs. punishment) sensitivity had better memory for associations encoded during positive (vs. negative) prediction errors when tested after 20 minutes, but a negative trend when tested after 24 hours. In conclusion, modelling trial-by-trial fluctuations in the magnitude of reward, as we did here for prediction errors and expected value computations, provides a comprehensive and biologically plausible description of the dynamic interplay between reward, dopamine, and associative memory formation. Our results also underline the importance of considering individual traits when assessing reward-related influences on memory.",
      "authors": [
        "Aberg, Kristoffer C.",
        "M\u00fcller, Julia",
        "Schwartz, Sophie"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2017.00056",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2017-02-15",
      "selected": null,
      "title": "Trial-by-Trial Modulation of Associative Memory Formation by Reward Prediction Error and Reward Anticipation as Revealed by a Biologically Plausible Computational Model",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014043056&origin=inward"
      ]
    },
    {
      "abstract": "<p>Motor imagery (MI) activates the sensorimotor system independent of actual movements and might be facilitated by neurofeedback. Knowledge on the interaction between feedback modality and the involved frequency bands during MI-related brain self-regulation is still scarce. Previous studies compared the cortical activity during the MI task with concurrent feedback (MI <italic>with</italic> feedback condition) to cortical activity during the relaxation task where no feedback was provided (relaxation <italic>without</italic> feedback condition). The observed differences might, therefore, be related to either the task or the feedback. A proper comparison would necessitate studying a relaxation condition with feedback and a MI task condition without feedback as well. Right-handed healthy subjects performed two tasks, i.e., MI and relaxation, in alternating order. Each of the tasks (MI vs. relaxation) was studied with and without feedback. The respective event-driven oscillatory activity, i.e., sensorimotor desynchronization (during MI) or synchronization (during relaxation), was rewarded with contingent feedback. Importantly, feedback onset was delayed to study the task-related cortical activity in the absence of feedback provision during the delay period. The reward modality was alternated every 15 trials between proprioceptive and visual feedback. Proprioceptive input was superior to visual input to increase the range of task-related spectral perturbations in the \u00ce\u00b1- and \u00ce\u00b2-band, and was necessary to consistently achieve MI-related sensorimotor desynchronization (ERD) significantly below baseline. These effects occurred in task periods without feedback as well. The increased accuracy and duration of learned brain self-regulation achieved in the proprioceptive condition was specific to the \u00ce\u00b2-band. MI-related operant learning of brain self-regulation is facilitated by proprioceptive feedback and mediated in the sensorimotor \u00ce\u00b2-band.</p>",
      "authors": [
        "Darvishi, Sam",
        "Gharabaghi, Alireza",
        "Boulay, Chadwick B.",
        "Ridding, Michael C.",
        "Abbott, Derek",
        "Baumert, Mathias"
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2017.00060",
      "keywords": [
        "operant conditioning",
        "Brain-computer interface",
        "Stroke",
        "reinforcement learning",
        "brain-robot interface",
        "Neurorehabilitation",
        "brain-machine interface",
        "Beta rhythms"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2017-02-09",
      "selected": null,
      "title": "Proprioceptive Feedback Facilitates Motor Imagery-Related Operant Learning of Sensorimotor \u00ce\u00b2-Band Modulation",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00060/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014304972&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kessler L."
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12783",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "260-269",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2017-02-01",
      "selected": null,
      "title": "Feedback negativity and decision-making behavior in the Balloon Analogue Risk Task (BART) in adolescents is modulated by peer presence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85001125050&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Nees F."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1097/j.pain.0000000000000720",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "212-219",
      "publication": {
        "category": "Journal",
        "cite_score": 12.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03043959",
        "publisher": "Lippincott Williams and Wilkins Ltd.",
        "sjr": 2.445,
        "snip": 3.151,
        "subject_areas": [
          "Neurology (clinical)",
          "Anesthesiology and Pain Medicine",
          "Neurology"
        ],
        "title": "Pain"
      },
      "publication_date": "2017-02-01",
      "selected": null,
      "title": "Brain substrates of reward processing and the \u03bc-opioid receptor: A pathway into pain?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018645363&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Schiffer A.M."
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2016.08.057",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "626-641",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2017-02-01",
      "selected": null,
      "title": "Adaptive behaviour and feedback processing integrate experience and instruction in reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85006086041&origin=inward"
      ]
    },
    {
      "abstract": "<p>Classical economic theory contends that the utility of a choice option should be independent of other options. This view is challenged by the attraction effect, in which the relative preference between two options is altered by the addition of a third, asymmetrically dominated option. Here, we leveraged the attraction effect in the context of intertemporal choices to test whether both decisions and reward prediction errors (RPE) in the absence of choice violate the independence of irrelevant alternatives principle. We first demonstrate that intertemporal decision making is prone to the attraction effect in humans. In an independent group of participants, we then investigated how this affects the neural and behavioral valuation of outcomes using a novel intertemporal lottery task and fMRI. Participants9 behavioral responses (i.e., satisfaction ratings) were modulated systematically by the attraction effect and this modulation was correlated across participants with the respective change of the RPE signal in the nucleus accumbens. Furthermore, we show that, because exponential and hyperbolic discounting models are unable to account for the attraction effect, recently proposed sequential sampling models might be more appropriate to describe intertemporal choices. Our findings demonstrate for the first time that the attraction effect modulates subjective valuation even in the absence of choice. The findings also challenge the prospect of using neuroscientific methods to measure utility in a context-free manner and have important implications for theories of reinforcement learning and delay discounting.</p><p><b>SIGNIFICANCE STATEMENT</b> Many theories of value-based decision making assume that people first assess the attractiveness of each option independently of each other and then pick the option with the highest subjective value. The attraction effect, however, shows that adding a new option to a choice set can change the relative value of the existing options, which is a violation of the independence principle. Using an intertemporal choice framework, we tested whether such violations also occur when the brain encodes the difference between expected and received rewards (i.e., the reward prediction error). Our results suggest that neither intertemporal choice nor valuation without choice adhere to the independence principle.</p>",
      "authors": [
        "Sebastian Gluth",
        "Jared M. Hotaling",
        "J\u00f6rg Rieskamp"
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.2532-16.2016",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "371-382",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2017-01-11",
      "selected": null,
      "title": "The Attraction Effect Modulates Reward Prediction Errors and Intertemporal Choices",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85043625475&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Nicart E."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICTAI.2016.99",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "635-639",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781509044597",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2016 IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016"
      },
      "publication_date": "2017-01-11",
      "selected": null,
      "title": "Building document treatment chains using reinforcement learning and intuitive feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85013648981&origin=inward"
      ]
    },
    {
      "abstract": "Controlling a bicycle without human interaction is still a challenge for researchers. Most of the studies on this topic focus on the physical area of bicycle or designing controllers based on automatic control knowledge such as feedback controller, LQR controller. This study focuses on applying a state-of-the-art deep reinforcement learning algorithm called Deep Deterministic Policy Gradient to control the bicycle. The bicycle can use the learned controller (agent) to keep balancing or reach a specified goal.",
      "authors": [
        "Le Pham Tuyen",
        "TaeChoong Chung"
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/URAI.2017.7992765",
      "keywords": [
        "Deep Deterministic Policy Gradient",
        "Balancing Bicycle",
        "Deep Reinforcement Learning",
        "Deep-Q Network",
        "Go-to Bicycle"
      ],
      "number_of_pages": 5,
      "pages": "413-417",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-3057-6",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2017"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Controlling bicycle using deep deterministic policy gradient algorithm",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85034236318&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7992765"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) has distinguished itself as a prominent learning method to augment the efficacy of autonomous systems. Recent advances in deep learning studies have complemented existing RL methods and led to a crucial breakthrough in the effort of applying RL to automation and robotics. Artificial agents based on deep RL can take selective and intelligent actions comparable with those of a human to maximize the feedback reward from the interactive environment. In this paper, we survey recent developments in the literature regarding deep RL methods for building human-level agents. As a result, prominent studies that involve modeling every aspect of a human-level agent will be examined. We also provide an overview of constructing a framework for prospective autonomous systems. Moreover, various toolkits and frameworks are suggested to facilitate the development of deep RL methods. Finally, we open a discussion that potentially raises a range of future research directions in deep RL.",
      "authors": [
        "Ngoc Duy Nguyen",
        "Thanh Nguyen",
        "Saeid Nahavandi"
      ],
      "categories": null,
      "citations": 73,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ACCESS.2017.2777827",
      "keywords": [
        "robotics",
        "human-level agents",
        "system design",
        "reinforcement learning",
        "Deep learning",
        "survey"
      ],
      "number_of_pages": 12,
      "pages": "27091-27102",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2169-3536",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Access"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "System Design Perspective for Human-Level Agents Using Deep Reinforcement Learning: A Survey",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85035762670&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8119919"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Furfaro R."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 16,
      "pages": "401-416",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 0.7,
        "is_potentially_predatory": false,
        "isbn": "9780877036432",
        "issn": "00653438",
        "publisher": "Univelt Inc.",
        "sjr": 0.143,
        "snip": 0.132,
        "subject_areas": [
          "Aerospace Engineering",
          "Space and Planetary Science"
        ],
        "title": "Advances in the Astronautical Sciences"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Waypoint-Based generalized ZEM/ZEV feedback guidance for planetary landing via a reinforcement learning approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85046338662&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mitsunaga N."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 12,
      "pages": "313-324",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781466506985",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Human-Robot Interaction in Social Robotics"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Adapting Nonverbal Behavior Parameters to Be Preferred by Individuals",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85132459000&origin=inward"
      ]
    },
    {
      "abstract": "It can be said that none of yet proposed methods for achieving artificial ethical reasoning is realistic, i.e. working outside very limited environments and scenarios. Whichever method one chooses, it will not work in various real world situations because it would be...",
      "authors": [
        "Rzepka, Rafal",
        "Araki, Kenji"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-319-63703-7_17",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "178-187",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "What People Say? Web-Based Casuistry for Artificial Morality Experiments",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-319-63703-7_17.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028467326&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zhu J.Q."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 6,
      "pages": "3658-3663",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780991196760",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "CogSci 2017 - Proceedings of the 39th Annual Meeting of the Cognitive Science Society: Computational Foundations of Cognition"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Information Seeking as Chasing Anticipated Prediction Errors",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85085842138&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sermanet P."
      ],
      "categories": null,
      "citations": 36,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.15607/rss.2017.xiii.050",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780992374730",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Robotics: Science and Systems"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Unsupervised perceptual rewards for imitation learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048763035&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Nguyen K."
      ],
      "categories": null,
      "citations": 43,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.18653/v1/d17-1153",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "1464-1474",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781945626838",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Reinforcement learning for bandit neural machine translation with simulated human feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056359899&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Majumdar A."
      ],
      "categories": null,
      "citations": 36,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.15607/rss.2017.xiii.069",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780992374730",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Robotics: Science and Systems"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Risk-sensitive inverse reinforcement learning via coherent risk models",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048804859&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hu X."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "930-939",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AMIA ... Annual Symposium proceedings. AMIA Symposium"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "A First Step Towards Behavioral Coaching for Managing Stress: A Case Study on Optimal Policy Estimation with Multi-stage Threshold Q-learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053475941&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Morris L.S."
      ],
      "categories": null,
      "citations": 25,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nicl.2017.08.007",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "286-294",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "NeuroImage: Clinical"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Disrupted avoidance learning in functional neurological disorder: Implications for harm avoidance theories",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027396707&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Mathewson K."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 5,
      "pages": "477-481",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357797",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAAI Spring Symposium - Technical Report"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Reinforcement learning based embodied agents modelling human users through interaction and multi-sensory perception",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028707999&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bakic J."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/da.22576",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "89-96",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10914269",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Depression and Anxiety"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Spared internal but impaired external reward prediction error signals in major depressive disorder during reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84996814623&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sermanet P."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Unsupervised perceptual rewards for imitation learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094309528&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Ligneul R."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-12-805308-9.00017-8",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "211-224",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9780128053317",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Decision Neuroscience: An Integrative Perspective"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Social dominance representations in the human brain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85020513085&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Nicart E."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3166/RIA.31.619-648",
      "keywords": [],
      "number_of_pages": 30,
      "pages": "619-648",
      "publication": {
        "category": "Journal",
        "cite_score": 2.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0992499X",
        "publisher": "Lavoisier",
        "sjr": 0.299,
        "snip": 0.665,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Revue d'Intelligence Artificielle"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Using reinforcement learning to continuously improve a document treatment chain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85042257581&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Li J."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings"
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "Dialogue learning with human-in-the-loop",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048594513&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.",
      "authors": [
        "Christian Wirth",
        "Riad Akrour",
        "Gerhard Neumann",
        "Johannes F\u00fcrnkranz"
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3122009.3208017",
      "keywords": [
        "qualitative feedback",
        "Markov decision process",
        "policy search",
        "preference-based reinforcement learning",
        "reinforcement learning",
        "temporal difference learning",
        "preference learning"
      ],
      "number_of_pages": 46,
      "pages": "4945-4990",
      "publication": {
        "category": "Journal",
        "cite_score": 9.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1532-4435",
        "publisher": "Microtome Publishing",
        "sjr": 2.281,
        "snip": 3.323,
        "subject_areas": [
          "Software",
          "Artificial Intelligence",
          "Statistics and Probability",
          "Control and Systems Engineering"
        ],
        "title": "J. Mach. Learn. Res."
      },
      "publication_date": "2017-01-01",
      "selected": null,
      "title": "A survey of preference-based reinforcement learning methods",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3122009.3208017"
      ]
    },
    {
      "abstract": "Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide feedback on these intermediate steps. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit specification of sub-goals. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also show that our method can be used to learn a real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. Supplementary material and data are available at https://sermanet.github.io/rewards",
      "authors": [
        "Sermanet, Pierre",
        "Xu, Kelvin",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2016-12-20",
      "selected": null,
      "title": "Unsupervised Perceptual Rewards for Imitation Learning",
      "urls": [
        "http://arxiv.org/pdf/1612.06699v3",
        "http://arxiv.org/pdf/1612.06699.pdf",
        "http://arxiv.org/abs/1612.06699v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Tremel J."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bbr.2016.08.023",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "51-65",
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01664328",
        "publisher": "Elsevier B.V.",
        "sjr": 0.881,
        "snip": 0.802,
        "subject_areas": [
          "Behavioral Neuroscience"
        ],
        "title": "Behavioural Brain Research"
      },
      "publication_date": "2016-12-15",
      "selected": null,
      "title": "Neural signatures of experience-based improvements in deterministic decision-making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84982095256&origin=inward"
      ]
    },
    {
      "abstract": "Recent advances have shown the capability of Fully Convolutional Neural Networks (FCN) to model cost functions for motion planning in the context of learning driving preferences purely based on demonstration data from human drivers. While pure learning from demonstrations in the framework of Inverse Reinforcement Learning (IRL) is a promising approach, we can benefit from well informed human priors and incorporate them into the learning process. Our work achieves this by pretraining a model to regress to a manual cost function and refining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When injecting prior knowledge as pretraining for the network, we achieve higher robustness, more visually distinct obstacle boundaries, and the ability to capture instances of obstacles that elude models that purely learn from demonstration data. Furthermore, by exploiting these human priors, the resulting model can more accurately handle corner cases that are scarcely seen in the demonstration data, such as stairs, slopes, and underpasses.",
      "authors": [
        "Wulfmeier, Markus",
        "Rao, Dushyant",
        "Posner, Ingmar"
      ],
      "categories": null,
      "citations": null,
      "comments": "Neural Information Processing Systems 2016, Deep Reinforcement\n  Learning Workshop",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2016-12-13",
      "selected": null,
      "title": "Incorporating Human Domain Knowledge into Large Scale Cost Function Learning",
      "urls": [
        "http://arxiv.org/pdf/1612.04318.pdf",
        "http://arxiv.org/pdf/1612.04318v1",
        "http://arxiv.org/abs/1612.04318v1"
      ]
    },
    {
      "abstract": "Threat detection is one of the basic mechanisms for protecting a network, as prevention does not suffice. Finding an attack is difficult because the most harmful ones are specially prepared against a specific victim and crafted for the first time. The contribution of a human expert is still needed for their detection, no matter how effective automatic methods used nowadays can appear. Moreover, in many occasions intrusions can only be efficiently detected by analyzing its effects on more than one element in the network. Event and alert recollection offers a way to centralize information from a heterogeneous set of sources. Then, it can be normalized to a common language and analyzed as a whole by a security system. In this paper we propose Morwilog, an ant-inspired method for standing out the relationship between actions belonging to the same complex attack. Morwilog is conceived as a framework for alert correlation to be integrated in a multi-modular security system. Reinforcement learning is incorporated to it thanks to feedback from a human security expert.",
      "authors": [
        "Julio Navarro-Lara",
        "Aline Deruyver",
        "Pierre Parrend"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/SSCI.2016.7849902",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-4241-8",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016"
      },
      "publication_date": "2016-12-06",
      "selected": null,
      "title": "Morwilog: an ACO-based system for outlining multi-step attacks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85016001769&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7849902"
      ]
    },
    {
      "abstract": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English \u2194 French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.",
      "authors": [
        "Di He",
        "Yingce Xia",
        "Tao Qin",
        "Liwei Wang",
        "Nenghai Yu",
        "Tie-Yan Liu",
        "Wei-Ying Ma"
      ],
      "categories": null,
      "citations": 40,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3157096.3157188",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "820-828",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781510838819",
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 30th International Conference on Neural Information Processing Systems"
      },
      "publication_date": "2016-12-05",
      "selected": null,
      "title": "Dual learning for machine translation",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3157096.3157188"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Han J."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.4218/etrij.16.0116.0106",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "1229-1239",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "12256463",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "ETRI Journal"
      },
      "publication_date": "2016-12-01",
      "selected": null,
      "title": "Interactive human intention reading by learning hierarchical behavior knowledge networks for human-robot interaction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85000842709&origin=inward"
      ]
    },
    {
      "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.",
      "authors": [
        "Li, Jiwei",
        "Miller, Alexander H.",
        "Chopra, Sumit",
        "Ranzato, Marc'Aurelio",
        "Weston, Jason"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2016-11-29",
      "selected": null,
      "title": "Dialogue Learning With Human-In-The-Loop",
      "urls": [
        "http://arxiv.org/abs/1611.09823v3",
        "http://arxiv.org/pdf/1611.09823v3",
        "http://arxiv.org/pdf/1611.09823.pdf"
      ]
    },
    {
      "abstract": "In recent years, unmanned systems reached a maturity level that resulted in the adoption of robotic system for both civil and military usages. As the amount of unmanned systems increases, the amount of visual information that is being sent to the end user increases, and the need to develop the unmanned systems ability to autonomously alert the user on predefined events is being emphasized. This ability is mainly based on their capability to detect and classify the objects in their field of operation. Recent studies shows that the mechanism behind the recognition process in the human brain includes continuous generation of predictions based on prior knowledge about the world. These predictions enable rapid generation of hypothesis that biases the outcome of the recognition process in situation of uncertainty. In addition, the visual system continuously updating its knowledge about the world based on visual feedback. In our work, we formed an architecture that is based on the concepts behind these top-down prediction and learning processes of the human visual system, together with state of the art bottom-up object recognition models, e.g. deep CNNs. We show that imitating these top-down reinforcement learning mechanisms, together with the state of the art bottom-up feed-forward algorithm, creates an accurate and continuously improving target recognition model.",
      "authors": [
        "Dan Malowany",
        "Hugo Guterman"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICSEE.2016.7806151",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "1-5",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-2153-6",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2016 IEEE International Conference on the Science of Electrical Engineering, ICSEE 2016"
      },
      "publication_date": "2016-11-16",
      "selected": null,
      "title": "Visual reinforcement learning for object recognition in robotics",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014185250&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7806151"
      ]
    },
    {
      "abstract": null,
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781509039296",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "25th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN 2016"
      },
      "publication_date": "2016-11-15",
      "selected": null,
      "title": "25th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN 2016",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85002553994&origin=inward"
      ]
    },
    {
      "abstract": "We model a document treatment chain as a Markov Decision Process, and use reinforcement learning to allow the agent to learn to construct and continuously improve custom-made chains \"on the fly\". We build a platform which enables us to measure the impact on the learning of various models, web services, algorithms, parameters, etc. We apply this in an industrial setting, specifically to an open source document treatment chain which extracts events from massive volumes of web pages and other open-source documents. Our emphasis is on minimising the burden of the human analysts, from whom the agent learns to improve guided by their feedback on the events extracted. For this, we investigate different types of feedback, from numerical feedback, which requires a lot of tuning, to partially and even fully qualitative feedback, which is much more intuitive, and demands little to no user calibration. We carry out experiments, first with numerical feedback, then demonstrate that intuitive feedback still allows the agent to learn effectively.",
      "authors": [
        "Esther Nicart",
        "Bruno Zanuttini",
        "Hugo Gilbert",
        "Bruno Grilh\u00e8res",
        "Frd\u00e9ric Praca"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": "10.1109/ICTAI.2016.0102",
      "keywords": [
        "Extraction and knowledge management",
        "Qualitative feedback",
        "Artificial intelligence",
        "Open source intelligence (OSINT)",
        "Reinforcement learning",
        "Man-machine interaction"
      ],
      "number_of_pages": 5,
      "pages": "635-639",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5090-4460-3",
        "issn": "2375-0197",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)"
      },
      "publication_date": "2016-11-06",
      "selected": null,
      "title": "Building Document Treatment Chains Using Reinforcement Learning and Intuitive Feedback",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7814662"
      ]
    },
    {
      "abstract": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \\emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.",
      "authors": [
        "Xia, Yingce",
        "He, Di",
        "Qin, Tao",
        "Wang, Liwei",
        "Yu, Nenghai",
        "Liu, Tie-Yan",
        "Ma, Wei-Ying"
      ],
      "categories": null,
      "citations": 612,
      "comments": null,
      "databases": [
        "Scopus",
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "820-828",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Computation and Language"
        ],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2016-11-01",
      "selected": null,
      "title": "Dual Learning for Machine Translation",
      "urls": [
        "http://arxiv.org/pdf/1611.00179.pdf",
        "http://arxiv.org/pdf/1611.00179v1",
        "http://arxiv.org/abs/1611.00179v1",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019265504&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chang W.C."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/schbul/sbw060",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "1476-1485",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "05867614",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Schizophrenia Bulletin"
      },
      "publication_date": "2016-11-01",
      "selected": null,
      "title": "Mild Reinforcement Learning Deficits in Patients with First-Episode Psychosis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84994444354&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Reinen J."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/schbul/sbw045",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1467-1475",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "05867614",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Schizophrenia Bulletin"
      },
      "publication_date": "2016-11-01",
      "selected": null,
      "title": "Motivational Context Modulates Prediction Error Response in Schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84994475121&origin=inward"
      ]
    },
    {
      "abstract": "Learning how to gain rewards (approach learning) and avoid punishments (avoidance learning) is fundamental for everyday life. While individual differences in approach and avoidance learning styles have been related to genetics and aging, the contribution of personality factors, such as traits, remains undetermined. Moreover, little is known about the computational mechanisms mediating differences in learning styles. Here, we used a probabilistic selection task with positive and negative feedbacks, in combination with computational modelling, to show that individuals displaying better approach (vs. avoidance) learning scored higher on measures of approach (vs. avoidance) trait motivation, but, paradoxically, also displayed reduced learning speed following positive (vs. negative) outcomes. These data suggest that learning different types of information depend on associated reward values and internal motivational drives, possibly determined by personality traits.",
      "authors": [
        "Kristoffer Carl Aberg",
        "Kimberly C. Doell",
        "Sophie Schwartz"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0166675",
      "keywords": [
        "Dopamine",
        "Learning",
        "Behavior",
        "Human learning",
        "Decision making",
        "Motivation",
        "Personality traits",
        "Working memory"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2016-11-01",
      "selected": null,
      "title": "Linking Individual Learning Styles to Approach-Avoidance Motivational Traits and Computational Aspects of Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84995605632&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0166675&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hobson N."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/scan/nsw082",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1698-1706",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17495016",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Social Cognitive and Affective Neuroscience"
      },
      "publication_date": "2016-11-01",
      "selected": null,
      "title": "The mere presence of an outgroup member disrupts the brain's feedback-monitoring system",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84996644818&origin=inward"
      ]
    },
    {
      "abstract": "Dopamine systems mediate key aspects of reward learning. Parkinson\u2019s disease (PD) represents a valuable model to study reward mechanisms because both the disease process and the anti-Parkinson medications influence dopamine neurotransmission. The aim of this pilot study was to investigate whether the level of levodopa differently modulates learning from positive and negative feedback and its electrophysiological correlate, the error related negativity (ERN), in PD. Ten PD patients and ten healthy participants performed a two-stage reinforcement learning task. In the Learning Phase they had to learn the correct stimulus within a stimulus pair on the basis of a probabilistic positive or negative feedback. Three sets of stimulus pairs were used. In the Testing Phase the participants were tested with novel combinations of the stimuli previously experienced to evaluate whether they learned more from positive or negative feedback. PD patients performed the task both ON- and OFF-levodopa in two separate sessions while they remained on stable therapy with dopamine agonists. The electroencephalogram was recorded during the task. PD patients were less accurate in negative than positive learning both OFF- and ON-levodopa. In the OFF-levodopa state they were less accurate than controls in negative learning. PD patients had a smaller ERN amplitude OFF- than ON-levodopa only in negative learning. In the OFF-levodopa state they had a smaller ERN amplitude than controls in negative learning. We hypothesize that high tonic dopaminergic stimulation due to the dopamine agonist medication, combined to the low level of phasic dopamine due to the OFF-levodopa state, could prevent phasic \u201cdopamine dips\u201d indicated by the ERN needed for learning from negative feedback.",
      "authors": [
        "Volpato, Chiara",
        "Schiff, Sami",
        "Facchini, Silvia",
        "Silvoni, Stefano",
        "Cavinato, Marianna",
        "Piccione, Francesco",
        "Antonini, Angelo",
        "Birbaumer, Niels"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2016.00205",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2016-10-24",
      "selected": null,
      "title": "Dopaminergic Medication Modulates Learning from Feedback and Error-Related Negativity in Parkinson\u2019s Disease: A Pilot Study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84992677755&origin=inward"
      ]
    },
    {
      "abstract": "Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a navigational guide model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then \"translate\" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18% when compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants that demonstrate that our method generates instructions that people follow as accurately and easily as those produced by humans.",
      "authors": [
        "Daniele, Andrea F.",
        "Bansal, Mohit",
        "Walter, Matthew R."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2016-10-11",
      "selected": null,
      "title": "Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation",
      "urls": [
        "http://arxiv.org/abs/1610.03164v1",
        "http://arxiv.org/pdf/1610.03164v1",
        "http://arxiv.org/pdf/1610.03164.pdf"
      ]
    },
    {
      "abstract": "Adaptive memory requires context-dependent control over how information is retrieved, evaluated and used to guide action, yet the signals that drive adjustments to memory decisions remain unknown. Here we show that prediction errors (PEs) coded by the striatum support control over memory decisions. Human participants completed a recognition memory test that incorporated biased feedback to influence participants\u2019 recognition criterion. Using model-based fMRI, we find that PEs\u2014the deviation between the outcome and expected value of a memory decision\u2014correlate with striatal activity and predict individuals\u2019 final criterion. Importantly, the striatal PEs are scaled relative to memory strength rather than the expected trial outcome. Follow-up experiments show that the learned recognition criterion transfers to free recall, and targeting biased feedback to experimentally manipulate the magnitude of PEs influences criterion consistent with PEs scaled relative to memory strength. This provides convergent evidence that declarative memory decisions can be regulated via striatally mediated reinforcement learning signals. The human brain can efficiently retrieve information from long-term memory and use it to guide action but how the brain selects the most useful information in each case is unclear. Here the authors show that reinforcement learning mechanisms, based on expected value and prediction error fMRI signals in striatum, play a role in memory control processes guiding behavior.",
      "authors": [
        "Scimeca, Jason M.",
        "Katzman, Perri L.",
        "Badre, David"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/ncomms13061",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 24.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2041-1723",
        "publisher": "Nature Publishing Group",
        "sjr": 5.116,
        "snip": 3.268,
        "subject_areas": [
          "Chemistry (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Physics and Astronomy (all)"
        ],
        "title": "Nature Communications"
      },
      "publication_date": "2016-10-07",
      "selected": null,
      "title": "Striatal prediction errors support dynamic control of declarative memory decisions",
      "urls": [
        "https://www.nature.com/articles/ncomms13061.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84990231595&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "DiMenichi B."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuron.2016.09.043",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "1-3",
      "publication": {
        "category": "Journal",
        "cite_score": 26.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08966273",
        "publisher": "Cell Press",
        "sjr": 7.736,
        "snip": 3.346,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Neuron"
      },
      "publication_date": "2016-10-05",
      "selected": null,
      "title": "Are You Smarter Than a Teenager? Maybe Not When It Comes to Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84992724108&origin=inward"
      ]
    },
    {
      "abstract": "A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training.",
      "authors": [
        "Finn, Chelsea",
        "Levine, Sergey"
      ],
      "categories": null,
      "citations": null,
      "comments": "ICRA 2017. Supplementary video:\n  https://sites.google.com/site/robotforesight/",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2016-10-03",
      "selected": null,
      "title": "Deep Visual Foresight for Planning Robot Motion",
      "urls": [
        "http://arxiv.org/pdf/1610.00696.pdf",
        "http://arxiv.org/abs/1610.00696v2",
        "http://arxiv.org/pdf/1610.00696v2"
      ]
    },
    {
      "abstract": "Stimuli associated with monetary reward can become powerful cues that effectively capture visual attention. We examined whether such value-driven attentional capture can be induced with monetary feedback in the absence of an expected cash payout. To this end, we implemented images of U.S. dollar bills as reward feedback. Participants knew in advance that they would not receive any money based on their performance. Our reward stimuli\u2014$5 and $20 bill images\u2014were thus dissociated from any practical utility. Strikingly, we observed a reliable attentional capture effect for the mere images of bills. Moreover, this finding generalized to Monopoly money. In two control experiments, we found no evidence in favor of nominal or symbolic monetary value. Hence, we claim that bill images are special monetary representations, such that there are strong associations between the defining visual features of bills and reward, probably due to a lifelong learning history. Together, we show that the motivation to earn cash plays a minor role when it comes to monetary rewards, while bill-defining visual features seem to be sufficient. These findings have the potential to influence human factor applications, such as gamification, and can be extended to novel value systems, such as the electronic cash Bitcoin being developed for use in mobile banking. Finally, our procedure represents a proof of concept on how images of money can be used to conserve expenditures in the experimental context.",
      "authors": [
        "Roper, Zachary J. J.",
        "Vecera, Shaun P."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13414-016-1147-y",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "2199-2212",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "19433921",
        "publisher": "Springer US",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Attention, Perception, and Psychophysics"
      },
      "publication_date": "2016-10-01",
      "selected": null,
      "title": "Funny money: the attentional role of monetary feedback detached from expected value",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13414-016-1147-y.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84973161089&origin=inward"
      ]
    },
    {
      "abstract": "The aim of this paper is threefold. First, it reappraises the major transformations which the utilitarian approach to human behavior has undergone in economics in search for a representation by utility functions and later by preference orders. Second, in the light of today\u2019s behavioral and human sciences, an attempt is made to restore some elements of early utilitarianism that were abandoned in these transformations. Third, in line with the interest of the early utilitarians in both explaining behavior and elaborating on its moral assessment, the present paper also discusses some normative implications of the suggested restoration of utilitarian theory.",
      "authors": [
        "Witt, Ulrich"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10818-016-9235-6",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "211-228",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13876996",
        "publisher": "Springer US",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Bioeconomics"
      },
      "publication_date": "2016-10-01",
      "selected": null,
      "title": "The transformations of utility theory: a behavioral perspective",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10818-016-9235-6.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84991111211&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "van de Vijver I."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neurobiolaging.2016.06.002",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1-12",
      "publication": {
        "category": "Journal",
        "cite_score": 8.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01974580",
        "publisher": "Elsevier Inc.",
        "sjr": 1.521,
        "snip": 1.058,
        "subject_areas": [
          "Neuroscience (all)",
          "Geriatrics and Gerontology",
          "Developmental Biology",
          "Neurology (clinical)",
          "Aging"
        ],
        "title": "Neurobiology of Aging"
      },
      "publication_date": "2016-10-01",
      "selected": null,
      "title": "Frontostriatal anatomical connections predict age- and difficulty-related differences in reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84978264452&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Padr\u00f3n I."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroscience.2016.07.025",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "264-276",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03064522",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Neuroscience"
      },
      "publication_date": "2016-10-01",
      "selected": null,
      "title": "Representing the consequences of our actions trial by trial: Complex and flexible encoding of feedback valence and magnitude",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84980017952&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Schiffler B."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00987",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "1539-1552",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2016-10-01",
      "selected": null,
      "title": "Memory-reliant post-error slowing is associated with successful learning and fronto-occipital activity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84984996880&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bauer R."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.clinph.2016.06.016",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "3033-3041",
      "publication": {
        "category": "Journal",
        "cite_score": 7.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13882457",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 1.212,
        "snip": 1.482,
        "subject_areas": [
          "Neurology (clinical)",
          "Physiology (medical)",
          "Sensory Systems",
          "Neurology"
        ],
        "title": "Clinical Neurophysiology"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "What is the optimal task difficulty for reinforcement learning of brain self-regulation?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85007001554&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zaki J."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00978",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "1270-1282",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "Social cognition as reinforcement learning: Feedback modulates emotion inference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84982786536&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Li P."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijpsycho.2016.06.018",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "37-43",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01678760",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Psychophysiology"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "Oscillatory profiles of positive, negative and neutral feedback stimuli during adaptive decision making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84977630909&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Dowd E.C."
      ],
      "categories": null,
      "citations": 81,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bpsc.2016.05.005",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "460-473",
      "publication": {
        "category": "Journal",
        "cite_score": 9.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "24519022",
        "publisher": "Elsevier Inc.",
        "sjr": 1.997,
        "snip": 1.337,
        "subject_areas": [
          "Neurology (clinical)",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "Probabilistic Reinforcement Learning in Patients With Schizophrenia: Relationships to Anhedonia and Avolition",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84994381600&origin=inward"
      ]
    },
    {
      "abstract": "In this work, we address a relatively unexplored aspect of designing agents that learn from human reward. We investigate how an agent\u2019s non-task behavior can affect a human trainer\u2019s training and agent learning. We use the TAMER framework, which facilitates the training of agents by human-generated reward signals, i.e., judgements of the quality of the agent\u2019s actions, as the foundation for our investigation. Then, starting from the premise that the interaction between the agent and the trainer should be bi-directional, we propose two new training interfaces to increase a human trainer\u2019s active involvement in the training process and thereby improve the agent\u2019s task performance. One provides information on the agent\u2019s uncertainty which is a metric calculated as data coverage, the other on its performance. Our results from a 51-subject user study show that these interfaces can induce the trainers to train longer and give more feedback. The agent\u2019s performance, however, increases only in response to the addition of performance-oriented information, not by sharing uncertainty levels. These results suggest that the organizational maxim about human behavior, \u201cyou get what you measure\u201d\u2014i.e., sharing metrics with people causes them to focus on optimizing those metrics while de-emphasizing other objectives\u2014also applies to the training of agents. Using principle component analysis, we show how trainers in the two conditions train agents differently. In addition, by simulating the influence of the agent\u2019s uncertainty\u2013informative behavior on a human\u2019s training behavior, we show that trainers could be distracted by the agent sharing its uncertainty levels about its actions, giving poor feedback for the sake of reducing the agent\u2019s uncertainty without improving the agent\u2019s performance.",
      "authors": [
        "Li, Guangliang",
        "Whiteson, Shimon",
        "Knox, W. Bradley",
        "Hung, Hayley"
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10458-015-9308-2",
      "keywords": [],
      "number_of_pages": 23,
      "pages": "826-848",
      "publication": {
        "category": "Journal",
        "cite_score": 5.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13872532",
        "publisher": "Springer Netherlands",
        "sjr": 0.927,
        "snip": 2.046,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Autonomous Agents and Multi-Agent Systems"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "Using informative behavior to increase engagement while learning from human reward",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10458-015-9308-2.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84939856385&origin=inward"
      ]
    },
    {
      "abstract": "A large body of evidence shows that buying behaviour is strongly determined by consumers\u2019 price expectations and the extent to which real prices violate these expectations. Despite the importance of this phenomenon, little is known regarding its neural mechanisms. Here we show that two patterns of electrical brain activity known to index prediction errors\u2013the Feedback-Related Negativity (FRN) and the feedback-related P300 \u2013were sensitive to price offers that were cheaper than participants\u2019 expectations. In addition, we also found that FRN amplitude time-locked to price offers predicted whether a product would be subsequently purchased or not, and further analyses suggest that this result was driven by the sensitivity of the FRN to positive price expectation violations. This finding strongly suggests that ensembles of neurons coding positive prediction errors play a critical role in real-life consumer behaviour. Further, these findings indicate that theoretical models based on the notion of prediction error, such as the Reinforcement Learning Theory, can provide a neurobiologically grounded account of consumer behavior.",
      "authors": [
        "Alexandre Schaefer",
        "Luciano G. Buratto",
        "Nobuhiko Goto",
        "Emilie V. Brotherhood"
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0163150",
      "keywords": [
        "Electroencephalography",
        "Scalp",
        "Neurons",
        "Learning",
        "Behavior",
        "Decision making",
        "Event-related potentials",
        "Analysis of variance"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "The Feedback-Related Negativity and the P300 Brain Potential Are Sensitive to Price Expectation Violations in a Virtual Shopping Task",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84992187078&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0163150&type=printable"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huys Q.J.M."
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bpsc.2016.08.001",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "382-385",
      "publication": {
        "category": "Journal",
        "cite_score": 9.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "24519022",
        "publisher": "Elsevier Inc.",
        "sjr": 1.997,
        "snip": 1.337,
        "subject_areas": [
          "Neurology (clinical)",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "Computational Psychiatry: From Mechanistic Insights to the Development of New Treatments",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84995685008&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Bauer R."
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.clinph.2016.06.020",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "3156-3164",
      "publication": {
        "category": "Journal",
        "cite_score": 7.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13882457",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 1.212,
        "snip": 1.482,
        "subject_areas": [
          "Neurology (clinical)",
          "Physiology (medical)",
          "Sensory Systems",
          "Neurology"
        ],
        "title": "Clinical Neurophysiology"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "Closed-loop adaptation of neurofeedback based on mental effort facilitates reinforcement learning of brain self-regulation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84979524743&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rodriguez Buritica J.M."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/desc.12317",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "699-709",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Developmental science"
      },
      "publication_date": "2016-09-01",
      "selected": null,
      "title": "Electrophysiological correlates of observational learning in children",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028281325&origin=inward"
      ]
    },
    {
      "abstract": "Intermittent feedback control for stabilizing human upright stance is a promising strategy, alternative to the standard time-continuous stiffness control. Here we show that such an intermittent controller can be established naturally through reinforcement learning. To this end, we used a single inverted pendulum model of the upright posture and a very simple reward function that gives a certain amount of punishments when the inverted pendulum falls or changes its position in the state space. We found that the acquired feedback controller exhibits hallmarks of the intermittent feedback control strategy, namely the action of the feedback controller is switched-off intermittently when the state of the pendulum is located near the stable manifold of the unstable saddle-type upright equilibrium of the inverted pendulum with no active control: this action provides an opportunity to exploit transiently converging dynamics toward the unstable upright position with no help of the active feedback control. We then speculate about a possible physiological mechanism of such reinforcement learning, and suggest that it may be related to the neural activity in the pedunculopontine tegmental nucleus (PPN) of the brainstem. This hypothesis is supported by recent evidence indicating that PPN might play critical roles for generation and regulation of postural tonus, reward prediction, as well as postural instability in patients with Parkinson's disease.",
      "authors": [
        "Kenjiro Michimoto",
        "Yasuyuki Suzuki",
        "Ken Kiyono",
        "Yasushi Kobayashi",
        "Pietro Morasso",
        "Taishin Nomura"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/EMBC.2016.7590634",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "37-40",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-4577-0219-8",
        "issn": "1557-170X",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS"
      },
      "publication_date": "2016-08-16",
      "selected": null,
      "title": "Reinforcement learning for stabilizing an inverted pendulum naturally leads to intermittent feedback control as in human quiet standing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85009079617&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7590634"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Aberg K.C."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuropsychologia.2016.05.023",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "1-13",
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00283932",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.995,
        "snip": 1.03,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neuropsychologia"
      },
      "publication_date": "2016-08-01",
      "selected": null,
      "title": "The left hemisphere learns what is right: Hemispatial reward learning depends on reinforcement learning processes in the contralateral hemisphere",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84971384339&origin=inward"
      ]
    },
    {
      "abstract": "A reinforcement learning agent learns an optimal policy under a certain environment with an evaluative reward function. In some applications, such an agent may lack sufficient adaptability to handle a variety of scenarios, similar to the one in the learning process. In other words, a learning agent is supposed to satisfy minor demands which are not part of the reward function. This paper proposes an interactive approach to accommodating human reinforcement and environmental rewards in a shaped reinforcement function, which can coach a robot despite goal modification or an inaccurate reward. The proposed approach coaches a robot, already equipped with a reinforcement learning mechanism, by human reinforcement feedback to conquer insufficiencies or shortsightedness in the environmental reward function. The proposed reinforcement learning algorithm links direct policy evaluation and human reinforcement to autonomous robots, accordingly shaping the reward function by combining both reinforcement signals. The technique of relative information entropy was applied to provide more effective learning for solving the conflict between human reinforcement and the robot\u2019s core policy. In this work, the human coaching is conveyed by a bystander\u2019s facial expressions. The transformation of facial expressions to a scalar index was processed by a type-2 fuzzy system. The simulated and experimental results show that a short-sighted robot could walk successfully through a swamp, and an under-powered car could reach the tip of a mountain with coaching from a bystander. The learning system worked quickly enough that the robot could continually adapt to an altered goal or environment.",
      "authors": [
        "Hwang, Kao-Shing",
        "Lin, Jin-Ling",
        "Shi, Haobin",
        "Chen, Yu-Ying"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s40815-016-0194-9",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "618-629",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15622479",
        "publisher": "Springer Berlin Heidelberg",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Journal of Fuzzy Systems"
      },
      "publication_date": "2016-08-01",
      "selected": null,
      "title": "Policy Learning with Human Reinforcement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84979656476&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s40815-016-0194-9.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Heydari S."
      ],
      "categories": null,
      "citations": 54,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12673",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1185-1192",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychophysiology"
      },
      "publication_date": "2016-08-01",
      "selected": null,
      "title": "Reward positivity: Reward prediction error or salience prediction error?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85027941748&origin=inward"
      ]
    },
    {
      "abstract": "The goal of this article is to investigate how human participants allocate their limited time to decisions with different properties. We report the results of two behavioral experiments. In each trial of the experiments, the participant must accumulate noisy information to make a decision. The participants received positive and negative rewards for their correct and incorrect decisions, respectively. The stimulus was designed such that decisions based on more accumulated information were more accurate but took longer. Therefore, the total outcome that a participant could achieve during the limited experiments' time depended on her \"decision threshold\", the amount of information she needed to make a decision. In the first experiment, two types of trials were intermixed randomly: hard and easy. Crucially, the hard trials were associated with smaller positive and negative rewards than the easy trials. A cue presented at the beginning of each trial would indicate the type of the upcoming trial. The optimal strategy was to adopt a small decision threshold for hard trials. The results showed that several of the participants did not learn this simple strategy. We then investigated how the participants adjusted their decision threshold based on the feedback they received in each trial. To this end, we developed and compared 10 computational models for adjusting the decision threshold. The models differ in their assumptions on the shape of the decision thresholds and the way the feedback is used to adjust the decision thresholds. The results of Bayesian model comparison showed that a model with time-varying thresholds whose parameters are updated by a reinforcement learning algorithm is the most likely model.",
      "authors": [
        "Khodadadi, Arash",
        "Fakhari, Pegah",
        "Busemeyer, Jerome R."
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2016-07-18",
      "selected": null,
      "title": "Learning to Allocate Limited Time to Decisions with Different Expected Outcomes",
      "urls": [
        "http://arxiv.org/pdf/1607.05334.pdf",
        "http://arxiv.org/abs/1607.05334v1",
        "http://arxiv.org/pdf/1607.05334v1"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Naros G."
      ],
      "categories": null,
      "citations": 58,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2016.03.016",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "142-152",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2016-07-01",
      "selected": null,
      "title": "Reinforcement learning of self-regulated sensorimotor \u03b2-oscillations improves motor performance",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84963958354&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Farreny A."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.comppsych.2016.04.011",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "119-128",
      "publication": {
        "category": "Journal",
        "cite_score": 11.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0010440X",
        "publisher": "W.B. Saunders",
        "sjr": 1.869,
        "snip": 2.058,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Comprehensive Psychiatry"
      },
      "publication_date": "2016-07-01",
      "selected": null,
      "title": "Study of positive and negative feedback sensitivity in psychosis using the Wisconsin Card Sorting Test",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964573736&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Collins A."
      ],
      "categories": null,
      "citations": 46,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.cognition.2016.04.002",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "160-169",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00100277",
        "publisher": "Elsevier B.V.",
        "sjr": 1.691,
        "snip": 1.813,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Developmental and Educational Psychology",
          "Cognitive Neuroscience",
          "Language and Linguistics",
          "Linguistics and Language"
        ],
        "title": "Cognition"
      },
      "publication_date": "2016-07-01",
      "selected": null,
      "title": "Neural signature of hierarchically structured expectations predicts clustering and transfer of rule sets in reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84962889642&origin=inward"
      ]
    },
    {
      "abstract": "In this paper, we present an interactive learning and adaptation framework. The framework combines Interactive Reinforcement Learning methods to effectively adapt and refine a learned policy to cope with new users. We argue that implicit feedback provided by the primary user and guidance from a secondary user can be integrated to the adaptation mechanism, resulting at a tailored and safe interaction. We illustrate this framework with a use case in Robot Assisted Therapy, presenting a Robot Yoga Trainer that monitors a yoga training session, adjusting the session parameters based on human motion activity recognition and evaluation through depth data, to assist the user complete the session, following a Reinforcement Learning approach.",
      "authors": [
        "Konstantinos Tsiakas",
        "Michalis Papakostas",
        "Benjamin Chebaa",
        "Dylan Ebert",
        "Vangelis Karkaletsis",
        "Fillia Makedon"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/2910674.2935857",
      "keywords": [
        "Interactive Reinforcement Learning",
        "Policy Adaptation",
        "Adaptive Robot Assisted Therapy"
      ],
      "number_of_pages": 4,
      "pages": "1-4",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450343374",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments"
      },
      "publication_date": "2016-06-29",
      "selected": null,
      "title": "An Interactive Learning and Adaptation Framework for Adaptive Robot Assisted Therapy",
      "urls": [
        "https://dl.acm.org/doi/10.1145/2910674.2935857",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85005990789&origin=inward"
      ]
    },
    {
      "abstract": "This paper contributes a preliminary report on the advantages and disadvantages of incorporating simultaneous human control and feedback signals in the training of a reinforcement learning robotic agent. While robotic human-machine interfaces have become increasingly complex in both form and function, control remains challenging for users. This has resulted in an increasing gap between user control approaches and the number of robotic motors which can be controlled. One way to address this gap is to shift some autonomy to the robot. Semi-autonomous actions of the robotic agent can then be shaped by human feedback, simplifying user control. Most prior work on agent shaping by humans has incorporated training with feedback, or has included indirect control signals. By contrast, in this paper we explore how a human can provide concurrent feedback signals and real-time myoelectric control signals to train a robot's actor-critic reinforcement learning control system. Using both a physical and a simulated robotic system, we compare training performance on a simple movement task when reward is derived from the environment, when reward is provided by the human, and combinations of these two approaches. Our results indicate that some benefit can be gained with the inclusion of human generated feedback.",
      "authors": [
        "Mathewson, Kory W.",
        "Pilarski, Patrick M."
      ],
      "categories": null,
      "citations": null,
      "comments": "7 pages, 3 figures, Accepted at the Interactive Machine Learning\n  Workshop at IJCAI 2016 (IML): Connecting Humans and Machines",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2016-06-22",
      "selected": null,
      "title": "Simultaneous Control and Human Feedback in the Training of a Robotic Agent with Actor-Critic Reinforcement Learning",
      "urls": [
        "http://arxiv.org/abs/1606.06979v1",
        "http://arxiv.org/pdf/1606.06979v1",
        "http://arxiv.org/pdf/1606.06979.pdf"
      ]
    },
    {
      "abstract": "<p>Dopamine D<sub>2/3</sub> receptor signaling is critical for flexible adaptive behavior; however, it is unclear whether D<sub>2</sub>, D<sub>3</sub>, or both receptor subtypes modulate precise signals of feedback and reward history that underlie optimal decision making. Here, PET with the radioligand [<sup>11</sup>C]-(+)-PHNO was used to quantify individual differences in putative D<sub>3</sub> receptor availability in rodents trained on a novel three-choice spatial acquisition and reversal-learning task with probabilistic reinforcement. Binding of [<sup>11</sup>C]-(+)-PHNO in the midbrain was negatively related to the ability of rats to adapt to changes in rewarded locations, but not to the initial learning. Computational modeling of choice behavior in the reversal phase indicated that [<sup>11</sup>C]-(+)-PHNO binding in the midbrain was related to the learning rate and sensitivity to positive, but not negative, feedback. Administration of a D<sub>3</sub>-preferring agonist likewise impaired reversal performance by reducing the learning rate and sensitivity to positive feedback. These results demonstrate a previously unrecognized role for D<sub>3</sub> receptors in select aspects of reinforcement learning and suggest that individual variation in midbrain D<sub>3</sub> receptors influences flexible behavior. Our combined neuroimaging, behavioral, pharmacological, and computational approach implicates the dopamine D<sub>3</sub> receptor in decision-making processes that are altered in psychiatric disorders.</p><p><b>SIGNIFICANCE STATEMENT</b> Flexible decision-making behavior is dependent upon dopamine D<sub>2/3</sub> signaling in corticostriatal brain regions. However, the role of D<sub>3</sub> receptors in adaptive, goal-directed behavior has not been thoroughly investigated. By combining PET imaging with the D<sub>3</sub>-preferring radioligand [<sup>11</sup>C]-(+)-PHNO, pharmacology, a novel three-choice probabilistic discrimination and reversal task and computational modeling of behavior in rats, we report that naturally occurring variation in [<sup>11</sup>C]-(+)-PHNO receptor availability relates to specific aspects of flexible decision making. We confirm these relationships using a D<sub>3</sub>-preferring agonist, thus identifying a unique role of midbrain D<sub>3</sub> receptors in decision-making processes.</p>",
      "authors": [
        "Stephanie M. Groman",
        "Nathaniel J. Smith",
        "J. Ryan Petrullli",
        "Bart Massi",
        "Lihui Chen",
        "Jim Ropchan",
        "Yiyun Huang",
        "Daeyeol Lee",
        "Evan D. Morris",
        "Jane R. Taylor"
      ],
      "categories": null,
      "citations": 27,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.3253-15.2016",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "6732-6741",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2016-06-22",
      "selected": null,
      "title": "Dopamine D3 Receptor Availability Is Associated with Inflexible Decision Making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84975231527&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Abrahamyan A."
      ],
      "categories": null,
      "citations": 100,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.1518786113",
      "keywords": [],
      "number_of_pages": null,
      "pages": "E3548-E3557",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2016-06-21",
      "selected": null,
      "title": "Adaptable history biases in human perceptual decisions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84975796904&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "McDougle S.D."
      ],
      "categories": null,
      "citations": 43,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.1523669113",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "6797-6802",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2016-06-14",
      "selected": null,
      "title": "Credit assignment in movement-dependent reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84974784188&origin=inward"
      ]
    },
    {
      "abstract": "Anhedonia has long been recognized as a central feature of major depression, yet its neurobiological underpinnings remain poorly understood. While clinical definitions of anhedonia have historically emphasized reductions in pleasure and positive emotionality, there...",
      "authors": [
        "Treadway, Michael T."
      ],
      "categories": null,
      "citations": 41,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/7854_2015_400",
      "keywords": [],
      "number_of_pages": 19,
      "pages": "337-355",
      "publication": {
        "category": "Book",
        "cite_score": 3.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18663370",
        "publisher": "Springer Verlag",
        "sjr": 1.545,
        "snip": 2.089,
        "subject_areas": [
          "Behavioral Neuroscience"
        ],
        "title": "Current Topics in Behavioral Neurosciences"
      },
      "publication_date": "2016-06-01",
      "selected": null,
      "title": "The Neurobiology of Motivational Deficits in Depression\u2014An Update on Candidate Pathomechanisms",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/7854_2015_400.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84975859239&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Shephard E."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.ijdevneu.2016.04.005",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "17-27",
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "07365748",
        "publisher": "John Wiley & Sons Inc.",
        "sjr": 0.64,
        "snip": 0.744,
        "subject_areas": [
          "Developmental Neuroscience",
          "Developmental Biology"
        ],
        "title": "International Journal of Developmental Neuroscience"
      },
      "publication_date": "2016-06-01",
      "selected": null,
      "title": "Electrophysiological correlates of reinforcement learning in young people with Tourette syndrome with and without co-occurring ADHD symptoms",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964389301&origin=inward"
      ]
    },
    {
      "abstract": "Could a pat on the back affect motor adaptation? Recent studies indeed suggest that rewards can boost motor adaptation. However, the rewards used were typically reward gradients that carried quite detailed information about performance. We investigated whether simple binary rewards affected how participants learned to correct for a visual rotation of performance feedback in a 3D pointing task. To do so, we asked participants to align their unseen hand with virtual target cubes in alternating blocks with and without spatial performance feedback. Forty participants were assigned to one of two groups: a \u2018spatial only\u2019 group, in which the feedback consisted of showing the (perturbed) endpoint of the hand, or to a \u2018spatial &amp; reward\u2019 group, in which a reward could be received in addition to the spatial feedback. In addition, six participants were tested in a \u2018reward only\u2019 group. Binary reward was given when the participants\u2019 hand landed in a virtual \u2018hit area\u2019 that was adapted to individual performance to reward about half the trials. The results show a typical pattern of adaptation in both the \u2018spatial only\u2019 and the \u2018spatial &amp; reward\u2019 groups, whereas the \u2018reward only\u2019 group was unable to adapt. The rewards did not affect the overall pattern of adaptation in the \u2018spatial &amp; reward\u2019 group. However, on a trial-by-trial basis, the rewards reduced adaptive changes to spatial errors.",
      "authors": [
        "van der Kooij, K.",
        "Overvliet, K. E."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00221-015-4540-1",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "1441-1450",
      "publication": {
        "category": "Journal",
        "cite_score": 3.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00144819",
        "publisher": "Springer Verlag",
        "sjr": 0.662,
        "snip": 0.85,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Experimental Brain Research"
      },
      "publication_date": "2016-06-01",
      "selected": null,
      "title": "Rewarding imperfect motor performance reduces adaptive changes",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00221-015-4540-1.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84954320617&origin=inward"
      ]
    },
    {
      "abstract": "Machine learning (ML) is the fastest growing field in computer science, and health informatics is among the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic machine learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive machine learning (iML) may be of help, having its roots in reinforcement learning, preference learning, and active learning. The term iML is not yet well used, so we define it as \u201calgorithms that can interact with agents and can optimize their learning behavior through these interactions, where the agents can also be human.\u201d This \u201chuman-in-the-loop\u201d can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.",
      "authors": [
        "Holzinger, Andreas"
      ],
      "categories": null,
      "citations": 581,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s40708-016-0042-6",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "119-131",
      "publication": {
        "category": "Journal",
        "cite_score": 7.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21984018",
        "publisher": "Springer Berlin",
        "sjr": 0.986,
        "snip": 1.609,
        "subject_areas": [
          "Computer Science Applications",
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "Brain Informatics"
      },
      "publication_date": "2016-06-01",
      "selected": null,
      "title": "Interactive machine learning for health informatics: when do we need the human-in-the-loop?",
      "urls": [
        "https://braininformatics.springeropen.com/counter/pdf/10.1007/s40708-016-0042-6",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85048560245&origin=inward"
      ]
    },
    {
      "abstract": "Adolescence is a period of life characterised by changes in learning and decision-making. Learning and decision-making do not rely on a unitary system, but instead require the coordination of different cognitive processes that can be mathematically formalised as dissociable computational modules. Here, we aimed to trace the developmental time-course of the computational modules responsible for learning from reward or punishment, and learning from counterfactual feedback. Adolescents and adults carried out a novel reinforcement learning paradigm in which participants learned the association between cues and probabilistic outcomes, where the outcomes differed in valence (reward versus punishment) and feedback was either partial or complete (either the outcome of the chosen option only, or the outcomes of both the chosen and unchosen option, were displayed). Computational strategies changed during development: whereas adolescents\u2019 behaviour was better explained by a basic reinforcement learning algorithm, adults\u2019 behaviour integrated increasingly complex computational features, namely a counterfactual learning module (enabling enhanced performance in the presence of complete feedback) and a value contextualisation module (enabling symmetrical reward and punishment learning). Unlike adults, adolescent performance did not benefit from counterfactual (complete) feedback. In addition, while adults learned symmetrically from both reward and punishment, adolescents learned from reward but were less likely to learn from punishment. This tendency to rely on rewards and not to consider alternative consequences of actions might contribute to our understanding of decision-making in adolescence.",
      "authors": [
        "Stefano Palminteri",
        "Emma J. Kilford",
        "Giorgio Coricelli",
        "Sarah-Jayne Blakemore"
      ],
      "categories": null,
      "citations": 70,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1004953",
      "keywords": [
        "Age groups",
        "Learning",
        "Human learning",
        "Decision making",
        "Simulation and modeling",
        "Reaction time",
        "Statistical models",
        "Adolescents"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2016-06-01",
      "selected": null,
      "title": "The Computational Development of Reinforcement Learning during Adolescence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84978872482&origin=inward",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1004953&type=printable"
      ]
    },
    {
      "abstract": "Background: Regular physical activity is known to be beneficial for people with type 2 diabetes. Nevertheless, most of the people who have diabetes lead a sedentary lifestyle. Smartphones create new possibilities for helping people to adhere to their physical activity goals through continuous monitoring and communication, coupled with personalized feedback. Objective: The aim of this study was to help type 2 diabetes patients increase the level of their physical activity. Methods: We provided 27 sedentary type 2 diabetes patients with a smartphone-based pedometer and a personal plan for physical activity. Patients were sent short message service messages to encourage physical activity between once a day and once per week. Messages were personalized through a Reinforcement Learning algorithm so as to improve each participant\u2019s compliance with the activity regimen. The algorithm was compared with a static policy for sending messages and weekly reminders. Results: Our results show that participants who received messages generated by the learning algorithm increased the amount of activity and pace of walking, whereas the control group patients did not. Patients assigned to the learning algorithm group experienced a superior reduction in blood glucose levels (glycated hemoglobin [HbA1c]) compared with control policies, and longer participation caused greater reductions in blood glucose levels. The learning algorithm improved gradually in predicting which messages would lead participants to exercise. Conclusions: Mobile phone apps coupled with a learning algorithm can improve adherence to exercise in diabetic patients. This algorithm can be used in large populations of diabetic patients to improve health and glycemic control. Our results can be expanded to other areas where computer-led health coaching of humans may have a positive impact. Summary of a part of this manuscript has been previously published as a letter in Diabetes Care, 2016. ",
      "authors": [
        "Irit Hochberg",
        "Guy Feraru",
        "Mark Kozdoba",
        "Shie Mannor",
        "Moshe Tennenholtz",
        "Elad Yom-Tov"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": "10.2196/jmir.7994",
      "keywords": [
        "physical activity",
        "reinforcement learning",
        "diabetes type 2"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "JMIR Publications Inc., Toronto, Canada",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Medical Internet Research"
      },
      "publication_date": "2016-05-13",
      "selected": null,
      "title": "Encouraging Physical Activity in Patients With Diabetes: Intervention Using a Reinforcement Learning System",
      "urls": [
        "http://arxiv.org/abs/1605.04070v1",
        "http://arxiv.org/pdf/1605.04070v1",
        "http://dx.doi.org/10.2196/jmir.7994"
      ]
    },
    {
      "abstract": "As robots become pervasive in human environments, it is important to enable users to effectively convey new skills without programming. Most existing work on Interactive Reinforcement Learning focuses on interpreting and incorporating non-expert human feedback to speed up learning; we aim to design a better representation of the learning agent that is able to elicit more natural and effective communication between the human trainer and the learner, while treating human feedback as discrete communication that depends probabilistically on the trainer's target policy. This work entails a user study where participants train a virtual agent to accomplish tasks by giving reward and/or punishment in a variety of simulated environments. We present results from 60 participants to show how a learner can ground natural language commands and adapt its action execution speed to learn more efficiently from human trainers. The agent's action execution speed can be successfully modulated to encourage more explicit feedback from a human trainer in areas of the state space where there is high uncertainty. Our results show that our novel adaptive speed agent dominates different fixed speed agents on several measures of performance. Additionally, we investigate the impact of instructions on user performance and user preference in training conditions.",
      "authors": [
        "Bei Peng",
        "James MacGlashan",
        "Robert Loftin",
        "Michael L. Littman",
        "David L. Roberts",
        "Matthew E. Taylor"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2936924.2937065",
      "keywords": [
        "human-agent interaction",
        "eliciting human feedback",
        "crowdsourcing experiments",
        "learning sequential decision tasks from humans",
        "variable speed agents"
      ],
      "number_of_pages": 9,
      "pages": "957-965",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450342391",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems"
      },
      "publication_date": "2016-05-09",
      "selected": null,
      "title": "A Need for Speed: Adapting Agent Action Speed to Improve Task Learning from Non-Expert Humans",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2936924.2937065"
      ]
    },
    {
      "abstract": "Reinforcement Learning (RL) has been effectively used to solve complex problems given careful design of the problem and algorithm parameters. However standard RL approaches do not scale particularly well with the size of the problem and often require extensive engineering on the part of the designer to minimize the search space. To alleviate this problem, we present a model-free policy-based approach called Exploration from Demonstration (EfD) that uses human demonstrations to guide search space exploration. We use statistical measures of RL algorithms to provide feedback to the user about the agent's uncertainty and use this to solicit targeted demonstrations useful from the agent's perspective. The demonstrations are used to learn an exploration policy that actively guides the agent towards important aspects of the problem. We instantiate our approach in a gridworld and a popular arcade game and validate its performance under different experimental conditions. We show how EfD scales to large problems and provides convergence speed-ups over traditional exploration and interactive learning methods.",
      "authors": [
        "Kaushik Subramanian",
        "Charles L. Isbell",
        "Andrea L. Thomaz"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2936924.2936990",
      "keywords": [
        "human-agent interaction",
        "active learning",
        "reinforcement learning",
        "exploration"
      ],
      "number_of_pages": 10,
      "pages": "447-456",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450342391",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems"
      },
      "publication_date": "2016-05-09",
      "selected": null,
      "title": "Exploration from Demonstration for Interactive Reinforcement Learning",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2936924.2936990"
      ]
    },
    {
      "abstract": "The TAMER framework provides a way for agents to learn to solve tasks using human-generated rewards. Previous research showed that humans give copious feedback early in training but very sparsely thereafter and that an agent's competitive feedback --- informing the trainer about its performance relative to other trainers --- can greatly affect the trainer's engagement and the agent's learning. In this paper, we present the first large-scale study of TAMER, involving 561 subjects, which investigates the effect of the agent's competitive feedback in a new setting as well as the potential for learning from trainers' facial expressions. Our results show for the first time that a TAMER agent can successfully learn to play Infinite Mario, a challenging reinforcement-learning benchmark problem. In addition, our study supports prior results demonstrating the importance of bi-directional feedback and competitive elements in the training interface. Finally, our results shed light on the potential for using trainers' facial expressions as reward signals, as well as the role of age and gender in trainer behavior and agent performance.",
      "authors": [
        "Guangliang Li",
        "Hamdi Dibeklioglu",
        "Shimon Whiteson",
        "Hayley Hung"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2936924.2937156",
      "keywords": [
        "human agent interaction",
        "reinforcement learning",
        "facial expression"
      ],
      "number_of_pages": 2,
      "pages": "1353-1354",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450342391",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems"
      },
      "publication_date": "2016-05-09",
      "selected": null,
      "title": "Towards Learning from Implicit Human Reward: (Extended Abstract)",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2936924.2937156"
      ]
    },
    {
      "abstract": "The challenges of robotic software testing extend beyond conventional\nsoftware testing. Valid, realistic and interesting tests need to be generated\nfor multiple programs and hardware running concurrently, deployed into dynamic\nenvironments with people. We investigate the use of Belief-Desire-Intention\n(BDI) agents as models for test generation, in the domain of human-robot\ninteraction (HRI) in simulations. These models provide rational agency,\ncausality, and a reasoning mechanism for planning, which emulate both\nintelligent and adaptive robots, as well as smart testing environments directed\nby humans. We introduce reinforcement learning (RL) to automate the exploration\nof the BDI models using a reward function based on coverage feedback. Our\napproach is evaluated using a collaborative manufacture example, where the\nrobotic software under test is stimulated indirectly via a simulated human\nco-worker. We conclude that BDI agents provide intuitive models for test\ngeneration in the HRI domain. Our results demonstrate that RL can fully\nautomate BDI model exploration, leading to very effective coverage-directed\ntest generation.",
      "authors": [
        "Dejanira Araiza-Illan",
        "Anthony G. Pipe",
        "Kerstin Eder"
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1145/3022099.3022101",
      "keywords": [
        "Verification agents",
        "Reinforcement learning",
        "Coverage-directed test generation",
        "Belief-Desire-Intention agents",
        "Simulation-based testing",
        "Human-robot interaction",
        "Model-based test generation"
      ],
      "number_of_pages": 8,
      "pages": "9-16",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450342599",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 3rd Workshop on Model-Driven Robot Software Engineering"
      },
      "publication_date": "2016-04-19",
      "selected": null,
      "title": "Intelligent Agent-Based Stimulation for Testing Robotic Software in Human-Robot Interactions",
      "urls": [
        "http://dx.doi.org/10.1145/3022099.3022101",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85018302263&origin=inward",
        "http://arxiv.org/abs/1604.05508v3",
        "https://dl.acm.org/doi/10.1145/3022099.3022101",
        "http://arxiv.org/pdf/1604.05508v3"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Zendehrouh S."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neulet.2016.02.062",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "110-114",
      "publication": {
        "category": "Journal",
        "cite_score": 5.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03043940",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 0.802,
        "snip": 0.777,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Neuroscience Letters"
      },
      "publication_date": "2016-04-08",
      "selected": null,
      "title": "The role of time in conflict-triggered control: Extending the theory of response-conflict monitoring",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84960415464&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Diekhof E.K."
      ],
      "categories": null,
      "citations": 42,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuropsychologia.2015.10.016",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "70-80",
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00283932",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.995,
        "snip": 1.03,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neuropsychologia"
      },
      "publication_date": "2016-04-01",
      "selected": null,
      "title": "Menstrual cycle phase modulates reward sensitivity and performance monitoring in young women: Preliminary fMRI evidence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84957584828&origin=inward"
      ]
    },
    {
      "abstract": "The negative symptoms of schizophrenia (SZ) are associated with a pattern of reinforcement learning (RL) deficits likely related to degraded representations of reward values. However, the RL tasks used to date have required active responses to both reward and punishing stimuli. Pavlovian biases have been shown to affect performance on these tasks through invigoration of action to reward and inhibition of action to punishment, and may be partially responsible for the effects found in patients. Forty-five patients with schizophrenia and 30 demographically-matched controls completed a four-stimulus reinforcement learning task that crossed action (\u201cGo\u201d or \u201cNoGo\u201d) and the valence of the optimal outcome (reward or punishment-avoidance), such that all combinations of action and outcome valence were tested. Behaviour was modelled using a six-parameter RL model and EEG was simultaneously recorded. Patients demonstrated a reduction in Pavlovian performance bias that was evident in a reduced Go bias across the full group. In a subset of patients administered clozapine, the reduction in Pavlovian bias was enhanced. The reduction in Pavlovian bias in SZ patients was accompanied by feedback processing differences at the time of the P3a component. The reduced Pavlovian bias in patients is suggested to be due to reduced fidelity in the communication between striatal regions and frontal cortex. It may also partially account for previous findings of poorer \u201cGo-learning\u201d in schizophrenia where \u201cGo\u201d responses or Pavlovian consistent responses are required for optimal performance. An attenuated P3a component dynamic in patients is consistent with a view that deficits in operant learning are due to impairments in adaptively using feedback to update representations of stimulus value.",
      "authors": [
        "Matthew A. Albrecht",
        "James A. Waltz",
        "James F. Cavanagh",
        "Michael J. Frank",
        "James M. Gold"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0152781",
      "keywords": [
        "Electroencephalography",
        "Schizophrenia",
        "Antipsychotics",
        "Dopamine",
        "Learning",
        "Behavior",
        "Human learning",
        "Event-related potentials"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2016-04-01",
      "selected": null,
      "title": "Reduction of Pavlovian Bias in Schizophrenia: Enhanced Effects in Clozapine-Administered Patients",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0152781&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84963699510&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hochberg I."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.2337/dc15-2340",
      "keywords": [],
      "number_of_pages": null,
      "pages": "e59-e60",
      "publication": {
        "category": "Journal",
        "cite_score": 27.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01495992",
        "publisher": "American Diabetes Association Inc.",
        "sjr": 6.008,
        "snip": 4.781,
        "subject_areas": [
          "Internal Medicine",
          "Endocrinology, Diabetes and Metabolism",
          "Advanced and Specialized Nursing"
        ],
        "title": "Diabetes Care"
      },
      "publication_date": "2016-04-01",
      "selected": null,
      "title": "Encouraging physical activity in patients with diabetes through automatic personalized feedback via reinforcement learning improves glycemic control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84962053103&origin=inward"
      ]
    },
    {
      "abstract": "Previous studies on the neurophysiological underpinnings of feedback processing almost exclusively used low-ambiguity feedback, which does not fully address the diversity of situations in everyday life. We therefore used a pseudo trial-and-error learning task to investigate ERPs of low-versus high-ambiguity feedback. Twenty-eight participants tried to deduce the rule governing visual feedback to their button presses in response to visual stimuli. In the blocked condition, the same two feedback words were presented across several consecutive trials, whereas in the random condition feedback was randomly drawn on each trial from sets of five positive and five negative words. The feedback-related negativity FRN-D, a frontocentral ERP difference between negative and positive feedback, was significantly larger in the blocked condition, whereas the centroparietal late positive complex indicating controlled attention was enhanced for negative feedback irrespective of condition. Moreover, FRN-D in the blocked condition was due to increased reward positivity Rew-P for positive feedback, rather than increased raw FRN for negative feedback. Our findings strongly support recent lines of evidence that the FRN-D, one of the most widely studied signatures of reinforcement learning in the human brain, critically depends on feedback discriminability and is primarily driven by the Rew-P. A novel finding concerned larger frontocentral P2 for negative feedback in the random but not the blocked condition. Although Rew-P points to a positivity bias in feedback processing under conditions of low feedback ambiguity, P2 suggests a specific adaptation of information processing in case of highly ambiguous feedback, involving an early negativity bias. Generalizability of the P2 findings was demonstrated in a second experiment using explicit valence categorization of highly emotional positive and negative adjectives.",
      "authors": [
        "Henning Gibbons",
        "Robert Schnuerch",
        "Jutta Stahl"
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1162/jocn_a_00921",
      "keywords": [],
      "number_of_pages": 16,
      "pages": "542-557",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898-929X",
        "publisher": "MIT Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2016-04-01",
      "selected": null,
      "title": "From positivity to negativity bias: Ambiguity affects the neurophysiological signatures of feedback processing",
      "urls": [
        "https://dl.acm.org/doi/10.1162/jocn_a_00921",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84959260795&origin=inward"
      ]
    },
    {
      "abstract": "<p>Learning in a new environment is influenced by prior learning and experience. Correctly applying a rule that maps a context to stimuli, actions, and outcomes enables faster learning and better outcomes compared to relying on strategies for learning that are ignorant of task structure. However, it is often difficult to know when and how to apply learned rules in new contexts. In our study we explored how subjects employ different strategies for learning the relationship between stimulus features and positive outcomes in a probabilistic task context. We test the hypothesis that task naive subjects will show enhanced learning of feature specific reward associations by switching to the use of an abstract rule that associates stimuli by feature type and restricts selections to that dimension. To test this hypothesis we designed a decision making task where subjects receive probabilistic feedback following choices between pairs of stimuli. In the task, trials are grouped in two contexts by blocks, where in one type of block there is no unique relationship between a specific feature dimension (stimulus shape or color) and positive outcomes, and following an un-cued transition, alternating blocks have outcomes that are linked to either stimulus shape or color. Two-thirds of subjects (<italic>n</italic> = 22/32) exhibited behavior that was best fit by a hierarchical feature-rule model. Supporting the prediction of the model mechanism these subjects showed significantly enhanced performance in feature-reward blocks, and rapidly switched their choice strategy to using abstract feature rules when reward contingencies changed. Choice behavior of other subjects (<italic>n</italic> = 10/32) was fit by a range of alternative reinforcement learning models representing strategies that do not benefit from applying previously learned rules. In summary, these results show that untrained subjects are capable of flexibly shifting between behavioral rules by leveraging simple model-free reinforcement learning and context-specific selections to drive responses.</p>",
      "authors": [
        "Balcarras, Matthew",
        "Womelsdorf, Thilo"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2016.00125",
      "keywords": [
        "flexible",
        "reinforcement learning",
        "rule selection",
        "Model-free",
        "Value-based Decision Making"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2016-03-30",
      "selected": null,
      "title": "A Flexible Mechanism of Rule Selection Enables Rapid Feature-Based Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964345551&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00125/pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Guggenmos M."
      ],
      "categories": null,
      "citations": 80,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7554/eLife.13388",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "eLife"
      },
      "publication_date": "2016-03-29",
      "selected": null,
      "title": "Mesolimbic confidence signals guide perceptual learning in the absence of external feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964267750&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "G\u00fctig R."
      ],
      "categories": null,
      "citations": 86,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1126/science.aab4113",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 59.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00368075",
        "publisher": "American Association for the Advancement of Science",
        "sjr": 13.328,
        "snip": 7.729,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Science"
      },
      "publication_date": "2016-03-04",
      "selected": null,
      "title": "Spiking neurons can discover predictive features by aggregate-label learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84961221485&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Baker T.E."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00905",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "460-471",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2016-03-01",
      "selected": null,
      "title": "Reward Sensitivity of ACC as an Intermediate Phenotype between DRD4-521T and Substance Misuse",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84956655137&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Deserno L."
      ],
      "categories": null,
      "citations": 33,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 13,
      "pages": "77-89",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "12948322",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Dialogues in Clinical Neuroscience"
      },
      "publication_date": "2016-03-01",
      "selected": null,
      "title": "Striatal dopamine, reward, and decision making in schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964587046&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Brudner S."
      ],
      "categories": null,
      "citations": 58,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00066.2015",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "1499-1511",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2016-03-01",
      "selected": null,
      "title": "Delayed feedback during sensorimotor learning selectively disrupts adaptation but not strategy use",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84984813329&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Happel M.F.K."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bbr.2015.11.016",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "32-41",
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01664328",
        "publisher": "Elsevier B.V.",
        "sjr": 0.881,
        "snip": 0.802,
        "subject_areas": [
          "Behavioral Neuroscience"
        ],
        "title": "Behavioural Brain Research"
      },
      "publication_date": "2016-02-15",
      "selected": null,
      "title": "Dopaminergic impact on local and global cortical circuit processing during learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84948783729&origin=inward"
      ]
    },
    {
      "abstract": "Large state spaces and the curse of dimensionality contribute to the complexity of a task. Learning from demonstration techniques can be combined with reinforcement learning to narrow the exploration space of an agent, but require consistent and accurate demonstrations, as well as the state-action pairs for an entire demonstration. Individuals with severe motor disabilities are often slow and prone to human errors in demonstrations while teaching. My dissertation develops tools to allow persons with severe motor disabilities, and individuals in general, to train these systems. To handle these large state spaces as well as human error, we developed Dimensionality Reduced Reinforcement Learning. To accommodate slower feedback, we will develop a movie-reel style learning from demonstration interface.",
      "authors": [
        "William Curran"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3016387.3016563",
      "keywords": [],
      "number_of_pages": 2,
      "pages": "4293-4294",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence"
      },
      "publication_date": "2016-02-12",
      "selected": null,
      "title": "Robust learning from demonstration techniques and tools",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3016387.3016563"
      ]
    },
    {
      "abstract": "Specifying a numeric reward function for reinforcement learning typically requires a lot of hand-tuning from a human expert. In contrast, preference-based reinforcement learning (PBRL) utilizes only pairwise comparisons between trajectories as a feedback signal, which are often more intuitive to specify. Currently available approaches to PBRL for control problems with continuous state/action spaces require a known or estimated model, which is often not available and hard to learn. In this paper, we integrate preference-based estimation of the reward function into a model-free reinforcement learning (RL) algorithm, resulting in a model-free PBRL algorithm. Our new algorithm is based on Relative Entropy Policy Search (REPS), enabling us to utilize stochastic policies and to directly control the greediness of the policy update. REPS decreases exploration of the policy slowly by limiting the relative entropy of the policy update, which ensures that the algorithm is provided with a versatile set of trajectories, and consequently with informative preferences. The preference-based estimation is computed using a sample-based Bayesian method, which can also estimate the uncertainty of the utility. Additionally, we also compare to a linear solvable approximation, based on inverse RL. We show that both approaches perform favourably to the current state-of-the-art. The overall result is an algorithm that can learn non-parametric continuous action policies from a small number of preferences.",
      "authors": [
        "Christian Wirth",
        "Johannes F\u00fcrnkranz",
        "Gerhard Neumann"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/3016100.3016209",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "2222-2228",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence"
      },
      "publication_date": "2016-02-12",
      "selected": null,
      "title": "Model-free preference-based reinforcement learning",
      "urls": [
        "https://dl.acm.org/doi/10.5555/3016100.3016209"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lam J.M."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroscience.2015.11.051",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "116-124",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03064522",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Neuroscience"
      },
      "publication_date": "2016-02-09",
      "selected": null,
      "title": "Impaired implicit learning and feedback processing after stroke",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84949921237&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Fr\u00f6mer R."
      ],
      "categories": null,
      "citations": 33,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2015.12.011",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "61-68",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2016-02-01",
      "selected": null,
      "title": "The better, the bigger: The effect of graded positive performance feedback on the reward positivity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84953236542&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Baker T.E."
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.clinph.2015.11.002",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1358-1365",
      "publication": {
        "category": "Journal",
        "cite_score": 7.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13882457",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 1.212,
        "snip": 1.482,
        "subject_areas": [
          "Neurology (clinical)",
          "Physiology (medical)",
          "Sensory Systems",
          "Neurology"
        ],
        "title": "Clinical Neurophysiology"
      },
      "publication_date": "2016-02-01",
      "selected": null,
      "title": "Atypical valuation of monetary and cigarette rewards in substance dependent smokers",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84957954614&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Kroemer N.B."
      ],
      "categories": null,
      "citations": 52,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.physbeh.2016.04.020",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "37-45",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00319384",
        "publisher": "Elsevier Inc.",
        "sjr": 0.753,
        "snip": 1.02,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience"
        ],
        "title": "Physiology and Behavior"
      },
      "publication_date": "2016-01-21",
      "selected": null,
      "title": "Fuel not fun: Reinterpreting attenuated brain responses to reward in obesity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964659796&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Williams C.C."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1097/WNR.0000000000000575",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "522-526",
      "publication": {
        "category": "Journal",
        "cite_score": 3.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09594965",
        "publisher": "Lippincott Williams and Wilkins Ltd.",
        "sjr": 0.426,
        "snip": 0.45,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "NeuroReport"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "The scarcity heuristic impacts reward processing within the medial-frontal cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84962073972&origin=inward"
      ]
    },
    {
      "abstract": "<p>Stroke patients with severe motor deficits of the upper extremity may practice rehabilitation exercises with the assistance of a multi-joint exoskeleton. Although this technology enables intensive task-oriented training, it may also lead to slacking when the assistance is too supportive. Preserving the engagement of the patients while providing \u201cassistance-as-needed\u201d during the exercises, therefore remains an ongoing challenge. We applied a commercially available seven degree-of-freedom arm exoskeleton to provide passive gravity compensation during task-oriented training in a virtual environment. During this 4-week pilot study, five severely affected chronic stroke patients performed reach-to-grasp exercises resembling activities of daily living. The subjects received virtual reality feedback from their three-dimensional movements. The level of difficulty for the exercise was adjusted by a performance-dependent real-time adaptation algorithm. The goal of this algorithm was the automated improvement of the range of motion. In the course of 20 training and feedback sessions, this unsupervised adaptive training concept led to a progressive increase of the virtual training space (<italic>p</italic> &lt; 0.001) in accordance with the subjects' abilities. This learning curve was paralleled by a concurrent improvement of real world kinematic parameters, i.e., range of motion (<italic>p</italic> = 0.008), accuracy of movement (<italic>p</italic> = 0.01), and movement velocity (<italic>p</italic> &lt; 0.001). Notably, these kinematic gains were paralleled by motor improvements such as increased elbow movement (<italic>p</italic> = 0.001), grip force (<italic>p</italic> &lt; 0.001), and upper extremity Fugl-Meyer-Assessment score from 14.3 \u00b1 5 to 16.9 \u00b1 6.1 (<italic>p</italic> = 0.026). Combining gravity-compensating assistance with adaptive closed-loop feedback in virtual reality provides customized rehabilitation environments for severely affected stroke patients. This approach may facilitate motor learning by progressively challenging the subject in accordance with the individual capacity for functional restoration. It might be necessary to apply concurrent restorative interventions to translate these improvements into relevant functional gains of severely motor impaired patients in activities of daily living.</p>",
      "authors": [
        "Grimm, Florian",
        "Naros, Georgios",
        "Gharabaghi, Alireza"
      ],
      "categories": null,
      "citations": 62,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2016.00518",
      "keywords": [
        "virtual reality",
        "motor recovery",
        "Functional restoration",
        "individualized therapy",
        "Feedback",
        "reinforcement learning",
        "hemiparesis",
        "Robot-assisted rehabilitation",
        "motor learning",
        "stroke rehabilitation",
        "upper-limb assistance"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Closed-Loop Task Difficulty Adaptation during Virtual Reality Reach-to-Grasp Training Assisted with an Exoskeleton for Stroke Rehabilitation",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00518/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85009810403&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Raza S.A."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 4,
      "pages": "414-417",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357568",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 29th International Florida Artificial Intelligence Research Society Conference, FLAIRS 2016"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Reward from demonstration in interactive reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85003837183&origin=inward"
      ]
    },
    {
      "abstract": "Although the use of neuroimaging techniques has revealed much about the neural correlates of social decision making (SDM) in humans, it remains poorly understood how social stimuli are represented, and how social decisions are implemented at the neural level in...",
      "authors": [
        "Hernandez-Lallement, Julen",
        "van Wingerden, Marijn",
        "Sch\u00e4ble, Sandra",
        "Kalenscher, Tobias"
      ],
      "categories": null,
      "citations": 18,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/7854_2016_436",
      "keywords": [],
      "number_of_pages": 18,
      "pages": "159-176",
      "publication": {
        "category": "Book",
        "cite_score": 3.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18663370",
        "publisher": "Springer Verlag",
        "sjr": 1.545,
        "snip": 2.089,
        "subject_areas": [
          "Behavioral Neuroscience"
        ],
        "title": "Current Topics in Behavioral Neurosciences"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "A Social Reinforcement Learning Hypothesis of Mutual Reward Preferences in Rats",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85013831261&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/7854_2016_436.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Wirth C."
      ],
      "categories": null,
      "citations": 44,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "2222-2228",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357605",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "30th AAAI Conference on Artificial Intelligence, AAAI 2016"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Model-free preference-based reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85007173791&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sotala K."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 11,
      "pages": "113-123",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357599",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAAI Workshop - Technical Report"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Defining human values for value learners",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85019229900&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "M\u00f6rkl S."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nlm.2015.12.001",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "48-55",
      "publication": {
        "category": "Journal",
        "cite_score": 5.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10747427",
        "publisher": "Academic Press Inc.",
        "sjr": 0.986,
        "snip": 0.765,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neurobiology of Learning and Memory"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Impaired probabilistic classification learning with feedback in patients with major depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84953433056&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Therrien A."
      ],
      "categories": null,
      "citations": 112,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/brain/awv329",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "101-114",
      "publication": {
        "category": "Journal",
        "cite_score": 20.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00068950",
        "publisher": "Oxford University Press",
        "sjr": 4.437,
        "snip": 3.147,
        "subject_areas": [
          "Neurology (clinical)"
        ],
        "title": "Brain"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Effective Reinforcement learning following cerebellar damage requires a balance between exploration and motor noise",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964612276&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Subramanian K."
      ],
      "categories": null,
      "citations": 60,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 10,
      "pages": "447-456",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Exploration from demonstration for interactive reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014185118&origin=inward"
      ]
    },
    {
      "abstract": "<p>In the domain of sports and motor rehabilitation, it is of major importance to regulate and control physiological processes and physical motion in most optimal ways. For that purpose, real-time auditory feedback of physiological and physical information based on sound signals, often termed \u201csonification,\u201d has been proven particularly useful. However, the use of music in biofeedback systems has been much less explored. In the current article, we assert that the use of music, and musical principles, can have a major added value, on top of mere sound signals, to the benefit of psychological and physical optimization of sports and motor rehabilitation tasks. In this article, we present the 3Mo model to describe three main functions of music that contribute to these benefits. These functions relate the power of music to Motivate, and to Monitor and Modify physiological and physical processes. The model brings together concepts and theories related to human sensorimotor interaction with music, and specifies the underlying psychological and physiological principles. This 3Mo model is intended to provide a conceptual framework that guides future research on musical biofeedback systems in the domain of sports and motor rehabilitation.</p>",
      "authors": [
        "Maes, Pieter-Jan",
        "Buhmann, Jeska",
        "Leman, Marc"
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2016.00548",
      "keywords": [
        "reinforcement learning4",
        "music interaction3",
        "auditory biofeedback2",
        "sonification1",
        "predictive processing5"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "3Mo: A Model for Music-Based Biofeedback",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00548/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85009812516&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Curran W."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 2,
      "pages": "4293-4294",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357605",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "30th AAAI Conference on Artificial Intelligence, AAAI 2016"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Robust learning from demonstration techniques and tools",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85007164441&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Li G."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 2,
      "pages": "1353-1354",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Towards learning from implicit human reward",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014285534&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Oudeyer P.Y."
      ],
      "categories": null,
      "citations": 191,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/bs.pbr.2016.05.005",
      "keywords": [],
      "number_of_pages": 28,
      "pages": "257-284",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00796123",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Progress in Brain Research"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Intrinsic motivation, curiosity, and learning: Theory and applications in educational technologies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84979942201&origin=inward"
      ]
    },
    {
      "abstract": "This chapter surveys the main approaches developed to date to endow robots with the ability to learn from human guidance. The field is best known as robot programming by demonstration, robot learning from/by demonstration, apprenticeship learning and imitation...",
      "authors": [
        "Billard, Aude G.",
        "Calinon, Sylvain",
        "Dillmann, R\u00fcdiger"
      ],
      "categories": null,
      "citations": 132,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-319-32552-1_74",
      "keywords": [],
      "number_of_pages": 20,
      "pages": "1995-2014",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9783319325521",
        "issn": "25228692",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Springer Handbook of Robotics"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Learning from Humans",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85021798480&origin=inward",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85136952623&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-319-32552-1_74.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Junges S."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 4,
      "pages": "185-188",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357759",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAAI Fall Symposium - Technical Report"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Probabilistic verification for cognitive models: Controller synthesis and model evaluation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85025834417&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Subbarao K."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.arcontrol.2016.09.021",
      "keywords": [],
      "number_of_pages": 13,
      "pages": "319-331",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13675788",
        "publisher": "Elsevier Ltd.",
        "sjr": 3.343,
        "snip": 4.271,
        "subject_areas": [
          "Software",
          "Control and Systems Engineering"
        ],
        "title": "Annual Reviews in Control"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Reinforcement learning based computational adaptive optimal control and system identification for linear systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84991199928&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Peng B."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "957-965",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "A need for speed: Adapting agent action speed to improve task learning from non-expert humans",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85014255335&origin=inward"
      ]
    },
    {
      "abstract": "For real-world applications, virtual agents must be able to learn new behaviors from non-technical users. Positive and negative feedback are an intuitive way to train new behaviors, and existing work has presented algorithms for learning from such feedback. That work, however, treats feedback as numeric reward to be maximized, and assumes that all trainers provide feedback in the same way. In this work, we show that users can provide feedback in many different ways, which we describe as \u201ctraining strategies.\u201d Specifically, users may not always give explicit feedback in response to an action, and may be more likely to provide explicit reward than explicit punishment, or vice versa, such that the lack of feedback itself conveys information about the behavior. We present a probabilistic model of trainer feedback that describes how a trainer chooses to provide explicit reward and/or explicit punishment and, based on this model, develop two novel learning algorithms (SABL and I-SABL) which take trainer strategy into account, and can therefore learn from cases where no feedback is provided. Through online user studies we demonstrate that these algorithms can learn with less feedback than algorithms based on a numerical interpretation of feedback. Furthermore, we conduct an empirical analysis of the training strategies employed by users, and of factors that can affect their choice of strategy.",
      "authors": [
        "Loftin, Robert",
        "Peng, Bei",
        "MacGlashan, James",
        "Littman, Michael L.",
        "Taylor, Matthew E.",
        "Huang, Jeff",
        "Roberts, David L."
      ],
      "categories": null,
      "citations": 68,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10458-015-9283-7",
      "keywords": [],
      "number_of_pages": 30,
      "pages": "30-59",
      "publication": {
        "category": "Journal",
        "cite_score": 5.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13872532",
        "publisher": "Springer Netherlands",
        "sjr": 0.927,
        "snip": 2.046,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Autonomous Agents and Multi-Agent Systems"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84953637884&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10458-015-9283-7.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Sambrook T."
      ],
      "categories": null,
      "citations": 45,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2015.07.032",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "276-286",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2016-01-01",
      "selected": null,
      "title": "Principal components analysis of reward prediction errors in a reinforcement learning task",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84941985376&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Franklin N.T."
      ],
      "categories": null,
      "citations": 48,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.7554/eLife.12029",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "eLife"
      },
      "publication_date": "2015-12-25",
      "selected": null,
      "title": "A cholinergic feedback circuit to regulate striatal population uncertainty and optimize reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84976276201&origin=inward"
      ]
    },
    {
      "abstract": "<p>The feedback-related negativity (FRN) is a commonly observed potential in scalp electroencephalography (EEG) studies related to the valence of feedback about a subject9s performance. This potential classically manifests as a negative deflection in medial frontocentral EEG contacts following negative feedback. Recent work has shown prominence of theta power in the spectral composition of the FRN, placing it within the larger class of \u201cfrontal midline theta\u201d cognitive control signals. Although the dorsal anterior cingulate cortex (dACC) is thought to be the cortical generator of the FRN, conclusive data regarding its origin and propagation are lacking. Here we examine intracranial electrophysiology from the human medial and lateral prefrontal cortex (PFC) to better understand the anatomical localization and communication patterns of the FRN. We show that the FRN is evident in both low- and high-frequency local field potentials (LFPs) recorded on electrocorticography. The FRN is larger in medial compared with lateral PFC, and coupling between theta band phase and high-frequency LFP power is also greater in medial PFC. Using Granger causality and conditional mutual information analyses, we provide evidence that feedback-related information propagates from medial to lateral PFC, and that this information transfer oscillates with theta-range periodicity. These results provide evidence for the dACC as the cortical source of the FRN, provide insight into the local computation of frontal midline theta, and have implications for reinforcement learning models of cognitive control.</p><p><b>SIGNIFICANCE STATEMENT</b> Using intracranial electrophysiology in humans, this work addresses questions about a frequently studied feedback-related electroencephalographic signal, illuminating anatomical and functional properties of the representation of feedback-related reinforcement during decision-making across the medial to lateral extent of the human prefrontal cortex.</p>",
      "authors": [
        "Elliot H. Smith",
        "Garrett P. Banks",
        "Charles B. Mikell",
        "Syndey S. Cash",
        "Shaun R. Patel",
        "Emad N. Eskandar",
        "Sameer A. Sheth"
      ],
      "categories": null,
      "citations": 36,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1864-15.2015",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "15827-15836",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2015-12-02",
      "selected": null,
      "title": "Frequency-Dependent Representation of Reinforcement-Related Information in the Human Medial and Lateral Prefrontal Cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84949469820&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Lu J."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.eatbeh.2015.07.003",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "94-97",
      "publication": {
        "category": "Journal",
        "cite_score": 4.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14710153",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.715,
        "snip": 1.028,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Clinical Psychology"
        ],
        "title": "Eating Behaviors"
      },
      "publication_date": "2015-12-01",
      "selected": null,
      "title": "Using food as reinforcer to shape children's non-food behavior: The adverse nutritional effect doubly moderated by reward sensitivity and gender",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84938695931&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Li P."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bandc.2015.08.004",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "15-20",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02782626",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Brain and Cognition"
      },
      "publication_date": "2015-11-01",
      "selected": null,
      "title": "Reduced sensitivity to neutral feedback versus negative feedback in subjects with mild depression: Evidence from event-related potentials study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84942574724&origin=inward"
      ]
    },
    {
      "abstract": "Recent work on decision-making field offers an account of dual-system theory for decision-making process. This theory holds that this process is conducted by two main controllers: a goal-directed system and a habitual system. In the reinforcement learning (RL) domain, the habitual behaviors are connected with model-free methods, in which appropriate actions are learned through trial-and-error experiences. However, goal-directed behaviors are associated with model-based methods of RL, in which actions are selected using a model of the environment. Studies on cognitive control also suggest that during processes like decision-making, some cortical and subcortical structures work in concert to monitor the consequences of decisions and to adjust control according to current task demands. Here a computational model is presented based on dual system theory and cognitive control perspective of decision-making. The proposed model is used to simulate human performance on a variant of probabilistic learning task. The basic proposal is that the brain implements a dual controller, while an accompanying monitoring system detects some kinds of conflict including a hypothetical cost-conflict one. The simulation results address existing theories about two event-related potentials, namely error related negativity (ERN) and feedback related negativity (FRN), and explore the best account of them. Based on the results, some testable predictions are also presented. Modeling decision-making from the perspectives of dual-system and cognitive control.The model simulates human performance on a variant of probabilistic learning task.The model addresses existing theories about the ERN and FRN components of ERP.The results show that the ERN is best described by the RL-ERN theory.The FRN is best described by a hypothetical cost-conflict signal.",
      "authors": [
        "Sareh Zendehrouh"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.neunet.2015.08.006",
      "keywords": [
        "Cognitive control",
        "Goal-directed behavior",
        "Dual system theory",
        "Reinforcement learning",
        "Probabilistic learning task",
        "Cost function"
      ],
      "number_of_pages": 12,
      "pages": "112-123",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0893-6080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2015-11-01",
      "selected": null,
      "title": "A new computational account of cognitive control over reinforcement-based decision-making",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84940535843&origin=inward",
        "https://dl.acm.org/doi/10.1016/j.neunet.2015.08.006"
      ]
    },
    {
      "abstract": "According to dominant neuropsychological theories of affect, emotions signal salience of events and in turn facilitate a wide spectrum of response options or action tendencies. Valence of an emotional experience is pivotal here, as it alters reward and punishment processing, as well as the balance between safety and risk taking, which can be translated into changes in the exploration-exploitation trade-off during reinforcement learning (RL). To test this idea, we compared the behavioral performance of three groups of participants that all completed a variant of a standard probabilistic learning task, but who differed regarding which mood state was actually induced and maintained (happy, sad or neutral). To foster a change from an exploration to an exploitation-based mode, we removed feedback information once learning was reliably established. Although changes in mood were successful, learning performance was balanced between the three groups. Critically, when focusing on exploitation-driven learning only, they did not differ either. Moreover, mood valence did not alter the learning rate or exploration per se, when titrated using complementing computational modeling. By comparing systematically these results to our previous study (Bakic, Jepma, De Raedt, & Pourtois, 2014), we found that arousal levels did differ between studies, which might account for limited modulatory effects of (positive) mood on RL in the present case. These results challenge the assumption that mood valence alone is enough to create strong shifts in the way exploitation or exploration is eventually carried out during (probabilistic) learning. In this context, we discuss the possibility that both valence and arousal are actually necessary components of the emotional mood state to yield changes in the use and exploration of incentives cues during RL.",
      "authors": [
        "Bakic, Jasmina",
        "De Raedt, Rudi",
        "Jepma, Marieke",
        "Pourtois, Gilles"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2015.00584",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2015-10-29",
      "selected": null,
      "title": "What is in the feedback? Effect of induced happiness vs. sadness on probabilistic learning with vs. without exploration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84947553567&origin=inward"
      ]
    },
    {
      "abstract": "Reward functions are an essential component of many robot learning methods. Defining such functions, however, remains hard in many practical applications. For tasks such as grasping, there are no reliable success measures available. Defining reward functions by hand requires extensive task knowledge and often leads to undesired emergent behavior. We introduce a framework, wherein the robot simultaneously learns an action policy and a model of the reward function by actively querying a human expert for ratings. We represent the reward model using a Gaussian process and evaluate several classical acquisition functions (AFs) from the Bayesian optimization literature in this context. Furthermore, we present a novel AF, expected policy divergence. We demonstrate results of our method for a robot grasping task and show that the learned reward function generalizes to a similar task. Additionally, we evaluate the proposed novel AF on a real robot pendulum swing-up task.",
      "authors": [
        "Daniel, Christian",
        "Kroemer, Oliver",
        "Viering, Malte",
        "Metz, Jan",
        "Peters, Jan"
      ],
      "categories": null,
      "citations": 34,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10514-015-9454-z",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "389-405",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09295593",
        "publisher": "Springer Netherlands",
        "sjr": 1.165,
        "snip": 1.523,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Autonomous Robots"
      },
      "publication_date": "2015-10-22",
      "selected": null,
      "title": "Active reward learning with a novel acquisition function",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10514-015-9454-z.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84941937390&origin=inward"
      ]
    },
    {
      "abstract": "Adaptation of saccades can be induced by different error signals, such as retinal position errors, prediction errors or reinforcement learning. Recently, we showed that a shift in the spatial goal of a perceptual task can induce saccadic adaptation, in the absence of a low-level position error. Here we investigated whether this top-down effect is mediated by the visibility of the task-relevant object, by reinforcement due to the feedback about the perceptual judgment or by a target selection mechanism. Participants were asked to discriminate visual stimuli arranged in a vertical compound. To induce adaptation, the discrimination target was presented at eccentric locations in the compound. In the first experiment, we compared adaptation with an easy and difficult discrimination. In the second experiment, we compared adaptation when feedback about the perceptual task was valid and when feedback was provided but was unrelated to performance. In the third experiment we compared adaptation with instructions to fixate one of the elements in the compound\u2014target selection\u2014to the perceptual task condition\u2014target selection and discrimination. To control for a bottom-up stimulus effect, we ran a fourth experiment in which the only instruction was to look at the compound. The saccade amplitude data were fitted by a two-state model distinguishing between an immediate and a gradual error correction process. We replicated our finding that a perceptual task can drive adaptation of saccades. Adaptation showed no effect of feedback reliability, nor an effect of the perceptual task beyond target selection. Adaptation was induced by a top-down signal since it was absent when there was no target selection instruction and no perceptual task. The immediate error correction was larger for the difficult than for the easy condition, suggesting that task difficulty affects mainly voluntary saccade targeting. In addition, the repetition of experiments one week later increased the magnitude of the gradual error correction. The results dissociate two distinct components of adaptation: an immediate and a gradual error correction. We conclude that perceptual-task induced adaptation is most likely due to top-down target selection within a larger object.",
      "authors": [
        "Sch\u00fctz, Alexander C.",
        "Souto, David"
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2015.00566",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2015-10-20",
      "selected": null,
      "title": "Perceptual task induces saccadic adaptation by target selection",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84947607307&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "De Kloet S."
      ],
      "categories": null,
      "citations": 50,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bcp.2015.07.014",
      "keywords": [],
      "number_of_pages": 14,
      "pages": "425-438",
      "publication": {
        "category": "Journal",
        "cite_score": 9.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00062952",
        "publisher": "Elsevier Inc.",
        "sjr": 1.329,
        "snip": 1.155,
        "subject_areas": [
          "Biochemistry",
          "Pharmacology"
        ],
        "title": "Biochemical Pharmacology"
      },
      "publication_date": "2015-10-15",
      "selected": null,
      "title": "Cholinergic modulation of dopamine pathways through nicotinic acetylcholine receptors",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84943366968&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "DePasque S."
      ],
      "categories": null,
      "citations": 62,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2015.06.046",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "175-186",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2015-10-01",
      "selected": null,
      "title": "Effects of intrinsic motivation on feedback processing during learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84937411157&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Schnuerch R."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12461",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "1328-1342",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2015-10-01",
      "selected": null,
      "title": "Social proof in the human brain: Electrophysiological signatures of agreement and disagreement with the majority",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84940797474&origin=inward"
      ]
    },
    {
      "abstract": "<p>For day-to-day decisions, multiple factors influence our choice between alternatives. Two dimensions of decision making that substantially affect choice are the objective perceptual properties of the stimulus (e.g., salience) and its subjective value. Here we measure EEGs in human subjects to relate their feedback-evoked EEG responses to estimates of prediction error given a neurally derived expected value for each trial. Unlike in traditional reinforcement learning paradigms, in our experiment the reward itself is not probabilistic; rather, it is a fixed value, which, when combined with the variable stimulus salience, yields uncertainty in the choice. We find that feedback-evoked event-related potentials (ERPs), specifically those classically termed feedback-related negativity, are modulated by both the reward level and stimulus salience. Using single-trial analysis of the EEG, we show stimulus-locked EEG components reflecting perceived stimulus salience can be combined with the level of reward to create an estimate of expected reward. This expected reward is used to form a prediction error that correlates with the trial-by-trial variability of the feedback ERPs for negative, but not positive, feedback. This suggests that the valence of prediction error is more important than the valence of the actual feedback, since only positive rewards were delivered in the experiment (no penalty or loss). Finally, we show that these subjectively defined prediction errors are informative of the riskiness of the subject9s choice on the subsequent trial. In summary, our work shows that neural correlates of stimulus salience interact with value information to yield neural representations of subjective expected reward.</p><p><b>SIGNIFICANCE STATEMENT</b> How we make perceptual decisions depends on sensory evidence and the value of our options. These two factors often interact to yield subjective decisions; i.e., individuals integrate sensory evidence and value to form their own estimates of expected reward. Here, we use electroencephelography to identify trial-by-trial neural activity of perceived stimulus salience, showing that this activity can be combined with the value of choice options to form a representation of expected reward. Our results provide insight into the neural processing governing the interaction between salience and value and the formation of subjective expected reward and prediction error. This work is potentially important for identifying neural markers of abnormal sensory/value processing, as is seen in some cases of psychiatric illnesses.</p>",
      "authors": [
        "Bin Lou",
        "Wha-Yin Hsu",
        "Paul Sajda"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.1601-15.2015",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "13064-13075",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2015-09-23",
      "selected": null,
      "title": "Perceptual Salience and Reward Both Influence Feedback-Related Neural Activity Arising from Choice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84945522408&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Moustafa A.A."
      ],
      "categories": null,
      "citations": 35,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bbr.2015.05.024",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "147-154",
      "publication": {
        "category": "Journal",
        "cite_score": 6.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01664328",
        "publisher": "Elsevier B.V.",
        "sjr": 0.881,
        "snip": 0.802,
        "subject_areas": [
          "Behavioral Neuroscience"
        ],
        "title": "Behavioural Brain Research"
      },
      "publication_date": "2015-09-05",
      "selected": null,
      "title": "Drift diffusion model of reward and punishment learning in schizophrenia: Modeling and experimental data",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84930659364&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Van De Vijver I."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/13825585.2015.1020917",
      "keywords": [],
      "number_of_pages": 25,
      "pages": "595-619",
      "publication": {
        "category": "Journal",
        "cite_score": 3.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13825585",
        "publisher": "Taylor and Francis Ltd.",
        "sjr": 0.595,
        "snip": 0.921,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Psychiatry and Mental Health",
          "Geriatrics and Gerontology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Aging, Neuropsychology, and Cognition"
      },
      "publication_date": "2015-09-03",
      "selected": null,
      "title": "Age-related changes in deterministic learning from positive versus negative performance feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84930863934&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Salim M."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2015.07.001",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "50-58",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2015-09-01",
      "selected": null,
      "title": "Brain activity elicited by reward and reward omission in individuals with psychopathic traits: An ERP study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84938078187&origin=inward"
      ]
    },
    {
      "abstract": "Many gait training programs are based on supervised learning principles: an individual is guided towards a desired gait pattern with directional error feedback. While this results in rapid adaptation, improvements quickly disappear. This study tested the hypothesis that a reinforcement learning approach improves retention and transfer of a new gait pattern. The results of a pilot study and larger experiment are presented. Healthy subjects were randomly assigned to either a supervised group, who received explicit instructions and directional error feedback while they learned a new gait pattern on a treadmill, or a reinforcement group, who was only shown whether they were close to or far from the desired gait. Subjects practiced for 10 min, followed by immediate and overnight retention and over-ground transfer tests. The pilot study showed that subjects could learn a new gait pattern under a reinforcement learning paradigm. The larger experiment, which had twice as many subjects (16 in each group) showed that the reinforcement group had better overnight retention than the supervised group (a 9% vs. 96% error increase, respectively), but there were no differences for over-ground transfer. These results suggest that encouraging participants to find rewarding actions through self-guided exploration is beneficial for retention.",
      "authors": [
        "Hasson, Christopher J.",
        "Manczurowsky, Julia",
        "Yen, Sheng-Che"
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2015.00459",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2015-08-27",
      "selected": null,
      "title": "A reinforcement learning approach to gait training improves retention",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84940994537&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "HajiHosseini A."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neulet.2015.06.054",
      "keywords": [],
      "number_of_pages": 5,
      "pages": "99-103",
      "publication": {
        "category": "Journal",
        "cite_score": 5.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03043940",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 0.802,
        "snip": 0.777,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Neuroscience Letters"
      },
      "publication_date": "2015-08-18",
      "selected": null,
      "title": "Sensitivity of frontal beta oscillations to reward valence but not probability",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84939521123&origin=inward"
      ]
    },
    {
      "abstract": "We introduce a new class of reinforcement learning methods referred to as {\\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\\em episodes}, each composed of several {\\em steps}, in which it chooses an action and observes a feedback signal. Moreover, in each step, it can take a special action, called the $stop$ action, that ends the current episode. After the $stop$ action is taken, the learner collects a terminal reward, and observes the costs and terminal rewards associated with each step of the episode. The goal of the learner is to maximize its cumulative gain (i.e., the terminal reward minus costs) over all episodes by learning to choose the best sequence of actions based on the feedback. First, we define an {\\em oracle} benchmark, which sequentially selects the actions that maximize the expected immediate gain. Then, we propose our online learning algorithm, named {\\em FeedBack Adaptive Learning} (FeedBAL), and prove that its regret with respect to the benchmark is bounded with high probability and increases logarithmically in expectation. Moreover, the regret only has polynomial dependence on the number of steps, actions and states. eMAB can be used to model applications that involve humans in the loop, ranging from personalized medical screening to personalized web-based education, where sequences of actions are taken in each episode, and optimal behavior requires adapting the chosen actions based on the feedback.",
      "authors": [
        "Tekin, Cem",
        "van der Schaar, Mihaela"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2015-08-04",
      "selected": null,
      "title": "Episodic Multi-armed Bandits",
      "urls": [
        "http://arxiv.org/abs/1508.00641v4",
        "http://arxiv.org/pdf/1508.00641v4",
        "http://arxiv.org/pdf/1508.00641.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Morita K."
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/ejn.12994",
      "keywords": [],
      "number_of_pages": 19,
      "pages": "2003-2021",
      "publication": {
        "category": "Journal",
        "cite_score": 6.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0953816X",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.044,
        "snip": 0.891,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "European Journal of Neuroscience"
      },
      "publication_date": "2015-08-01",
      "selected": null,
      "title": "Computing reward-prediction error: An integrated account of cortical timing and basal-ganglia pathways for appetitive and aversive learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84939262688&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chen C."
      ],
      "categories": null,
      "citations": 125,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neubiorev.2015.05.005",
      "keywords": [],
      "number_of_pages": 21,
      "pages": "247-267",
      "publication": {
        "category": "Journal",
        "cite_score": 13.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01497634",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.599,
        "snip": 2.49,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Neuroscience and Biobehavioral Reviews"
      },
      "publication_date": "2015-08-01",
      "selected": null,
      "title": "Reinforcement learning in depression: A review of computational research",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84929583571&origin=inward"
      ]
    },
    {
      "abstract": "The ultimate goal of this study is to develop a method that can accomplish dexterous manipulation of various non-rigid objects by a robotic hand. In this paper, we propose a novel model-free approach using reinforcement learning to learn a shared control policy for dexterous telemanipulation by a human operator. A shared control policy is a probabilistic mapping from the human operator's (master) action and complementary sensor data to the robot (slave) control input for robot actuators. Through the learning process, our method can optimize the shared control policy so that it cooperates to the operator's policy and compensates the lack of sensory information of the operator using complementary sensor data to enhance the dexterity. To validate our method, we adopted a page turning task by telemanipulation and developed an experimental platform with a paper page model and a robot fingertip in simulation. Since the human operator cannot perceive the tactile information of the robot, it may not be as easy as humans do directly. Experimental results suggest that our method is able to learn task-relevant shared control for flexible and enhanced dexterous manipulation by a teleoperated robotic fingertip without tactile feedback to the operator.",
      "authors": [
        "Takamitsu Matsubara",
        "Takahiro Hasegawa",
        "Kenji Sugimoto"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ROMAN.2015.7333587",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "343-348",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-4673-6703-5",
        "issn": null,
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)"
      },
      "publication_date": "2015-08-01",
      "selected": null,
      "title": "Reinforcement learning of shared control for dexterous telemanipulation: Application to a page turning skill",
      "urls": [
        "https://dl.acm.org/doi/10.1109/ROMAN.2015.7333587",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84954073712&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7333587"
      ]
    },
    {
      "abstract": "Several studies have demonstrated that reward from a human trainer can be a powerful feedback signal for control-learning algorithms. However, the space of algorithms for learning from such human reward has hitherto not been explored systematically. Using model-based reinforcement learning from human reward, this article investigates the problem of learning from human reward through six experiments, focusing on the relationships between reward positivity, which is how generally positive a trainer's reward values are; temporal discounting, the extent to which future reward is discounted in value; episodicity, whether task learning occurs in discrete learning episodes instead of one continuing session; and task performance, the agent's performance on the task the trainer intends to teach. This investigation is motivated by the observation that an agent can pursue different learning objectives, leading to different resulting behaviors. We search for learning objectives that lead the agent to behave as the trainer intends.We identify and empirically support a \"positive circuits\" problem with low discounting (i.e., high discount factors) for episodic, goal-based tasks that arises from an observed bias among humans towards giving positive reward, resulting in an endorsement of myopic learning for such domains. We then show that converting simple episodic tasks to be non-episodic (i.e., continuing) reduces and in some cases resolves issues present in episodic tasks with generally positive reward and-relatedly-enables highly successful learning with non-myopic valuation in multiple user studies. The primary learning algorithm introduced in this article, which we call \"vi-tamer\", is the first algorithm to successfully learn non-myopically from reward generated by a human trainer; we also empirically show that such non-myopic valuation facilitates higher-level understanding of the task. Anticipating the complexity of real-world problems, we perform further studies-one with a failure state added-that compare (1) learning when states are updated asynchronously with local bias-i.e., states quickly reachable from the agent's current state are updated more often than other states-to (2) learning with the fully synchronous sweeps across each state in the vi-tamer algorithm. With these locally biased updates, we find that the general positivity of human reward creates problems even for continuing tasks, revealing a distinct research challenge for future work.",
      "authors": [
        "W. Bradley Knox",
        "Peter Stone"
      ],
      "categories": null,
      "citations": 45,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1016/j.artint.2015.03.009",
      "keywords": [
        "Human-agent interaction",
        "End-user programming",
        "Modeling user behavior",
        "Reinforcement learning",
        "Human teachers",
        "Interactive machine learning"
      ],
      "number_of_pages": 27,
      "pages": "24-50",
      "publication": {
        "category": "Journal",
        "cite_score": 11.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0004-3702",
        "publisher": "Elsevier B.V.",
        "sjr": 1.819,
        "snip": 3.416,
        "subject_areas": [
          "Linguistics and Language",
          "Artificial Intelligence",
          "Language and Linguistics"
        ],
        "title": "Artificial Intelligence"
      },
      "publication_date": "2015-08-01",
      "selected": null,
      "title": "Framing reinforcement learning from human reward",
      "urls": [
        "https://dl.acm.org/doi/10.1016/j.artint.2015.03.009",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84927928457&origin=inward"
      ]
    },
    {
      "abstract": "COACH (COrrective Advice Communicated by Humans), a new interactive learning framework that allows non-expert humans to shape a policy through corrective advice, using a binary signal in the action domain of the agent, is proposed. One of the main innovative features of COACH is a mechanism for adaptively adjusting the amount of human feedback that a given action receives, taking into consideration past feedback. The performance of COACH is compared with the one of TAMER (Teaching an Agent Manually via Evaluative Reinforcement), ACTAMER (Actor-Critic TAMER), and an autonomous agent trained using SARSA(?) in two reinforcement learning problems. COACH outperforms all other learning frameworks in the reported experiments. In addition, results show that COACH is able to transfer successfully human knowledge to agents with continuous actions, being a complementary approach to TAMER, which is appropriate for teaching in discrete action domains.",
      "authors": [
        "Carlos Celemin",
        "Javier Ruiz-del-Solar"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ICAR.2015.7251514",
      "keywords": [
        "Robot learning",
        "human teachers",
        "human feedback in action domains",
        "interactive learning"
      ],
      "number_of_pages": 6,
      "pages": "581-586",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-4673-7508-5",
        "issn": null,
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 17th International Conference on Advanced Robotics, ICAR 2015"
      },
      "publication_date": "2015-07-27",
      "selected": null,
      "title": "COACH: Learning continuous actions from COrrective Advice Communicated by Humans",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7251514",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84957690343&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning describes how a learning agent can achieve optimal behaviour based on interactions with its environment and reward feedback. A limiting factor in reinforcement learning as employed in artificial intelligence is the need for an often prohibitively large number of environment samples before the agent reaches a desirable level of performance. Learning from demonstration is an approach that provides the agent with demonstrations by a supposed expert, from which it should derive suitable behaviour. Yet, one of the challenges of learning from demonstration is that no guarantees can be provided for the quality of the demonstrations, and thus the learned behavior. In this paper, we investigate the intersection of these two approaches, leveraging the theoretical guarantees provided by reinforcement learning, and using expert demonstrations to speed up this learning by biasing exploration through a process called reward shaping. This approach allows us to leverage human input without making an erroneous assumption regarding demonstration optimality. We show experimentally that this approach requires significantly fewer demonstrations, is more robust against suboptimality of demonstrations, and achieves much faster learning than the recently developed HAT algorithm.",
      "authors": [
        "Tim Brys",
        "Anna Harutyunyan",
        "Halit Bener Suay",
        "Sonia Chernova",
        "Matthew E. Taylor",
        "Ann Now\u00e9"
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2832581.2832716",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "3352-3358",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357384",
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 24th International Conference on Artificial Intelligence"
      },
      "publication_date": "2015-07-25",
      "selected": null,
      "title": "Reinforcement learning from demonstration through shaping",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2832581.2832716"
      ]
    },
    {
      "abstract": "An interactive learning framework that allows non-expert humans to shape a policy through corrective advice, using a binary signal in the action domain of the robot/agent, is proposed. One of the most innovative features of COACH (COrrective Advice Communicated by...",
      "authors": [
        "Celemin, Carlos",
        "Ruiz-del-Solar, Javier"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/978-3-319-29339-4_2",
      "keywords": [
        "Human feedback in action domains",
        "Ball dribbling",
        "Interactive learning",
        "Human teachers",
        "Robot learning",
        "Robot soccer"
      ],
      "number_of_pages": 12,
      "pages": "16-27",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "978-3-319-29338-7",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2015-07-23",
      "selected": null,
      "title": "Interactive Learning of Continuous Actions from Corrective Advice Communicated by Humans",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/978-3-319-29339-4_2.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84958040274&origin=inward",
        "https://dl.acm.org/doi/10.1007/978-3-319-29339-4_2"
      ]
    },
    {
      "abstract": "Recently robots are being used more frequently as assistants in domestic scenarios. In this context we train an apprentice robot to perform a cleaning task using interactive reinforcement learning since it has been shown to be an efficient learning approach benefiting from human expertise for performing domestic tasks. The robotic agent obtains interactive feedback via a speech recognition system which is tested to work with five different microphones concerning their polar patterns and distance to the teacher to recognize sentences in different instruction classes. Moreover, the reinforcement learning approach uses situated affordances to allow the robot to complete the cleaning task in every episode anticipating when chosen actions are possible to be performed. Situated affordances and interaction allow to improve the convergence speed of reinforcement learning, and the results also show that the system is robust against wrong instructions that result from errors of the speech recognition system.",
      "authors": [
        "Francisco Cruz",
        "Johannes Twiefel",
        "Sven Magg",
        "Cornelius Weber",
        "Stefan Wermter"
      ],
      "categories": null,
      "citations": 37,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/IJCNN.2015.7280477",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-6654-8868-6",
        "issn": "2161-4393",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Neural Networks"
      },
      "publication_date": "2015-07-12",
      "selected": null,
      "title": "Interactive reinforcement learning through speech guidance in a domestic scenario",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7280477",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84950995385&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Huys Q.J.M."
      ],
      "categories": null,
      "citations": 119,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1146/annurev-neuro-071714-033928",
      "keywords": [],
      "number_of_pages": 23,
      "pages": "1-23",
      "publication": {
        "category": "Book",
        "cite_score": 24.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0147006X",
        "publisher": "Annual Reviews Inc.",
        "sjr": 8.389,
        "snip": 3.679,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Annual Review of Neuroscience"
      },
      "publication_date": "2015-07-08",
      "selected": null,
      "title": "Depression: A Decision-Theoretic Analysis",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84929597054&origin=inward"
      ]
    },
    {
      "abstract": "Neurofeedback training of motor imagery-related brain-states with brain-machine interfaces (BMI) is currently being explored prior to standard physiotherapy to improve the motor outcome of stroke rehabilitation. Pilot studies suggest that such a priming intervention before physiotherapy might increase the responsiveness of the brain to the subsequent physiotherapy, thereby improving the clinical outcome. However, there is little evidence up to now that these BMI-based interventions have achieved operate conditioning of specific brain states that facilitate task-specific functional gains beyond the practice of primed physiotherapy. In this context, we argue that BMI technology needs to aim at physiological features relevant for the targeted behavioral gain. Moreover, this therapeutic intervention has to be informed by concepts of reinforcement learning to develop its full potential. Such a refined neurofeedback approach would need to address the following issues (1) Defining a physiological feedback target specific to the intended behavioral gain, e.g. \u03b2-band oscillations for cortico-muscular communication. This targeted brain state could well be different from the brain state optimal for the neurofeedback task (2) Selecting a BMI classification and thresholding approach on the basis of learning principles, i.e. balancing challenge and reward of the neurofeedback task instead of maximizing the classification accuracy of the feedback device (3) Adjusting the feedback in the course of the training period to account for the cognitive load and the learning experience of the participant. The proposed neurofeedback strategy provides evidence for the feasibility of the suggested approach by demonstrating that dynamic threshold adaptation based on reinforcement learning may lead to frequency-specific operant conditioning of \u03b2-band oscillations paralleled by task-specific motor improvement; a proposal that requires investigation in a larger cohort of stroke patients.",
      "authors": [
        "Naros, Georgios",
        "Gharabaghi, Alireza"
      ],
      "categories": null,
      "citations": 54,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2015.00391",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2015-07-03",
      "selected": null,
      "title": "Reinforcement learning of self-regulated \u03b2-oscillations for motor restoration in chronic stroke",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84947572857&origin=inward"
      ]
    },
    {
      "abstract": "In this paper we computationally study the relation between adaptive behaviour and emotion. Using the reinforcement learning framework, we propose that learned state utility, <inline-formula><inline-graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink=\"ccos_a_1031081_ilm0001.gif\"/></inline-formula>, models fear negative and hope positive based on the fact that both signals are about anticipation of loss or gain. Further, we propose that joy/distress is a signal similar to the error signal. We present agent-based simulation experiments that show that this model replicates psychological and behavioural dynamics of emotion. This work distinguishes itself by assessing the dynamics of emotion in an adaptive agent framework \u2013 coupling it to the literature on habituation, development, extinction and hope theory. Our results support the idea that the function of emotion is to provide a complex feedback signal for an organism to adapt its behaviour. Our work is relevant for understanding the relation between emotion and adaptation in animals, as well as for human\u2013robot interaction, in particular how emotional signals can be used to communicate between adaptive agents and humans.",
      "authors": [
        "Joost Broekens",
        "Elmer Jacobs",
        "Catholijn M. Jonker"
      ],
      "categories": null,
      "citations": 25,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1080/09540091.2015.1031081",
      "keywords": [
        "emotion",
        "reinforcement learning",
        "emotion dynamics",
        "affective computing"
      ],
      "number_of_pages": 19,
      "pages": "215-233",
      "publication": {
        "category": "Journal",
        "cite_score": 5.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0954-0091",
        "publisher": "Taylor and Francis A.S.",
        "sjr": 0.853,
        "snip": 1.676,
        "subject_areas": [
          "Software",
          "Human-Computer Interaction",
          "Artificial Intelligence"
        ],
        "title": "Connection Science"
      },
      "publication_date": "2015-07-01",
      "selected": null,
      "title": "A reinforcement learning model of joy, distress, hope and fear",
      "urls": [
        "https://dl.acm.org/doi/10.1080/09540091.2015.1031081",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84943197277&origin=inward"
      ]
    },
    {
      "abstract": "Pavlovian-to-instrumental transfer (PIT) refers to the process of a Pavlovian reward-paired cue acquiring incentive motivational proprieties that drive choices. It represents a crucial phenomenon for understanding cue-controlled behavior, and it has both adaptive and maladaptive implications (i.e., drug-taking). In animals, individual differences in the degree to which such cues bias performance have been identified in two types of individuals that exhibit distinct Conditioned Responses during Pavlovian conditioning: Sign-Trackers (ST) and Goal-Trackers (GT). Using an appetitive PIT procedure with a monetary reward, the present study investigated, for the first time, the extent to which such individual differences might affect the influence of reward-paired cues in humans. In a first task, participants learned an instrumental response leading to reward; then, in a second task, a visual Pavlovian cue was associated with the same reward; finally, in a third task, PIT was tested by measuring the preference for the reward-paired instrumental response when the task-irrelevant reward-paired cue was presented, in the absence of the reward itself. In ST individuals, but not in GT individuals, reward-related cues biased behavior, resulting in an increased likelihood to perform the instrumental response independently paired with the same reward when presented with the task-irrelevant reward-paired cue, even if the reward itself was no longer available (i.e., stronger PIT effect). This finding has important implications for developing individualized treatment for maladaptive behaviors, such as addiction.",
      "authors": [
        "Garofalo, Sara",
        "di Pellegrino, Giuseppe"
      ],
      "categories": null,
      "citations": 72,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2015.00163",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2015-06-24",
      "selected": null,
      "title": "Individual differences in the influence of task-irrelevant Pavlovian cues on human behavior",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84933039584&origin=inward"
      ]
    },
    {
      "abstract": "Throughout our lives, we face the important task of distinguishing rewarding actions from those that are best avoided. Importantly, there are multiple means by which we acquire this information. Through trial and error, we use experiential feedback to evaluate our actions. We also learn which actions are advantageous through explicit instruction from others. Here, we examined whether the influence of these two forms of learning on choice changes across development by placing instruction and experience in competition in a probabilistic-learning task. Whereas inaccurate instruction markedly biased adults\u2019 estimations of a stimulus\u2019s value, children and adolescents were better able to objectively estimate stimulus values through experience. Instructional control of learning is thought to recruit prefrontal\u2013striatal brain circuitry, which continues to mature into adulthood. Our behavioral data suggest that this protracted neurocognitive maturation may cause the motivated actions of children and adolescents to be less influenced by explicit instruction than are those of adults. This absence of a confirmation bias in children and adolescents represents a paradoxical developmental advantage of youth over adults in the unbiased evaluation of actions through positive and negative experience.",
      "authors": [
        "Decker, Johannes H.",
        "Lourenco, Frederico S.",
        "Doll, Bradley B.",
        "Hartley, Catherine A."
      ],
      "categories": null,
      "citations": 58,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-014-0332-5",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "310-320",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2015-06-22",
      "selected": null,
      "title": "Experiential reward learning outweighs instruction prior to adulthood",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84939969840&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-014-0332-5.pdf"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Hu J."
      ],
      "categories": null,
      "citations": 71,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/hbm.22760",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "2132-2146",
      "publication": {
        "category": "Journal",
        "cite_score": 9.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10659471",
        "publisher": "Wiley-Liss Inc.",
        "sjr": 1.688,
        "snip": 1.395,
        "subject_areas": [
          "Radiological and Ultrasound Technology",
          "Anatomy",
          "Neurology",
          "Neurology (clinical)",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Human Brain Mapping"
      },
      "publication_date": "2015-06-01",
      "selected": null,
      "title": "Oxytocin selectively facilitates learning with social feedback and increases activity and functional connectivity in emotional memory and reward processing regions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84928755621&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Chen C."
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00600.2014",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "3459-3461",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2015-06-01",
      "selected": null,
      "title": "Intelligence moderates reinforcement learning: A mini-review of the neural evidence",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84930854843&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning is a branch of machine learning concerned with using experience gained through interacting with the world and evaluative feedback to improve a system's ability to make behavioural decisions. It has been called the artificial intelligence problem in a microcosm because learning algorithms must act autonomously to perform well and achieve their goals. Partly driven by the increasing availability of rich data, recent years have seen exciting advances in the theory and practice of reinforcement learning, including developments in fundamental technical areas such as generalization, planning, exploration and empirical methodology, leading to increasing applicability to real-life problems.",
      "authors": [
        "Littman, Michael L."
      ],
      "categories": null,
      "citations": 237,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/nature14540",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "445-451",
      "publication": {
        "category": "Journal",
        "cite_score": 83.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00280836",
        "publisher": "Nature Publishing Group",
        "sjr": 20.957,
        "snip": 11.591,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Nature"
      },
      "publication_date": "2015-05-27",
      "selected": null,
      "title": "Reinforcement learning improves behaviour from evaluative feedback",
      "urls": [
        "https://www.nature.com/articles/nature14540.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84930647882&origin=inward"
      ]
    },
    {
      "abstract": "The TAMER framework, which provides a way for agents to learn to solve tasks using human-generated rewards, has been examined in several small-scale studies, each with a few dozen subjects. In this paper, we present the results of the first large-scale study of TAMER, which was performed at the NEMO science museum in Amsterdam and involved 561 subjects. Our results show for the first time that an agent using TAMER can successfully learn to play Infinite Mario, a challenging reinforcement-learning benchmark problem based on the popular video game, given feedback from both adult (N=209) and child (N=352) trainers. In addition, our study supports prior studies demonstrating the importance of bidirectional feedback and competitive elements in the training interface. Finally, our results also shed light on the potential for using trainers' facial expressions as a reward signal, as well as the role of age and gender in trainer behavior and agent performance.",
      "authors": [
        "Guangliang Li",
        "Hayley Hung",
        "Shimon Whiteson"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2772879.2773428",
      "keywords": [
        "human-agent interaction",
        "reinforcement learning"
      ],
      "number_of_pages": 2,
      "pages": "1771-1772",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450334136",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems"
      },
      "publication_date": "2015-05-04",
      "selected": null,
      "title": "A Large-Scale Study of Agents Learning from Human Reward",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2772879.2773428"
      ]
    },
    {
      "abstract": "In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A fundamental aspect of music perception is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a novel reinforcement-learning framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a model of preferences for both songs and song transitions. The model is learned online and is uniquely adapted for each listener. To reduce exploration time, DJ-MC exploits user feedback to initialize a model, which it subsequently updates by reinforcement. We evaluate our framework with human participants using both real song and playlist data. Our results indicate that DJ-MC's ability to recommend sequences of songs provides a significant improvement over more straightforward approaches, which do not take transitions into account.",
      "authors": [
        "Elad Liebman",
        "Maytal Saar-Tsechansky",
        "Peter Stone"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2772879.2772954",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "591-599",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450334136",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems"
      },
      "publication_date": "2015-05-04",
      "selected": null,
      "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2772879.2772954"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Moreno-L\u00f5pez L."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/adb.12143",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "546-556",
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13556215",
        "publisher": "Wiley-Blackwell",
        "sjr": 1.049,
        "snip": 0.941,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Psychiatry and Mental Health",
          "Pharmacology"
        ],
        "title": "Addiction Biology"
      },
      "publication_date": "2015-05-01",
      "selected": null,
      "title": "Cocaine use severity and cerebellar gray matter are associated with reversal learning deficits in cocaine-dependent individuals",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84926520724&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Slagter H."
      ],
      "categories": null,
      "citations": 48,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuropsychologia.2015.03.028",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "126-132",
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00283932",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.995,
        "snip": 1.03,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neuropsychologia"
      },
      "publication_date": "2015-05-01",
      "selected": null,
      "title": "Spontaneous eye blink rate predicts learning from negative, but not positive, outcomes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964240198&origin=inward"
      ]
    },
    {
      "abstract": "Humans can learn under a wide variety of feedback conditions. Reinforcement learning (RL), where a series of rewarded decisions must be made, is a particularly important type of learning. Computational and behavioral studies of RL have focused mainly on Markovian decision processes, where the next state depends on only the current state and action. Little is known about non-Markovian decision making, where the next state depends on more than the current state and action. Learning is non-Markovian, for example, when there is no unique mapping between actions and feedback. We have produced a model based on spiking neurons that can handle these non-Markovian conditions by performing policy gradient descent [1]. Here, we examine the model\u2019s performance and compare it with human learning and a Bayes optimal reference, which provides an upper-bound on performance. We find that in all cases, our population of spiking neurons model well-describes human performance.",
      "authors": [
        "Aaron Michael Clarke",
        "Johannes Friedrich",
        "Elisa M. Tartaglia",
        "Silvia Marchesotti",
        "Walter Senn",
        "Michael H. Herzog"
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0123105",
      "keywords": [
        "Machine learning",
        "Neurons",
        "Learning",
        "Human performance",
        "Human learning",
        "Decision making",
        "Memory",
        "Neural networks"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2015-04-21",
      "selected": null,
      "title": "Human and Machine Learning in Non-Markovian Decision Making",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0123105&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84928242573&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Cavanagh J."
      ],
      "categories": null,
      "citations": 88,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2015.02.007",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "205-216",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2015-04-05",
      "selected": null,
      "title": "Cortical delta activity reflects reward prediction error and related behavioral adjustments, but at different times",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84923034711&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Rafferty A.N."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/cogs.12157",
      "keywords": [],
      "number_of_pages": 35,
      "pages": "584-618",
      "publication": {
        "category": "Journal",
        "cite_score": 3.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03640213",
        "publisher": "Wiley-Blackwell",
        "sjr": 1.057,
        "snip": 1.257,
        "subject_areas": [
          "Artificial Intelligence",
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive Science"
      },
      "publication_date": "2015-04-01",
      "selected": null,
      "title": "Inferring learners' knowledge from their actions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84926685199&origin=inward"
      ]
    },
    {
      "abstract": "Background: Although cognitive theories of depression have postulated enhanced processing of negatively valenced information, previous EEG studies have shown both increased and reduced sensitivity for negative performance feedback in MDD. To reconcile these paradoxical findings, it has been speculated that sensitivity for negative feedback is potentiated in moderate MDD, but reduced in highly anhedonic subjects. The goal of this study was to test this hypothesis by analyzing the feedback-related negativity (FRN), frontomedial theta power (FMT), and source-localized anterior midcingulate cortex (aMCC) activity after negative feedback. Methods: Fourteen unmedicated participants with Major Depressive Disorder (MDD) and 15 control participants performed a reinforcement learning task while 128-channel Electroencephalogram (EEG) was recorded. FRN, FMT, and LORETA source-localized aMCC activity after negative and positive feedback were compared between groups. Results: The MDD group showed higher FRN amplitudes and aMCC activation to negative feedback than controls. Moreover, aMCC activation to negative feedback was inversely related to self-reported anhedonia. In contrast, self-reported anxiety correlated with feedback-evoked frontomedial theta (FMT) within the depression group. Conclusions: The present findings suggest that, among depressed and anxious individuals, enhanced processing of negative feedback occurs relatively early in the information processing stream. These results extend prior work and indicate that although moderate depression is associated with elevated sensitivity for negative feedback, high levels of anhedonia may attenuate this effect. \u00c2\u00a9 2015 Wiley Periodicals, Inc.",
      "authors": [
        "Mueller, E.M.",
        "Pechtel, P.",
        "Cohen, A.L.",
        "Douglas, S.R.",
        "Pizzagalli, D.A."
      ],
      "categories": null,
      "citations": 37,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/da.22338",
      "keywords": [
        "theta",
        "error-related negativity",
        "depression",
        "action monitoring",
        "LORETA",
        "feedback-related negativity (FRN)"
      ],
      "number_of_pages": 10,
      "pages": "296-305",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10914269",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Depression and Anxiety"
      },
      "publication_date": "2015-04-01",
      "selected": null,
      "title": "Potentiated processing of negative feedback in depression is attenuated by anhedonia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84925620435&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning is a powerful machine learning paradigm that allows agents to autonomously learn to maximize a scalar reward. However, it often suffers from poor initial performance and long learning times. This paper discusses how collecting on-line human feedback, both in real time and post hoc, can potentially improve the performance of such learning systems. We use the game Pac-Man to simulate a navigation setting and show that workers are able to accurately identify both when a sub-optimal action is executed, and what action should have been performed instead. Demonstrating that the crowd is capable of generating this input, and discussing the types of errors that occur, serves as a critical first step in designing systems that use this real-time feedback to improve systems' learning performance on-the-fly. \u00c2\u00a9 Copyright 2015 by the Association for Computing Machinery, Inc. (ACM).",
      "authors": [
        "Gabriel V. de la Cruz",
        "Bei Peng",
        "Walter S. Lasecki",
        "Matthew E. Taylor"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/2732158.2732180",
      "keywords": [],
      "number_of_pages": 4,
      "pages": "17-20",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450333085",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 20th International Conference on Intelligent User Interfaces Companion"
      },
      "publication_date": "2015-03-29",
      "selected": null,
      "title": "Towards Integrating Real-Time Crowd Advice with Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84958697606&origin=inward",
        "https://dl.acm.org/doi/10.1145/2732158.2732180"
      ]
    },
    {
      "abstract": "Adapting behavior to accommodate changes in the environment is an important function of the nervous system. A universal problem for motile animals is the discovery that a learned route is blocked and detour is required. Given the substantial neuroscience research on spatial navigation and decision-making it is surprising that so little is known about how the brain solves the detour problem. Here we review the limited number of relevant functional neuroimaging, single unit recording and lesion studies. We find that while the prefrontal cortex consistently responds to detours, the hippocampus does not. Recent evidence suggests the hippocampus tracks information about the future path distance to the goal. Based on this evidence we postulate a conceptual model in which: Lateral prefrontal cortex provides a prediction error signal about the change in the path, frontopolar and superior prefrontal cortex support the re-formulation of the route plan as a novel subgoal and the hippocampus simulates the new path. More data will be required to validate this model and understand 1) how the system processes the different options and 2) deals with situations where a new path becomes available (i.e. shortcuts).",
      "authors": [
        "Spiers, Hugo J.",
        "Gilbert, Sam J."
      ],
      "categories": null,
      "citations": 57,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2015.00125",
      "keywords": [
        "Hippocampus",
        "Artificial intelligence",
        "Goals",
        "Reinforcement learning",
        "Virtual reality",
        "Planning",
        "Prediction error",
        "Place cells"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2015-03-20",
      "selected": null,
      "title": "Solving the detour problem in navigation: a model of prefrontal and hippocampal interactions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84933674731&origin=inward"
      ]
    },
    {
      "abstract": "We present a framework for automatically learning human user models from joint-action demonstrations that enables a robot to compute a robust policy for a collaborative task with a human. First, the demonstrated action sequences are clustered into different human types using an unsupervised learning algorithm. A reward function is then learned for each type through the employment of an inverse reinforcement learning algorithm. The learned model is then incorporated into a mixed-observability Markov decision process (MOMDP) formulation, wherein the human type is a partially observable variable. With this framework, we can infer online the human type of a new user that was not included in the training set, and can compute a policy for the robot that will be aligned to the preference of this user. In a human subject experiment (n=30), participants agreed more strongly that the robot anticipated their actions when working with a robot incorporating the proposed framework (p<;0.01), compared to manually annotating robot actions. In trials where participants faced difficulty annotating the robot actions to complete the task, the proposed framework significantly improved team efficiency (p <;0.01). The robot incorporating the framework was also found to be more responsive to human actions compared to policies computed using a hand-coded reward function by a domain expert (p<;0.01). These results indicate that learning human user models from joint-action demonstrations and encoding them in a MOMDP formalism can support effective teaming in human-robot collaborative tasks.",
      "authors": [
        "Stefanos Nikolaidis",
        "Ramya Ramakrishnan",
        "Keren Gu",
        "Julie Shah"
      ],
      "categories": null,
      "citations": 40,
      "comments": null,
      "databases": [
        "IEEE"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 8,
      "pages": "189-196",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-5386-8556-3",
        "issn": "2167-2121",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction"
      },
      "publication_date": "2015-03-02",
      "selected": null,
      "title": "Efficient Model Learning from Joint-Action Demonstrations for Human-Robot Collaborative Tasks",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8520627"
      ]
    },
    {
      "abstract": "Learning from demonstration research often assumes that the demonstrator can quickly give feedback or demonstrations. Individuals with severe motor disabilities are often slow and prone to human errors in demonstrations while teaching. Our work develops tools to allow persons with severe motor disabilities, who stand to benefit most from assistive robots, to train these systems. To accommodate slower feedback, we will develop a movie-reel style learning from demonstration interface. To handle human error, we will use dimensionality reduction to develop new reinforcement learning techniques. \u00c2\u00a9 2015 Author.",
      "authors": [
        "William Curran"
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/2701973.2702710",
      "keywords": [
        "human-robot interaction",
        "learning from demonstration"
      ],
      "number_of_pages": 2,
      "pages": "233-234",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450333184",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts"
      },
      "publication_date": "2015-03-02",
      "selected": null,
      "title": "Developing Learning from Demonstration Techniques for Individuals with Physical Disabilities",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84969270811&origin=inward",
        "https://dl.acm.org/doi/10.1145/2701973.2702710"
      ]
    },
    {
      "abstract": "Demis Hassabis sold his London-based DeepMind Technologies to find how human brains and computers learn to master complex tasks. DeepMind made use of a newly fashionable machine-learning technique called deep learning, which involves processing data through networks of crudely simulated neurons. But it combined deep learning with other tricks to make something with an unexpected level of intelligence. In particular, DeepMind had used a technique called reinforcement learning, which is inspired by the work of animal psychologists such as B. F. Skinner. This led to software that learns by taking actions and receiving feedback on the results of those actions. DeepMind made use of a newly fashionable machine-learning technique called deep learning, which involves processing data through networks of crudely simulated neurons. But it combined deep learning with other tricks to make something with an unexpected level of intelligence.",
      "authors": [
        "Simonite, T."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 3,
      "pages": "16-18",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1099274X",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Technology Review"
      },
      "publication_date": "2015-03-01",
      "selected": null,
      "title": "Google's intelligence designer",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85022332350&origin=inward"
      ]
    },
    {
      "abstract": "<p>Reinforcement learning (RL) theory posits that learning is driven by discrepancies between the predicted and actual outcomes of actions (prediction errors [PEs]). In social environments, learning is often guided by similar RL mechanisms. For example, teachers monitor the actions of students and provide feedback to them. This feedback evokes PEs in students that guide their learning. We report the first study that investigates the neural mechanisms that underpin RL signals in the brain of a teacher. Neurons in the anterior cingulate cortex (ACC) signal PEs when learning from the outcomes of one9s own actions but also signal information when outcomes are received by others. Does a teacher9s ACC signal PEs when monitoring a student9s learning? Using fMRI, we studied brain activity in human subjects (teachers) as they taught a confederate (student) action\u2013outcome associations by providing positive or negative feedback. We examined activity time-locked to the students9 responses, when teachers infer student predictions and know actual outcomes. We fitted a RL-based computational model to the behavior of the student to characterize their learning, and examined whether a teacher9s ACC signals when a student9s predictions are wrong. In line with our hypothesis, activity in the teacher9s ACC covaried with the PE values in the model. Additionally, activity in the teacher9s insula and ventromedial prefrontal cortex covaried with the predicted value according to the student. Our findings highlight that the ACC signals PEs vicariously for others9 erroneous predictions, when monitoring and instructing their learning. These results suggest that RL mechanisms, processed vicariously, may underpin and facilitate teaching behaviors.</p>",
      "authors": [
        "Matthew A.J. Apps",
        "Elise Lesage",
        "Narender Ramnani"
      ],
      "categories": null,
      "citations": 53,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.3669-14.2015",
      "keywords": [
        "Cingulate",
        "Teaching",
        "Reinforcement learning",
        "fMRI",
        "Social",
        "Prediction error"
      ],
      "number_of_pages": 10,
      "pages": "2904-2913",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2015-02-18",
      "selected": null,
      "title": "Vicarious Reinforcement Learning Signals When Instructing Others",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84923012094&origin=inward"
      ]
    },
    {
      "abstract": "Reward signal plays an important role in guiding human learning behaviour. Recent studies have provided evidence that reward signal modulates perceptual learning of basic visual features. Typically, the reward effects on perceptual learning were accompanied with consciously presented reward during the learning process. However, whether an unconsciously presented reward signal that minimizes the contribution of attentional and motivational factors can facilitate perceptual learning remains less well understood. We trained human subjects on a visual motion detection task and subliminally delivered a monetary reward for correct response during the training. The results showed significantly larger learning effect for high reward-associated motion direction than low reward-associated motion direction. Importantly, subjects could neither discriminate the relative values of the subliminal monetary reward nor correctly report the reward-direction contingencies. Our findings suggest that reward signal plays an important modulatory role in perceptual learning even if the magnitude of the reward was not consciously perceived. \u00c2\u00a9 2015 Taylor & Francis.",
      "authors": [
        "Xue, X.",
        "Zhou, X.",
        "Li, S."
      ],
      "categories": null,
      "citations": 7,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/13506285.2014.981625",
      "keywords": [
        "Reward",
        "Feedback",
        "Unconscious",
        "Reinforcement learning",
        "Perceptual learning"
      ],
      "number_of_pages": 18,
      "pages": "161-178",
      "publication": {
        "category": "Journal",
        "cite_score": 3.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13506285",
        "publisher": "Taylor and Francis Ltd.",
        "sjr": 0.904,
        "snip": 0.836,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience",
          "Arts and Humanities (miscellaneous)"
        ],
        "title": "Visual Cognition"
      },
      "publication_date": "2015-02-07",
      "selected": null,
      "title": "Unconscious reward facilitates motion perceptual learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84928938475&origin=inward"
      ]
    },
    {
      "abstract": "Selection mechanisms for WM are ordinarily studied by explicitly cueing a subset of memory items. However, we might also expect the reward associations of stimuli we encounter to modulate their probability of being represented in working memory (WM). Theoretical and computational models explicitly predict that reward value should determine which items will be gated into WM. For example, a model by Braver and colleagues in which phasic dopamine signalling gates WM updating predicts a temporally-specific but not item-specific reward-driven boost to encoding. In contrast, Hazy and colleagues invoke reinforcement learning in cortico-striatal loops and predict an item-wise reward-driven encoding bias. Furthermore, a body of prior work has demonstrated that reward-associated items can capture attention, and it has been shown that attentional capture biases WM encoding. We directly investigated the relationship between reward history and WM encoding. In our first experiment, we found an encoding benefit associated with reward-associated items, but the benefit generalized to all items in the memory array. In a second experiment this effect was shown to be highly temporally specific. We speculate that in real-world contexts in which the environment is sampled sequentially with saccades/shifts in attention, this mechanism could effectively mediate an item-wise encoding bias, because encoding boosts would occur when rewarded items were fixated. \u00c2\u00a9 2015 Taylor & Francis.",
      "authors": [
        "Wallis, G.",
        "Stokes, M.G.",
        "Arnold, C.",
        "Nobre, A.C."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/13506285.2015.1013168",
      "keywords": [
        "Attention",
        "Reward",
        "Working memory"
      ],
      "number_of_pages": 22,
      "pages": "291-312",
      "publication": {
        "category": "Journal",
        "cite_score": 3.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "13506285",
        "publisher": "Taylor and Francis Ltd.",
        "sjr": 0.904,
        "snip": 0.836,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience",
          "Arts and Humanities (miscellaneous)"
        ],
        "title": "Visual Cognition"
      },
      "publication_date": "2015-02-07",
      "selected": null,
      "title": "Reward boosts working memory encoding over a brief temporal window",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84928938496&origin=inward"
      ]
    },
    {
      "abstract": "Fluid intelligence (fluid IQ), defined as the capacity for rapid problem solving and behavioral adaptation, is known to be modulated by learning and experience. Both stressful life events (SLES) and neural correlates of learning [specifically, a key mediator of adaptive learning in the brain, namely the ventral striatal representation of prediction errors (PE)] have been shown to be associated with individual differences in fluid IQ. Here, we examine the interaction between adaptive learning signals (using a well-characterized probabilistic reversal learning task in combination with fMRI) and SLES on fluid IQ measures. We find that the correlation between ventral striatal BOLD PE and fluid IQ, which we have previously reported, is quantitatively modulated by the amount of reported SLES. Thus, after experiencing adversity, basic neuronal learning signatures appear to align more closely with a general measure of flexible learning (fluid IQ), a finding complementing studies on the effects of acute stress on learning. The results suggest that an understanding of the neurobiological correlates of trait variables like fluid IQ needs to take socioemotional influences such as chronic stress into account.",
      "authors": [
        "Friedel, Eva",
        "Schlagenhauf, Florian",
        "Beck, Anne",
        "Dolan, Raymond J.",
        "Huys, Quentin J.M.",
        "Rapp, Michael A.",
        "Heinz, Andreas"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00406-014-0519-3",
      "keywords": [
        "Intelligence",
        "Stress",
        "Ventral striatum",
        "Reinforcement learning",
        "Prediction error signal"
      ],
      "number_of_pages": 9,
      "pages": "35-43",
      "publication": {
        "category": "Journal",
        "cite_score": 9.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09401334",
        "publisher": "D. Steinkopff-Verlag",
        "sjr": 1.356,
        "snip": 1.39,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry",
          "Pharmacology (medical)"
        ],
        "title": "European Archives of Psychiatry and Clinical Neuroscience"
      },
      "publication_date": "2015-02-01",
      "selected": null,
      "title": "The effects of life stress and neural learning signals on fluid intelligence",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00406-014-0519-3.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84922230346&origin=inward"
      ]
    },
    {
      "abstract": "In reinforcement learning (RL), discriminative stimuli (S) allow agents to anticipate the value of a future outcome, and the response that will produce that outcome. We examined this processing by recording EEG locked to S during RL. Incentive value of outcomes and predictive value of S were manipulated, allowing us to discriminate between outcome-related and response-related activity. S predicting the correct response differed from nonpredictive S in the P2. S paired with high-value outcomes differed from those paired with low-value outcomes in a frontocentral positivity and in the P3b. A slow negativity then distinguished between predictive and nonpredictive S. These results suggest that, first, attention prioritizes detection of informative S. Activation of mental representations of these informative S then retrieves representations of outcomes, which in turn retrieve representations of responses that previously produced those outcomes. \u00c2\u00a9 2014 Society for Psychophysiological Research.",
      "authors": [
        "Luque, D.",
        "Mor\u00c3\u00ads, J.",
        "Rushby, J.A.",
        "Le Pelley, M.E."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12302",
      "keywords": [
        "P3b",
        "Cognitive neuroscience",
        "Feedback correct-related positivity",
        "P2",
        "Goal-directed action",
        "P3",
        "Event-related potentials",
        "Reinforcement learning"
      ],
      "number_of_pages": 11,
      "pages": "238-248",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2015-02-01",
      "selected": null,
      "title": "Goal-directed EEG activity evoked by discriminative stimuli in reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84921539024&origin=inward"
      ]
    },
    {
      "abstract": "Animals including human often prefer immediate returns to larger delayed returns. It holds true in the human communications. Standard interpretation of the immediate return preference is that an animal might subjectively discount the value of a delayed reward, and that might choose the larger valued one. The interpretation has been successfully applied to explain behavior of many species including human. However, the description is not necessarily sufficient to apply for interactions of individuals. This study adopts a different approach to seek a possibility that immediate return preference may be reproduced by learning rule to maximize objective outcomes. We show that a synaptic learning rule to achieve the temporal difference (TD) learning for outcome maximization fails the maximization and exhibits immediate return preference if the context is not properly represented as a internal state. \u00c2\u00a9 2014 Elsevier Ltd.",
      "authors": [
        "Yamaguchi, Y.",
        "Aihara, T.",
        "Sakai, Y."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2014.04.004",
      "keywords": [
        "Inter-temporal choice",
        "Reinforcement learning",
        "Synaptic plasticity",
        "Delay discount"
      ],
      "number_of_pages": 8,
      "pages": "83-90",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2015-02-01",
      "selected": null,
      "title": "Immediate return preference emerged from a synaptic learning rule for return maximization",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84921409544&origin=inward"
      ]
    },
    {
      "abstract": "Objective: Impairments in learning are central to autism spectrum disorders. The authors investigated the cognitive and neural basis of these deficits in young adults with autism spectrum disorders using a well-characterized probabilistic reinforcement learning paradigm. Method: The probabilistic selection task was implemented among matched participants with autism spectrum disorders (N=22) and with typical development (N=25), aged 18-40 years, using rapid event-related functional MRI. Participants were trained to choose the correct stimulus in high-probability (AB), medium-probability (CD), and low-probability (EF) pairs, presented with valid feedback 80%, 70%, and 60% of the time, respectively. Whole-brain voxel-wise and parametric modulator analyses examined early and late learning during the stimulus and feedback epochs of the task. Results: The groups exhibited comparable performance on medium-and low-probability pairs. Typically developing persons showed higher accuracy on the high-probability pair, better win-stay performance (selection of the previously rewarded stimulus on the next trial of that type), and more robust recruitment of the anterior and medial prefrontal cortex during the stimulus epoch, suggesting development of an intact reward-based working memory for recent stimulus values. Throughout the feedback epoch, individuals with autism spectrum disorders exhibited greater recruitment of the anterior cingulate and orbito-frontal cortices compared with individuals with typical development, indicating continuing trial-by-trial activity related to feedback processing. Conclusions: Individuals with autism spectrum disorders exhibit learning deficits reflecting impaired ability to develop an effective reward-based working memory to guide stimulus selection. Instead, they continue to rely on trial-by-trial feedback processing to support learning dependent upon engagement of the anterior cingulate and orbito-frontal cortices.",
      "authors": [
        "Solomon, M.",
        "Frank, M.J.",
        "Daniel Ragland, J.",
        "Smith, A.C.",
        "Niendam, T.A.",
        "Lesh, T.A.",
        "Grayson, D.S.",
        "Beck, J.S.",
        "Matter, J.C.",
        "Carter, C.S."
      ],
      "categories": null,
      "citations": 34,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1176/appi.ajp.2014.14010036",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "173-181",
      "publication": {
        "category": "Journal",
        "cite_score": 20.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0002953X",
        "publisher": "American Psychiatric Association",
        "sjr": 4.231,
        "snip": 3.582,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "American Journal of Psychiatry"
      },
      "publication_date": "2015-02-01",
      "selected": null,
      "title": "Feedback-driven trial-by-trial learning in autism spectrum disorders",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84922241218&origin=inward"
      ]
    },
    {
      "abstract": "Recent findings have demonstrated that reward feedback alone can drive motor learning. However, it is not yet clear whether reward feedback alone can lead to learning when a perturbation is introduced abruptly, or how a reward gradient can modulate learning. In this study, we provide reward feedback that decays continuously with increasing error. We asked whether it is possible to learn an abrupt visuomotor rotation by reward alone, and if the learning process could be modulated by combining reward and sensory feedback and/or by using different reward landscapes. We designed a novel visuomotor learning protocol during which subjects experienced an abruptly introduced rotational perturbation. Subjects received either visual feedback or reward feedback, or a combination of the two. Two different reward landscapes, where the reward decayed either linearly or cubically with distance from the target, were tested. Results demonstrate that it is possible to learn from reward feedback alone and that the combination of reward and sensory feedback accelerates learning. An analysis of the underlying mechanisms reveals that although reward feedback alone does not allow for sensorimotor remapping, it can nonetheless lead to broad generalization, highlighting a dissociation between remapping and generalization. Also, the combination of reward and sensory feedback accelerates learning without compromising sensorimotor remapping. These findings suggest that the use of reward feedback is a promising approach to either supplement or substitute sensory feedback in the development of improved neurorehabilitation techniques. More generally, they point to an important role played by reward in the motor learning process. \u00c2\u00a9 2015 the American Physiological Society.",
      "authors": [
        "Nikooyan, A.A.",
        "Ahmed, A.A."
      ],
      "categories": null,
      "citations": 94,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00032.2014",
      "keywords": [
        "Decision-making",
        "Dopaminergic",
        "Temporal-difference model",
        "Reinforcement learning",
        "Sensorimotor mapping"
      ],
      "number_of_pages": 14,
      "pages": "633-646",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2015-01-15",
      "selected": null,
      "title": "Reward feedback accelerates motor learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84928226644&origin=inward"
      ]
    },
    {
      "abstract": "The TAMER framework, which provides a way for agents to learn to solve tasks using human-generated rewards, has been examined in several small-scale studies, each with a few dozen subjects. In this paper, we present the results of the first large-scale study of TAMER, which was performed at the NEMO science museum in Amsterdam and involved 561 subjects. Our results show for the first time that an agent using TAMER can successfully learn to play Infinite Mario, a challenging reinforcement-learning benchmark problem based on the popular video game, given feedback from both adult (N = 209) and child (N = 352) trainers. In addition, our study supports prior studies demonstrating the importance of bidirectional feedback and competitive elements in the training interface. Finally, our results also shed light on the potential for using trainers' facial expressions as a reward signal, as well as the role of age and gender in trainer behavior and agent performance. Copyright \u00c2\u00a9 2015, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
      "authors": [
        "Li, G.",
        "Hung, H.",
        "Whiteson, S."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Human-agent interaction",
        "Reinforcement learning"
      ],
      "number_of_pages": 2,
      "pages": "1771-1772",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "A large-scale study of agents learning from human reward",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84944688839&origin=inward"
      ]
    },
    {
      "abstract": "The neural response to positive and negative feedback differs in their event-related potentials. Most often this difference is interpreted as the result of a negative voltage deflection after negative feedback. This deflection has been referred to as the feedback-related negativity component. The reinforcement learning model of the feedback-related negativity establishes that this component reflects an error monitoring process aimed to increase behavior adjustment progressively. However, a recent proposal suggests that the difference observed is actually due to a positivity reflecting the rewarding value of positive feedbacks - that is, the reward positivity component (RewP). From this it follows that RewP could be found even in the absence of any actionmonitoring processes. We tested this prediction by means of an experiment in which visual target stimuli were intermixed with nontarget stimuli. Three types of targets signaled money gains, money losses, or the absence of either money gain or money loss, respectively. No motor response was required. Event-related potential analyses showed a central positivity in a 270-370ms time window that was elicited by target stimuli signaling money gains, as compared with both stimuli signaling losses and no-gain/no-loss neutral stimuli. This is the first evidence to show that RewP is obtained when stimuli with rewarding values are passively perceived. Copyright \u00c2\u00a9 2015 Wolters Kluwer Health, Inc. All rights reserved.",
      "authors": [
        "Varona-Moya, S.",
        "Mor\u00c3\u00ads, J.",
        "Luque, D."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1097/WNR.0000000000000317",
      "keywords": [
        "Reward positivity component",
        "Feedback-related negativity",
        "Reward",
        "Cingulate cortex"
      ],
      "number_of_pages": 5,
      "pages": "152-156",
      "publication": {
        "category": "Journal",
        "cite_score": 3.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09594965",
        "publisher": "Lippincott Williams and Wilkins Ltd.",
        "sjr": 0.426,
        "snip": 0.45,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "NeuroReport"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Reward positivity is elicited by monetary reward in the absence of response choice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84926134960&origin=inward"
      ]
    },
    {
      "abstract": "Feedback-related negativity (FRN) is an event-related brain potential (ERP) component elicited by errors and negative outcomes. Previous studies proposed that FRN reflects the activity of a general error-processing system that incorporates reward prediction error (RPE). However, other studies reported inconsistent results on this issue - namely, that FRN only reflects the valence of feedback and that the magnitude of RPE is reflected by the other ERP component called P300. The present study focused on the relationship between the FRN amplitude and RPE. ERPs were recorded during a reversal learning task performed by the participants, and a computational model was used to estimate trial-by-trial RPEs, which we correlated with the ERPs. The results indicated that FRN and P300 reflected the magnitude of RPE in negative outcomes and positive outcomes, respectively. In addition, the correlation between RPE and the P300 amplitude was stronger than the correlation between RPE and the FRN amplitude. These differences in the correlation between ERP and RPE components may explain the inconsistent results reported by previous studies; the asymmetry in the correlations might make it difficult to detect the effect of the RPE magnitude on the FRN and makes it appear that the FRN only reflects the valence of feedback. Copyright \u00c2\u00a9 2015 Wolters Kluwer Health, Inc. All rights reserved.",
      "authors": [
        "Bai, Y.",
        "Katahira, K.",
        "Ohira, H."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1097/WNR.0000000000000318",
      "keywords": [
        "P300",
        "Feedback-related negativity",
        "Hybrid model",
        "Reinforcement learning theory"
      ],
      "number_of_pages": 6,
      "pages": "157-162",
      "publication": {
        "category": "Journal",
        "cite_score": 3.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09594965",
        "publisher": "Lippincott Williams and Wilkins Ltd.",
        "sjr": 0.426,
        "snip": 0.45,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "NeuroReport"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Valence-separated representation of reward prediction error in feedback-related negativity and positivity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84925935039&origin=inward"
      ]
    },
    {
      "abstract": "The proceedings contain 57 papers. The topics discussed include: outdoor UAV control and coordination system supported by biological inspired method; an underactuated mechanical hand; modeling of AS/RS using hierarchical and timed colored Petri Nets; multitasking robot-vision for supply Holon execution in intelligent manufacturing; adapting periodic motion primitives to external feedback: modulating and changing the motion; estimation of mass parameters for cooperative human and soft-robots as basis; estimation of contact information using nonlinear optimization; railtruck robotic spring end process operating system; making a map for mobile robot using laser rangefinder; real-time path planning for the robot in known environment; and reinforcement learning approach to generate goal-directed locomotion of a snake-like robot with screw-drive units.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": "368p",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781479967988",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "23rd International Conference on Robotics in Alpe-Adria-Danube Region, IEEE RAAD 2014 - Conference Proceedings"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "23rd International Conference on Robotics in Alpe-Adria-Danube Region, IEEE RAAD 2014 - Conference Proceedings",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84921985996&origin=inward"
      ]
    },
    {
      "abstract": "The proceedings contain 92 papers. The topics discussed include: reinforcement learning with internal reward for multi-agent cooperation: a theoretical approach; revisiting BEECLUST: aggregation of swarm robots with adaptiveness to different light settings; optimization of aircraft landing route and order: an approach of hierarchical evolutionary computation; ant colony optimization based model checking extended by smell-like pheromone; testing software using swarm intelligence: a bee colony optimization approach; a dynamic step-size adaptation roach infestation algorithm for constrained engineering optimization problems; analysis of gait changes caused by simulated left knee disorder; breathing expression for intimate communication corresponding to the physical distance and contact between human and robot; assessing the use of communication robots for recreational activities at nursing homes; biologically-inspired adaptive routing protocol with stochastic route exploration; abundance of connected motifs in transcriptional networks, a case study using random forests regression; recognition for switching of feedback and feedforward process in motor internal model; bio-inspired computation approach for tumor growth with spatial randomness analysis of kidney cancer xenograft pathology slides; and designing behaviour in bio-inspired robots using associative topologies of spiking-neuralnetworks.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "EAI International Conference on Bio-inspired Information and Communications Technologies (BICT)"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "BICT 2015 - 9th EAI International Conference on Bio-Inspired Information and Communications Technologies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85052166734&origin=inward"
      ]
    },
    {
      "abstract": "Transitive inference (the ability to infer that B > D given that B > C and C > D) is a widespread characteristic of serial learning, observed in dozens of species. Despite these robust behavioral effects, reinforcement learning models reliant on reward prediction error or associative strength routinely fail to perform these inferences. We propose an algorithm called betasort, inspired by cognitive processes, which performs transitive inference at low computational cost. This is accomplished by (1) representing stimulus positions along a unit span using beta distributions, (2) treating positive and negative feedback asymmetrically, and (3) updating the position of every stimulus during every trial, whether that stimulus was visible or not. Performance was compared for rhesus macaques, humans, and the betasort algorithm, as well as Q-learning, an established reward-prediction error (RPE) model. Of these, only Q-learning failed to respond above chance during critical test trials. Betasort\u2019s success (when compared to RPE models) and its computational efficiency (when compared to full Markov decision process implementations) suggests that the study of reinforcement learning in organisms will be best served by a feature-driven approach to comparing formal models.",
      "authors": [
        "Greg Jensen",
        "Fabian Mu\u00f1oz",
        "Yelda Alkan",
        "Vincent P. Ferrera",
        "Herbert S. Terrace"
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pcbi.1004523",
      "keywords": [
        "Machine learning",
        "Eye movements",
        "Machine learning algorithms",
        "Human performance",
        "Animal performance",
        "Human learning",
        "Algorithms",
        "Monkeys"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1553734X",
        "publisher": "Public Library of Science",
        "sjr": 1.872,
        "snip": 1.278,
        "subject_areas": [
          "Computational Theory and Mathematics",
          "Ecology",
          "Molecular Biology",
          "Ecology, Evolution, Behavior and Systematics",
          "Cellular and Molecular Neuroscience",
          "Modeling and Simulation",
          "Genetics"
        ],
        "title": "PLoS Computational Biology"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Implicit Value Updating Explains Transitive Inference Performance: The Betasort Model",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84943556406&origin=inward",
        "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1004523&type=printable"
      ]
    },
    {
      "abstract": "In nonmonotonic decision problems, the magnitude of outcomes can both increase and decrease over time depending on the state of the decision problem. These increases and decreases may occur repeatedly and result in a variety of possible outcome distributions. In many previously investigated sequential decision problems, in contrast, outcomes (or the probabilities of obtaining specific outcomes) change monotonically in 1 direction. To investigate how and to what extent people learn in nonmonotonic decision problems, we developed a new task, the Sequential Investment Task (SIT), in which people sequentially decide whether or not to sell shares at several selling points over the course of virtual days. Across trials, they can learn which selling point yields the highest payoff in a specific market. The results of 2 experiments suggest that a reinforcement-learning model generally describes participants' learning processes best. Learning largely depends on an interaction of the complexity of the stochastic process that generates the outcome distribution (i.e., whether the peak selling point is early or late in the selling period and whether there are single or multiple payoff maxima) and the amount of feedback that is available for learning. Although the risk profile in nonmonotonic decision problems renders exploration relatively safe, a clear gap persisted between the choices of people receiving partial feedback (thus facing an exploration- exploitation trade-off) and those of people receiving full feedback: Only the choices of the latter consistently approximated the peak selling points. \u00c2\u00a9 2014 American Psychological Association.",
      "authors": [
        "Frey, R.",
        "Rieskamp, J.",
        "Hertwig, R."
      ],
      "categories": null,
      "citations": 15,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/a0038118",
      "keywords": [
        "Computational modeling",
        "Risk taking",
        "Sequential decision making",
        "Exploration-exploitation trade-off",
        "Reinforcement learning"
      ],
      "number_of_pages": 16,
      "pages": "193-208",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02787393",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Experimental Psychology: Learning Memory and Cognition"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Sell in may and go away? Learning and risk taking in nonmonotonic decision problems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84925822747&origin=inward"
      ]
    },
    {
      "abstract": "<p>Restorative brain-computer interfaces (BCI) are increasingly used to provide feedback of neuronal states in a bid to normalize pathological brain activity and achieve behavioral gains. However, patients and healthy subjects alike often show a large variability, or even inability, of brain self-regulation for BCI control, known as BCI illiteracy. Although current co-adaptive algorithms are powerful for <italic>assistive</italic> BCIs, their inherent class switching clashes with the operant conditioning goal of <italic>restorative</italic> BCIs. Moreover, due to the treatment rationale, the classifier of restorative BCIs usually has a constrained feature space, thus limiting the possibility of classifier adaptation. In this context, we applied a Bayesian model of neurofeedback and reinforcement learning for different threshold selection strategies to study the impact of threshold adaptation of a linear classifier on optimizing restorative BCIs. For each feedback iteration, we first determined the thresholds that result in minimal action entropy and maximal instructional efficiency. We then used the resulting vector for the simulation of continuous threshold adaptation. We could thus show that threshold adaptation can improve reinforcement learning, particularly in cases of BCI illiteracy. Finally, on the basis of information-theory, we provided an explanation for the achieved benefits of adaptive threshold setting.</p>",
      "authors": [
        "Bauer, Robert",
        "Gharabaghi, Alireza"
      ],
      "categories": null,
      "citations": 48,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2015.00036",
      "keywords": [
        "Classification Accuracy",
        "Functional restoration",
        "reinforcement learning model",
        "Brain-Machine-Interface (BMI)",
        "reinforcement learning",
        "Brain-Computer-Interface",
        "Neurofeedback",
        "Bayesian Control Rule",
        "Neurorehabilitation",
        "brain-robot interface"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Reinforcement learning for adaptive threshold control of restorative brain-computer interfaces: a Bayesian simulation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84949117611&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2015.00036/pdf"
      ]
    },
    {
      "abstract": "In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks. \u00c2\u00a9 2014.",
      "authors": [
        "Schmidhuber, J."
      ],
      "categories": null,
      "citations": 12320,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2014.09.003",
      "keywords": [
        "Supervised learning",
        "Deep learning",
        "Evolutionary computation",
        "Unsupervised learning",
        "Reinforcement learning"
      ],
      "number_of_pages": 33,
      "pages": "85-117",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Deep Learning in neural networks: An overview",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84910651844&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning is a powerful machine learning paradigm that allows agents to autonomously learn to maximize a scalar reward. However, it often suffers from poor initial performance and long learning times. This paper discusses how collecting on-line human feedback, both in real time and post hoc, can potentially improve the performance of such learning systems. We use the game Pac-Man to simulate a navigation setting and show that workers are able to accurately identify both when a sub-optimal action is executed, and what action should have been performed instead. Our results demonstrate that the crowd is capable of generating helpful input. We conclude with a discussion the types of errors that occur most commonly when engaging human workers for this task, and a discussion of how such data could be used to improve learning. Our work serves as a critical first step in designing systems that use real-time human feedback to improve the learning performance of automated systems on-the-fly. Copyright \u00c2\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "authors": [
        "De La Cruz, G.V.",
        "Lasecki, W.S.",
        "Peng, B.",
        "Taylor, M.E."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "12-18",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577357216",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAAI Workshop - Technical Report"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Generating real-time crowd advice to improve reinforcement learning agents",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84964603312&origin=inward"
      ]
    },
    {
      "abstract": "Adolescence is associated with quickly changing environmental demands which require excellent adaptive skills and high cognitive flexibility. Feedback-guided adaptive learning and cognitive flexibility are driven by reward prediction error (RPE) signals, which indicate the accuracy of expectations and can be estimated using computational models. Despite the importance of cognitive flexibility during adolescence, only little is known about how RPE processing in cognitive flexibility deviates between adolescence and adulthood.In this study, we investigated the developmental aspects of cognitive flexibility by means of computational models and functional magnetic resonance imaging (fMRI). We compared the neural and behavioral correlates of cognitive flexibility in healthy adolescents (12-16. years) to adults performing a probabilistic reversal learning task. Using a modified risk-sensitive reinforcement learning model, we found that adolescents learned faster from negative RPEs than adults. The fMRI analysis revealed that within the RPE network, the adolescents had a significantly altered RPE-response in the anterior insula. This effect seemed to be mainly driven by increased responses to negative prediction errors.In summary, our findings indicate that decision making in adolescence goes beyond merely increased reward-seeking behavior and provides a developmental perspective to the behavioral and neural mechanisms underlying cognitive flexibility in the context of reinforcement learning. \u00c2\u00a9 2014 The Authors. Published by Elsevier Inc.",
      "authors": [
        "Hauser, T.U.",
        "Iannaccone, R.",
        "Walitza, S.",
        "Brandeis, D.",
        "Brem, S."
      ],
      "categories": null,
      "citations": 115,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2014.09.018",
      "keywords": [
        "Cognitive flexibility",
        "Development",
        "Adolescence",
        "Reward prediction errors",
        "Functional magnetic resonance imaging (fMRI)"
      ],
      "number_of_pages": 8,
      "pages": "347-354",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Cognitive flexibility in adolescence: Neural and behavioral mechanisms of reward prediction error processing in adaptive decision making during development",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84925652948&origin=inward"
      ]
    },
    {
      "abstract": null,
      "authors": [
        "Stopper C.M."
      ],
      "categories": null,
      "citations": 35,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/schbul/sbu165",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "9-14",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "05867614",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Schizophrenia Bulletin"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Dopaminergic circuitry and risk/reward decision making: Implications for schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84925867910&origin=inward"
      ]
    },
    {
      "abstract": "In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A fundamental aspect of music perception is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a novel reinforcement-learning frame-work for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a model of preferences for both songs and song transitions. The model is learned online and is uniquely adapted for each listener. To reduce exploration time, DJ-MC exploits user feedback to initialize a model, which it subsequently updates by reinforcement. We evaluate our framework with human participants using both real song and playlist data. Our results indicate that DJ-MC's ability to recommend sequences of songs provides a significant improvement over more straightforward approaches, which do not take transitions into account. Copyright \u00c2\u00a9 2015, International Foundation for Autonomous Agents and Multiagent Systems.",
      "authors": [
        "Liebman, E.",
        "Saar-Tsechansky, M.",
        "Stone, P."
      ],
      "categories": null,
      "citations": 36,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 9,
      "pages": "591-599",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713854333",
        "issn": "15488403",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "DJ-MC: A reinforcement-learning agent for music playlist recommendation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84944679240&origin=inward"
      ]
    },
    {
      "abstract": "We present ongoing work in the Autonomy Incubator at NASA Langley Research Center (LaRC) exploring the efficacy of a data set aggregation approach to reinforcement learning for small unmanned aerial vehicle (sUAV) flight in dense and cluttered environments with reactive obstacle avoidance. The goal is to learn an autonomous flight model using training experiences from a human piloting a sUAV around static obstacles. The training approach uses video data from a forward-facing camera that records the human pilot\u00e2\u0080\u0099s flight. Various computer vision based features are extracted from the video relating to edge and gradient information. The recorded human-controlled inputs are used to train an autonomous control model that correlates the extracted feature vector to a yaw command. As part of the reinforcement learning approach, the autonomous control model is iteratively updated with feedback from a human agent who corrects undesired model output. This data driven approach to autonomous obstacle avoidance is explored for simulated forest environments furthering autonomous flight under the tree canopy research. This enables flight in previously inaccessible environments which are of interest to NASA researchers in Earth and Atmospheric sciences. \u00c2\u00a9 2015 American Institute of Aeronautics and Astronautics Inc, AIAA. All right reserved.",
      "authors": [
        "Tran, L.",
        "Crossy, C.",
        "Montaguez, G.",
        "Motterx, M.",
        "Neilan, J.",
        "Quallsk, G.",
        "Rothhaar, P.",
        "Trujillo, A.",
        "Allen, B.D."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781624103698",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "15th AIAA Aviation Technology, Integration, and Operations Conference"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Reinforcement learning with autonomous small unmanned aerial vehicles in cluttered environments: \u201cAfter all these years among humans, you still haven\u2019t learned to smile.\u201d - Star Trek\u00ae: The original series, ep. 39",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85026989461&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning describes how a learning agent can achieve optimal behaviour based on interactions with its environment and reward feedback. A limiting factor in reinforcement learning as employed in artificial intelligence is the need for an often prohibitively large number of environment samples before the agent reaches a desirable level of performance. Learning from demonstration is an approach that provides the agent with demonstrations by a supposed expert, from which it should derive suitable behaviour. Yet, one of the challenges of learning from demonstration is that no guarantees can be provided for the quality of the demonstrations, and thus the learned behavior. In this paper, we investigate the intersection of these two approaches, leveraging the theoretical guarantees provided by reinforcement learning, and using expert demonstrations to speed up this learning by biasing exploration through a process called reward shaping. This approach allows us to leverage human input without making an erroneous assumption regarding demonstration optimality. We show experimentally that this approach requires significantly fewer demonstrations, is more robust against suboptimality of demonstrations, and achieves much faster learning than the recently developed HAT algorithm.",
      "authors": [
        "Brys, T.",
        "Harutyunyan, A.",
        "Suay, H.B.",
        "Chernova, S.",
        "Taylor, M.E.",
        "Now\u00c3\u00a9, A."
      ],
      "categories": null,
      "citations": 140,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "3352-3358",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781956792034",
        "issn": "10450823",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IJCAI International Joint Conference on Artificial Intelligence"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Reinforcement learning from demonstration through shaping",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84949777213&origin=inward"
      ]
    },
    {
      "abstract": "With the fast development of human society, on one hand, environmental issues have drawn incomparable attention, so energy efficiency plays a significant role in smart buildings; on the other hand, spending more and more time in buildings leads occupants constantly...",
      "authors": [
        "Zhu, Jiawei",
        "Lauri, Fabrice",
        "Koukam, Abderrafiaa",
        "Hilaire, Vincent"
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-319-17996-4_3",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9789811599521",
        "issn": "21945357",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent Human Systems Integration"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "A Hybrid Intelligent Control System Based on PMV Optimization for Thermal Comfort in Smart Buildings",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84942582404&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-319-17996-4_3.pdf"
      ]
    },
    {
      "abstract": "Feedback-related negativity (FRN) is a negative deflection that appears around 250 ms after the gain or loss of feedback to chosen alternatives in a gambling task in frontocentral regions following outcomes. Few studies have reported FRN enhancement in adolescents compared with adults in a gambling task without probabilistic reinforcement learning, despite the fact that learning from positive or negative consequences is crucial for decision-making during adolescence. Therefore, the aim of the present research was to identify differences in FRN amplitude and latency between adolescents and adults on a gambling task with favorable and unfavorable probabilistic reinforcement learning conditions, in addition to a nonlearning condition with monetary gains and losses. Higher rate scores of high-magnitude choices during the final 30 trials compared with the first 30 trials were observed during the favorable condition, whereas lower rates were observed during the unfavorable condition in both groups. Higher FRN amplitude in all conditions and longer latency in the nonlearning condition were observed in adolescents compared with adults and in relation to losses. Results indicate that both the adolescents and the adults improved their performance in relation to positive and negative feedback. However, the FRN findings suggest an increased sensitivity to external feedback to losses in adolescents compared with adults, irrespective of the presence or absence of probabilistic reinforcement learning. These results reflect processing differences on the neural monitoring system and provide new perspectives on the dynamic development of an adolescent's brain. \u00c2\u00a9 2015 Wolters Kluwer Health, Inc. All rights reserved.",
      "authors": [
        "Mart\u00c3\u00adnez-Vel\u00c3\u00a1zquez, E.S.",
        "Ramos-Loyo, J.",
        "Gonz\u00c3\u00a1lez-Garrido, A.A.",
        "Sequeira, H."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1097/WNR.0000000000000291",
      "keywords": [
        "adolescence",
        "reward",
        "feedback-related negativity",
        "gambling",
        "probabilistic learning"
      ],
      "number_of_pages": 5,
      "pages": "45-49",
      "publication": {
        "category": "Journal",
        "cite_score": 3.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09594965",
        "publisher": "Lippincott Williams and Wilkins Ltd.",
        "sjr": 0.426,
        "snip": 0.45,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "NeuroReport"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "Feedback-related negativity is enhanced in adolescence during a gambling task with and without probabilistic reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84920775332&origin=inward"
      ]
    },
    {
      "abstract": "<p>What are the neural dynamics of choice processes during reinforcement learning? Two largely separate literatures have examined dynamics of reinforcement learning (RL) as a function of experience but assuming a static choice process, or conversely, the dynamics of choice processes in decision making but based on static decision values. Here we show that human choice processes during RL are well described by a drift diffusion model (DDM) of decision making in which the learned trial-by-trial reward values are sequentially sampled, with a choice made when the value signal crosses a decision threshold. Moreover, simultaneous fMRI and EEG recordings revealed that this decision threshold is not fixed across trials but varies as a function of activity in the subthalamic nucleus (STN) and is further modulated by trial-by-trial measures of decision conflict and activity in the dorsomedial frontal cortex (pre-SMA BOLD and mediofrontal theta in EEG). These findings provide converging multimodal evidence for a model in which decision threshold in reward-based tasks is adjusted as a function of communication from pre-SMA to STN when choices differ subtly in reward values, allowing more time to choose the statistically more rewarding option.</p>",
      "authors": [
        "Michael J. Frank",
        "Chris Gagne",
        "Erika Nyhus",
        "Sean Masters",
        "Thomas V. Wiecki",
        "James F. Cavanagh",
        "David Badre"
      ],
      "categories": null,
      "citations": 160,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.2036-14.2015",
      "keywords": [
        "Drift diffusion model",
        "Subthalamic nucleus",
        "Decision making",
        "Basal ganglia",
        "Prefrontal cortex"
      ],
      "number_of_pages": 10,
      "pages": "485-494",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2015-01-01",
      "selected": null,
      "title": "fMRI and EEG Predictors of Dynamic Decision Parameters during Human Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84920972979&origin=inward"
      ]
    },
    {
      "abstract": "Previous research suggests that people with schizophrenia have difficulty learning from positive feedback and when learning needs to occur rapidly. However, they seem to have relatively intact learning from negative feedback when learning occurs gradually. Participants are typically given a limited amount of acquisition trials to learn the reward contingencies and then tested about what they learned. The current study examined whether participants with schizophrenia continue to display these deficits when given extra time to learn the contingences. Participants with schizophrenia and matched healthy controls completed the Probabilistic Selection Task, which measures positive and negative feedback learning separately. Participants with schizophrenia showed a deficit in learning from both positive feedback and negative feedback. These reward learning deficits persisted even if people with schizophrenia are given extra time (up to 10 blocks of 60 trials) to learn the reward contingencies. These results suggest that the observed deficits cannot be attributed solely to slower learning and instead reflect a specific deficit in reinforcement learning. \u00c2\u00a9 2014 Elsevier Ireland Ltd.",
      "authors": [
        "Cicero, D.C.",
        "Martin, E.A.",
        "Becker, T.M.",
        "Kerns, J.G."
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.psychres.2014.08.013",
      "keywords": [
        "Avoidance",
        "Punishment",
        "Reward",
        "Dopamine",
        "Approach"
      ],
      "number_of_pages": 5,
      "pages": "760-764",
      "publication": {
        "category": "Journal",
        "cite_score": 13.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01651781",
        "publisher": "Elsevier Ireland Ltd",
        "sjr": 2.139,
        "snip": 2.134,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "Psychiatry Research"
      },
      "publication_date": "2014-12-30",
      "selected": null,
      "title": "Reinforcement learning deficits in people with schizophrenia persist after extended trials",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84919687699&origin=inward"
      ]
    },
    {
      "abstract": "\u00c2 It has been suggested that adolescents process rewards differently from adults, both cognitively and affectively. In an fMRI study we recorded brain BOLD activity of adolescents (age range = 14\u00e2\u0080\u009315 years) and adults (age range = 20\u00e2\u0080\u009339 years) to investigate the developmental changes in reward processing and decision-making. In a probabilistic reversal learning task, adolescents and adults adapted to changes in reward contingencies. We used a reinforcement learning model with an adaptive learning rate for each trial to model the adolescents\u00ca\u00bc and adults\u00ca\u00bc behavior. Results showed that adolescents possessed a shallower slope in the sigmoid curve governing the relation between expected value (the value of the expected feedback, +1 and \u00e2\u0088\u00921 representing rewarding and punishing feedback, respectively) and probability of stay (selecting the same option as in the previous trial). Trial-by-trial change in expected values after being correct or wrong was significantly different between adolescents and adults. These values were closer to certainty for adults. Additionally, absolute value of model-derived prediction error for adolescents was significantly higher after a correct response but a punishing feedback. At the neural level, BOLD correlates of learning rate, expected value, and prediction error did not significantly differ between adolescents and adults. Nor did we see group differences in the prediction error-related BOLD signal for different trial types. Our results indicate that adults seem to behaviorally integrate punishing feedback better than adolescents in their estimation of the current state of the contingencies. On the basis of these results, we argue that adolescents made decisions with less certainty when compared with adults and speculate that adolescents acquired a less accurate knowledge of their current state, that is, of being correct or wrong. \u00c2  \u00c2\u00a9 2014 Massachusetts Institute of Technology.",
      "authors": [
        "Javadi, A.H.",
        "Schmidt, D.H.K.",
        "Smolka, M.N."
      ],
      "categories": null,
      "citations": 27,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00677",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "2670-2681",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2014-12-20",
      "selected": null,
      "title": "Adolescents adapt more slowly than adults to varying reward contingencies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84949115358&origin=inward"
      ]
    },
    {
      "abstract": "The present research moves in the direction of enabling a bidirectional communication between the subject brain and the prosthetic limb, by providing the prosthesis with an artificial cutaneous sensing through an electronic skin. In this preliminary study, the skin response to the applied mechanical stimuli is conveyed to the human subject using electrotactile stimulation. Experimental tests on two healthy subjects show that a short training through reinforced learning increased considerably the success rate in the identification of the impact location. This preliminary study demonstrates the feasibility of communicating the tactile information from the electronic skin to the human subject using multichannel electrocutaneous stimulation. The result is promising since it implies that it might be possible to achieve the embodiment of the artificial skin into the body scheme of the human subject, relying on the brain ability to successfully process the artificial tactile information. \u00c2\u00a9 2014 IEEE.",
      "authors": [
        "Hartmann, C.",
        "Linde, J.",
        "Dosen, S.",
        "Farina, D.",
        "Seminara, L.",
        "Pinna, L.",
        "Valle, M.",
        "Capurro, M."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/BioCAS.2014.6981802",
      "keywords": [
        "sensor fusion",
        "prosthesis embodiment",
        "electronic skin",
        "electro-tactile feedback",
        "spatial and intensity coding",
        "closed-loop myoelectric prostheses"
      ],
      "number_of_pages": 4,
      "pages": "620-623",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781479923465",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE 2014 Biomedical Circuits and Systems Conference, BioCAS 2014 - Proceedings"
      },
      "publication_date": "2014-12-09",
      "selected": null,
      "title": "Towards prosthetic systems providing comprehensive tactile feedback for utility and embodiment",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84920517469&origin=inward"
      ]
    },
    {
      "abstract": "Introduction: Different techniques for neurofeedback of voluntary brain activations are currently being explored for clinical application in brain-related disorders. One of the most frequently used approaches is the self-regulation of oscillatory signals recorded with electroencephalography (EEG). Many patients are, however, not in a position to use such tools. This could be due to the specific anatomical and physiological properties of the patient's brain after the lesion, as well as to methodological issues related to the technique chosen for recording brain signals. Methods: A patient with extended ischemic lesions of the cortex was unable to gain volitional control of sensorimotor oscillations when using a standard EEG-based approach. We provided him with a neurofeedback set-up with which his brain activity could be recorded from the epidural space by electrocorticography (ECoG). Results: Ipsilesional epidural recordings of field potentials facilitated learned self-regulation of brain oscillations in an online closed-loop paradigm and allowed swift and reliable neurofeedback training for a period of four weeks on a daily basis. Conclusion: Epidural implants may decode and train brain activity even when the cortical physiology is distorted following severe brain injury. Such practice would allow for reinforcement learning of preserved neural networks and may well provide restorative tools for those patients who are worst afflicted.",
      "authors": [
        "Gharabaghi, Alireza",
        "Naros, Georgios",
        "Khademi, Fatemeh",
        "Jesser, Jessica",
        "Sp\u00fcler, Martin",
        "Walter, Armin",
        "Bogdan, Martin",
        "Rosenstiel, Wolfgang",
        "Birbaumer, Niels"
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2014.00429",
      "keywords": [
        "Brain-machine interface",
        "Stroke",
        "Electrocorticography",
        "Neuroprosthetics",
        "Neurofeedback",
        "Cortical lesion",
        "Epidural implant"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2014-12-09",
      "selected": null,
      "title": "Learned self-regulation of the lesioned brain with epidural electrocorticography",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84917736909&origin=inward"
      ]
    },
    {
      "abstract": "Standard economic thinking postulates that increased monetary incentives should increase performance. Human decision makers, however, frequently focus on past performance, a form of reinforcement learning occasionally at odds with rational decision making. We used an incentivized belief-updating task from economics to investigate this conflict through measurements of neural correlates of reward processing. We found that higher incentives fail to improve performance when immediate feedback on decision outcomes is provided. Subsequent analysis of the feedback-related negativity, an early event-related potential following feedback, revealed the mechanism behind this paradoxical effect. As incentives increase, the win/lose feedback becomes more prominent, leading to an increased reliance on reinforcement and more errors. This mechanism is relevant for economic decision making and the debate on performance-based payment. \u00c2\u00a9 The Author (2015). Published by Oxford University Press.",
      "authors": [
        "Achtziger, A.",
        "Al\u00c3\u00b3s-Ferrer, C.",
        "H\u00c3\u00bcgelsch\u00c3\u00a4fer, S.",
        "Steinhauser, M."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/scan/nsv036",
      "keywords": [
        "Bayesian updating",
        "FRN",
        "Incentives",
        "Reinforcement",
        "ERPs"
      ],
      "number_of_pages": 7,
      "pages": "1477-1483",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17495016",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Social Cognitive and Affective Neuroscience"
      },
      "publication_date": "2014-12-05",
      "selected": null,
      "title": "Higher incentives can impair performance: Neural evidence on reinforcement and rationality",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84947761972&origin=inward"
      ]
    },
    {
      "abstract": "Several lines of evidence implicate the striatum in learning from experience on the basis of positive and negative feedback. However, the necessity of the striatum for such learning has been difficult to demonstrate in humans, because brain damage is rarely restricted to this structure. Here we test a rare individual with widespread bilateral damage restricted to the dorsal striatum. His performance was impaired and not significantly different from chance on several classic learning tasks, consistent with current theories regarding the role of the striatum. However, he also exhibited remarkably intact performance on a different subset of learning paradigms. The tasks he could perform can all be solved by learning the value of actions, while those he could not perform can only be solved by learning the value of stimuli. Although dorsal striatum is often thought to play a specific role in action-value learning, we find surprisingly that dorsal striatum is necessary for stimulus-value but not action-value learning in humans. \u00c2\u00a9 2014 The Author.",
      "authors": [
        "Vo, K.",
        "Rutledge, R.B.",
        "Chatterjee, A.",
        "Kable, J.W."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/brain/awu277",
      "keywords": [
        "Stimulus-value",
        "Striatum",
        "Reinforcement learning",
        "Action-value"
      ],
      "number_of_pages": 7,
      "pages": "3129-3135",
      "publication": {
        "category": "Journal",
        "cite_score": 20.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00068950",
        "publisher": "Oxford University Press",
        "sjr": 4.437,
        "snip": 3.147,
        "subject_areas": [
          "Neurology (clinical)"
        ],
        "title": "Brain"
      },
      "publication_date": "2014-12-01",
      "selected": null,
      "title": "Dorsal striatum is necessary for stimulus-value but not action-value learning in humans",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84922255825&origin=inward"
      ]
    },
    {
      "abstract": "Whether positive mood can change reinforcement learning or not remains an open question. In this study, we used a probabilistic learning task and explored whether positive mood could alter the way positive versus negative feedback was used to guide learning. This process was characterized both at the behavioral and electro-encephalographic levels. Thirty two participants were randomly allocated either to a positive or a neutral (control) mood condition. Behavioral results showed that while learning performance was balanced between the two groups, participants in the positive mood group had a higher learning rate than participants in the neutral mood group. At the electrophysiological level, we found that positive mood increased the error-related negativity when the stimulus-response associations were deterministic, selectively (as opposed to random or probabilistic). However, it did not influence the feedback-related negativity. These new findings are discussed in terms of an enhanced internal reward prediction error signal after the induction of positive mood when the probability of getting a reward is high. \u00c2\u00a9 2014 Elsevier B.V.",
      "authors": [
        "Bakic, J.",
        "Jepma, M.",
        "De Raedt, R.",
        "Pourtois, G."
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2014.09.012",
      "keywords": [
        "EEG",
        "Mood",
        "ERP",
        "Positive affect",
        "FRN",
        "ERN",
        "Reinforcement learning"
      ],
      "number_of_pages": 10,
      "pages": "223-232",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2014-12-01",
      "selected": null,
      "title": "Effects of positive mood on probabilistic learning: Behavioral and electrophysiological correlates",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84908024147&origin=inward"
      ]
    },
    {
      "abstract": "Semi-autonomous control schemes can address the limitations of both teleoperation and fully autonomous robotic control of rescue robots in disaster environments by allowing a human operator to cooperate and share such tasks with a rescue robot as navigation, exploration, and victim identification. In this paper, we present a unique hierarchical reinforcement learning-based semi-autonomous control architecture for rescue robots operating in cluttered and unknown urban search and rescue (USAR) environments. The aim of the controller is to enable a rescue robot to continuously learn from its own experiences in an environment in order to improve its overall performance in exploration of unknown disaster scenes. A direction-based exploration technique is integrated in the controller to expand the search area of the robot via the classification of regions and the rubble piles within these regions. Both simulations and physical experiments in USAR-like environments verify the robustness of the proposed HRL-based semi-autonomous controller to unknown cluttered scenes with different sizes and varying types of configurations. \u00c2\u00a9 2014 IEEE.",
      "authors": [
        "Doroodgar, B.",
        "Liu, Y.",
        "Nejat, G."
      ],
      "categories": null,
      "citations": 72,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TCYB.2014.2314294",
      "keywords": [
        "Urban search and rescue",
        "Semi-autonomous control",
        "Hierarchical reinforcement learning",
        "Rescue robots"
      ],
      "number_of_pages": 14,
      "pages": "2719-2732",
      "publication": {
        "category": "Journal",
        "cite_score": 22.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21682267",
        "publisher": "IEEE Advancing Technology for Humanity",
        "sjr": 5.365,
        "snip": 4.286,
        "subject_areas": [
          "Software",
          "Information Systems",
          "Computer Science Applications",
          "Human-Computer Interaction",
          "Electrical and Electronic Engineering",
          "Control and Systems Engineering"
        ],
        "title": "IEEE Transactions on Cybernetics"
      },
      "publication_date": "2014-12-01",
      "selected": null,
      "title": "A learning-based semi-autonomous controller for robotic exploration of unknown disaster scenes while searching for victims",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84942570412&origin=inward"
      ]
    },
    {
      "abstract": "Motor error evaluation appears to be a hierarchically organized process subserved by 2 distinct systems: a higher level system within medial-frontal cortex responsible for movement outcome evaluation (high-level error evaluation) and a lower level posterior system(s) responsible for the mediation of within-movement errors (low-level error evaluation). While a growing body of evidence suggests that a reinforcement learning system within medial-frontal cortex plays a crucial role in the evaluation of high-level errors made during discrete reaching movements and continuous motor tracking, the role of this system in postural control is currently unclear. Participants learned a postural control task via a feedback-driven trial-and-error shaping process. In line with previous findings, electroencephalographic recordings revealed that feedback about movement outcomes elicited a feedback error-related negativity: a component of the human event-related brain potential associated with high-level outcome evaluation within medial-frontal cortex. Thus, the data provide evidence that a high-level error-evaluation system within medial-frontal cortex plays a key role in learning to control our body posture. \u00c2\u00a9 2014 Taylor & Francis Group, LLC.",
      "authors": [
        "Hassall, C.D.",
        "Maclean, S.",
        "Krigolson, O.E."
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/00222895.2014.918021",
      "keywords": [
        "ERP",
        "Balance",
        "fERN",
        "Reinforcement learning",
        "Outcome evaluation",
        "Posture"
      ],
      "number_of_pages": 7,
      "pages": "381-387",
      "publication": {
        "category": "Journal",
        "cite_score": 2.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00222895",
        "publisher": "Routledge",
        "sjr": 0.356,
        "snip": 0.699,
        "subject_areas": [
          "Biophysics",
          "Experimental and Cognitive Psychology",
          "Orthopedics and Sports Medicine",
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Motor Behavior"
      },
      "publication_date": "2014-11-26",
      "selected": null,
      "title": "Hierarchical error evaluation: The role of medial-frontal cortex in postural control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84919819845&origin=inward"
      ]
    },
    {
      "abstract": "Balancing a game is a long process and relies mainly on subjective feedback from human testers and selective interpretation from game developers. As a first step for completely automate game balancing, we propose a methodology to algorithmically choose features and calibrate their parameters for the procedural level generation of a simple runner game based on testers feedback.This methodology is used in a 30 seconds game demo with survey and each playthrough is recorded and fed to a reinforcement learning algorithm. We show that the average fun grade steadily grows, proving the effectiveness of the proposed method. The collected data can be further analysed for insights on new features and other major changes. \u00c2\u00a9 2014 IEEE.",
      "authors": [
        "Rubem Jos\u00e9 Vasconcelos de Medeiros",
        "T\u00e1cio Filipe Vasconcelos de Medeiros"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/SBGAMES.2014.30",
      "keywords": [
        "Game Balancing",
        "Machine Learning",
        "Game Flow",
        "Runner",
        "Reinforcement Learning"
      ],
      "number_of_pages": 6,
      "pages": "109-114",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781479980659",
        "issn": "2159-6654",
        "publisher": "IEEE Computer Society",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2014 Brazilian Symposium on Computer Games and Digital Entertainment"
      },
      "publication_date": "2014-11-12",
      "selected": null,
      "title": "Procedural Level Balancing in Runner Games",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7000038",
        "https://dl.acm.org/doi/10.1109/SBGAMES.2014.30",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84937559219&origin=inward"
      ]
    },
    {
      "abstract": "Objective: Converging research suggests that individuals with schizophrenia show a marked impairment in reinforcement learning, particularly in tasks requiring flexibility and adaptation. The problem has been associated with dopamine reward systems. This study explores, for the first time, the characteristics of this impairment and how it is affected by a behavioral intervention - cognitive remediation. Method: Using computational modelling, 3 reinforcement learning parameters based on the Wisconsin Card Sorting Test (WCST) trial-by-trial performance were estimated: R (reward sensitivity), P (punishment sensitivity), and D (choice consistency). In Study 1 the parameters were compared between a group of individuals with schizophrenia (n = 100) and a healthy control group (n = 50). In Study 2 the effect of cognitive remediation therapy (CRT) on these parameters was assessed in 2 groups of individuals with schizophrenia, one receiving CRT (n = 37) and the other receiving treatment as usual (TAU, n = 34). Results: In Study 1 individuals with schizophrenia showed impairment in the R and P parameters compared with healthy controls. Study 2 demonstrated that sensitivity to negative feedback (P) and reward (R) improved in the CRT group after therapy compared with the TAU group. R and P parameter change correlated with WCST outputs. Improvements in R and P after CRT were associated with working memory gains and reduction of negative symptoms, respectively. Conclusion: Schizophrenia reinforcement learning difficulties negatively influence performance in shift learning tasks. CRT can improve sensitivity to reward and punishment. Identifying parameters that show change may be useful in experimental medicine studies to identify cognitive domains susceptible to improvement. \u00c2\u00a9 The Author 2013.reserved.",
      "authors": [
        "Cella, M.",
        "Bishara, A.J.",
        "Medin, E.",
        "Swan, S.",
        "Reeder, C.",
        "Wykes, T."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/schbul/sbt152",
      "keywords": [
        "Wisconsin Card Sorting test",
        "dopamine",
        "therapy",
        "sensitivity",
        "cognitive remediation",
        "reward systems",
        "reward sensitivity"
      ],
      "number_of_pages": 11,
      "pages": "1422-1432",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "05867614",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Schizophrenia Bulletin"
      },
      "publication_date": "2014-11-01",
      "selected": null,
      "title": "Identifying cognitive remediation change through computational modelling - Effects on reinforcement learning in schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84899961402&origin=inward"
      ]
    },
    {
      "abstract": "Amygdala hemodynamic responses to positive stimuli are attenuated in major depressive disorder (MDD) and normalize with remission. Real-time functional magnetic resonance imaging neurofeedback (rtfMRI-nf) training with the goal of upregulating amygdala activity during recall of happy autobiographical memories (AMs) has been suggested, and recently explored, as a novel therapeutic approach that resulted in improvement in self-reported mood in depressed subjects. In this study, we assessed the possibility of sustained brain changes as well as the neuromodulatory effects of rtfMRI-nf training of the amygdala during recall of positive AMs in MDD and matched healthy subjects. MDD and healthy subjects went through one visit of rtfMRI-nf training. Subjects were assigned to receive active neurofeedback from the left amygdale (LA) or from a control region putatively not modulated by AM recall or emotion regulation, that is, the left horizontal segment of the intraparietal sulcus. To assess lasting effects of neurofeedback in MDD, the resting-state functional connectivity before and after rtfMRI-nf in 27 depressed subjects, as well as in 27 matched healthy subjects before rtfMRI-nf was measured. Results show that abnormal hypo-connectivity with LA in MDD is reversed after rtfMRI-nf training by recalling positive AMs. Although such neuromodulatory changes are observed in both MDD groups receiving feedback from respective active and control brain regions, only in the active group are larger decreases of depression severity associated with larger increases of amygdala connectivity and a significant, positive correlation is found between the connectivity changes and the days after neurofeedback. In addition, active neurofeedback training of the amygdala enhances connectivity with temporal cortical regions, including the hippocampus. These results demonstrate lasting brain changes induced by amygdala rtfMRI-nf training and suggest the importance of reinforcement learning in rehabilitating emotion regulation in depression. \u00c2\u00a9 Copyright 2014, Mary Ann Liebert, Inc. 2014.",
      "authors": [
        "Yuan, H.",
        "Young, K.D.",
        "Phillips, R.",
        "Zotev, V.",
        "Misaki, M.",
        "Bodurka, J."
      ],
      "categories": null,
      "citations": 85,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1089/brain.2014.0262",
      "keywords": [
        "major depressive disorder",
        "resting state",
        "functional connectivity",
        "amygdala",
        "real-time fMRI",
        "neurofeedback"
      ],
      "number_of_pages": 12,
      "pages": "690-701",
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "21580014",
        "publisher": "Mary Ann Liebert Inc.",
        "sjr": 0.893,
        "snip": 0.898,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Brain Connectivity"
      },
      "publication_date": "2014-11-01",
      "selected": null,
      "title": "Resting-state functional connectivity modulation and sustained changes after real-time functional magnetic resonance imaging neurofeedback training in depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84937215932&origin=inward"
      ]
    },
    {
      "abstract": "Learning to control the planar three-link musculoskeletal arm by using an Actor-Critic learning algorithm during reaching movements to stationary target is presented. The arm model used in this study includes three skeletal links (hand, forearm and upper arm), three joints (wrist, elbow and shoulder without redundancy) and six nonlinear monoarticular muscles with redundancy which are modeled based on Hill model. The learning system is composed of Actor and Critic parts. For each part, a single layer neural network is used. This learning system applies six activation commands to six muscles at each instant of time. It also uses a reinforcement (reward) feedback for learning process and controlling the arm movement direction. The results showed that with a learning rate \u00ce\u00b1 = 0.9 and after 20 episodes, Mean square error (MSE), average reward and average time of reaching the target are gradually converged to the values: 0.0056, 0.02262 and 187 s, respectively. After the 20th episode, the learning will be completed. The research suggests a new direction for designation of learning-based controllers for functional electrical stimulation (FES) applications and for arm movement of autonomous robots. \u00c2\u00a9 2014 National Taiwan University.",
      "authors": [
        "Tahami, E.",
        "Jafari, A.H.",
        "Fallah, A."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.4015/S1016237214500641",
      "keywords": [
        "Actor-Critic",
        "Three-link musculoskeletal arm",
        "Reinforcement learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10162372",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biomedical Engineering - Applications, Basis and Communications"
      },
      "publication_date": "2014-10-14",
      "selected": null,
      "title": "Learning to Control the Three-Link Musculoskeletal Arm Using Actor-Critic Reinforcement Learning Algorithm during Reaching Movement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85028232402&origin=inward"
      ]
    },
    {
      "abstract": "Behavioral evidence from human studies suggests that the \u00ce\u00b3-aminobutyric acid type B receptor (GABAB receptor) agonist baclofen modulates reinforcement learning and reduces craving in patients with addiction spectrum disorders. However, in contrast to the well established role of dopamine in reinforcement learning, the mechanisms by which the GABAB receptor influences reinforcement learning in humans remain completely unknown. To further elucidate this issue, a cross-over, double-blind, placebo-controlled study was performed in healthy human subjects (N=15) to test the effects of baclofen (20 and 50mg p.o.) on probabilistic reinforcement learning. Outcomes were the feedback-induced P2 component of the event-related potential, the feedback-related negativity, and the P300 component of the event-related potential. Baclofen produced a reduction of P2 amplitude over the course of the experiment, but did not modulate the feedback-related negativity. Furthermore, there was a trend towards increased learning after baclofen administration relative to placebo over the course of the experiment. The present results extend previous theories of reinforcement learning, which focus on the importance of mesolimbic dopamine signaling, and indicate that stimulation of cortical GABAB receptors in a fronto-parietal network leads to better attentional allocation in reinforcement learning. This observation is a first step in our understanding of how baclofen may improve reinforcement learning in healthy subjects. Further studies with bigger sample sizes are needed to corroborate this conclusion and furthermore, test this effect in patients with addiction spectrum disorder. \u00c2\u00a9 2014 Elsevier B.V. and ECNP.",
      "authors": [
        "Ort, A.",
        "Kometer, M.",
        "Rohde, J.",
        "Seifritz, E.",
        "Vollenweider, F.X."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.euroneuro.2014.08.013",
      "keywords": [
        "GABA",
        "Baclofen",
        "Reinforcement learning",
        "P2"
      ],
      "number_of_pages": 9,
      "pages": "1606-1614",
      "publication": {
        "category": "Journal",
        "cite_score": 8.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0924977X",
        "publisher": "Elsevier B.V.",
        "sjr": 1.442,
        "snip": 1.185,
        "subject_areas": [
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neurology (clinical)",
          "Pharmacology",
          "Pharmacology (medical)"
        ],
        "title": "European Neuropsychopharmacology"
      },
      "publication_date": "2014-10-01",
      "selected": null,
      "title": "The role of GABA<inf>B</inf> receptors in human reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84908010350&origin=inward"
      ]
    },
    {
      "abstract": "IMPORTANCE Attention-deficit/hyperactivity disorder (ADHD) has been associated with deficient decision making and learning. Models of ADHD have suggested that these deficits could be caused by impaired reward prediction errors (RPEs). Reward prediction errors are signals that indicate violations of expectations and are known to be encoded by the dopaminergic system. However, the precise learning and decision-making deficits and their neurobiological correlates in ADHD are not well known. Copyright 2014 American Medical Association. All rights reserved.OBJECTIVE To determine the impaired decision-making and learning mechanisms in juvenile ADHD using advanced computational models, as well as the related neural RPE processes using multimodal neuroimaging.DESIGN, SETTING, AND PARTICIPANTS Twenty adolescents with ADHD and 20 healthy adolescents serving as controls (aged 12-16 years) were examined using a probabilistic reversal learning task while simultaneous functional magnetic resonance imaging and electroencephalogram were recorded.MAIN OUTCOMES AND MEASURES Learning and decision makingwere investigated by contrasting a hierarchical Bayesian model with an advanced reinforcement learning model and by comparing the model parameters. The neural correlates of RPEs were studied in functional magnetic resonance imaging and electroencephalogram.RESULTS Adolescents with ADHD showed more simplistic learning as reflected by the reinforcement learning model (exceedance probability, Px = .92) and had increased exploratory behavior compared with healthy controls (mean [SD] decision steepness parameter \u00ce\u00b2: ADHD, 4.83 [2.97]; controls, 6.04 [2.53]; P = .02). The functional magnetic resonance imaging analysis revealed impaired RPE processing in the medial prefrontal cortex during cue as well as during outcome presentation (P < .05, family-wise error correction). The outcome-related impairment in the medial prefrontal cortex could be attributed to deficient processing at 200 to 400 milliseconds after feedback presentation as reflected by reduced feedback-related negativity (ADHD, 0.61 [3.90] \u00ce\u00bcV; controls, -1.68 [2.52] \u00ce\u00bcV; P = .04).CONCLUSIONS AND RELEVANCE The combination of computational modeling of behavior and multimodal neuroimaging revealed that impaired decision making and learning mechanisms in adolescents with ADHD are driven by impaired RPE processing in the medial prefrontal cortex. This novel, combined approach furthers the understanding of the pathomechanisms in ADHD and may advance treatment strategies.",
      "authors": [
        "Hauser, T.U.",
        "Iannaccone, R.",
        "Ball, J.",
        "Mathys, C.",
        "Brandeis, D.",
        "Walitza, S.",
        "Brem, S."
      ],
      "categories": null,
      "citations": 113,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1001/jamapsychiatry.2014.1093",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "1165-1173",
      "publication": {
        "category": "Journal",
        "cite_score": 31.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2168622X",
        "publisher": "American Medical Association",
        "sjr": 6.578,
        "snip": 6.04,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "JAMA Psychiatry"
      },
      "publication_date": "2014-10-01",
      "selected": null,
      "title": "Role of the medial prefrontal cortex in impaired decision making in juvenile attention-deficit/hyperactivity disorder",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84907720583&origin=inward"
      ]
    },
    {
      "abstract": "Due to information revolution, huge amount of data is available over internet but retrieving correct and relevant data is not an easy task. The information retrieval from search engines is still far greater than that a user can handle and manage. Thus there is need of presenting the information in an abstract way so that one can easily infer the meaning without reading the whole document. In this paper, Human aided text summarizer \"SAAR\" is being proposed for single document. From the document, a term-sentence matrix is generated. The entries in the matrix are weight from Reinforcement Learning. Thus generated summary is shown to the user and if the user approve it then it is the final summary, otherwise new summary is generated as per the user feedback in form of keywords. Results of experiments on DUC2006 documents indicate that the performance of the proposed approach compares very favorably with other approaches in terms of precision, recall, and F-score. \u00c2\u00a9 2014 IEEE.",
      "authors": [
        "Chandra Prakash",
        "Anupam Shukla"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/ISCMI.2014.22",
      "keywords": [
        "information gain",
        "Automated Text Summarization",
        "single document",
        "abstractive summarization technique",
        "text summarization",
        "Human Aided Text summarizer \"Saar\""
      ],
      "number_of_pages": 5,
      "pages": "83-87",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781467367516",
        "issn": null,
        "publisher": "IEEE Computer Society",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - 2014 International Conference on Soft Computing and Machine Intelligence, ISCMI 2014"
      },
      "publication_date": "2014-09-26",
      "selected": null,
      "title": "Human Aided Text Summarizer \"SAAR\" Using Reinforcement Learning",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7079359",
        "https://dl.acm.org/doi/10.1109/ISCMI.2014.22",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84946531442&origin=inward"
      ]
    },
    {
      "abstract": "The human substantia nigra (SN) is thought to consist of two functionally distinct neuronal populations--dopaminergic (DA) neurons in the pars compacta subregion and GABA-ergic neurons in the pars reticulata subregion. However, a functional dissociation between these neuronal populations has not previously been demonstrated in the awake human. Here we obtained microelectrode recordings from the SN of patients undergoing deep brain stimulation (DBS) surgery for Parkinson's disease as they performed a two-alternative reinforcement learning task. Following positive feedback presentation, we found that putative DA and GABA neurons demonstrated distinct temporal dynamics. DA neurons demonstrated phasic increases in activity (250-500 ms post-feedback) whereas putative GABA neurons demonstrated more delayed and sustained increases in activity (500-1000 ms post-feedback). These results provide the first electrophysiological evidence for a functional dissociation between DA and GABA neurons in the human SN. We discuss possible functions for these neuronal responses based on previous findings in human and animal studies.",
      "authors": [
        "Ramayya, Ashwin G.",
        "Zaghloul, Kareem A.",
        "Weidemann, Christoph T.",
        "Baltuch, Gordon H.",
        "Kahana, Michael J."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2014.00655",
      "keywords": [
        "GABA",
        "Human",
        "Dopamine",
        "Neuron",
        "Reinforcement learning",
        "Substantia nigra"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2014-09-09",
      "selected": null,
      "title": "Electrophysiological evidence for functionally distinct neuronal populations in the human substantia nigra",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84973229945&origin=inward"
      ]
    },
    {
      "abstract": "As robotic technologies of varying shapes and forms continue to make their way into our everyday lives, the significance of a humanoid robot's ability to make a human interaction feel natural, engaging and entertaining becomes an area of keen interest in sociable robotics. In this paper, we present our findings on how affective feedback can be used to drive reinforcement learning in human-robot interactions (HRI) and other dialogue systems. We implemented a system where a humanoid robot, named ZOEI, acts as a standup comedian by entertaining a human audience in a bid to generate humor and positively influence the emotional state of the humans. The mood rating of the audience is recorded prior to the interaction session. Using a survey, the eventual emotional state of the human participant is captured after the HRI session. For each audience member, we capture feedback regarding how funny each joke was. We present the implementation of the content selection framework. We share our findings to substantiate the idea that by using expressive behaviors of the humanoid to influence the delivery of content (in this case, jokes) as well as employing reinforcement learning techniques for driving targeted content selection, the robot was able to improve the human mood score progressively across the 16 people who engaged in the study. \u00c2\u00a9 2014 IEEE.",
      "authors": [
        "Ivor D. Addo",
        "Sheikh I. Ahamed"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/ROMAN.2014.6926289",
      "keywords": [
        "HRI",
        "Affective Content",
        "Humanoids",
        "Internet of Things",
        "Affect Generation",
        "Collective Intelligence",
        "Ubiquitous Computing",
        "Affective Computing"
      ],
      "number_of_pages": 6,
      "pages": "423-428",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-4799-6763-6",
        "issn": "1944-9437",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE RO-MAN 2014 - 23rd IEEE International Symposium on Robot and Human Interactive Communication: Human-Robot Co-Existence: Adaptive Interfaces and Systems for Daily Life, Therapy, Assistance and Socially Engaging Interactions"
      },
      "publication_date": "2014-08-25",
      "selected": null,
      "title": "Applying affective feedback to reinforcement learning in ZOEI, a comic humanoid robot",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6926289",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84937574882&origin=inward"
      ]
    },
    {
      "abstract": "Evaluating the positive and negative outcomes of our behaviour is important for action selection and learning. Such reinforcement learning has been shown to engage a specific neural circuitry including the mesencephalic dopamine system and its target areas, the striatum and medial frontal cortex, especially the anterior cingulate cortex (ACC). An intensively pursued debate regards the prevailing influence of feedback expectancy and feedback valence on the engagement of these two brain regions in reinforcement learning and their respective roles are far from being understood. To this end, we used a time estimation task with three different types of feedback that allows disentangling the effect of feedback valence and expectancy using functional magnetic resonance imaging (fMRI). Our results show greater ACC activation after unexpected positive and unexpected negative feedback than after expected feedback and by this sensitivity to unexpected events in general irrespective of their valence.",
      "authors": [
        "Ferdinand, Nicola K.",
        "Opitz, Bertram"
      ],
      "categories": null,
      "citations": 34,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/srep05986",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2045-2322",
        "publisher": "Nature Publishing Group",
        "sjr": 0.973,
        "snip": 1.312,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Scientific Reports"
      },
      "publication_date": "2014-08-07",
      "selected": null,
      "title": "Different aspects of performance feedback engage different brain areas: Disentangling valence and expectancy in feedback processing",
      "urls": [
        "https://www.nature.com/articles/srep05986.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84905823045&origin=inward"
      ]
    },
    {
      "abstract": "Humans exhibit a preference for options they have freely chosen over equally valued options they have not; however, the neural mechanism that drives this bias and its functional significance have yet to be identified. Here, we propose a model in which choice biases arise due to amplified positive reward prediction errors associated with free choice. Using a novel variant of a probabilistic learning task, we show that choice biases are selective to options that are predominantly associated with positive outcomes. A polymorphism in DARPP-32, a gene linked to dopaminergic striatal plasticity and individual differences in reinforcement learning, was found to predict the effect of choice as a function of value. We propose that these choice biases are the behavioral byproduct of a credit assignment mechanism responsible for ensuring the effective delivery of dopaminergic reinforcement learning signals broadcast to the striatum. \u00c2\u00a9 2014 Elsevier Inc.",
      "authors": [
        "Cockburn, J.",
        "Collins, A.G.E.",
        "Frank, M.J."
      ],
      "categories": null,
      "citations": 64,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuron.2014.06.035",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "551-557",
      "publication": {
        "category": "Journal",
        "cite_score": 26.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08966273",
        "publisher": "Cell Press",
        "sjr": 7.736,
        "snip": 3.346,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Neuron"
      },
      "publication_date": "2014-08-06",
      "selected": null,
      "title": "A Reinforcement Learning Mechanism Responsible for the Valuation of Free Choice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84905569399&origin=inward"
      ]
    },
    {
      "abstract": "In situated dialogue with artificial agents (e.g., robots), although a human and an agent are co-present, the agent's representation and the human's representation of the shared environment are significantly mismatched. Because of this misalignment, our previous work has shown that when the agent applies traditional approaches to generate referring expressions for describing target objects with minimum descriptions, the intended objects often cannot be correctly identified by the human. To address this problem, motivated by collaborative behaviors in human referential communication, we have developed two collaborative models - an episodic model and an installment model - for referring expression generation. Both models, instead of generating a single referring expression to describe a target object as in the previous work, generate multiple small expressions that lead to the target object with the goal of minimizing the collaborative effort. In particular, our installment model incorporates human feedback in a reinforcement learning framework to learn the optimal generation strategies. Our empirical results have shown that the episodic model and the installment model outperform previous non-collaborative models with an absolute gain of 6% and 21% respectively.",
      "authors": [
        "Rui Fang",
        "Malcolm Doering",
        "Joyce Y. Chai"
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2892753.2892767",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "1544-1550",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "AAAI Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence"
      },
      "publication_date": "2014-07-27",
      "selected": null,
      "title": "Collaborative models for referring expression generation in situated dialogue",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2892753.2892767"
      ]
    },
    {
      "abstract": "The rationality assumption that underlies mainstream economic theory has proved to be a useful approximation, despite the fact that systematic violations to its predictions can be found. That is, the assumption of rational behavior is useful in understanding the ways in which many successful economic institutions function, although it is also true that actual human behavior falls systematically short of perfect rationality. We consider a possible explanation of this apparent inconsistency, suggesting that mechanisms that rest on the rationality assumption are likely to be successful when they create an environment in which the behavior they try to facilitate leads to the best payoff for all agents on average, and most of the time. Review of basic learning research suggests that, under these conditions, people quickly learn to maximize expected return. This review also shows that there are many situations in which experience does not increase maximization. In many cases, experience leads people to underweight rare events. In addition, the current paper suggests that it is convenient to distinguish between two behavioral approaches to improve economic analyses. The first, and more conventional approach among behavioral economists and psychologists interested in judgment and decision making, highlights violations of the rational model and proposes descriptive models that capture these violations. The second approach studies human learning to clarify the conditions under which people quickly learn to maximize expected return. The current review highlights one set of conditions of this type and shows how the understanding of these conditions can facilitate market design.",
      "authors": [
        "Erev, I.",
        "Roth, A.E."
      ],
      "categories": null,
      "citations": 109,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1073/pnas.1402846111",
      "keywords": [
        "Decisions from experience",
        "Mechanism design",
        "Reinforcement learning",
        "Contingencies of reinforcements",
        "Experience-description gap"
      ],
      "number_of_pages": 8,
      "pages": "10818-10825",
      "publication": {
        "category": "Journal",
        "cite_score": 19.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00278424",
        "publisher": "National Academy of Sciences",
        "sjr": 4.026,
        "snip": 2.765,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "Proceedings of the National Academy of Sciences of the United States of America"
      },
      "publication_date": "2014-07-22",
      "selected": null,
      "title": "Maximization, learning, and economic behavior",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84904623675&origin=inward"
      ]
    },
    {
      "abstract": "Recent research shows that humans are heavily influenced by online social interactions: We are more likely to perform actions which, in the past, have led to positive social feedback. We introduce a quantitative model of behavior changes in response to such feedback, drawing on inverse reinforcement learning and studies of human game playing. The model allows us to make predictions, particularly in the context of social media, about which community a user will select, and to quantify how future selections change based on the feedback a user receives. We show that our model predicts real-world changes in behavior on a dataset gathered from reddit. We also explore how this relatively simple model of individual behavior can lead to complex collective dynamics when there is a population of users, each individual learning in response to feedback and in turn providing feedback to others.",
      "authors": [
        "Das, Sanmay",
        "Lavoie, Allen"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2014-06-30",
      "selected": null,
      "title": "Home Is Where the Up-Votes Are: Behavior Changes in Response to Feedback in Social Media",
      "urls": [
        "http://arxiv.org/pdf/1406.7738.pdf",
        "http://arxiv.org/abs/1406.7738v1",
        "http://arxiv.org/pdf/1406.7738v1"
      ]
    },
    {
      "abstract": "We present a framework for automatically learning human user models from joint-action demonstrations that enables a robot to compute a robust policy for a collaborative task with a human. First, the demonstrated action sequences are clustered into different human types using an unsupervised learning algorithm. A reward function is then learned for each type through the employment of an inverse reinforcement learning algorithm. The learned model is then incorporated into a mixed-observability Markov decision process (MOMDP) formulation, wherein the human type is a partially observable variable. With this framework, we can infer online the human type of a new user that was not included in the training set, and can compute a policy for the robot that will be aligned to the preference of this user. In a human subject experiment (n=30), participants agreed more strongly that the robot anticipated their actions when working with a robot incorporating the proposed framework (p<0.01), compared to manually annotating robot actions. In trials where participants faced difficulty annotating the robot actions to complete the task, the proposed framework significantly improved team efficiency (p<0.01). The robot incorporating the framework was also found to be more responsive to human actions compared to policies computed using a hand-coded reward function by a domain expert (p<0.01). These results indicate that learning human user models from joint-action demonstrations and encoding them in a MOMDP formalism can support effective teaming in human-robot collaborative tasks.",
      "authors": [
        "Stefanos Nikolaidis",
        "Keren Gu",
        "Ramya Ramakrishnan",
        "Julie Shah"
      ],
      "categories": null,
      "citations": 137,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM",
        "arXiv"
      ],
      "doi": "10.1145/2696454.2696455",
      "keywords": [
        "human-robot collaboration",
        "model learning",
        "mixed observability markov decision process"
      ],
      "number_of_pages": 8,
      "pages": "189-196",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450328838",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Artificial Intelligence",
          "Robotics",
          "Learning",
          "Systems and Control"
        ],
        "title": "Proceedings of the Tenth Annual ACM/IEEE International Conference\n  on Human-Robot Interaction (HRI 2015)"
      },
      "publication_date": "2014-05-24",
      "selected": null,
      "title": "Efficient Model Learning for Human-Robot Collaborative Tasks",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84943534743&origin=inward",
        "http://arxiv.org/pdf/1405.6341v1",
        "http://arxiv.org/abs/1405.6341v1",
        "http://dx.doi.org/10.1145/2696454.2696455",
        "https://dl.acm.org/doi/10.1145/2696454.2696455"
      ]
    },
    {
      "abstract": "We introduce and validate a learning model of human behavior change in response to feedback on social media. People who participate in these types of websites, like Wikipedia, Reddit, and others, are learning agents whose choices about how to allocate their effort are dynamic and responsive to how they feel their efforts were received in the past. By explicitly taking into account the reinforcement effects of different types of feedback received on prior contributions, our model is able to significantly outperform all known baselines in predicting future contributions both on synthetic data and on real data collected from the social news site reddit.com. Our model has an intuitive interpretation as users playing mixed strategies in a game-like setting with thousands of other users and thousands of available pure strategies. In this interpretation, our task is then inverse reinforcement learning: recovering users' reward functions based on observed behavior.",
      "authors": [
        "Sanmay Das",
        "Allen Lavoie"
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2615731.2615837",
      "keywords": [
        "social simulation",
        "social media",
        "multi-agent learning"
      ],
      "number_of_pages": 8,
      "pages": "653-660",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450327381",
        "issn": null,
        "publisher": "International Foundation for Autonomous Agents and Multiagent Systems",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems"
      },
      "publication_date": "2014-05-05",
      "selected": null,
      "title": "The effects of feedback on human behavior in social media: an inverse reinforcement learning model",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2615731.2615837"
      ]
    },
    {
      "abstract": "This paper proposes a novel robotic trainer for motor skill learning. It is user-adaptive inspired by the assist-as-needed principle well known in the field of physical therapy. Most previous studies in the field of the robotic assistance of motor skill learning have used predetermined desired trajectories, and it has not been examined intensively whether these trajectories were optimal for each user. Furthermore, the guidance hypothesis states that humans tend to rely too much on external assistive feedback, resulting in interference with the internal feedback necessary for motor skill learning. A few studies have proposed a system that adjusts its assistive strength according to the user's performance in order to prevent the user from relying too much on the robotic assistance. There are, however, problems in these studies, in that a physical model of the user's motor system is required, which is inherently difficult to construct. In this paper, we propose a framework for a robotic trainer that is user-adaptive and that neither requires a specific desired trajectory nor a physical model of the user's motor system, and we achieve this using model-free reinforcement learning. We chose dart-throwing as an example motor-learning task as it is one of the simplest throwing tasks, and its performance can easily be and quantitatively measured. Training experiments with novices, aiming at maximizing the score with the darts and minimizing the physical robotic assistance, demonstrate the feasibility and plausibility of the proposed framework. \u00c2\u00a9 2014 Elsevier Ltd.",
      "authors": [
        "Obayashi, C.",
        "Tamei, T.",
        "Shibata, T."
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2014.01.012",
      "keywords": [
        "Assist-as-needed",
        "Assistive robotics",
        "Reinforcement learning",
        "Motor skill learning"
      ],
      "number_of_pages": 9,
      "pages": "52-60",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2014-05-01",
      "selected": null,
      "title": "Assist-as-needed robotic trainer based on reinforcement learning and its application to dart-throwing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84894061021&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning requires the convergence of signals representing context, action, and reward. While models of basal ganglia function have well-founded hypotheses about the neural origin of signals representing context and reward, the function and origin of signals representing action are less clear. Recent findings suggest that exploratory or variable behaviors are initiated by a wide array of 'action-generating' circuits in the midbrain, brainstem, and cortex. Thus, in order to learn, the striatum must incorporate an efference copy of action decisions made in these action-generating circuits. Here we review several recent neural models of reinforcement learning that emphasize the role of efference copy signals. Also described are ideas about how these signals might be integrated with inputs signaling context and reward. \u00c2\u00a9 2014 Elsevier Ltd.",
      "authors": [
        "Fee, M.S."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.conb.2014.01.012",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "194-200",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09594388",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Current Opinion in Neurobiology"
      },
      "publication_date": "2014-04-01",
      "selected": null,
      "title": "The role of efference copy in striatal learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84896724329&origin=inward"
      ]
    },
    {
      "abstract": "A fundamental challenge for computational and cognitive neuroscience is to understand how reward-based learning and decision-making are made and how accrued knowledge and internal models of the environment are incorporated. Remarkable progress has been made in the field, guided by the midbrain dopamine reward prediction error hypothesis and the underlying reinforcement learning framework, which does not involve internal models ('model-free'). Recent studies, however, have begun not only to address more complex decision-making processes that are integrated with model-free decision-making, but also to include internal models about environmental reward structures and the minds of other agents, including model-based reinforcement learning and using generalized prediction errors. Even dopamine, a classic model-free signal, may work as multiplexed signals using model-based information and contribute to representational learning of reward structure. \u00c2\u00a9 2014 Elsevier Ltd.",
      "authors": [
        "Hiroyuki, N."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.conb.2014.01.001",
      "keywords": [],
      "number_of_pages": 7,
      "pages": "123-129",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09594388",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Current Opinion in Neurobiology"
      },
      "publication_date": "2014-04-01",
      "selected": null,
      "title": "Multiplexing signals in reinforcement learning with internal models and dopamine",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84892891610&origin=inward"
      ]
    },
    {
      "abstract": "The tendency to make unhealthy choices is hypothesized to be related to an individual's temporal discount rate, the theoretical rate at which they devalue delayed rewards. Furthermore, a particular form of temporal discounting, hyperbolic discounting, has been proposed to explain why unhealthy behavior can occur despite healthy intentions. We examine these two hypotheses in turn. We first systematically review studies which investigate whether discount rates can predict unhealthy behavior. These studies reveal that high discount rates for money (and in some instances food or drug rewards) are associated with several unhealthy behaviors and markers of health status, establishing discounting as a promising predictive measure. We secondly examine whether intention-incongruent unhealthy actions are consistent with hyperbolic discounting. We conclude that intention-incongruent actions are often triggered by environmental cues or changes in motivational state, whose effects are not parameterized by hyperbolic discounting. We propose a framework for understanding these state-based effects in terms of the interplay of two distinct reinforcement learning mechanisms: a \u201cmodel-based\u201d (or goal-directed) system and a \u201cmodel-free\u201d (or habitual) system. Under this framework, while discounting of delayed health may contribute to the initiation of unhealthy behavior, with repetition, many unhealthy behaviors become habitual; if health goals then change, habitual behavior can still arise in response to environmental cues. We propose that the burgeoning development of computational models of these processes will permit further identification of health decision-making phenotypes.",
      "authors": [
        "Story, Giles",
        "Vlaev, Ivo",
        "Seymour, Ben",
        "Darzi, Ara",
        "Dolan, Ray"
      ],
      "categories": null,
      "citations": 176,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2014.00076",
      "keywords": [
        "Habit",
        "Addiction",
        "Discounting",
        "Preference reversal",
        "Model-free",
        "Hyperbolic",
        "Model-based",
        "Health"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2014-03-12",
      "selected": null,
      "title": "Does temporal discounting explain unhealthy behavior? A systematic review and reinforcement learning perspective",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84896932271&origin=inward"
      ]
    },
    {
      "abstract": "Aging induces a decline in the ties that bind anatomical networks centered on the prefrontal cortex, which are critical for reinforcement learning and decision making. At the neurophysiological level, the prefrontal cortex may engage electrophysiological oscillatory synchronization to coordinate other brain systems during learning. We recorded scalp EEG from 21 older (mean age 69 years) and 20 young (mean age 22 years) healthy human adults while they learned stimulus-response mappings by trial-and-error using feedback. In young adults, theta-band (4-8 Hz) oscillatory power over medial frontal and anterior frontal cortex predicted learning after errors. Older adults demonstrated a decrease in the theta-band learning-predictive signals over medial frontal but not anterior frontal cortex. This age-related decrease in task-relevant medial frontal theta power may be related to the more general decrease in medial frontal theta power that we observed during rest. These results demonstrate a shift in cortical networks that support reinforcement learning in older adults, and shed new light on the changes in neurophysiological (oscillatory) mechanisms with neurocognitive aging. \u00c2\u00a9 2014 Elsevier Inc.",
      "authors": [
        "Van de Vijver, I.",
        "Cohen, M.X.",
        "Ridderinkhof, K.R."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neurobiolaging.2013.09.006",
      "keywords": [
        "EEG",
        "Medial frontal cortex",
        "Anterior frontal cortex",
        "Theta",
        "Reinforcement learning",
        "Aging",
        "Oscillations"
      ],
      "number_of_pages": 13,
      "pages": "692-704",
      "publication": {
        "category": "Journal",
        "cite_score": 8.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01974580",
        "publisher": "Elsevier Inc.",
        "sjr": 1.521,
        "snip": 1.058,
        "subject_areas": [
          "Neuroscience (all)",
          "Geriatrics and Gerontology",
          "Developmental Biology",
          "Neurology (clinical)",
          "Aging"
        ],
        "title": "Neurobiology of Aging"
      },
      "publication_date": "2014-03-01",
      "selected": null,
      "title": "Aging affects medial but not anterior frontal learning-related theta oscillations",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84889560824&origin=inward"
      ]
    },
    {
      "abstract": "'Anthropomimetic' robots mimic both human morphology and internal structure - skeleton, muscles, compliance and high redundancy - thus presenting a formidable challenge to conventional control. Here we derive a novel controller for this class of robot which learns effective reaching actions through the sustained activation of weighted muscle synergies, an approach which draws upon compelling, recent evidence from animal and human studies, but is almost unexplored to date in the musculoskeletal robot literature. Since the effective synergy patterns for a given robot will be unknown, we derive a reinforcement-learning approach intended to allow their emergence, in particular those patterns aiding linearization of control. Using an extensive physics-based model of the anthropomimetic ECCERobot, we find that effective reaching actions can be learned comprising only two sequential motor co-activation patterns, each controlled by just a single common driving signal. Factor analysis shows the emergent muscle co-activations can be largely reconstructed using weighted combinations of only 13 common fragments. Testing these 'candidate' synergies as drivable units, the same controller now learns the reaching task both faster and better. \u00c2\u00a9 2014 IOP Publishing Ltd.",
      "authors": [
        "Diamond, A.",
        "Holland, O.E."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1088/1748-3182/9/1/016015",
      "keywords": [
        "robotics",
        "musculoskeletal",
        "reinforcement learning",
        "muscle synergies",
        "biomimetic"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17483182",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Bioinspiration and Biomimetics"
      },
      "publication_date": "2014-03-01",
      "selected": null,
      "title": "Reaching control of a full-torso, modelled musculoskeletal robot using muscle synergies emergent under reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84894469747&origin=inward"
      ]
    },
    {
      "abstract": "Background and aims: Individuals with methamphetamine dependence (MD) exhibit dysfunction in brain regions involved in goal maintenance and reward processing when compared with healthy individuals. We examined whether these characteristics also reflect relapse vulnerability within a sample of MD patients. Design: Longitudinal, with functional magnetic resonance imaging (fMRI) and clinical interview data collected at baseline and relapse status collected at 1-year follow-up interview. Setting: Keck Imaging Center, University of California San Diego, USA. Participants: MD patients (n=60) enrolled into an in-patient drug treatment program at baseline. MD participants remaining abstinent at 1-year follow-up (abstinent MD group; n=42) were compared with MD participants who relapsed within this period (relapsed MD group; n=18). Measurements: Behavioral and neural responses to a reinforcement learning (paper-scissors-rock) paradigm recorded during an fMRI session at time of treatment. Findings: The relapsed MD group exhibited greater bilateral inferior frontal gyrus (IFG) and right striatal activation than the abstinent MD group during the learning of reward contingencies (Cohen's d range: 0.60-0.83). In contrast, the relapsed MD group displayed lower bilateral striatum, bilateral insula, left IFG and left anterior cingulate activation than the abstinent MD group (Cohen's d range: 0.90-1.23) in response to winning, tying and losing feedback. Conclusions: Methamphetamine-dependent individuals who achieve abstinence and then relapse show greater inferior frontal gyrus activation during learning, and relatively attenuated striatal, insular and frontal activation in response to feedback, compared with methamphetamine-dependent people who remain abstinent. \u00c2\u00a9 2013 Society for the Study of Addiction.",
      "authors": [
        "Stewart, J.L.",
        "Connolly, C.G.",
        "May, A.C.",
        "Tapert, S.F.",
        "Wittmann, M.",
        "Paulus, M.P."
      ],
      "categories": null,
      "citations": 50,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/add.12403",
      "keywords": [
        "Decision-making",
        "Reward",
        "Learning",
        "Methamphetamine",
        "fMRI",
        "Relapse"
      ],
      "number_of_pages": 12,
      "pages": "460-471",
      "publication": {
        "category": "Journal",
        "cite_score": 10.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09652140",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 2.177,
        "snip": 2.021,
        "subject_areas": [
          "Medicine (miscellaneous)",
          "Psychiatry and Mental Health"
        ],
        "title": "Addiction"
      },
      "publication_date": "2014-03-01",
      "selected": null,
      "title": "Striatum and insula dysfunction during reinforcement learning differentiates abstinent and relapsed methamphetamine-dependent individuals",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84893820981&origin=inward"
      ]
    },
    {
      "abstract": "The anterior insula (AI) plays a key role in affective processing, and insular dysfunction has been noted in several clinical conditions. Real-time functional MRI neurofeedback (rtfMRI-NF) provides a means of helping people learn to self-regulate activation in this brain region. Using the Blood Oxygenated Level Dependant (BOLD) signal from the right AI (RAI) as neurofeedback, we trained participants to increase RAI activation. In contrast, another group of participants was shown 'control' feedback from another brain area. Pre- and post-training affective probes were shown, with subjective ratings and skin conductance response (SCR) measured. We also investigated a reward-related reinforcement learning model of rtfMRI-NF. In contrast to the controls, we hypothesised a positive linear increase in RAI activation in participants shown feedback from this region, alongside increases in valence ratings and SCR to affective probes. Hypothesis-driven analyses showed a significant interaction between the RAI/control neurofeedback groups and the effect of self-regulation. Whole-brain analyses revealed a significant linear increase in RAI activation across four training runs in the group who received feedback from RAI. Increased activation was also observed in the caudate body and thalamus, likely representing feedback-related learning. No positive linear trend was observed in the RAI in the group receiving control feedback, suggesting that these data are not a general effect of cognitive strategy or control feedback. The control group did, however, show diffuse activation across the putamen, caudate and posterior insula which may indicate the representation of false feedback. No significant training-related behavioural differences were observed for valence ratings, or SCR. In addition, correlational analyses based on a reinforcement learning model showed that the dorsal anterior cingulate cortex underpinned learning in both groups. In summary, these data demonstrate that it is possible to regulate the RAI using rtfMRI-NF within one scanning session, and that such reward-related learning is mediated by the dorsal anterior cingulate. \u00c2\u00a9 2013 Elsevier Inc.",
      "authors": [
        "Lawrence, E.J.",
        "Su, L.",
        "Barker, G.J.",
        "Medford, N.",
        "Dalton, J.",
        "Williams, S.C.R.",
        "Birbaumer, N.",
        "Veit, R.",
        "Ranganatha, S.",
        "Bodurka, J.",
        "Brammer, M.",
        "Giampietro, V.",
        "David, A.S."
      ],
      "categories": null,
      "citations": 67,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2013.10.069",
      "keywords": [
        "Real-time fMRI",
        "Self-regulation",
        "Insula",
        "Neurofeedback",
        "Reinforcement learning",
        "Emotion"
      ],
      "number_of_pages": 12,
      "pages": "113-124",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2014-03-01",
      "selected": null,
      "title": "Self-regulation of the anterior insula: Reinforcement learning using real-time fMRI neurofeedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84892899976&origin=inward"
      ]
    },
    {
      "abstract": "To behave adaptively, we must learn from the consequences of our actions. Doing so is difficult when the consequences of an action follow a delay. This introduces the problem of temporal credit assignment. When feedback follows a sequence of decisions, how should the individual assign credit to the intermediate actions that comprise the sequence? Research in reinforcement learning provides 2 general solutions to this problem: model-free reinforcement learning and model-based reinforcement learning. In this review, we examine connections between stimulus-response and cognitive learning theories, habitual and goal-directed control, and model-free and model-based reinforcement learning. We then consider a range of problems related to temporal credit assignment. These include second-order conditioning and secondary reinforcers, latent learning and detour behavior, partially observable Markov decision processes, actions with distributed outcomes, and hierarchical learning. We ask whether humans and animals, when faced with these problems, behave in a manner consistent with reinforcement learning techniques. Throughout, we seek to identify neural substrates of model-free and model-based reinforcement learning. The former class of techniques is understood in terms of the neurotransmitter dopamine and its effects in the basal ganglia. The latter is understood in terms of a distributed network of regions including the prefrontal cortex, medial temporal lobes, cerebellum, and basal ganglia. Not only do reinforcement learning techniques have a natural interpretation in terms of human and animal behavior but they also provide a useful framework for understanding neural reward valuation and action selection. \u00c2\u00a9 2013 American Psychological Association.",
      "authors": [
        "Walsh, M.M.",
        "Anderson, J.R."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/a0033455",
      "keywords": [
        "Sequential choice",
        "Temporal credit assignment",
        "Reinforcement learning"
      ],
      "number_of_pages": 21,
      "pages": "466-486",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00332909",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychological Bulletin"
      },
      "publication_date": "2014-03-01",
      "selected": null,
      "title": "Navigating complex decision spaces: Problems and paradigms in sequential choice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84894423717&origin=inward"
      ]
    },
    {
      "abstract": "Our ability to make decisions is predicated upon our knowledge of the outcomes of the actions available to us. Reinforcement learning theory posits that actions followed by a reward or punishment acquire value through the computation of prediction errors-discrepancies between the predicted and the actual reward. A multitude of neuroimaging studies have demonstrated that rewards and punishments evoke neural responses that appear to reflect reinforcement learning prediction errors [e.g., Krigolson, O. E., Pierce, L. J., Holroyd, C. B., & Tanaka, J. W. Learning to become an expert: Reinforcement learning and the acquisition of perceptual expertise. Journal of Cognitive Neuroscience, 21, 1833-1840, 2009; Bayer, H. M., & Glimcher, P. W. Midbrain dopamine neurons encode a quantitative reward prediction error signal. Neuron, 47, 129-141, 2005; O'Doherty, J. P. Reward representations and reward-related learning in the human brain: Insights from neuroimaging. Current Opinion in Neurobiology, 14, 769-776, 2004; Holroyd, C. B., & Coles, M. G. H. The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity. Psychological Review, 109, 679-709, 2002]. Here, we used the brain ERP technique to demonstrate that not only do rewards elicit a neural response akin to a prediction error but also that this signal rapidly diminished and propagated to the time of choice presentation with learning. Specifically, in a simple, learnable gambling task, we show that novel rewards elicited a feedback error-related negativity that rapidly decreased in amplitude with learning. Furthermore, we demonstrate the existence of a reward positivity at choice presentation, a previously unreported ERP component that has a similar timing and topography as the feedback error-related negativity that increased in amplitude with learning. The pattern of results we observed mirrored the output of a computational model that we implemented to compute reward prediction errors and the changes in amplitude of these prediction errors at the time of choice presentation and reward delivery. Our results provide further support that the computations that underlie human learning and decision-making follow reinforcement learning principles. \u00c2\u00a9 2014 Massachusetts Institute of Technology.",
      "authors": [
        "Olav E. Krigolson",
        "Cameron D. Hassall",
        "Todd C. Handy"
      ],
      "categories": null,
      "citations": 51,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00509",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "635-644",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898-929X",
        "publisher": "MIT Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2014-03-01",
      "selected": null,
      "title": "How we learn to make decisions: Rapid propagation of reinforcement learning prediction errors in humans",
      "urls": [
        "https://dl.acm.org/doi/10.1162/jocn_a_00509",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84893380358&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6818616"
      ]
    },
    {
      "abstract": "We present an approach to subjective computing for the design of future robots that exhibit more adaptive and flexible behavior in terms of subjective intelligence. Instead of encapsulating subjectivity into higher order states, we show by means of a relational approach how subjective intelligence can be implemented in terms of the reciprocity of autonomous self-referentiality and direct world-coupling. Subjectivity concerns the relational arrangement of an agent's cognitive space. This theoretical concept is narrowed down to the problem of coaching a reinforcement learning agent by means of binary feedback. Algorithms are presented that implement subjective computing. The relational characteristic of subjectivity is further confirmed by a questionnaire on human perception of the robot's behavior. The results imply that subjective intelligence cannot be externally observed. In sum, we conclude that subjective intelligence in relational terms is fully tractable and therefore implementable in artificial agents. \u00c2\u00a9 2014 IEEE.",
      "authors": [
        "Patrick Gruneberg",
        "Kenji Suzuki"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "IEEE",
        "ACM",
        "Scopus"
      ],
      "doi": "10.1109/TAMD.2013.2271739",
      "keywords": [
        "subjective computing",
        "Affective computing",
        "human-robot-interaction (HRI)",
        "coaching",
        "social robotics",
        "cognitive robotics"
      ],
      "number_of_pages": 14,
      "pages": "5-18",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1943-0604",
        "publisher": "IEEE Press",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IEEE Transactions on Autonomous Mental Development"
      },
      "publication_date": "2014-03-01",
      "selected": null,
      "title": "An Approach to Subjective Computing: A Robot That Learns From Interaction With Humans",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6565385",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84897853674&origin=inward",
        "https://dl.acm.org/doi/10.1109/TAMD.2013.2271739"
      ]
    },
    {
      "abstract": "The ability to adapt face-to-face social behavior in response to an interaction's changing contingencies is an important aspect of social skill. Individual differences in social ability may depend on how well people learn from social rewards and punishments. Here we relate people's social aptitude to their ability to learn from differences in the reward values of two common social reinforcers, genuine and polite smiles. In a series of experiments, participants experienced a hidden social contingency in which they either learned to repeat actions that received genuine smile feedback and switch after polite smiles or the reverse. A condition with nonsocial feedback served as a comparison measure. Participants showed better ability to repeat actions reinforced with genuine smile feedback than with nonsocial feedback. When participants were required to switch actions following genuine smiles, performance was inhibited relative to nonsocial reinforcement. The ability to detect task contingencies and learn from social rewards predicted self-reported social ability. These novel results suggest that individual differences in reinforcement learning, and particularly in people's motivation to receive social rewards, may relate to social ability in face-to-face interactions. This finding has important implications for understanding the social difficulties that characterize disorders such as autism, depression, and schizophrenia, in which the ability to learn from rewards may be compromised.\u00c2\u00a9 2013 American Psychological Association.",
      "authors": [
        "Heerey, E.A."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/a0031511",
      "keywords": [
        "Social rewards",
        "Learning",
        "Social cognition",
        "Social skill",
        "Smiles"
      ],
      "number_of_pages": 8,
      "pages": "332-339",
      "publication": {
        "category": "Journal",
        "cite_score": 7.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00963445",
        "publisher": "American Psychological Association",
        "sjr": 2.023,
        "snip": 1.954,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Psychology (all)"
        ],
        "title": "Journal of Experimental Psychology: General"
      },
      "publication_date": "2014-02-01",
      "selected": null,
      "title": "Learning from social rewards predicts individual differences in self-reported social ability",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84893378497&origin=inward"
      ]
    },
    {
      "abstract": "In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A fundamental aspect of music perception is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a novel reinforcement-learning framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a model of preferences for both songs and song transitions. The model is learned online and is uniquely adapted for each listener. To reduce exploration time, DJ-MC exploits user feedback to initialize a model, which it subsequently updates by reinforcement. We evaluate our framework with human participants using both real song and playlist data. Our results indicate that DJ-MC's ability to recommend sequences of songs provides a significant improvement over more straightforward approaches, which do not take transitions into account.",
      "authors": [
        "Liebman, Elad",
        "Saar-Tsechansky, Maytal",
        "Stone, Peter"
      ],
      "categories": null,
      "citations": null,
      "comments": "-Updated to the most recent and completed version (to be presented at\n  AAMAS 2015) -Updated author list. in Autonomous Agents and Multiagent Systems\n  (AAMAS) 2015, Istanbul, Turkey, May 2015",
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2014-01-09",
      "selected": null,
      "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation",
      "urls": [
        "http://arxiv.org/abs/1401.1880v2",
        "http://arxiv.org/pdf/1401.1880.pdf",
        "http://arxiv.org/pdf/1401.1880v2"
      ]
    },
    {
      "abstract": "<p>Dopamine (DA) plays a major role in reinforcement learning with increases promoting reward sensitivity (<italic>Go learning</italic>) while decreases facilitate the avoidance of negative outcomes (<italic>NoGo learning</italic>). This is also reflected in adaptations of response time: higher levels of DA enhance speeding up to get a reward, whereas lower levels favor slowing down. The steroid hormones estradiol and progesterone have been shown to modulate dopaminergic tone. Here, we tested 14 women twice during their menstrual cycle, during the follicular (FP) and the luteal phase (LP), applying functional magnetic resonance imaging while they performed a feedback learning task. Subsequent behavioral testing assessed response time preferences with a clock task, in which subjects had to explore the optimal response time (RT) to maximize reward. In the FP subjects displayed a greater learning-related change of their RT than during the LP, when they were required to slow down. Final RTs in the slow condition were also predicted by feedback-related brain activation, but only in the FP. Increased activation of the inferior frontal junction and rostral cingulate zone was thereby predictive of slower and thus better adapted final RTs. Conversely, final RT was faster and less optimal for reward maximization if activation in the ventromedial prefrontal cortex was enhanced. These findings show that hormonal shifts across the menstrual cycle affect adaptation of response speed during reward acquisition with higher RT adjustment in the FP in the condition that requires slowing down. Since high estradiol levels during the FP increase synaptic DA levels, this conforms well to our hypothesis that estradiol supports <italic>Go learning</italic> at the expense of <italic>NoGo learning</italic>. Brain-behavior correlations further indicated that the compensatory capacity to counteract the follicular <italic>Go bias</italic> may be linked to the ability to more effectively monitor action outcomes and suppress bottom-up reward desiring during feedback processing.</p>",
      "authors": [
        "Reimers, Luise",
        "B\u00fcchel, Christian",
        "Diekhof, Esther K."
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2014.00401",
      "keywords": [
        "Estradiol",
        "Reward",
        "Dopamine",
        "RCZ",
        "reinforcement learning",
        "Menstrual Cycle",
        "Time Perception",
        "vmPFC",
        "fMRI"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "How to be patient. The ability to wait for a reward depends on menstrual cycle phase and feedback-related activity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84920601898&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2014.00401/pdf"
      ]
    },
    {
      "abstract": "The proceedings contain 223 papers. The special focus in this conference is on Computers and Information Processing Technologies. The topics include: A large-scale and flexible BMS design; research on map reduce task dynamic balancing strategy based on file label; researches on data privacy in cloud computing based on game theory; the MPI and OpenMP implementation of parallel algorithm for generating Mandelbrot set; adaptive congestion control via dynamic output feedback; an advanced ECC dynamic password-based remote authentication scheme for cloud computing; an audio hiding algorithm based on spline interpolation and wavelet transform; evaluation method for anti-interference performance of measuring based on entropy loss; RE-based weighted distributed equipment layout; algorithms of mining maximum frequent itemsets based on compression matrix; a secret sharing scheme on access structure; a tag-based signature scheme with shorter signature length; an identity-based conditional proxy re-encryption in cloud computing environments; fine-grained access control with efficient revocation in cloud storage; integrity and fidelity evaluation of digital evidence in live forensics; MANET-based stable clustering algorithm and its performance analysis;; reinforcement learning for cloud computing digital library; research on design and construction method of bent functions; a novelty model for reliability assessment of complex system; a multi-attribute decision-making method based on SPAOWA operator; the forecasting research of Beijing tourism demand based on the BP neural network; setting of academic warning based on multivariate copula functions; the application of iteration learning control in silkworm incubation chamber system; fault diagnoses using inverse fuzzy model; a community detection method based on multi-objective optimization method; a high precision indoor positioning system based on VLC and smart handheld; a hybrid IGA-SA algorithm for optimization problems in fault diagnosis; efficient particle swarm optimization algorithm based on affinity propagation; fault diagnosis of transformer based on RBF neural network; features extraction for Lhasa Tibetan speech recognition; FECG extraction algorithm based on BSS using temporal structure and DWT; fuzzy clustering segmentation algorithm research for biomedical image based on artificial life; hourly solar radiation forecast based on k-NN nonparametric regression model; multi-thread memory particle swarm optimization for dynamic optimization problems; recognizing event in short text based on decision tree; reliability analysis based on Markov process for repairable systems; short-term prediction of ship motion based on EMD-SVM; design of a two-wheel self-balanced vehicle system; moving target detection and tracking control simulation platform; new research for traffic state identification; large naval ship evaluation method attended by multiple experts; fuzzy comprehensive evaluation method of experimental teaching quality based on AHP; the planning system based on the postponement manufacturing theory; Tibetan-Chinese named entity extraction based on comparable corpus; the research on the development of smart library; an Iot-based remote health monitoring and management system; analysis of user influence using user behavior and random walk; tax revenue loss under electronic commerce in China; tax administration problem in the e-business environment; research on the elements of e-government performance evaluation; application of information technology in the county government; a study on simulative system of mobile payment; casting process solutions optimization of LFC forklift box; FEM analysis of sheet incremental forming process; application research of PBL method in PLC control technology; a phase anti-ambiguity method for USBL system; smart community security system based on sensor web; research on static game theory based secure routing algorithm in WSN; human activity recognition using smart-phone sensors; design of circuit for alcohol measurements using three-electrode biosensor; study on passivity-based control of TNPC PV grid-connected inverter; theoretical studies of novel high power millimeter generator based on vacuum electron devices; the study on intelligent insecticidal lamp with LED; the intelligent desk lamp designed for special populations; the design of freeform surface Fresnel lens used for LED uniform illumination; stabilization of a class of chaotic systems via single-state adaptive feedback controller; research on 5V internal power supply circuit of switching power supply; optimization design of photovoltaic system MPPT controller; modeling hydraulic power system with surge tank; dispersion curves and fields for a chiral negative refraction parallel-plate waveguide under PMC boundary; a novel dead-time control method for double end converter; a low standby power consumption control circuit for switching power supply; a flyback 25W switching power supply for electric vehicle; a distributed DC power devise based on DSP; a 3 GHz semi-digital delay locked loop with high resolution; a dual-DSP sonobuoy signal processing system; transmission of image information in the network multimedia teaching; context-assisted fast face detection; the EMD analysis AE signals of rock failure under uniaxial compression; simulation of dynamic light scattering signal based on AR Model; disparity estimation of 3-D mesh for stereo video coding; taxi bidirectional search system based on smart phone; dynamic biomedical image segmentation based on wavelet transform; research on issues of across border area in network games; an RFID-assisted digital souvenir generation system; rock image pore identification based on fuzzy C-means clustering and neural networks; a video characteristics watermarking algorithm based on bees evolutional computation; study of slice cell counting system; upper-body pose recognition using cylinder pattern model; tomato disease image retrieval based on composite features the research of ortho-rectification to QuickBird image with more mountains based on ERDAS10.0; pedestrian detection optimization algorithm based on low-altitude UAV; improved face recognition using 2D-LDA with weighted covariance scatter; challenging the recognition of facial expression via deep learning and applied study of size measurement based on image.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9783038351399",
        "issn": "16609336",
        "publisher": "Trans Tech Publications",
        "sjr": 0.112,
        "snip": 0.0,
        "subject_areas": [
          "Engineering (all)"
        ],
        "title": "Applied Mechanics and Materials"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "International Conference on Computers and Information Processing Technologies, ICCIPT 2014",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84903445629&origin=inward"
      ]
    },
    {
      "abstract": "In situated dialogue with artificial agents (e.g., robots), although a human and an agent are co-present, the agent's representation and the human's representation of the shared environment are significantly mismatched. Because of this misalignment, our previous work has shown that when the agent applies traditional approaches to generate referring expressions for describing target objects with minimum descriptions, the intended objects often cannot be correctly identified by the human. To address this problem, motivated by collaborative behaviors in human referential communication, we have developed two collaborative models - An episodic model and an installment model - for referring expression generation. Both models, instead of generating a single referring expression to describe a target object as in the previous work, generate multiple small expressions that lead to the target object with the goal of minimizing the collaborative effort. In particular, our installment model incorporates human feedback in a reinforcement learning framework to learn the optimal generation strategies. Our empirical results have shown that the episodic model and the installment model outperform previous non-collaborative models with an absolute gain of 6% and 21% respectively. Copyright \u00c2\u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
      "authors": [
        "Fang, R.",
        "Doering, M.",
        "Chai, J.Y."
      ],
      "categories": null,
      "citations": 23,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 7,
      "pages": "1544-1550",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577356783",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the National Conference on Artificial Intelligence"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Collaborative models for referring expression generation in situated dialogue",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84908190418&origin=inward"
      ]
    },
    {
      "abstract": "In decision-making processes, the relevance of the information yielded by outcomes varies across time and situations. It increases when previous predictions are not accurate and in contexts with high environmental uncertainty. Previous fMRI studies have shown an important role of medial pFC in coding both reward prediction errors and the impact of this information to guide future decisions. However, it is unclear whether these two processes are dissociated in time or occur simultaneously, suggesting that a common mechanism is engaged. In the present work, we studied the modulation of two electrophysiological responses associated to outcome processing-the feedback-related negativity ERP and frontocentral theta oscillatory activity-with the reward prediction error and the learning rate. Twenty-six participants performed two learning tasks differing in the degree of predictability of the outcomes: a reversal learning task and a probabilistic learning task with multiple blocks of novel cue-outcome associations. We implemented a reinforcement learning model to obtain the single-trial reward prediction error and the learning rate for each participant and task. Our results indicated that midfrontal theta activity and feedback-related negativity increased linearly with the unsigned prediction error. In addition, variations of frontal theta oscillatory activity predicted the learning rate across tasks and participants. These results support the existence of a common brain mechanism for the computation of unsigned prediction error and learning rate. \u00c2\u00a9 2014 Massachusetts Institute of Technology.",
      "authors": [
        "Mas-Herrero, E.",
        "Marco-Pallar\u00c3\u00a9s, J."
      ],
      "categories": null,
      "citations": 55,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00516",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "447-458",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Frontal theta oscillatory activity is a common mechanism for the computation of unexpected outcomes and learning rate",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84893389472&origin=inward"
      ]
    },
    {
      "abstract": "Traditionally, motor learning has been studied as an implicit learning process, one in which movement errors are used to improve performance in a continuous, gradual manner. The cerebellum figures prominently in this literature given well-established ideas about the role of this system in error-based learning and the production of automatized skills. Recent developments have brought into focus the relevance of multiple learning mechanisms for sensorimotor learning. These include processes involving repetition, reinforcement learning, and strategy utilization. We examine these developments, considering their implications for understanding cerebellar function and how this structure interacts with other neural systems to support motor learning. Converging lines of evidence from behavioral, computational, and neuropsychological studies suggest a fundamental distinction between processes that use error information to improve action execution or action selection. While the cerebellum is clearly linked to the former, its role in the latter remains an open question. \u00c2\u00a9 2014 Elsevier B.V.",
      "authors": [
        "Taylor, J.A.",
        "Ivry, R.B."
      ],
      "categories": null,
      "citations": 126,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-444-63356-9.00009-1",
      "keywords": [
        "Adaptation",
        "Cerebellum",
        "Sensorimotor learning",
        "Basal ganglia",
        "Systems interaction",
        "Error-based learning",
        "Ataxia",
        "Reinforcement learning",
        "Prefrontal cortex"
      ],
      "number_of_pages": 37,
      "pages": "217-253",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00796123",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Progress in Brain Research"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Cerebellar and Prefrontal Cortex Contributions to Adaptation, Strategies, and Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84901911152&origin=inward"
      ]
    },
    {
      "abstract": "In a world rich with stimuli and potential actions, organisms must learn which objects are rewarding and which actions produce rewards. Dopamine neurons may play a key role in learning the values of stimuli and actions by representing reward prediction errors (RPEs), the difference between",
      "authors": [
        "Archy O. de Berker",
        "Robb B. Rutledge"
      ],
      "categories": null,
      "citations": 6,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.2854-14.2014",
      "keywords": [],
      "number_of_pages": 3,
      "pages": "12947-12949",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "A Role for the Human Substantia Nigra in Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84907298238&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning enables organisms to adjust their behavior in order to maximize rewards. Electrophysiological recordings of dopaminergic midbrain neurons have shown that they code the difference between actual and predicted rewards, i.e., the reward prediction error, in many species. This error signal is conveyed to both the striatum and cortical areas and is thought to play a central role in learning to optimize behavior. However, in human daily life rewards are diverse and often only indirect feedback is available. Here we explore the range of rewards that are processed by the dopaminergic system in human participants, and examine whether it is also involved in learning in the absence of explicit rewards. While results from electrophysiological recordings in humans are sparse, evidence linking dopaminergic activity to the metabolic signal recorded from the midbrain and striatum with functional magnetic resonance imaging (fMRI) is available. Results from fMRI studies suggest that the human ventral striatum (VS) receives valuation information for a diverse set of rewarding stimuli. These range from simple primary reinforcers such as juice rewards over abstract social rewards to internally generated signals on perceived correctness, suggesting that the VS is involved in learning from trial-and-error irrespective of the specific nature of provided rewards. In addition, we summarize evidence that the VS can also be implicated when learning from observing others, and in tasks that go beyond simple stimulus-action-outcome learning, indicating that the reward system is also recruited in more complex learning tasks. \u00c2\u00a9 2014 Elsevier Inc.",
      "authors": [
        "Daniel, R.",
        "Pollmann, S."
      ],
      "categories": null,
      "citations": 114,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nlm.2014.05.002",
      "keywords": [
        "Human",
        "Reward",
        "Ventral striatum",
        "Learning",
        "Feedback",
        "FMRI"
      ],
      "number_of_pages": 11,
      "pages": "90-100",
      "publication": {
        "category": "Journal",
        "cite_score": 5.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10747427",
        "publisher": "Academic Press Inc.",
        "sjr": 0.986,
        "snip": 0.765,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neurobiology of Learning and Memory"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "A universal role of the ventral striatum in reward-based learning: Evidence from human studies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84902109003&origin=inward"
      ]
    },
    {
      "abstract": "This paper presents a reinforcement learning algorithm, which is inspired by human team dynamics, for autonomous robotic multi agent applications. Individual agents on the team have heterogeneous capabilities and responsibilities. The learning algorithm assigns strictly local credit assignments to individual agents promoting scalability of the team size. The Personality Adjusted Learner (PAL) algorithm is applied to heterogeneous teams of robots with reward adjustments modified from earlier work on homogeneous teams and an information-based action personality type assignment algorithm has been incorporated. The PAL algorithm was tested in a robot combat scenario against both static and learning opponent teams. The PAL team studied included distinct commander, driver, and gunner agents for each robot. The personality preferences for each agent were varied systematically to uncover team performance sensitivities to agent personality preference assignments. The results show a significant sensitivity for the commander agent. This agent selected the robot strategy, and it was noted that the better performing commander personalities were linked to team oriented actions, rather than more selfish strategies. The driver and gunner agent performance remained insensitive to personality assignment. The driver and gunner actions did not apply at the strategic level, indicating that personality preferences may be important for agents responsible for learning to cooperate intentionally with teammates. \u00c2\u00a9 2013 Elsevier B.V. All rights reserved.",
      "authors": [
        "Recchia, T.",
        "Chung, J.",
        "Pochiraju, K."
      ],
      "categories": null,
      "citations": 3,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.bica.2013.10.003",
      "keywords": [
        "Heterogeneous robot team",
        "Robot teaming",
        "Multi-agent system",
        "Reinforcement learning",
        "Myers-Briggs Type Indicator"
      ],
      "number_of_pages": 11,
      "pages": "87-97",
      "publication": {
        "category": "Journal",
        "cite_score": 3.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2212683X",
        "publisher": "Elsevier B.V.",
        "sjr": 0.556,
        "snip": 1.24,
        "subject_areas": [
          "Artificial Intelligence",
          "Experimental and Cognitive Psychology",
          "Cognitive Neuroscience"
        ],
        "title": "Biologically Inspired Cognitive Architectures"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Performance of heterogeneous robot teams with personality adjusted learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84892435134&origin=inward"
      ]
    },
    {
      "abstract": "The communication among heterogeneous embedded devices could lead to correctness problems in M2M environment. Sometimes, it is not easy to classify the data because they may provide wrong or uncertain information. The data from these devices should be gathered in a safe, efficient, and right manner without the help of server or human intervention; even the low-level information from each device causes interoperability problems. This data gathering or data fusion process is very important because the data mapping result could be understood as totally different situation and hence cause different reaction, feedback, and controls. In this paper, we propose a hierarchical aggregation for uncertain sensor data using reinforcement learning to get correct and efficient data gathering result for reliable wireless sensor network. In our proposal, we add a new category for uncertain data and classify them through reinforcement learning using hierarchical subcategories. By adopting our proposed aggregation, false classification caused by uncertain data can be decreased and the correctness of data gathering can be enhanced. \u00c2\u00a9 2014 Yunjeong Choi and Inshil Doh.",
      "authors": [
        "Choi, Y.",
        "Doh, I."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1155/2014/535707",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15501329",
        "publisher": "Sage-Hindawi",
        "sjr": 0.562,
        "snip": 0.84,
        "subject_areas": [
          "Engineering (all)",
          "Computer Networks and Communications"
        ],
        "title": "International Journal of Distributed Sensor Networks"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Hierarchical aggregation of uncertain sensor data for M2M wireless sensor network using reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84902147322&origin=inward"
      ]
    },
    {
      "abstract": "This study examined neurocognitive differences between children and adults in the ability to learn and adapt simple stimulus-response associations through feedback. Fourteen typically developing children (mean age = 10.2) and 15 healthy adults (mean age = 25.5) completed a simple task in which they learned to associate visually presented stimuli with manual responses based on performance feedback (acquisition phase), and then reversed and re-learned those associations following an unexpected change in reinforcement contingencies (reversal phase). Electrophysiological activity was recorded throughout task performance. We found no group differences in learning-related changes in performance (reaction time, accuracy) or in the amplitude of event-related potentials (ERPs) associated with stimulus processing (P3 ERP) or feedback processing (feedback-related negativity; FRN) during the acquisition phase. However, children's performance was significantly more disrupted by the reversal than adults and FRN amplitudes were significantly modulated by the reversal phase in children but not adults. These findings indicate that children have specific difficulties with reinforcement learning when acquired behaviours must be altered. This may be caused by the added demands on immature executive functioning, specifically response monitoring, created by the requirement to reverse the associations, or a developmental difference in the way in which children and adults approach reinforcement learning. \u00c2\u00a9 2013 The Authors.",
      "authors": [
        "Shephard, E.",
        "Jackson, G.M.",
        "Groom, M.J."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.dcn.2013.12.001",
      "keywords": [
        "P3",
        "Development",
        "Reinforcement learning",
        "Feedback-related negativity (FRN)"
      ],
      "number_of_pages": 12,
      "pages": "94-105",
      "publication": {
        "category": "Journal",
        "cite_score": 8.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18789293",
        "publisher": "Elsevier B.V.",
        "sjr": 1.792,
        "snip": 1.42,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Developmental Cognitive Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Learning and altering behaviours by reinforcement: Neurocognitive differences between children and adults",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891290553&origin=inward"
      ]
    },
    {
      "abstract": "Surprise drives learning. Various neural \"prediction error\" signals are believed to underpin surprise-based reinforcement learning. Here, we report a surprise signal that reflects reinforcement learning but is neither un/signed reward prediction error (RPE) nor un/signed state prediction error (SPE). To exclude these alternatives, we measured surprise responses in the absence of RPE and accounted for a host of potential SPE confounds. This new surprise signal was evident in ventral striatum, primary sensory cortex, frontal poles, and amygdala. We interpret these findings via a normative model of surprise. \u00c2\u00a9 2014 The Authors.",
      "authors": [
        "Chumbley, J.R.",
        "Burke, C.J.",
        "Stephan, K.E.",
        "Friston, K.J.",
        "Tobler, P.N.",
        "Fehr, E."
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/hbm.22513",
      "keywords": [
        "Learning",
        "Reward",
        "Prediction error"
      ],
      "number_of_pages": 10,
      "pages": "4805-4814",
      "publication": {
        "category": "Journal",
        "cite_score": 9.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10659471",
        "publisher": "Wiley-Liss Inc.",
        "sjr": 1.688,
        "snip": 1.395,
        "subject_areas": [
          "Radiological and Ultrasound Technology",
          "Anatomy",
          "Neurology",
          "Neurology (clinical)",
          "Radiology, Nuclear Medicine and Imaging"
        ],
        "title": "Human Brain Mapping"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Surprise beyond prediction error",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84904510838&origin=inward"
      ]
    },
    {
      "abstract": "<p>Computational models of learning have proved largely successful in characterizing potential mechanisms which allow humans to make decisions in uncertain and volatile contexts. We report here findings that extend existing knowledge and show that a modified reinforcement learning model, which has separate parameters according to whether the previous trial gave a reward or a punishment, can provide the best fit to human behavior in decision making under uncertainty. More specifically, we examined the fit of our modified reinforcement learning model to human behavioral data in a probabilistic two-alternative decision making task with rule reversals. Our results demonstrate that this model predicted human behavior better than a series of other models based on reinforcement learning or Bayesian reasoning. Unlike the Bayesian models, our modified reinforcement learning model does not include any representation of rule switches. When our task is considered purely as a machine learning task, to gain as many rewards as possible without trying to describe human behavior, the performance of modified reinforcement learning and Bayesian methods is similar. Others have used various computational models to describe human behavior in similar tasks, however, we are not aware of any who have compared Bayesian reasoning with reinforcement learning modified to differentiate rewards and punishments.</p>",
      "authors": [
        "Duffin, Elaine",
        "Bland, Amy R.",
        "Schaefer, Alexandre",
        "De Kamps, Marc"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2014.00030",
      "keywords": [
        "volatility",
        "Reward and punishment",
        "reinforcement learning",
        "uncertainty",
        "Decision Making",
        "Bayesian learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Differential effects of reward and punishment in decision making under uncertainty: a computational study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84898034046&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2014.00030/pdf"
      ]
    },
    {
      "abstract": "In this brief, a novel adaptive-critic-based neural network (NN) controller is investigated for nonlinear pure-feedback systems. The controller design is based on the transformed predictor form, and the actor-critic NN control architecture includes two NNs, whereas the critic NN is used to approximate the strategic utility function, and the action NN is employed to minimize both the strategic utility function and the tracking error. A deterministic learning technique has been employed to guarantee that the partial persistent excitation condition of internal states is satisfied during tracking control to a periodic reference orbit. The uniformly ultimate boundedness of closed-loop signals is shown via Lyapunov stability analysis. Simulation results are presented to demonstrate the effectiveness of the proposed control. \u00c2\u00a9 2012 IEEE.",
      "authors": [
        "Xu, B.",
        "Yang, C.",
        "Shi, Z."
      ],
      "categories": null,
      "citations": 231,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2013.2292704",
      "keywords": [
        "pure-feedback system",
        "radial basis function neural network (RBF NN)",
        "output feedback control",
        "discrete-time system",
        "Approximate dynamic programming"
      ],
      "number_of_pages": 7,
      "pages": "635-641",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Reinforcement learning output feedback NN control using deterministic learning technique",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84897663275&origin=inward"
      ]
    },
    {
      "abstract": "Visuo-motor adaptation suffers at older working age. The age-related decline of behavioural adjustments is accompanied by reduced explicit knowledge of the visuo-motor transformation. It disappears when explicit knowledge is kept constant across the age range, except for particularly high levels of explicit knowledge. According to these findings, at older adult age both the acquisition of explicit knowledge and its application for strategic corrections become poorer. Recently it has been posited that visuo-motor adaptation can involve model-free reinforcement mechanisms of learning in addition to model-based mechanisms. We tested whether age-related declines of reinforcement learning can also contribute to the age-related changes of visuo-motor adaptation. Therefore we enhanced the contribution of reinforcement learning to visuo-motor adaptation by way of introducing salient markers of success and failure during practice. With such modified practice conditions, there were residual age-related variations of behavioural adjustments at all levels of explicit knowledge, even when explicit knowledge was absent. The residual age-related variations were observed for practiced target directions only, but not for new target directions. These findings are consistent with an age-related decline of model-free reinforcement learning as a third factor in the age-related decline of visuo-motor adaptation. Under practice conditions, which spur model-free reward-based learning, this factor adds to the decrements of the acquisition of explicit knowledge and its use for strategic corrections.",
      "authors": [
        "Heuer, Herbert",
        "Hegele, Mathias"
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnagi.2014.00152",
      "keywords": [
        "After-effect",
        "Model-based learning",
        "Explicit knowledge",
        "Reinforcement learning",
        "Visuo-motor rotation"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.2,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1663-4365",
        "publisher": "Frontiers Media SA",
        "sjr": 1.211,
        "snip": 1.118,
        "subject_areas": [
          "Aging",
          "Cognitive Neuroscience"
        ],
        "title": "Frontiers in Aging Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Age-related variations of visuo-motor adaptation beyond explicit knowledge",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84904602214&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning models make use of reward prediction errors (RPEs), the difference between an expected and obtained reward. There is evidence that the brain computes RPEs, but an outstanding question is whether positive RPEs (\"better than expected\") and negative RPEs (\"worse than expected\") are represented in a single integrated system. An electrophysiological component, feedback related negativity, has been claimed to encode an RPE but its relative sensitivity to the utility of positive and negative RPEs remains unclear. This study explored the question by varying the utility of positive and negative RPEs in a design that controlled for other closely related properties of feedback and could distinguish utility from salience. It revealed a mediofrontal sensitivity to utility, for positive RPEs at 275-310. ms and for negative RPEs at 310-390. ms. These effects were preceded and succeeded by a response consistent with an unsigned prediction error, or \"salience\" coding. \u00c2\u00a9 2014 Elsevier Ltd.",
      "authors": [
        "Sambrook, T.D.",
        "Goslin, J."
      ],
      "categories": null,
      "citations": 48,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuropsychologia.2014.06.004",
      "keywords": [
        "Reward prediction error (RPE)",
        "Dopamine",
        "Saliency",
        "Feedback related negativity (FRN)",
        "Event-related potential (ERP)",
        "Unsigned prediction error"
      ],
      "number_of_pages": 10,
      "pages": "1-10",
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00283932",
        "publisher": "Elsevier Ltd.",
        "sjr": 0.995,
        "snip": 1.03,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neuropsychologia"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Mediofrontal event-related potentials in response to positive, negative and unsigned prediction errors",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84903374669&origin=inward"
      ]
    },
    {
      "abstract": "Robots deployed to assist and collaborate with humans in complex domains need the ability to represent and reason with incomplete domain knowledge, and to learn from minimal feedback obtained from non-expert human participants. This paper presents an architecture...",
      "authors": [
        "Sridharan, Mohan",
        "Rainge, Sarah"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-3-319-11973-1_33",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "320-329",
      "publication": {
        "category": "Book",
        "cite_score": 2.2,
        "is_potentially_predatory": false,
        "isbn": "9789819964796",
        "issn": "03029743",
        "publisher": "Springer Verlag",
        "sjr": 0.32,
        "snip": 0.542,
        "subject_areas": [
          "Theoretical Computer Science",
          "Computer Science (all)"
        ],
        "title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Integrating Reinforcement Learning and Declarative Programming to Learn Causal Laws in Dynamic Domains",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84910008021&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-3-319-11973-1_33.pdf"
      ]
    },
    {
      "abstract": "In order to understand the development of non-genetically encoded actions during an animal's lifespan, it is necessary to analyze the dynamics and evolution of learning rules producing behavior. Owing to the intrinsic stochastic and frequency-dependent nature of learning dynamics, these rules are often studied in evolutionary biology via agent-based computer simulations. In this paper, we show that stochastic approximation theory can help to qualitatively understand learning dynamics and formulate analytical models for the evolution of learning rules. We consider a population of individuals repeatedly interacting during their lifespan, and where the stage game faced by the individuals fluctuates according to an environmental stochastic process. Individuals adjust their behavioral actions according to learning rules belonging to the class of experience-weighted attraction learning mechanisms, which includes standard reinforcement and Bayesian learning as special cases. We use stochastic approximation theory in order to derive differential equations governing action play probabilities, which turn out to have qualitative features of mutator-selection equations. We then perform agent-based simulations to find the conditions where the deterministic approximation is closest to the original stochastic learning process for standard 2-action 2-player fluctuating games, where interaction between learning rules and preference reversal may occur. Finally, we analyze a simplified model for the evolution of learning in a producer-scrounger game, which shows that the exploration rate can interact in a non-intuitive way with other features of co-evolving learning rules. Overall, our analyses illustrate the usefulness of applying stochastic approximation theory in the study of animal learning. \u00c2\u00a9 2013 Elsevier Inc.",
      "authors": [
        "Dridi, S.",
        "Lehmann, L."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.tpb.2013.09.003",
      "keywords": [
        "Stochastic approximation",
        "Fluctuating environments",
        "Fictitious play",
        "Evolutionary game theory",
        "Producer-scrounger game",
        "Reinforcement learning"
      ],
      "number_of_pages": 17,
      "pages": "20-36",
      "publication": {
        "category": "Journal",
        "cite_score": 2.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00405809",
        "publisher": "Academic Press Inc.",
        "sjr": 0.445,
        "snip": 0.714,
        "subject_areas": [
          "Ecology, Evolution, Behavior and Systematics"
        ],
        "title": "Theoretical Population Biology"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "On learning dynamics underlying the evolution of learning rules",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891158799&origin=inward"
      ]
    },
    {
      "abstract": "<p>For people living with paralysis, restoration of hand function remains the top priority because it leads to independence and improvement in quality of life. In approaches to restore hand and arm function, a goal is to better engage voluntary control and counteract maladaptive brain reorganization that results from non-use. Standard rehabilitation augmented with developments from the study of brain-computer interfaces could provide a combined therapy approach for motor cortex rehabilitation and to alleviate motor impairments. In this paper, an adaptive brain-computer interface system intended for application to control a functional electrical stimulation (FES) device is developed as an experimental test bed for augmenting rehabilitation with a brain-computer interface. The system's performance is improved throughout rehabilitation by passive user feedback and reinforcement learning. By continuously adapting to the user's brain activity, similar adaptive systems could be used to support clinical brain-computer interface neurorehabilitation over multiple days.</p>",
      "authors": [
        "Roset, Scott A.",
        "Gant, Katie",
        "Prasad, Abhishek",
        "Sanchez, Justin C."
      ],
      "categories": null,
      "citations": 13,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2014.00415",
      "keywords": [
        "Brain-computer interface",
        "reinforcement learning",
        "Neurorehabilitation",
        "Spinal Cord Injuries",
        "Error-related potentials"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "An adaptive brain actuated system for augmenting rehabilitation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84920558115&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2014.00415/pdf"
      ]
    },
    {
      "abstract": "We introduce and validate a learning model of human behavior change in response to feedback on social media. People who participate in these types of websites, like Wikipedia, Reddit, and others, are learning agents whose choices about how to allocate their effort are dynamic and responsive to how they feel their efforts were received in the past. By explicitly taking into account the reinforcement effects of different types of feedback received on prior contributions, our model is able to significantly outperform all known baselines in predicting future contributions both on synthetic data and on real data collected from the social news site reddit.com. Our model has an intuitive interpretation as users playing mixed strategies in a game-like setting with thousands of other users and thousands of available pure strategies. In this interpretation, our task is then inverse reinforcement learning: recovering users' reward functions based on observed behavior. Copyright \u00c2\u00a9 2014, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
      "authors": [
        "Das, S.",
        "Lavoie, A."
      ],
      "categories": null,
      "citations": 19,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Social media",
        "Social simulation",
        "Multi-agent learning"
      ],
      "number_of_pages": 8,
      "pages": "653-660",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781634391313",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "The effects of feedback on human behavior in social media: An inverse reinforcement learning model",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84911426890&origin=inward"
      ]
    },
    {
      "abstract": "Behavioural traditions have only been described for a small subset of species, and the factors responsible for the maintenance of traditions over time are unclear. Redfronted lemurs are known to learn socially but traditions have not been described in the wild. We conducted a social diffusion experiment over three experimental years with artificial feeding boxes that could be opened in two different ways (pushing or pulling a door). Six out of 14 individuals that participated in at least 2&nbsp;years exhibited a stable preference: five lemurs maintained a pull and one lemur a push preference, suggesting that habit formation and reinforcement learning may have lead to preferences over time. The remaining individuals exhibited fluctuating preferences and switched between showing a preference or no preference, but never switched between preferences. This instability might have been due to the low level of difficulty and/or the low object specificity of the task. The majority of lemurs additionally scrounged. Scrounging was not influenced by age, sex or success in manipulating the boxes. Thus, redfronted lemurs appear to use the two techniques flexibly but also scrounged opportunistically to get access to the rewards, indicating that traditions might be stabilized by multiple factors.",
      "authors": [
        "Schnoell, Anna Viktoria",
        "Dittmann, Marie T.",
        "Fichtel, Claudia"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10071-013-0636-9",
      "keywords": [
        "Eulemur rufifrons",
        "Wild lemurs",
        "Long-term study",
        "Social learning",
        "Stability of traditions",
        "Scrounging"
      ],
      "number_of_pages": 10,
      "pages": "45-54",
      "publication": {
        "category": "Journal",
        "cite_score": 4.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "14359448",
        "publisher": "Springer Verlag",
        "sjr": 0.919,
        "snip": 1.014,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Ecology, Evolution, Behavior and Systematics"
        ],
        "title": "Animal Cognition"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Human-introduced long-term traditions in wild redfronted lemurs?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891625622&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10071-013-0636-9.pdf"
      ]
    },
    {
      "abstract": "Children and adolescents learn to regulate their behavior by utilizing feedback from the environment but exactly how this ability develops remains unclear. To investigate this question, we recorded the event-related brain potential (ERP) from children (8-13 years), adolescents (14-17 years) and young adults (18-23 years) while they navigated a \"virtual maze\" in pursuit of monetary rewards. The amplitude of the reward positivity, an ERP component elicited by feedback stimuli, was evaluated for each age group. A current theory suggests the reward positivity is produced by the impact of reinforcement learning signals carried by the midbrain dopamine system on anterior cingulate cortex, which utilizes the signals to learn and execute extended behaviors. We found that the three groups produced a reward positivity of comparable size despite relatively longer ERP component latencies for the children, suggesting that the reward processing system reaches maturity early in development. We propose that early development of the midbrain dopamine system facilitates the development of extended goal-directed behaviors in anterior cingulate cortex. \u00c2\u00a9 2014 The Authors.",
      "authors": [
        "Lukie, C.N.",
        "Montazer-Hojat, S.",
        "Holroyd, C.B."
      ],
      "categories": null,
      "citations": 44,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.dcn.2014.04.003",
      "keywords": [
        "Development",
        "Dopamine",
        "Cognitive control",
        "Anterior cingulate cortex",
        "Reward positivity",
        "Reinforcement learning"
      ],
      "number_of_pages": 9,
      "pages": "191-199",
      "publication": {
        "category": "Journal",
        "cite_score": 8.3,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18789293",
        "publisher": "Elsevier B.V.",
        "sjr": 1.792,
        "snip": 1.42,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Developmental Cognitive Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Developmental changes in the reward positivity: An electrophysiological trajectory of reward processing",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84901664621&origin=inward"
      ]
    },
    {
      "abstract": "Learning from rewards generated by a human trainer observing the agent in action has been demonstrated to be an effective method for humans to teach an agent to perform challenging tasks. However, how to make the agent learn most efficiently from these kinds of human reward is still under-addressed. In this paper, we investigate the effect of providing social-network-based feedback intended to engender trainer competitiveness, focusing on its impact on the trainer's behavior. The results of our user study with 85 subjects show that the agent's social feedback can induce the trainer to train longer and give more feedback. Furthermore, the agent's performance was much better when social-competitive feedback was provided. The results also show that making the feedback active further increases the amount of time trainers spend training but does not further improve agent performance. Copyright \u00c2\u00a9 2014, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
      "authors": [
        "Li, G.",
        "Hung, H.",
        "Whiteson, S.",
        "Knox, W.B."
      ],
      "categories": null,
      "citations": 2,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Social agent",
        "Human-agent interaction",
        "Learning from human reward",
        "Reinforcement learning"
      ],
      "number_of_pages": 2,
      "pages": "1571-1572",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781634391313",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Leveraging social networks to motivate humans to train agents",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84911394751&origin=inward"
      ]
    },
    {
      "abstract": "Many characteristics of sensorimotor control can be explained by models based on optimization and optimal control theories. However, most of the previous models assume that the central nervous system has access to the precise knowledge of the sensorimotor system and its interacting environment. This viewpoint is difficult to be justified theoretically and has not been convincingly validated by experiments. To address this problem, this paper presents a new computational mechanism for sensorimotor control from a perspective of adaptive dynamic programming (ADP), which shares some features of reinforcement learning. The ADP-based model for sensorimotor control suggests that a command signal for the human movement is derived directly from the real-time sensory data, without the need to identify the system dynamics. An iterative learning scheme based on the proposed ADP theory is developed, along with rigorous convergence analysis. Interestingly, the computational model as advocated here is able to reproduce the motor learning behavior observed in experiments where a divergent force field or velocity-dependent force field was present. In addition, this modeling strategy provides a clear way to perform stability analysis of the overall system. Hence, we conjecture that human sensorimotor systems use an ADP-type mechanism to control movements and to achieve successful adaptation to uncertainties present in the environment.",
      "authors": [
        "Jiang, Yu",
        "Jiang, Zhong-Ping"
      ],
      "categories": null,
      "citations": 22,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00422-014-0613-7",
      "keywords": [
        "Optimal control",
        "Adaptive dynamic programming",
        "Motor learning",
        "Endpoint stiffness"
      ],
      "number_of_pages": 15,
      "pages": "459-473",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03401200",
        "publisher": "Springer Verlag",
        "sjr": 0.537,
        "snip": 1.126,
        "subject_areas": [
          "Computer Science (all)",
          "Biotechnology"
        ],
        "title": "Biological Cybernetics"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Adaptive dynamic programming as a theory of sensorimotor control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84906946687&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s00422-014-0613-7.pdf"
      ]
    },
    {
      "abstract": "Successful goal-directed behavior requires not only correct action selection, planning, and execution but also the ability to flexibly adapt behavior when performance problems occur or the environment changes. A prerequisite for determining the necessity, type, and magnitude of adjustments is to continuously monitor the course and outcome of one's actions. Feedback-control loops correcting deviations from intended states constitute a basic functional principle of adaptation at all levels of the nervous system. Here, we review the neurophysiology of evaluating action course and outcome with respect to their valence, i.e., reward and punishment, and initiating short- and long-term adaptations, learning, and decisions. Based on studies in humans and other mammals, we outline the physiological principles of performance monitoring and subsequent cognitive, motivational, autonomic, and behavioral adaptation and link them to the underlying neuroanatomy, neurochemistry, psychological theories, and computational models. We provide an overview of invasive and noninvasive systemic measures, such as electrophysiological, neuroimaging, and lesion data. We describe how a wide network of brain areas encompassing frontal cortices, basal ganglia, thalamus, and monoaminergic brain stem nuclei detects and evaluates deviations of actual from predicted states indicating changed action costs or outcomes. This information is used to learn and update stimulus and action values, guide action selection, and recruit adaptive mechanisms that compensate errors and optimize goal achievement. \u00c2\u00a9 2014 the American Physiological Society.",
      "authors": [
        "Ullsperger, M.",
        "Danielmeier, C.",
        "Jocham, G."
      ],
      "categories": null,
      "citations": 419,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/physrev.00041.2012",
      "keywords": [],
      "number_of_pages": 45,
      "pages": "35-79",
      "publication": {
        "category": "Journal",
        "cite_score": 79.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00319333",
        "publisher": "American Physiological Society",
        "sjr": 13.693,
        "snip": 11.448,
        "subject_areas": [
          "Physiology",
          "Molecular Biology",
          "Physiology (medical)"
        ],
        "title": "Physiological Reviews"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Neurophysiology of performance monitoring and adaptive behavior",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891531390&origin=inward"
      ]
    },
    {
      "abstract": "Humans are sophisticated social beings. Social cues from others are exceptionally salient, particularly during adolescence. Understanding how adolescents interpret and learn from variable social signals can provide insight into the observed shift in social sensitivity during this period. The present study tested 120 participants between the ages of 8 and 25&nbsp;years on a social reinforcement learning task where the probability of receiving positive social feedback was parametrically manipulated. Seventy-eight of these participants completed the task during fMRI scanning. Modeling trial-by-trial learning, children and adults showed higher positive learning rates than did adolescents, suggesting that adolescents demonstrated less differentiation in their reaction times for peers who provided more positive feedback. Forming expectations about receiving positive social reinforcement correlated with neural activity within the medial prefrontal cortex and ventral striatum across age. Adolescents, unlike children and adults, showed greater insular activity during positive prediction error learning and increased activity in the supplementary motor cortex and the putamen when receiving positive social feedback regardless of the expected outcome, suggesting that peer approval may motivate adolescents toward action. While different amounts of positive social reinforcement enhanced learning in children and adults, all positive social reinforcement equally motivated adolescents. Together, these findings indicate that sensitivity to peer approval during adolescence goes beyond simple reinforcement theory accounts and suggest possible explanations for how peers may motivate adolescent behavior.",
      "authors": [
        "Jones, Rebecca M.",
        "Somerville, Leah H.",
        "Li, Jian",
        "Ruberry, Erika J.",
        "Powers, Alisa",
        "Mehta, Natasha",
        "Dyke, Jonathan",
        "Casey, B. J."
      ],
      "categories": null,
      "citations": 90,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-014-0257-z",
      "keywords": [
        "Reinforcement",
        "Peers",
        "Adolescence",
        "fMRI",
        "Social acceptance",
        "Brain"
      ],
      "number_of_pages": 15,
      "pages": "683-697",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Adolescent-specific patterns of behavior and neural activity during social reinforcement learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84904406204&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-014-0257-z.pdf"
      ]
    },
    {
      "abstract": "Neural responses to performance errors and external feedback have been suggested to be altered in obsessive-compulsive disorder. In the current study, an associative learning task was used in healthy participants assessed for obsessive-compulsive symptoms by the OCI-R questionnaire. The task included a condition with equivocal feedback that did not inform about the participants' performance. Following incorrect responses, an error-related negativity and an error positivity were observed. In the feedback phase, the largest feedback-related negativity was observed following equivocal feedback. Theta and beta oscillatory components were found following incorrect and correct responses, respectively, and an increase in theta power was associated with negative and equivocal feedback. Changes over time were also explored as an indicator for possible learning effects. Finally, event-related potentials and oscillatory components were found to be uncorrelated with OCI-R scores in the current non-clinical sample. \u00c2\u00a9 2014 Elsevier B.V.",
      "authors": [
        "Do\u00c3\u00b1amayor, N.",
        "Dinani, J.",
        "R\u00c3\u00b6misch, M.",
        "Ye, Z.",
        "M\u00c3\u00bcnte, T.F."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.biopsycho.2014.07.013",
      "keywords": [
        "Electroencephalography",
        "Obsessive-compulsive disorder",
        "Reinforcement learning",
        "Feedback-related negativity",
        "Error-related negativity"
      ],
      "number_of_pages": 15,
      "pages": "73-87",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "03010511",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Biological Psychology"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Performance monitoring during associative learning and its relation to obsessive-compulsive characteristics",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84905457799&origin=inward"
      ]
    },
    {
      "abstract": "Learning complex skills by repeating and generalizing expert behavior is a fundamental problem in robotics. However, the usual approaches do not answer the question of what are appropriate representations to generate motion for a specific task. Since it is time-consuming for a human expert to manually design the motion control representation for a task, we propose to uncover such structure from data-observed motion trajectories. Inspired by Inverse Optimal Control, we present a novel method to learn a latent value function, imitate and generalize demonstrated behavior, and discover a task relevant motion representation. We test our method, called Task Space Retrieval Using Inverse Feedback Control (TRIC), on several challenging high-dimensional tasks. TRIC learns the important control dimensions for the tasks from a few example movements and is able to robustly generalize to new situations.",
      "authors": [
        "Jetchev, Nikolay",
        "Toussaint, Marc"
      ],
      "categories": null,
      "citations": 8,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s10514-014-9384-1",
      "keywords": [
        "Robot motion generation",
        "Machine learning and robotics",
        "Inverse reinforcement learning",
        "Task spaces for motion",
        "Motion rate control",
        "Imitation learning"
      ],
      "number_of_pages": 21,
      "pages": "169-189",
      "publication": {
        "category": "Journal",
        "cite_score": 8.8,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09295593",
        "publisher": "Springer Netherlands",
        "sjr": 1.165,
        "snip": 1.523,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Autonomous Robots"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Discovering relevant task spaces using inverse feedback control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84902287582&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/s10514-014-9384-1.pdf"
      ]
    },
    {
      "abstract": "<p>Brain-Machine Interfaces (BMIs) can be used to restore function in people living with paralysis. Current BMIs require extensive calibration that increase the set-up times and external inputs for decoder training that may be difficult to produce in paralyzed individuals. Both these factors have presented challenges in transitioning the technology from research environments to activities of daily living (ADL). For BMIs to be seamlessly used in ADL, these issues should be handled with minimal external input thus reducing the need for a technician/caregiver to calibrate the system. Reinforcement Learning (RL) based BMIs are a good tool to be used when there is no external training signal and can provide an adaptive modality to train BMI decoders. However, RL based BMIs are sensitive to the feedback provided to adapt the BMI. In actor-critic BMIs, this feedback is provided by the critic and the overall system performance is limited by the critic accuracy. In this work, we developed an adaptive BMI that could handle inaccuracies in the critic feedback in an effort to produce more accurate RL based BMIs. We developed a confidence measure, which indicated how appropriate the feedback is for updating the decoding parameters of the actor. The results show that with the new update formulation, the critic accuracy is no longer a limiting factor for the overall performance. We tested and validated the system onthree different data sets: synthetic data generated by an Izhikevich neural spiking model, synthetic data with a Gaussian noise distribution, and data collected from a non-human primate engaged in a reaching task. All results indicated that the system with the critic confidence built in always outperformed the system without the critic confidence. Results of this study suggest the potential application of the technique in developing an autonomous BMI that does not need an external signal for training or extensive calibration.</p>",
      "authors": [
        "Prins, Noeline W.",
        "Sanchez, Justin C.",
        "Prasad, Abhishek"
      ],
      "categories": null,
      "citations": 11,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2014.00111",
      "keywords": [
        "Hebbian",
        "actor-critic",
        "Feedback",
        "reinforcement learning",
        "Brain machine interface (BMI)"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "A confidence metric for using neurobiological feedback in actor-critic reinforcement learning based brain-machine interfaces",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84905028627&origin=inward",
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2014.00111/pdf"
      ]
    },
    {
      "abstract": "In this paper, we investigated an approach for robots to learn to adapt dance actions to human\u2019s preferences through interaction and feedback. Human\u2019s preferences were extracted by analysing the common action patterns with positive or negative feedback from the human during robot dancing. By using a buffering technique to store the dance actions before a feedback, each individual\u2019s preferences can be extracted even when a reward is received late. The extracted preferred dance actions from different people were then combined to generate improved dance sequences, i.e. performing more of what was preferred and less of that was not preferred. Together with Softmax action-selection method, the Sarsa reinforcement learning algorithm was used as the underlining learning algorithm and to effectively control the trade-off between exploitation of the learnt dance skills and exploration of new dance actions. The results showed that the robot learnt, using interactive reinforcement learning, the preferences of human partners, and the dance improved with the extracted preferences from more human partners.",
      "authors": [
        "Meng, Qinggang",
        "Tholley, Ibrahim",
        "Chung, Paul W. H."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/s00521-013-1504-x",
      "keywords": [
        "Robot adaptation",
        "Robot dancing",
        "Robot interaction with humans",
        "Robot learning"
      ],
      "number_of_pages": 8,
      "pages": "117-124",
      "publication": {
        "category": "Journal",
        "cite_score": 10.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09410643",
        "publisher": "Springer London",
        "sjr": 1.169,
        "snip": 1.825,
        "subject_areas": [
          "Software",
          "Artificial Intelligence"
        ],
        "title": "Neural Computing and Applications"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Robots learn to dance through interaction with humans",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s00521-013-1504-x.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891842001&origin=inward"
      ]
    },
    {
      "abstract": "Teaching new motor tasks to robots through physical interactions is an important goal for both robotics and machine learning. Most monolithic machine learning approaches fail to scale when going beyond basic skills. In this paper we present a simple framework for teaching the robot (to play tennis) through direct physical interaction with a human teacher (i.e. Kinesthetic Teaching). Current popular established method of kinesthetic teaching generally uses a two-stage approach: First, a library of motor primitives is generated through direct physical manipulation of the robot. In second stage, a reinforced learning (\"reward\" stage) is implemented to dynamically adjust the policy of choosing from motor primitive library. In this paper, we show that by proper modification of the first stage of Kinesthetic Teaching and incorporating the domain experience of the human teacher, we can remove the necessity of the second stage. This approach has multiple advantages: (i) We can make the whole training process much simpler. This would go a long way in making our training algorithm scalable (for much increased number of basic moves, etc). (ii) One potential problem with the \"reward\" learning phase is that there may be subjective difference of what is a \"good\" shot from the kinesthetic teaching and from the bystander viewpoint (later in \"reward\" stage). Even a little difference in this regard will result in a confusing feedback (\"reward\"), and hence it would be difficult to correctly figure out which library training samples should be reassigned what weight values. Our approach eliminates this problem altogether. \u00c2\u00a9 2014 IFAC.",
      "authors": [
        "Ghoshal, D.P.",
        "Das, N.",
        "Dutta, S.",
        "Behera, L."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3182/20140313-3-IN-3024.00225",
      "keywords": [
        "Learning by demonstration",
        "Kinesthetic teaching",
        "Generalizing movements",
        "Robot learning",
        "Motor skill learning"
      ],
      "number_of_pages": 8,
      "pages": "773-780",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9783902823601",
        "issn": "14746670",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "IFAC Proceedings Volumes (IFAC-PapersOnline)"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Robot learns from human teacher through modified kinesthetic teaching",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84899540655&origin=inward"
      ]
    },
    {
      "abstract": "Emerging evidence from decision neuroscience suggests that although younger and older adults show similar frontostriatal representations of reward magnitude, older adults often show deficits in feedback-driven reinforcement learning. In the present study, healthy adults completed reward-based tasks that did or did not depend on probabilistic learning, while undergoing functional neuroimaging. We observed reductions in the frontostriatal representation of prediction errors during probabilistic learning in older adults. In contrast, we found evidence for stability across adulthood in the representation of reward outcome in a task that did not require learning. Together, the results identify changes across adulthood in the dynamic coding of relational representations of feedback, in spite of preserved reward sensitivity in old age. Overall, the results suggest that the neural representation of prediction error, but not reward outcome, is reduced in old age. These findings reveal a potential dissociation between cognition and motivation with age and identify a potential mechanism for explaining changes in learning-dependent decision making in old adulthood.",
      "authors": [
        "Samanez-Larkin, Gregory R.",
        "Worthy, Darrell A.",
        "Mata, Rui",
        "McClure, Samuel M.",
        "Knutson, Brian"
      ],
      "categories": null,
      "citations": 79,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-014-0297-4",
      "keywords": [
        "Medial prefrontal cortex",
        "Reward",
        "Ventral striatum",
        "Learning",
        "Decision making",
        "Motivation",
        "Aging"
      ],
      "number_of_pages": 11,
      "pages": "672-682",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Adult age differences in frontostriatal representation of prediction error but not reward outcome",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-014-0297-4.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84904389160&origin=inward"
      ]
    },
    {
      "abstract": "<p>Animal studies have shown that substantia nigra (SN) dopaminergic (DA) neurons strengthen action\u2013reward associations during reinforcement learning, but their role in human learning is not known. Here, we applied microstimulation in the SN of 11 patients undergoing deep brain stimulation surgery for the treatment of Parkinson9s disease as they performed a two-alternative probability learning task in which rewards were contingent on stimuli, rather than actions. Subjects demonstrated decreased learning from reward trials that were accompanied by phasic SN microstimulation compared with reward trials without stimulation. Subjects who showed large decreases in learning also showed an increased bias toward repeating actions after stimulation trials; therefore, stimulation may have decreased learning by strengthening action\u2013reward associations rather than stimulus\u2013reward associations. Our findings build on previous studies implicating SN DA neurons in preferentially strengthening action\u2013reward associations during reinforcement learning.</p>",
      "authors": [
        "Ashwin G. Ramayya",
        "Amrit Misra",
        "Gordon H. Baltuch",
        "Michael J. Kahana"
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.5445-13.2014",
      "keywords": [
        "Microstimulation",
        "Human",
        "Dopamine",
        "Parkinson's disease",
        "Reinforcement learning",
        "Substantia nigra"
      ],
      "number_of_pages": 9,
      "pages": "6887-6895",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Microstimulation of the Human Substantia Nigra Alters Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84900385713&origin=inward"
      ]
    },
    {
      "abstract": "Decades of research have examined the neurocognitive mechanisms of cognitive control, but the motivational factors underlying task selection and performance remain to be elucidated. We recently proposed that anterior cingulate cortex (ACC) utilizes reward prediction error signals carried by the midbrain dopamine system to learn the value of tasks according to the principles of hierarchical reinforcement learning. According to this position, disruption of the ACC\u2013dopamine interface can disrupt the selection and execution of extended, task-related behaviors. To investigate this issue, we recorded the event-related brain potential (ERP) from children with attention deficit hyperactivity disorder (ADHD), which is strongly associated with ACC\u2013dopamine dysfunction, and from typically developing children while they navigated a simple \u201cvirtual T-maze\u201d to find rewards. Depending on the condition, the feedback stimuli on each trial indicated that the children earned or failed to earn either money or points. We found that the reward positivity, an ERP component proposed to index the impact of dopamine-related reward signals on ACC, was significantly larger with money feedback than with points feedback for the children with ADHD, but not for the typically developing children. These results suggest that disruption of the ACC\u2013dopamine interface may underlie the impairments in motivational control observed in childhood ADHD.",
      "authors": [
        "Umemoto, Akina",
        "Lukie, Carmen N.",
        "Kerns, Kimberly A.",
        "M\u00fcller, Ulrich",
        "Holroyd, Clay B."
      ],
      "categories": null,
      "citations": 29,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-014-0298-3",
      "keywords": [
        "Motivation",
        "ADHD",
        "Reinforcement learning",
        "Cognitive control"
      ],
      "number_of_pages": 17,
      "pages": "698-714",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Impaired reward processing by anterior cingulate cortex in children with attention deficit hyperactivity disorder",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-014-0298-3.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84904389356&origin=inward"
      ]
    },
    {
      "abstract": "<p>Humans make predictions and use feedback to update their subsequent predictions. The feedback-related negativity (FRN) has been found to be sensitive to negative feedback as well as negative prediction error, such that the FRN is larger for outcomes that are worse than expected. The present study examined prediction errors in both appetitive and aversive conditions. We found that the FRN was more negative for reward omission vs. wins and for loss omission vs. losses, suggesting that the FRN might classify outcomes in a \u00e2\u0080\u009cmore-or-less than expected\u00e2\u0080\u009d fashion rather than in the \u00e2\u0080\u009cbetter-or-worse than expected\u00e2\u0080\u009d dimension. Our findings challenge the previous notion that the FRN only encodes negative feedback and \u00e2\u0080\u009cworse than expected\u00e2\u0080\u009d negative prediction error.</p>",
      "authors": [
        "Huang, Yi",
        "Yu, Rongjun"
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnins.2014.00108",
      "keywords": [
        "ERP",
        "aversive",
        "reinforcement learning",
        "appetitive",
        "feedback-related negative"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16624548",
        "publisher": "Frontiers Media SA",
        "sjr": 1.161,
        "snip": 1.221,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Frontiers in Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "The feedback-related negativity reflects \u00e2\u0080\u009cmore or less\u00e2\u0080\u009d prediction error in appetitive and aversive conditions",
      "urls": [
        "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2014.00108/pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84905027368&origin=inward"
      ]
    },
    {
      "abstract": "Schizophrenia is characterized by an abnormal dopamine system, and dopamine blockade is the primary mechanism of antipsychotic treatment. Consistent with the known role of dopamine in reward processing, prior research has demonstrated that patients with schizophrenia exhibit impairments in reward-based learning. However, it remains unknown how treatment with antipsychotic medication impacts the behavioral and neural signatures of reinforcement learning in schizophrenia. The goal of this study was to examine whether antipsychotic medication modulates behavioral and neural responses to prediction error coding during reinforcement learning. Patients with schizophrenia completed a reinforcement learning task while undergoing functional magnetic resonance imaging. The task consisted of two separate conditions in which participants accumulated monetary gain or avoided monetary loss. Behavioral results indicated that antipsychotic medication dose was associated with altered behavioral approaches to learning, such that patients taking higher doses of medication showed increased sensitivity to negative reinforcement. Higher doses of antipsychotic medication were also associated with higher learning rates (LRs), suggesting that medication enhanced sensitivity to trial-by-trial feedback. Neuroimaging data demonstrated that antipsychotic dose was related to differences in neural signatures of feedback prediction error during the loss condition. Specifically, patients taking higher doses of medication showed attenuated prediction error responses in the striatum and the medial prefrontal cortex. These findings indicate that antipsychotic medication treatment may influence motivational processes in patients with schizophrenia.",
      "authors": [
        "Insel, Catherine",
        "Reinen, Jenna",
        "Weber, Jochen",
        "Wager, Tor D.",
        "Jarskog, L. Fredrik",
        "Shohamy, Daphna",
        "Smith, Edward E."
      ],
      "categories": null,
      "citations": 27,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-014-0261-3",
      "keywords": [
        "Motivation",
        "Schizophrenia",
        "Reward",
        "Dopamine"
      ],
      "number_of_pages": 13,
      "pages": "189-201",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Antipsychotic dose modulates behavioral and neural responses to feedback during reinforcement learning in schizophrenia",
      "urls": [
        "https://link.springer.com/content/pdf/10.3758/s13415-014-0261-3.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84897541178&origin=inward"
      ]
    },
    {
      "abstract": "Phasic firing changes of midbrain dopamine neurons have been widely characterised as reflecting a reward prediction error (RPE). Major personality traits (e.g. extraversion) have been linked to inter-individual variations in dopaminergic neurotransmission. Consistent with these two claims, recent research (Smillie, Cooper, & Pickering, 2011; Cooper, Duke, Pickering, & Smillie, 2014) found that extraverts exhibited larger RPEs than introverts, as reflected in feedback related negativity (FRN) effects in EEG recordings. Using an established, biologically-localised RPE computational model, we successfully simulated dopaminergic cell firing changes which are thought to modulate the FRN. We introduced simulated individual differences into the model: parameters were systematically varied, with stable values for each simulated individual. We explored whether a model parameter might be responsible for the observed covariance between extraversion and the FRN changes in real data, and argued that a parameter is a plausible source of such covariance if parameter variance, across simulated individuals, correlated almost perfectly with the size of the simulated dopaminergic FRN modulation, and created as much variance as possible in this simulated output. Several model parameters met these criteria, while others did not. In particular, variations in the strength of connections carrying excitatory reward drive inputs to midbrain dopaminergic cells were considered plausible candidates, along with variations in a parameter which scales the effects of dopamine cell firing bursts on synaptic modification in ventral striatum. We suggest possible neurotransmitter mechanisms underpinning these model parameters. Finally, the limitations and possible extensions of our approach are discussed.",
      "authors": [
        "Pickering, Alan D.",
        "Pesola, Francesca"
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnhum.2014.00740",
      "keywords": [
        "Computational modeling of human behavior",
        "Dopamine",
        "Reward prediction error",
        "Extraversion",
        "Feedback related negativity (FRN)",
        "Reinforcement learning models"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5161",
        "publisher": "Frontiers Media SA",
        "sjr": 0.787,
        "snip": 1.033,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Biological Psychiatry",
          "Psychiatry and Mental Health",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Human Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Modeling dopaminergic and other processes involved in learning from reward prediction error: contributions from an individual differences perspective",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84933674611&origin=inward"
      ]
    },
    {
      "abstract": "Automated negotiation techniques can greatly improve the negotiation efficiency and quality of our human being, and a lot of automated negotiation strategies and mechanisms have been proposed in different negotiation scenarios until now. To achieve efficient...",
      "authors": [
        "Hao, Jianye",
        "Leung, Ho-fung"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1007/978-4-431-54758-7_11",
      "keywords": [
        "Adaption",
        "Negotiation",
        "Reinforcement learning"
      ],
      "number_of_pages": 9,
      "pages": "171-179",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9783030716158",
        "issn": "1860949X",
        "publisher": "Springer, Cham",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Annual Meeting of the Bulgarian Section of SIAM"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "CUHKAgent: An Adaptive Negotiation Strategy for Bilateral Negotiations over Multiple Items",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84927137082&origin=inward",
        "https://link.springer.com/content/pdf/10.1007/978-4-431-54758-7_11.pdf"
      ]
    },
    {
      "abstract": "Conflict has been proposed to act as a cost in action selection, implying a general function of medio\u2013frontal cortex in the adaptation to aversive events. Here we investigate if response conflict acts as a cost during reinforcement learning by modulating experienced reward values in cortical and striatal systems. Electroencephalography recordings show that conflict diminishes the relationship between reward-related frontal theta power and cue preference yet it enhances the relationship between punishment and cue avoidance. Individual differences in the cost of conflict on reward versus punishment sensitivity are also related to a genetic polymorphism associated with striatal D1 versus D2 pathway balance (DARPP-32). We manipulate these patterns with the D2 agent cabergoline, which induces a strong bias to amplify the aversive value of punishment outcomes following conflict. Collectively, these findings demonstrate that interactive cortico\u2013striatal systems implicitly modulate experienced reward and punishment values as a function of conflict. Conflict monitoring and value learning are often researched as separate processes within psychology, but they share many common neural mechanisms. Here Cavanagh et al.reveal that conflict acts as a cost during value learning, therefore suggesting a general link between conflict monitoring and value learning.",
      "authors": [
        "Cavanagh, James F.",
        "Masters, Sean E.",
        "Bath, Kevin",
        "Frank, Michael J."
      ],
      "categories": null,
      "citations": 62,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1038/ncomms6394",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 24.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2041-1723",
        "publisher": "Nature Publishing Group",
        "sjr": 5.116,
        "snip": 3.268,
        "subject_areas": [
          "Chemistry (all)",
          "Biochemistry, Genetics and Molecular Biology (all)",
          "Physics and Astronomy (all)"
        ],
        "title": "Nature Communications"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Conflict acts as an implicit cost in reinforcement learning",
      "urls": [
        "https://www.nature.com/articles/ncomms6394.pdf",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84922975531&origin=inward"
      ]
    },
    {
      "abstract": "We report on a study that shows plausible emotion dynamics for joy, distress, hope and fear, emerging in an adaptive agent that uses Reinforcement Learning (RL) to adapt to a task. Joy/distress is a signal that is derived from the RL update signal, while hope/fear is derived from the utility of the current state. Agent-based simulation experiments replicate psychological and behavioral dynamics of emotion including: joy and distress reactions that develop prior to hope and fear; fear extinction; habituation of joy; and, task randomness that increases the intensity of joy and distress. This work distinguishes itself by assessing the dynamics of emotion in an adaptive agent framework - coupling it to the literature on habituation, development, and extinction. Our results support the idea that the function of emotion is to provide a complex feedback signal for an organism to adapt its behavior. We show this feedback signal can be operationalized for RL agents. This is important because (a) RL-based models can help understand the relation between emotion and adaptation in animals, (b) the emotional state might be used to increase adaptive potential, and (c) expression of an emotion to a human observer that it is grounded in the learning mechanism of the agent should help interpret the meaning of the emotion.",
      "authors": [
        "Jacobs, E.",
        "Broekens, J.",
        "Jonker, C."
      ],
      "categories": null,
      "citations": 12,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Emotion dynamics",
        "Reinforcement learning",
        "Affective computing"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781943580125",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAMAS 2014 Workshop on Adaptive and Learning Agents, ALA 2014"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "Emergent dynamics of joy, distress, hope and fear in reinforcement learning agents",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84921636716&origin=inward"
      ]
    },
    {
      "abstract": "Purpose: Today's implanted brain-computer interfaces make direct contact with the brain or even penetrate the tissue, bearing additional risks with regard to safety and stability. What is more, these approaches aim to control prosthetic devices as as",
      "authors": [
        "Gharabaghi, Alireza",
        "Naros, Georgios",
        "Walter, Armin",
        "Grimm, Florian",
        "Schuermeyer, Marc",
        "Roth, Alexander",
        "Bogdan, Martin",
        "Rosenstiel, Wolfgang",
        "Birbaumer, Niels"
      ],
      "categories": null,
      "citations": 32,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3233/RNN-140387",
      "keywords": [
        "stroke",
        "neuroprosthetics",
        "brain-computer interface",
        "Electrocorticography",
        "neurorehabilitation",
        "brain-machine interface",
        "epidural implant"
      ],
      "number_of_pages": 9,
      "pages": "517-525",
      "publication": {
        "category": "Journal",
        "cite_score": 5.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09226028",
        "publisher": "IOS Press BV",
        "sjr": 0.789,
        "snip": 0.943,
        "subject_areas": [
          "Developmental Neuroscience",
          "Neurology (clinical)",
          "Neurology"
        ],
        "title": "Restorative Neurology and Neuroscience"
      },
      "publication_date": "2014-01-01",
      "selected": null,
      "title": "From assistance towards restoration with epidural brain-computer interfacing",
      "urls": [
        "https://content.iospress.com:443/download/restorative-neurology-and-neuroscience/rnn140387?id=restorative-neurology-and-neuroscience%2Frnn140387",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84923224567&origin=inward"
      ]
    },
    {
      "abstract": "A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.",
      "authors": [
        "Shane Griffith",
        "Kaushik Subramanian",
        "Jonathan Scholz",
        "Charles L. Isbell",
        "Andrea Thomaz"
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "ACM"
      ],
      "doi": "10.5555/2999792.2999905",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "2625-2633",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": "Curran Associates Inc.",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2"
      },
      "publication_date": "2013-12-05",
      "selected": null,
      "title": "Policy shaping: integrating human feedback with reinforcement learning",
      "urls": [
        "https://dl.acm.org/doi/10.5555/2999792.2999905"
      ]
    },
    {
      "abstract": "We propose a computational model of Precision Grip (PG) performance in normal subjects and Parkinson\u2019s Disease (PD) patients. Prior studies on grip force generation in PD patients show an increase in grip force during ON medication and an increase in the variability of the grip force during OFF medication (Fellows et al 1998; Ingvarsson et al 1997). Changes in grip force generation in dopamine-deficient PD conditions strongly suggest contribution of the Basal Ganglia, a deep brain system having a crucial role in translating dopamine signals to decision making. The present approach is to treat the problem of modeling grip force generation as a problem of action selection, which is one of the key functions of the Basal Ganglia. The model consists of two components: 1) the sensory-motor loop component, and 2) the Basal Ganglia component. The sensory-motor loop component converts a reference position and a reference grip force, into lift force and grip force profiles, respectively. These two forces cooperate in grip-lifting a load. The sensory-motor loop component also includes a plant model that represents the interaction between two fingers involved in PG, and the object to be lifted. The Basal Ganglia component is modeled using Reinforcement Learning with the significant difference that the action selection is performed using utility distribution instead of using purely Value-based distribution, thereby incorporating risk-based decision making. The proposed model is able to account for the precision grip results from normal and PD patients accurately (Fellows et. al. 1998; Ingvarsson et. al. 1997). To our knowledge the model is the first model of precision grip in PD conditions.",
      "authors": [
        "Gupta, Ankur",
        "Balasubramani, Pragathi Priyadharsini",
        "Chakravarthy, Srinivasa"
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2013.00172",
      "keywords": [
        "Precision grip",
        "Decision making",
        "Basal ganglia",
        "Parkinson's disease",
        "Reinforcement learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2013-12-02",
      "selected": null,
      "title": "Computational model of precision grip in Parkinson's disease: a utility based approach",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84889595693&origin=inward"
      ]
    },
    {
      "abstract": "The dopaminergic system is involved in reward encoding and reinforcement learning. Dopaminergic neurons from this system in the substantia nigra/ventral tegmental area complex (SN/VTA) fire in response to unexpected reinforcing cues. The goal of this study was to investigate whether individuals can gain voluntary control of SN/VTA activity, thereby potentially enhancing dopamine release to target brain regions. Neurofeedback and mental imagery were used to self-regulate the SN/VTA. Real-time functional magnetic resonance imaging (rtfMRI) provided abstract visual feedback of the SN/VTA activity while the subject imagined rewarding scenes. Skin conductance response (SCR) was recorded as a measure of emotional arousal. To examine the effect of neurofeedback, subjects were assigned to either receiving feedback directly proportional (n. = 15, veridical feedback) or inversely proportional (n. = 17, inverted feedback) to SN/VTA activity. Both groups of subjects were able to up-regulate SN/VTA activity initially without feedback. Veridical feedback improved the ability to up-regulate SN/VTA compared to baseline while inverted feedback did not. Additional dopaminergic regions were activated in both groups. The ability to self-regulate SN/VTA was differentially correlated with SCR depending on the group, suggesting an association between emotional arousal and neurofeedback performance. These findings indicate that SN/VTA can be voluntarily activated by imagery and voluntary activation is further enhanced by neurofeedback. The findings may lead the way towards a non-invasive strategy for endogenous control of dopamine. \u00c2\u00a9 2013 Elsevier Inc.",
      "authors": [
        "Sulzer, J.",
        "Sitaram, R.",
        "Blefari, M.L.",
        "Kollias, S.",
        "Birbaumer, N.",
        "Stephan, K.E.",
        "Luft, A.",
        "Gassert, R."
      ],
      "categories": null,
      "citations": 70,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2013.05.115",
      "keywords": [
        "Real-time fMRI",
        "Skin conductance response",
        "Dopamine",
        "Neurofeedback",
        "Substantia nigra"
      ],
      "number_of_pages": 9,
      "pages": "817-825",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2013-12-01",
      "selected": null,
      "title": "Neurofeedback-mediated self-regulation of the dopaminergic midbrain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84881483675&origin=inward"
      ]
    },
    {
      "abstract": "<p>Abnormalities in reinforcement learning are a key finding in schizophrenia and have been proposed to be linked to elevated levels of dopamine neurotransmission. Behavioral deficits in reinforcement learning and their neural correlates may contribute to the formation of clinical characteristics of schizophrenia. The ability to form predictions about future outcomes is fundamental for environmental interactions and depends on neuronal teaching signals, like reward prediction errors. While aberrant prediction errors, that encode non-salient events as surprising, have been proposed to contribute to the formation of positive symptoms, a failure to build neural representations of decision values may result in negative symptoms. Here, we review behavioral and neuroimaging research in schizophrenia and focus on studies that implemented reinforcement learning models. In addition, we discuss studies that combined reinforcement learning with measures of dopamine. Thereby, we suggest how reinforcement learning abnormalities in schizophrenia may contribute to the formation of psychotic symptoms and may interact with cognitive deficits. These ideas point toward an interplay of more rigid versus flexible control over reinforcement learning. Pronounced deficits in the flexible or model-based domain may allow for a detailed characterization of well-established cognitive deficits in schizophrenia patients based on computational models of learning. Finally, we propose a framework based on the potentially crucial contribution of dopamine to dysfunctional reinforcement learning on the level of neural networks. Future research may strongly benefit from computational modeling but also requires further methodological improvement for clinical group studies. These research tools may help to improve our understanding of disease-specific mechanisms and may help to identify clinically relevant subgroups of the heterogeneous entity schizophrenia.</p>",
      "authors": [
        "Deserno, Lorenz",
        "Boehme, Rebecca",
        "Heinz, Andreas",
        "Schlagenhauf, Florian"
      ],
      "categories": null,
      "citations": 66,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fpsyt.2013.00172",
      "keywords": [
        "computational modeling",
        "Schizophrenia",
        "Dopamine",
        "Computational Psychiatry",
        "reinforcement learning",
        "PET imaging",
        "aberrant salience",
        "fMRI",
        "Reinforcement learning models",
        "Prediction error"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.4,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1664-0640",
        "publisher": "Frontiers Media SA",
        "sjr": 1.222,
        "snip": 1.265,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "Frontiers in Psychiatry"
      },
      "publication_date": "2013-12-01",
      "selected": null,
      "title": "Reinforcement Learning and Dopamine in Schizophrenia: Dimensions of Symptoms or Specific Features of a Disease Group?",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891680236&origin=inward",
        "https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2013.00172/pdf"
      ]
    },
    {
      "abstract": "Research has demonstrated that individuals with schizophrenia fail to appropriately use negative feedback to guide learning. These learning deficits are thought to arise from abnormalities in midbrain dopamine activity. Primary and enduring negative symptoms are also associated with abnormal dopamine activity and are expected to produce more severe deficits in learning when they present in individuals with schizophrenia. The current study examines this matter by comparing individuals with deficit syndrome schizophrenia, which is characterized by primary and enduring negative symptoms, to individuals with nondeficit syndrome schizophrenia and to normal controls in their use of positive feedback and negative feedback to guide learning on the first four cards of the WCST. Participants included 67 individuals with schizophrenia (15 deficit; 52 nondeficit syndrome) and 51 healthy controls. Accuracy data from the first 4 cards of the WCST and measures of global test performance were examined. Individuals with schizophrenia were significantly less accurate than controls in their performance on early (pre-shift) WCST trials, and this impairment was significantly greater in patients with deficit than nondeficit schizophrenia. Additionally, accuracy across the first 4 WCST cards significantly predicted the number of categories completed and percentage of perseverative errors across the entire test. These findings suggest that negative symptoms of schizophrenia are associated with difficulty using negative feedback to adaptively guide behavior, and are consistent with the notion that abnormal DA signaling contributes to the higher-order executive functioning impairments seen in schizophrenia with severe negative symptoms. \u00c2\u00a9 2013 Elsevier B.V.",
      "authors": [
        "Vogel, S.J.",
        "Strauss, G.P.",
        "Allen, D.N."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.schres.2013.07.052",
      "keywords": [
        "Decision-making",
        "Reward",
        "Avolition",
        "Schizophrenia",
        "Deficit syndrome",
        "Anhedonia",
        "Reinforcement learning"
      ],
      "number_of_pages": 5,
      "pages": "97-101",
      "publication": {
        "category": "Journal",
        "cite_score": 7.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "09209964",
        "publisher": "Elsevier B.V.",
        "sjr": 1.429,
        "snip": 1.262,
        "subject_areas": [
          "Psychiatry and Mental Health",
          "Biological Psychiatry"
        ],
        "title": "Schizophrenia Research"
      },
      "publication_date": "2013-12-01",
      "selected": null,
      "title": "Using negative feedback to guide behavior: Impairments on the first 4 cards of the Wisconsin Card Sorting Test predict negative symptoms of schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84889096642&origin=inward"
      ]
    },
    {
      "abstract": "Inverse Reinforcement Learning (IRL) is an approach for domain-reward discovery from demonstration, where an agent mines the reward function of a Markov decision process by observing an expert acting in the domain. In the standard setting, it is assumed that the expert acts (nearly) optimally, and a large number of trajectories, i.e., training examples are available for reward discovery (and consequently, learning domain behavior). These are not practical assumptions: trajectories are often noisy, and there can be a paucity of examples. Our novel approach incorporates advice-giving into the IRL framework to address these issues. Inspired by preference elicitation, a domain expert provides advice on states and actions (features) by stating preferences over them. We evaluate our approach on several domains and show that with small amounts of targeted preference advice, learning is possible from noisy demonstrations, and requires far fewer trajectories compared to simply learning from trajectories alone. \u00c2\u00a9 2013 IEEE.",
      "authors": [
        "Kunapuli, G.",
        "Odom, P.",
        "Shavlik, J.W.",
        "Natarajan, S."
      ],
      "categories": null,
      "citations": 27,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/ICDM.2013.79",
      "keywords": [],
      "number_of_pages": 10,
      "pages": "409-418",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781728183169",
        "issn": "15504786",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings - IEEE International Conference on Data Mining, ICDM"
      },
      "publication_date": "2013-12-01",
      "selected": null,
      "title": "Guiding autonomous agents to better behaviors through human advice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84894669441&origin=inward"
      ]
    },
    {
      "abstract": "Serotonin and dopamine are speculated to subserve motivationally opponent functions, but this hypothesis has not been directly tested. We studied the role of these neurotransmitters in probabilistic reversal learning in nearly 700 individuals as a function of two polymorphisms in the genes encoding the serotonin and dopamine transporters (. SERT: 5HTTLPR plus rs25531; DAT1 3@UTR VNTR). A double dissociation was observed. The SERT polymorphism altered behavioral adaptation after losses, with increased lose-shift associated with L@ homozygosity, while leaving unaffected perseveration after reversal. In contrast, the DAT1 genotype affected the influence of prior choices on perseveration, while leaving lose-shifting unaltered. A model of reinforcement learning captured the dose-dependent effect of DAT1 genotype, such that an increasing number of 9R-alleles resulted in a stronger reliance on previous experience and therefore reluctance to update learned associations. These data provide direct evidence for doubly dissociable effects of serotonin and dopamine systems.",
      "authors": [
        "DenOuden, H.E.M.",
        "Daw, N.D.",
        "Fernandez, G.",
        "Elshout, J.A.",
        "Rijpkema, M.",
        "Hoogman, M.",
        "Franke, B.",
        "Cools, R."
      ],
      "categories": null,
      "citations": 162,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuron.2013.08.030",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "1090-1100",
      "publication": {
        "category": "Journal",
        "cite_score": 26.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08966273",
        "publisher": "Cell Press",
        "sjr": 7.736,
        "snip": 3.346,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Neuron"
      },
      "publication_date": "2013-11-20",
      "selected": null,
      "title": "Dissociable Effects of Dopamine and Serotonin on Reversal Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84888068462&origin=inward"
      ]
    },
    {
      "abstract": "Decision making under uncertainty is challenging for any autonomous agent. The challenge increases when the environment\u2019s stochastic properties change over time, i.e. when the environment is volatile. In order to efficiently adapt to volatile environments, agents must primarily rely on recent outcomes to quickly change their decision strategies; in other words, they need to increase their knowledge plasticity. On the contrary, in stable environments, knowledge stability must be preferred to preserve useful information against noise. Here we propose that in mammalian brain, the locus coeruleus (LC) is one of the nuclei involved in volatility estimation and in the subsequent control of neural plasticity. During a reinforcement learning task, LC activation, measured by means of pupil diameter, coded both for environmental volatility and learning rate. We hypothesize that LC could be responsible, through norepinephrinic modulation, for adaptations to optimize decision making in volatile environments. We also suggest a computational model on the interaction between the anterior cingulate cortex and LC for volatility estimation.",
      "authors": [
        "Silvetti, Massimo",
        "Seurinck, Ruth",
        "van Bochove, Marlies",
        "Verguts, Tom"
      ],
      "categories": null,
      "citations": 40,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2013.00160",
      "keywords": [
        "ACC",
        "Learning rate",
        "Volatility",
        "Locus coeruleus",
        "Plasticity",
        "Norepinephrine",
        "Reinforcement learning",
        "Prediction error"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2013-11-13",
      "selected": null,
      "title": "The influence of the noradrenergic system on optimal control of neural plasticity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84887868790&origin=inward"
      ]
    },
    {
      "abstract": "We derive a family of risk-sensitive reinforcement learning methods for\nagents, who face sequential decision-making tasks in uncertain environments. By\napplying a utility function to the temporal difference (TD) error, nonlinear\ntransformations are effectively applied not only to the received rewards but\nalso to the true transition probabilities of the underlying Markov decision\nprocess. When appropriate utility functions are chosen, the agents' behaviors\nexpress key features of human behavior as predicted by prospect theory\n(Kahneman and Tversky, 1979), for example different risk-preferences for gains\nand losses as well as the shape of subjective probability curves. We derive a\nrisk-sensitive Q-learning algorithm, which is necessary for modeling human\nbehavior when transition probabilities are unknown, and prove its convergence.\nAs a proof of principle for the applicability of the new framework we apply it\nto quantify human behavior in a sequential investment task. We find, that the\nrisk-sensitive variant provides a significantly better fit to the behavioral\ndata and that it leads to an interpretation of the subject's responses which is\nindeed consistent with prospect theory. The analysis of simultaneously measured\nfMRI signals show a significant correlation of the risk-sensitive TD error with\nBOLD signal change in the ventral striatum. In addition we find a significant\ncorrelation of the risk-sensitive Q-values with neural activity in the\nstriatum, cingulate cortex and insula, which is not present if standard\nQ-values are used.",
      "authors": [
        "Yun Shen",
        "Michael J. Tobia",
        "Tobias Sommer",
        "Klaus Obermayer"
      ],
      "categories": null,
      "citations": null,
      "comments": "27 pages, 7 figures",
      "databases": [
        "arXiv"
      ],
      "doi": "10.1162/NECO_a_00600",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": null,
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [
          "Learning"
        ],
        "title": "Neural Computation, Vol. 26, Nr. 7, pp. 1298--1328, 2014"
      },
      "publication_date": "2013-11-08",
      "selected": null,
      "title": "Risk-sensitive Reinforcement Learning",
      "urls": [
        "http://dx.doi.org/10.1162/NECO_a_00600",
        "http://arxiv.org/pdf/1311.2097v3",
        "http://arxiv.org/abs/1311.2097v3"
      ]
    },
    {
      "abstract": "Theories of reinforcement learning have proposed that the association of reward to visual stimuli may cause these objects to become fundamentally salient and thus attention-drawing. A number of recent studies have investigated the oculomotor correlates of this reward-priming effect, but there is some ambiguity in this literature regarding the involvement of top-down attentional set. Existing paradigms tend to create a situation where participants are actively looking for a reward-associated stimulus before subsequently showing that this selective bias sustains when it no longer has strategic purpose. This perseveration of attentional set is potentially different in nature than the direct impact of reward proposed by theory. Here we investigate the effect of reward on saccadic selection in a paradigm where strategic attentional set is decoupled from the effect of reward. We find that during search for a uniquely oriented target, the receipt of reward following selection of a target characterized by an irrelevant unique color causes subsequent stimuli characterized by this color to be preferentially selected. Importantly, this occurs regardless of whether the color characterizes the target or distractor. Other analyses demonstrate that only features associated with correct selection of the target prime the target representation, and that the magnitude of this effect can be predicted by variability in saccadic indices of feedback processing. These results add to a growing literature demonstrating that reward guides visual selection, often in spite of our strategic efforts otherwise. \u00c2\u00a9 2013 Elsevier B.V.",
      "authors": [
        "Hickey, C.",
        "van Zoest, W."
      ],
      "categories": null,
      "citations": 43,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.visres.2013.09.008",
      "keywords": [
        "Reward priming",
        "Oculomotor capture",
        "Eye movements",
        "Reward"
      ],
      "number_of_pages": 8,
      "pages": "67-74",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00426989",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Vision Research"
      },
      "publication_date": "2013-11-01",
      "selected": null,
      "title": "Reward-associated stimuli capture the eyes in spite of strategic attentional set",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84885827715&origin=inward"
      ]
    },
    {
      "abstract": "Developments in sensor technology and sensory input processing algorithms have enabled the use of mobile robots in real-world domains. As they are increasingly deployed to interact with humans in our homes and offices, robots need the ability to operate autonomously based on sensory cues and high-level feedback from non-expert human participants. Towards this objective, this chapter describes an integrated framework that jointly addresses the learning, adaptation, and interaction challenges associated with robust human-robot interaction in real-world application domains. The novel probabilistic framework consists of: (a) a bootstrap learning algorithm that enables a robot to learn layered graphical models of environmental objects and adapt to unforeseen dynamic changes; (b) a hierarchical planning algorithm based on partially observable Markov decision processes (POMDPs) that enables the robot to reliably and efficiently tailor learning, sensing, and processing to the task at hand; and (c) an augmented reinforcement learning algorithm that enables the robot to acquire limited high-level feedback from non-expert human participants, and merge human feedback with the information extracted from sensory cues. Instances of these algorithms are implemented and fully evaluated on mobile robots and in simulated domains using vision as the primary source of information in conjunction with range data and simplistic verbal inputs. Furthermore, a strategy is outlined to integrate these components to achieve robust human-robot interaction in real-world application domains. \u00c2\u00a9 2014, IGI Global. All rights reserved.",
      "authors": [
        "Sridharan, M."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.4018/978-1-4666-4607-0.ch060",
      "keywords": [],
      "number_of_pages": 21,
      "pages": "1255-1275",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781466646087",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Robotics: Concepts, Methodologies, Tools, and Applications"
      },
      "publication_date": "2013-10-31",
      "selected": null,
      "title": "An integrated framework for robust human-robot interaction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84944872273&origin=inward"
      ]
    },
    {
      "abstract": "A salient feature of human motor skill learning is the ability to exploit similarities across related tasks. In biological motor control, it has been hypothesized that muscle synergies, coherent activations of groups of muscles, allow for exploiting shared knowledge. Recent studies have shown that a rich set of complex motor skills can be generated by a combination of a small number of muscle synergies. In robotics, dynamic movement primitives are commonly used for motor skill learning. This machine learning approach implements a stable attractor system that facilitates learning and it can be used in high-dimensional continuous spaces. However, it does not allow for reusing shared knowledge, i.e. for each task an individual set of parameters has to be learned. We propose a novel movement primitive representation that employs parametrized basis functions, which combines the benefits of muscle synergies and dynamic movement primitives. For each task a superposition of synergies modulates a stable attractor system. This approach leads to a compact representation of multiple motor skills and at the same time enables efficient learning in high-dimensional continuous systems. The movement representation supports discrete and rhythmic movements and in particular includes the dynamic movement primitive approach as a special case. We demonstrate the feasibility of the movement representation in three multi-task learning simulated scenarios. First, the characteristics of the proposed representation are illustrated in a point-mass task. Second, in complex humanoid walking experiments, multiple walking patterns with different step heights are learned robustly and efficiently. Finally, in a multi-directional reaching task simulated with a musculoskeletal model of the human arm, we show how the proposed movement primitives can be used to learn appropriate muscle excitation patterns and to generalize effectively to new reaching skills.",
      "authors": [
        "R\u00fcckert, Elmar",
        "D'Avella, Andrea"
      ],
      "categories": null,
      "citations": 54,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fncom.2013.00138",
      "keywords": [
        "Motor control",
        "Dynamic movement primitives",
        "Musculoskeletal model",
        "Muscle synergies",
        "Reinforcement learning"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.8,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625188",
        "publisher": "Frontiers Media SA",
        "sjr": 0.69,
        "snip": 1.07,
        "subject_areas": [
          "Neuroscience (miscellaneous)",
          "Cellular and Molecular Neuroscience"
        ],
        "title": "Frontiers in Computational Neuroscience"
      },
      "publication_date": "2013-10-17",
      "selected": null,
      "title": "Learned parametrized dynamic movement primitives with shared synergies for controlling robotic and musculoskeletal systems",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84886246837&origin=inward"
      ]
    },
    {
      "abstract": "In the past decade, adaptive critic designs (ACDs), including heuristic dynamic programming (HDP), dual heuristic programming (DHP), and their action-dependent ones, have been widely studied to realize online learning control of dynamical systems. However, because neural networks with manually designed features are commonly used to deal with continuous state and action spaces, the generalization capability and learning efficiency of previous ACDs still need to be improved. In this paper, a novel framework of ACDs with sparse kernel machines is presented by integrating kernel methods into the critic of ACDs. To improve the generalization capability as well as the computational efficiency of kernel machines, a sparsification method based on the approximately linear dependence analysis is used. Using the sparse kernel machines, two kernel-based ACD algorithms, that is, kernel HDP (KHDP) and kernel DHP (KDHP), are proposed and their performance is analyzed both theoretically and empirically. Because of the representation learning and generalization capability of sparse kernel machines, KHDP and KDHP can obtain much better performance than previous HDP and DHP with manually designed neural networks. Simulation and experimental results of two nonlinear control problems, that is, a continuous-action inverted pendulum problem and a ball and plate control problem, demonstrate the effectiveness of the proposed kernel ACD methods. \u00c2\u00a9 2013 IEEE.",
      "authors": [
        "Xu, X.",
        "Hou, Z.",
        "Lian, C.",
        "He, H."
      ],
      "categories": null,
      "citations": 117,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1109/TNNLS.2012.2236354",
      "keywords": [
        "Adaptive critic designs",
        "Learning control",
        "Kernel machines",
        "Markov decision processes",
        "Reinforcement learning",
        "Approximate dynamic programming"
      ],
      "number_of_pages": 14,
      "pages": "762-775",
      "publication": {
        "category": "Journal",
        "cite_score": 21.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2162237X",
        "publisher": "IEEE Computational Intelligence Society",
        "sjr": 3.447,
        "snip": 3.984,
        "subject_areas": [
          "Software",
          "Computer Networks and Communications",
          "Computer Science Applications",
          "Artificial Intelligence"
        ],
        "title": "IEEE Transactions on Neural Networks and Learning Systems"
      },
      "publication_date": "2013-10-08",
      "selected": null,
      "title": "Online learning control using adaptive critic designs with sparse kernel machines",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84884922436&origin=inward"
      ]
    },
    {
      "abstract": "Human functional magnetic resonance imaging (fMRI) studies, as well as lesion, drug, and single-cell recording studies in animals, suggest that the striatum plays a key role in associating sensory events with rewarding actions, both by facilitating reward processing and prediction (i.e., reinforcement learning) and by biasing and later updating action selection. Previous human neuroimaging research has failed to dissociate striatal activity associated with reward, stimulus, and response processing, and previous electrophysiological research in nonhuman animals has typically only examined single striatal subregions. Overcoming both these limitations, we isolated blood oxygen level-dependent (BOLD) signal associated with four intratrial processes (stimulus, preparation of response, response, and feedback) in a visuomotor learning task and examined activity associated with each within four striatal subregions (ventral striatum, putamen, head of the caudate nucleus, and body of the caudate) and the lateral premotor cortex. Overall, the striatum and lateral premotor cortex were recruited during all trial components, confirming their importance in all aspects of visuomotor learning. However, the caudate was most active at stimulus and feedback, whereas the putamen peaked in activity at response. Activation in the lateral premotor cortex was, surprisingly, strongest during stimulus and following response as feedback approached. Activity was additionally examined at three reward magnitudes. Reward magnitude affected neural activity only during stimulus in the caudate, putamen, and premotor cortex, whereas the ventral striatum showed reward sensitivity during both stimulus and feedback. Collectively, these results indicate that each striatal region makes a unique contribution to visuomotor learning through functions performed at different points within single trials. \u00c2\u00a9 2013 the American Physiological Society.",
      "authors": [
        "Peterson, E.J.",
        "Seger, C.A."
      ],
      "categories": null,
      "citations": 16,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1152/jn.00164.2012",
      "keywords": [
        "Learning",
        "Response",
        "Striatum",
        "Reward"
      ],
      "number_of_pages": 14,
      "pages": "1689-1702",
      "publication": {
        "category": "Journal",
        "cite_score": 5.1,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00223077",
        "publisher": "American Physiological Society",
        "sjr": 1.067,
        "snip": 0.974,
        "subject_areas": [
          "Neuroscience (all)",
          "Physiology"
        ],
        "title": "Journal of Neurophysiology"
      },
      "publication_date": "2013-10-01",
      "selected": null,
      "title": "Many hats: Intratrial and reward level-dependent BOLD activity in the striatum and premotor cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84884952834&origin=inward"
      ]
    },
    {
      "abstract": "Humans often change their beliefs or behavior due to the behavior or opinions of others. This study explored, with the use of human event-related potentials (ERPs), whether social conformity is based on a general performance-monitoring mechanism. We tested the hypothesis that conflicts with a normative group opinion evoke a feedback-related negativity (FRN) often associated with performance monitoring and subsequent adjustment of behavior. The experimental results show that individual judgments of facial attractiveness were adjusted in line with a normative group opinion. A mismatch between individual and group opinions triggered a frontocentral negative deflection with the maximum at 200 ms, similar to FRN. Overall, a conflict with a normative group opinion triggered a cascade of neuronal responses: from an earlier FRN response reflecting a conflict with the normative opinion to a later ERP component (peaking at 380 ms) reflecting a conforming behavioral adjustment. These results add to the growing literature on neuronal mechanisms of social influence by disentangling the conflict-monitoring signal in response to the perceived violation of social norms and the neural signal of a conforming behavioral adjustment. \u00c2\u00a9 The Author (2012). Published by Oxford University Press.",
      "authors": [
        "Shestakova, A.",
        "Rieskamp, J.",
        "Tugin, S.",
        "Ossadtchi, A.",
        "Krutitskaya, J.",
        "Klucharev, V."
      ],
      "categories": null,
      "citations": 62,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/scan/nss064",
      "keywords": [
        "Social influence",
        "Conformity",
        "Medial frontal cortex",
        "Reinforcement learning",
        "Feedback-related negativity (FRN)"
      ],
      "number_of_pages": 8,
      "pages": "756-763",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17495016",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Social Cognitive and Affective Neuroscience"
      },
      "publication_date": "2013-10-01",
      "selected": null,
      "title": "Electrophysiological precursors of social conformity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84885783283&origin=inward"
      ]
    },
    {
      "abstract": "Mobile robots deployed in real-world domains frequently find it difficult to process all sensor inputs, or to operate without human input and domain knowledge. At the same time, complex domains make it difficult to provide robots all relevant domain knowledge in advance, and humans are unlikely to have the time and expertise to provide elaborate and accurate feedback. This paper presents an integrated framework that creates novel opportunities for addressing these learning, adaptation and collaboration challenges associated with human-robot collaboration. The framework consists of hierarchical planning, bootstrap learning and online reinforcement learning algorithms that inform and guide each other. As a result, robots are able to make best use of sensor inputs, soliciting high-level feedback from non-expert humans when such feedback is necessary and available. All algorithms are evaluated in simulation and on wheeled robots in dynamic indoor domains. \u00c2\u00a9 2013, Association for the Advancement of artificial intelligence.",
      "authors": [
        "Sridharan, M."
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": 6,
      "pages": "64-69",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781577356011",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "AAAI Spring Symposium - Technical Report"
      },
      "publication_date": "2013-09-05",
      "selected": null,
      "title": "Integrating visual learning and hierarchical planning for autonomy in human-robot collaboration",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84883308098&origin=inward"
      ]
    },
    {
      "abstract": "An influential neurocomputational theory of the biological mechanisms of decision making, the \u201cbasal ganglia go/no-go model,\u201d holds that individual variability in decision making is determined by differences in the makeup of a striatal system for approach and avoidance learning. The model has been tested empirically with the probabilistic selection task (PST), which determines whether individuals learn better from positive or negative feedback. In accordance with the model, in the present study we examined whether an individual\u2019s ability to learn from positive and negative reinforcement can be predicted by genetic factors related to the midbrain dopamine system. We also asked whether psychiatric and personality factors related to substance dependence and dopamine affect PST performance. Although we found characteristics that predicted individual differences in approach versus avoidance learning, these observations were qualified by additional findings that appear inconsistent with the predictions of the go/no-go model. These results highlight a need for future research to validate the PST as a measure of basal ganglia reward learning.",
      "authors": [
        "Baker, Travis E.",
        "Stockwell, Tim",
        "Holroyd, Clay B."
      ],
      "categories": null,
      "citations": 20,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3758/s13415-013-0164-8",
      "keywords": [
        "Individual differences",
        "Addiction",
        "Decision making",
        "Basal ganglia",
        "Personality",
        "Reinforcement learning",
        "Midbrain dopamine system",
        "Probabilistic selection task"
      ],
      "number_of_pages": 20,
      "pages": "417-436",
      "publication": {
        "category": "Journal",
        "cite_score": 5.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "15307026",
        "publisher": "Springer New York",
        "sjr": 1.085,
        "snip": 1.062,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Cognitive, Affective and Behavioral Neuroscience"
      },
      "publication_date": "2013-09-01",
      "selected": null,
      "title": "Constraints on decision making: Implications from genetics, personality, and addiction",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84879383264&origin=inward",
        "https://link.springer.com/content/pdf/10.3758/s13415-013-0164-8.pdf"
      ]
    },
    {
      "abstract": "This paper considers the problem of extending Training an Agent Manually via Evaluative Reinforcement (TAMER) in continuous state and action spaces. Investigative research using the TAMER framework enables a non-technical human to train an agent through a natural form of human feedback (negative or positive). The advantages of TAMER have been shown on tasks of training agents by only human feedback or combining human feedback with environment rewards. However, these methods are originally designed for discrete state-action, or continuous state-discrete action problems. This paper proposes an extension of TAMER to allow both continuous states and actions, called ACTAMER. The new framework utilizes any general function approximation of a human trainer\u2019s feedback signal. Moreover, a combined capability of ACTAMER and reinforcement learning is also investigated and evaluated. The combination of human feedback and reinforcement learning is studied in both settings: sequential and simultaneous. Our experimental results demonstrate the proposed method successfully allowing a human to train an agent in two continuous state-action domains: Mountain Car and Cart-pole (balancing).",
      "authors": [
        "Vien, Ngo Anh",
        "Ertel, Wolfgang",
        "Chung, Tae Choong"
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1007/s10489-012-0412-6",
      "keywords": [
        "Continuous actions",
        "Reward shaping",
        "Human-agent interaction",
        "Continuous states",
        "Interactive learning",
        "Reinforcement learning",
        "Human teachers"
      ],
      "number_of_pages": 12,
      "pages": "267-278",
      "publication": {
        "category": "Journal",
        "cite_score": 6.9,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0924-669X",
        "publisher": "Springer Netherlands",
        "sjr": 1.145,
        "snip": 1.78,
        "subject_areas": [
          "Artificial Intelligence"
        ],
        "title": "Applied Intelligence"
      },
      "publication_date": "2013-09-01",
      "selected": null,
      "title": "Learning via human feedback in continuous state and action spaces",
      "urls": [
        "https://link.springer.com/content/pdf/10.1007/s10489-012-0412-6.pdf",
        "https://dl.acm.org/doi/10.1007/s10489-012-0412-6",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84882925805&origin=inward"
      ]
    },
    {
      "abstract": "Post-traumatic stress disorder (PTSD) symptoms include behavioral avoidance which is acquired and tends to increase with time. This avoidance may represent a general learning bias; indeed, individuals with PTSD are often faster than controls on acquiring conditioned responses based on physiologically-aversive feedback. However, it is not clear whether this learning bias extends to cognitive feedback, or to learning from both reward and punishment. Here, male veterans with self-reported current, severe PTSD symptoms (PTSS group) or with few or no PTSD symptoms (control group) completed a probabilistic classification task that included both reward-based and punishment-based trials, where feedback could take the form of reward, punishment, or an ambiguous \u201cno-feedback\u201d outcome that could signal either successful avoidance of punishment or failure to obtain reward. The PTSS group outperformed the control group in total points obtained; the PTSS group specifically performed better than the control group on reward-based trials, with no difference on punishment-based trials. To better understand possible mechanisms underlying observed performance, we used a reinforcement learning model of the task, and applied maximum likelihood estimation techniques to derive estimated parameters describing individual participants\u2019 behavior. Estimations of the reinforcement value of the no-feedback outcome were significantly greater in the control group than the PTSS group, suggesting that the control group was more likely to value this outcome as positively reinforcing (i.e., signaling successful avoidance of punishment). This is consistent with the control group\u2019s generally poorer performance on reward trials, where reward feedback was to be obtained in preference to the no-feedback outcome. Differences in the interpretation of ambiguous feedback may contribute to the facilitated reinforcement learning often observed in PTSD patients, and may in turn provide new insight into how pathological behaviors are acquired and maintained in PTSD.",
      "authors": [
        "Catherine E. Myers",
        "Ahmed A. Moustafa",
        "Jony Sheynin",
        "Kirsten M. VanMeenen",
        "Mark W. Gilbertson",
        "Scott P. Orr",
        "Kevin D. Beck",
        "Kevin C. H. Pang",
        "Richard J. Servatius"
      ],
      "categories": null,
      "citations": 38,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0072508",
      "keywords": [
        "Mental health and psychiatry",
        "Antidepressants",
        "Learning",
        "Behavior",
        "Human learning",
        "Veterans",
        "Drug interactions",
        "Post-traumatic stress disorder"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2013-08-27",
      "selected": null,
      "title": "Learning to Obtain Reward, but Not Avoid Punishment, Is Affected by Presence of PTSD Symptoms in Male Veterans: Empirical Data and Computational Model",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0072508&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84883180119&origin=inward"
      ]
    },
    {
      "abstract": "This paper presents an algorithm to bootstrap shared understanding in a human-robot interaction scenario where the user teaches a robot a new task using teaching instructions yet unknown to it. In such cases, the robot needs to estimate simultaneously what the task is and the associated meaning of instructions received from the user. For this work, we consider a scenario where a human teacher uses initially unknown spoken words, whose associated unknown meaning is either a feedback (good/bad) or a guidance (go left, right,...). We present computational results, within an inverse reinforcement learning framework, showing that a) it is possible to learn the meaning of unknown and noisy teaching instructions, as well as a new task at the same time, b) it is possible to reuse the acquired knowledge about instructions for learning new tasks, and c) even if the robot initially knows some of the instructions' meanings, the use of extra unknown teaching instructions improves learning efficiency. \u00c2\u00a9 2013 IEEE.",
      "authors": [
        "Jonathan Grizou",
        "Manuel Lopes",
        "Pierre-Yves Oudeyer"
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/DevLrn.2013.6652523",
      "keywords": [],
      "number_of_pages": 8,
      "pages": "1-8",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-4799-1036-6",
        "issn": "2161-9476",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2013 IEEE 3rd Joint International Conference on Development and Learning and Epigenetic Robotics, ICDL 2013 - Electronic Conference Proceedings"
      },
      "publication_date": "2013-08-18",
      "selected": null,
      "title": "Robot learning simultaneously a task and how to interpret human instructions",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891137725&origin=inward",
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6652523"
      ]
    },
    {
      "abstract": "Intelligent building system attracts more and more attention in both academic and industrial communities. Learning human comfort requirements and incorporating it into building control system is one of the important issues. In the traditional HVAC control system, the thermal comfort and the acoustic comfort are often conflicted and we lack of a scheme to trade off them well. In this paper, we propose a unified control framework based on reinforcement learning to balance the multiple dimension comforts, including the thermal and acoustic comforts. We utilize the user's complaints in thermal and acoustic sensations as feedback and combine the current environment and devices information to learn the personalized optimal control policy using online Q-learning. The challenge caused by the complaints is coped with an incorporated perception estimation scheme in the Q-learning reward design. Both simulation results and the field experimental results demonstrate the effectiveness of the algorithm, especially in the adaptivity to the individual tradeoff between thermal and acoustic comfort. \u00c2\u00a9 2013 IEEE.",
      "authors": [
        "Yin Zhao",
        "Qianchuan Zhao",
        "Li Xia",
        "Zhijin Cheng",
        "Fulin Wang",
        "Fangting Song"
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "IEEE",
        "Scopus"
      ],
      "doi": "10.1109/CoASE.2013.6653964",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "416-421",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "978-1-4799-1515-6",
        "issn": "2161-8089",
        "publisher": "IEEE",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "2013 IEEE International Conference on Automation Science and Engineering (CASE)"
      },
      "publication_date": "2013-08-17",
      "selected": null,
      "title": "A unified control framework of HVAC system for thermal and acoustic comforts in office building",
      "urls": [
        "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6653964",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891521654&origin=inward"
      ]
    },
    {
      "abstract": "Animal research and human neuroimaging studies indicate that stress increases dopamine levels in brain regions involved in reward processing, and stress also appears to increase the attractiveness of addictive drugs. The current study tested the hypothesis that stress increases reward salience, leading to more effective learning about positive than negative outcomes in a probabilistic selection task. Changes to dopamine pathways with age raise the question of whether stress effects on incentive-based learning differ by age. Thus, the present study also examined whether effects of stress on reinforcement learning differed for younger (age 18-34) and older participants (age 65-85). Cold pressor stress was administered to half of the participants in each age group, and salivary cortisol levels were used to confirm biophysiological response to cold stress. After the manipulation, participants completed a probabilistic learning task involving positive and negative feedback. In both younger and older adults, stress enhanced learning about cues that predicted positive outcomes. In addition, during the initial learning phase, stress diminished sensitivity to recent feedback across age groups. These results indicate that stress affects reinforcement learning in both younger and older adults and suggests that stress exerts different effects on specific components of reinforcement learning depending on their neural underpinnings. \u00c2\u00a9 2012 American Psychological Association.",
      "authors": [
        "Lighthall, N.R.",
        "Gorlick, M.R.",
        "Schoeke, A.",
        "Frank, M.J.",
        "Mather, M."
      ],
      "categories": null,
      "citations": 80,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1037/a0029823",
      "keywords": [
        "Stress",
        "Cortisol",
        "Probabilistic reinforcement learning",
        "Reward",
        "Aging"
      ],
      "number_of_pages": 12,
      "pages": "35-46",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08827974",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Psychology and Aging"
      },
      "publication_date": "2013-08-13",
      "selected": null,
      "title": "Stress modulates reinforcement learning in younger and older adults",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84881235499&origin=inward"
      ]
    },
    {
      "abstract": "In the last years there has been an increasing interest on using human feedback during robot operation to incorporate non-expert human expertise while learning complex tasks. Most work has considered reinforcement learning frameworks were human feedback, provided through multiple modalities (speech, graphical interfaces, gestures) is converted into a reward. This paper explores a different communication channel: cognitive EEG brain signals related to the perception of errors by humans. In particular, we consider error potentials (ErrP), voltage deflections appearing when a user perceives an error, either committed by herself or by an external machine, thus encoding binary information about how a robot is performing a task. Based on this potential, we propose an algorithm based on policy matching for inverse reinforcement learning to infer the user goal from brain signals. We present two cases of study involving a target reaching task in a grid world and using a real mobile robot, respectively. For discrete worlds, the results show that the robot is able to infer and reach the target using only error potentials as feedback elicited from human observation. Finally, promising preliminary results were obtained for continuous states and actions in real scenarios. \u00c2\u00a9 2013 ACM.",
      "authors": [
        "I\u00f1aki Iturrate",
        "Jason Omedes",
        "Luis Montesano"
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus",
        "ACM"
      ],
      "doi": "10.1145/2493525.2493533",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "45-50",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450320191",
        "issn": null,
        "publisher": "Association for Computing Machinery",
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication"
      },
      "publication_date": "2013-08-04",
      "selected": null,
      "title": "Shared control of a robot using EEG-based feedback signals",
      "urls": [
        "https://dl.acm.org/doi/10.1145/2493525.2493533",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84882932193&origin=inward"
      ]
    },
    {
      "abstract": "In everyday life, expert advice has a great impact on individual decision making. Although often beneficial, advice may sometimes be misleading and cause people to pursue actions that entail suboptimal outcomes. This detrimental effect may diminish over time, when individuals have gathered sufficient contradicting evidence. Given the strong influence initial information has on opinion and personality impression formation, we aimed to investigate whether initial advice-confirmatory experience potentiates the rigidity with which persons stick to misleading advice. Furthermore, we intended to characterize the neuronal basis of such putative primacy effect. While undergoing functional magnetic resonance imaging (fMRI), participants selected between probabilistically reinforced symbols and were given the misleading tip that two low-probability symbols had a high reinforcement probability. One of these symbols initially received manipulated advice-congruent positive feedback (PF), the other one advice-incongruent negative feedback. Behaviorally, participants were impaired at learning to avoid advice-receiving symbols and overvalued them in terms of willingness to pay (WTP) in an auction market. Crucially, initial PF potentiated all effects. Greater ventral pallidal response to initial but not later PF during learning predicted higher behavioral WTP. Our results demonstrate that the nature of the very first advice-related experience already determines how strongly misleading advice will influence learning and ensuing decision making-an effect that is mediated by the ventral pallidum. Thus, in contrast to conventional reinforcement learning, learning under the influence of advice is susceptible to primacy effects. The present findings advance our understanding of why false beliefs are particularly difficult to change once they have been reinforced. \u00c2\u00a9 2013 Elsevier Inc.",
      "authors": [
        "Staudinger, M.R.",
        "B\u00c3\u00bcchel, C."
      ],
      "categories": null,
      "citations": 26,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2013.02.074",
      "keywords": [
        "Willingness to pay",
        "Ventral pallidum",
        "Primacy effect",
        "FMRI",
        "Reinforcement learning",
        "Advice"
      ],
      "number_of_pages": 9,
      "pages": "125-133",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2013-08-01",
      "selected": null,
      "title": "How initial confirmatory experience potentiates the detrimental influence of bad advice",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84876336768&origin=inward"
      ]
    },
    {
      "abstract": "Activation likelihood estimation (ALE) meta-analyses were used to examine the neural correlates of prediction error in reinforcement learning. The findings are interpreted in the light of current computational models of learning and action selection. In this context, particular consideration is given to the comparison of activation patterns from studies using instrumental and Pavlovian conditioning, and where reinforcement involved rewarding or punishing feedback. The striatum was the key brain area encoding for prediction error, with activity encompassing dorsal and ventral regions for instrumental and Pavlovian reinforcement alike, a finding which challenges the functional separation of the striatum into a dorsal 'actor' and a ventral 'critic'. Prediction error activity was further observed in diverse areas of predominantly anterior cerebral cortex including medial prefrontal cortex and anterior cingulate cortex. Distinct patterns of prediction error activity were found for studies using rewarding and aversive reinforcers; reward prediction errors were observed primarily in the striatum while aversive prediction errors were found more widely including insula and habenula. \u00c2\u00a9 2013 Elsevier Ltd.",
      "authors": [
        "Garrison, J.",
        "Erdeniz, B.",
        "Done, J."
      ],
      "categories": null,
      "citations": 296,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neubiorev.2013.03.023",
      "keywords": [
        "Punishment",
        "Reward",
        "Reinforcement learning",
        "Habenula",
        "Striatum",
        "Actor-critic",
        "Prediction error"
      ],
      "number_of_pages": 14,
      "pages": "1297-1310",
      "publication": {
        "category": "Journal",
        "cite_score": 13.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "01497634",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.599,
        "snip": 2.49,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Neuroscience and Biobehavioral Reviews"
      },
      "publication_date": "2013-08-01",
      "selected": null,
      "title": "Prediction error in reinforcement learning: A meta-analysis of neuroimaging studies",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84878887354&origin=inward"
      ]
    },
    {
      "abstract": "Timescales of learning in the basal ganglia and the hippocampus",
      "authors": [
        "Ortu, Daniele",
        "Skavhaug, Ida-Maria",
        "Vaidya, Manish"
      ],
      "categories": null,
      "citations": 1,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbeh.2013.00098",
      "keywords": [
        "Hippocampus",
        "Ventral tegmental area",
        "Basal ganglia",
        "Delayed feedback",
        "Reinforcement learning",
        "Degeneracy"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 4.6,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "16625153",
        "publisher": "Frontiers Media SA",
        "sjr": 0.975,
        "snip": 0.965,
        "subject_areas": [
          "Behavioral Neuroscience",
          "Cognitive Neuroscience",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Frontiers in Behavioral Neuroscience"
      },
      "publication_date": "2013-07-16",
      "selected": null,
      "title": "Timescales of learning in the basal ganglia and the hippocampus",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84880642267&origin=inward"
      ]
    },
    {
      "abstract": "The dopaminergic system is involved in reward encoding and reinforcement learning. Dopaminergic neurons from this system in the substantia nigra/ventral tegmental area complex (SN/VTA) fire in response to unexpected reinforcing cues. The goal of this study was to investigate whether individuals can gain voluntary control of SN/VTA activity, thereby potentially enhancing dopamine release to target brain regions. Neurofeedback and mental imagery were used to self-regulate the SN/VTA. Real-time functional magnetic resonance imaging (rtfMRI) provided abstract visual feedback of the SN/VTA activity while the subject imagined rewarding scenes. Skin conductance response (SCR) was recorded as a measure of emotional arousal. To examine the effect of neurofeedback, subjects were assigned to either receiving feedback directly proportional (n. =. 15, veridical feedback) or inversely proportional (n. =. 17, inverted feedback) to SN/VTA activity. Both groups of subjects were able to up-regulate SN/VTA activity initially without feedback. Veridical feedback improved the ability to up-regulate SN/VTA compared to baseline while inverted feedback did not. Additional dopaminergic regions were activated in both groups. The ability to self-regulate SN/VTA was differentially correlated with SCR depending on the group, suggesting an association between emotional arousal and neurofeedback performance. These findings indicate that SN/VTA can be voluntarily activated by imagery and voluntary activation is further enhanced by neurofeedback. The findings may lead the way towards a non-invasive strategy for endogenous control of dopamine. \u00c2\u00a9 2013 Elsevier Inc.",
      "authors": [
        "Sulzer, J.",
        "Sitaram, R.",
        "Blefari, M.L.",
        "Kollias, S.",
        "Birbaumer, N.",
        "Stephan, K.E.",
        "Luft, A.",
        "Gassert, R."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neuroimage.2013.02.041",
      "keywords": [
        "Real-time fMRI",
        "Skin conductance response",
        "Dopamine",
        "Neurofeedback",
        "Substantia nigra"
      ],
      "number_of_pages": 9,
      "pages": "176-184",
      "publication": {
        "category": "Journal",
        "cite_score": 11.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10538119",
        "publisher": "Academic Press Inc.",
        "sjr": 2.512,
        "snip": 1.782,
        "subject_areas": [
          "Cognitive Neuroscience",
          "Neurology"
        ],
        "title": "NeuroImage"
      },
      "publication_date": "2013-07-15",
      "selected": null,
      "title": "Neurofeedback-mediated self-regulation of the dopaminergic midbrain",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84875774892&origin=inward"
      ]
    },
    {
      "abstract": "Songbirds, long of interest to basic neuroscience, have great potential as a model system for translational neuroscience. Songbirds learn their complex vocal behavior in a manner that exemplifies general processes of perceptual and motor skill learning and, more specifically, resembles human speech learning. Song is subserved by circuitry that is specialized for vocal learning and production but that has strong similarities to mammalian brain pathways. The combination of highly quantifiable behavior and discrete neural substrates facilitates understanding links between brain and behavior, both in normal states and in disease. Here we highlight (a) behavioral and mechanistic parallels between birdsong and aspects of speech and social communication, including insights into mirror neurons, the function of auditory feedback, and genes underlying social communication disorders, and (b) contributions of songbirds to understanding cortical-basal ganglia circuit function and dysfunction, including the possibility of harnessing adult neurogenesis for brain repair. \u00c2\u00a9 2013 by Annual Reviews. All rights reserved.",
      "authors": [
        "Brainard, M.S.",
        "Doupe, A.J."
      ],
      "categories": null,
      "citations": 157,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1146/annurev-neuro-060909-152826",
      "keywords": [
        "Speech",
        "Mirror neurons",
        "Basal ganglia",
        "Hearing",
        "Reinforcement learning",
        "Neurogenesis"
      ],
      "number_of_pages": 29,
      "pages": "489-517",
      "publication": {
        "category": "Book",
        "cite_score": 24.4,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0147006X",
        "publisher": "Annual Reviews Inc.",
        "sjr": 8.389,
        "snip": 3.679,
        "subject_areas": [
          "Neuroscience (all)"
        ],
        "title": "Annual Review of Neuroscience"
      },
      "publication_date": "2013-07-01",
      "selected": null,
      "title": "Translating birdsong: Songbirds as a model for basic and applied medical research",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84880310191&origin=inward"
      ]
    },
    {
      "abstract": "The authors investigated the ability of human participants to discover novel actions under conditions of delayed reinforcement. Participants used a joystick to search for a target indicated by visual or auditory reinforcement. Reinforcement delays of 75-150 ms were found to significantly impair action acquisition. They also found an effect of modality, with acquisition superior with auditory feedback. The duration at which delay was found to impede action discovery is, to the authors knowledge, shorter than that previously reported from work with operant and causal learning paradigms. The sensitivity to delay reported, and the difference between modalities, is consistent with accounts of action discovery that emphasize the importance of a time stamp in the motor record for solving the credit assignment problem. Copyright \u00c2\u00a9 2013 Taylor and Francis Group, LLC.",
      "authors": [
        "Walton, T.",
        "Thirkettle, M.",
        "Redgrave, P.",
        "Gurney, K.N.",
        "Stafford, T."
      ],
      "categories": null,
      "citations": 4,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1080/00222895.2013.806108",
      "keywords": [
        "delayed reinforcement",
        "action acquisition",
        "reinforcement learning",
        "credit assignment problem"
      ],
      "number_of_pages": 10,
      "pages": "351-360",
      "publication": {
        "category": "Journal",
        "cite_score": 2.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00222895",
        "publisher": "Routledge",
        "sjr": 0.356,
        "snip": 0.699,
        "subject_areas": [
          "Biophysics",
          "Experimental and Cognitive Psychology",
          "Orthopedics and Sports Medicine",
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Motor Behavior"
      },
      "publication_date": "2013-07-01",
      "selected": null,
      "title": "The discovery of novel actions is affected by very brief reinforcement delays and reinforcement modality",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84880196523&origin=inward"
      ]
    },
    {
      "abstract": "This chapter describes the interactive learning system to assist positive change in the preference of a human toward the true preference. First, an introduction to interactive reinforcement learning with human in robot learning is given; then, the need to estimate the human's preference and to consider its changes by interactive learning system is described. Second, requirements for interactive system as being human adaptive and friendly are discussed. Then, the passive interaction design of the system to assist the awareness for a human is proposed. The system behaves passively to reflect the human intelligence by visualizing the traces of his/her behaviors. Experimental results show that subjects are divided into two groups, heavy users and light users, and that there are different effects between them under the same visualizing condition. They also show that the system improves the efficiency for deciding the most preferred plan for both heavy users and light users. \u00c2\u00a9 2013 by IGI Global. All rights reserved.",
      "authors": [
        "Yamaguchi, T.",
        "Nishimura, T.",
        "Takadama, K."
      ],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.4018/978-1-4666-4225-6.ch006",
      "keywords": [],
      "number_of_pages": 15,
      "pages": "86-100",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781466642263",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Engineering Creative Design in Robotics and Mechatronics"
      },
      "publication_date": "2013-06-30",
      "selected": null,
      "title": "Awareness-based recommendation: Toward the human adaptive and friendly interactive learning system",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84944724854&origin=inward"
      ]
    },
    {
      "abstract": "<p>Modulations of the feedback-related negativity (FRN) event-related potential (ERP) have been suggested as a potential biomarker in psychopathology. A dominant theory about this signal contends that it reflects the operation of the neural system underlying reinforcement learning in humans. The theory suggests that this frontocentral negative deflection in the ERP 230\u2013270 ms after the delivery of a probabilistic reward expresses a prediction error signal derived from midbrain dopaminergic projections to the anterior cingulate cortex. We tested this theory by investigating whether FRN will also be observed for an inherently aversive outcome: physical pain. In another session, the outcome was monetary reward instead of pain. As predicted, unexpected reward omissions (a negative reward prediction error) yielded a more negative deflection relative to unexpected reward delivery. Surprisingly, unexpected pain omission (a positive reward prediction error) also yielded a negative deflection relative to unexpected pain delivery. Our data challenge the theory by showing that the FRN expresses aversive prediction errors with the same sign as reward prediction errors. Both FRNs were spatiotemporally and functionally equivalent. We suggest that FRN expresses salience prediction errors rather than reward prediction errors.</p>",
      "authors": [
        "Deborah Talmi",
        "Ryan Atkinson",
        "Wael El-Deredy"
      ],
      "categories": null,
      "citations": 153,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1523/JNEUROSCI.5695-12.2013",
      "keywords": [],
      "number_of_pages": 6,
      "pages": "8264-8269",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "02706474",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Neuroscience"
      },
      "publication_date": "2013-05-13",
      "selected": null,
      "title": "The Feedback-Related Negativity Signals Salience Prediction Errors, Not Reward Prediction Errors",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84877300383&origin=inward"
      ]
    },
    {
      "abstract": "Importance: Childhood sexual abuse (CSA) has been associated with psychopathology, particularly major depressive disorder (MDD), and high-risk behaviors. Despite the epidemiological data available, the mechanisms underlying these maladaptive outcomes remain poorly understood. Objective: We examined whether a history of CSA, particularly in conjunction with a past episode of MDD, is associated with behavioral and neural dysfunction in reinforcement learning, and whether such dysfunction is linked to maladaptive behavior. Design: Participants completed a clinical evaluation and a probabilistic reinforcement task while 128-channel event-related potentials were recorded. Setting: Academic setting; participants recruited from the community. Participants: Fifteen women with a history of CSA and remitted MDD (CSA + rMDD), 16 women with remitted MDD with no history of CSA (rMDD), and 18 healthy women (controls). Exposure: Three or more episodes of coerced sexual contact (mean [SD] duration, 3.00 [2.20] years) between the ages of 7 and 12 years by at least 1 male perpetrator. Main Outcomes and Measures: Participants' preference for choosing the most rewarded stimulus and avoiding the most punished stimulus was evaluated. The feedback-related negativity and error-related negativity - hypothesized to reflect activation in the anterior cingulate cortex - were used as electrophysiological indices of reinforcement learning. Results: No group differences emerged in the acquisition of reinforcement contingencies. In trials requiring participants to rely partially or exclusively on previously rewarded information, the CSA + rMDD group showed (1) lower accuracy (relative to both controls and the rMDD group), (2) blunted electrophysiological differentiation between correct and incorrect responses (relative to controls), and (3) increased activation in the subgenual anterior cingulate cortex (relative to the rMDD group). A history of CSA was not associated with impairments in avoiding the most punished stimulus. Self-harm and suicidal behaviors correlated with poorer performance of previously rewarded, but not previously punished, trials. Conclusions and Relevance: Irrespective of past MDD episodes, women with a history of CSA showed neural and behavioral deficits in utilizing previous reinforcement to optimize decision making in the absence of feedback (blunted \"Go learning\"). Although our study provides initial evidence for reward-specific deficits associated with CSA, future research is warranted to determine if disrupted positive reinforcement learning predicts high-risk behavior following CSA. \u00c2\u00a92013 American Medical Association. All rights reserved.",
      "authors": [
        "Pechtel, P.",
        "Pizzagalli, D.A."
      ],
      "categories": null,
      "citations": 51,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1001/jamapsychiatry.2013.728",
      "keywords": [],
      "number_of_pages": 9,
      "pages": "499-507",
      "publication": {
        "category": "Journal",
        "cite_score": 31.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2168622X",
        "publisher": "American Medical Association",
        "sjr": 6.578,
        "snip": 6.04,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "JAMA Psychiatry"
      },
      "publication_date": "2013-05-01",
      "selected": null,
      "title": "Disrupted reinforcement learning and maladaptive behavior in women with a history of childhood sexual abuse: A high-density event-related potential study",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84876907197&origin=inward"
      ]
    },
    {
      "abstract": "Despite explicitly wanting to quit, long-term addicts find themselves powerless to resist drugs, despite knowing that drug-taking may be a harmful course of action. Such inconsistency between the explicit knowledge of negative consequences and the compulsive behavioral patterns represents a cognitive/behavioral conflict that is a central characteristic of addiction. Neurobiologically, differential cue-induced activity in distinct striatal subregions, as well as the dopamine connectivity spiraling from ventral striatal regions to the dorsal regions, play critical roles in compulsive drug seeking. However, the functional mechanism that integrates these neuropharmacological observations with the above-mentioned cognitive/behavioral conflict is unknown. Here we provide a formal computational explanation for the drug-induced cognitive inconsistency that is apparent in the addicts' \u201cself-described mistake\u201d. We show that addictive drugs gradually produce a motivational bias toward drug-seeking at low-level habitual decision processes, despite the low abstract cognitive valuation of this behavior. This pathology emerges within the hierarchical reinforcement learning framework when chronic exposure to the drug pharmacologically produces pathologicaly persistent phasic dopamine signals. Thereby the drug hijacks the dopaminergic spirals that cascade the reinforcement signals down the ventro-dorsal cortico-striatal hierarchy. Neurobiologically, our theory accounts for rapid development of drug cue-elicited dopamine efflux in the ventral striatum and a delayed response in the dorsal striatum. Our theory also shows how this response pattern depends critically on the dopamine spiraling circuitry. Behaviorally, our framework explains gradual insensitivity of drug-seeking to drug-associated punishments, the blocking phenomenon for drug outcomes, and the persistent preference for drugs over natural rewards by addicts. The model suggests testable predictions and beyond that, sets the stage for a view of addiction as a pathology of hierarchical decision-making processes. This view is complementary to the traditional interpretation of addiction as interaction between habitual and goal-directed decision systems.",
      "authors": [
        "Mehdi Keramati",
        "Boris Gutkin"
      ],
      "categories": null,
      "citations": 34,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0061489",
      "keywords": [
        "Drug research and development",
        "Dopamine",
        "Learning",
        "Behavior",
        "Decision making",
        "Neostriatum",
        "Addicts",
        "Drug addiction"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2013-04-24",
      "selected": null,
      "title": "Imbalanced Decision Hierarchy in Addicts Emerging from Drug-Hijacked Dopamine Spiraling Circuit",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84876562294&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0061489&type=printable"
      ]
    },
    {
      "abstract": "Incorporating human interaction into agent learning yields two crucial benefits. First, human knowledge can greatly improve the speed and final result of learning compared to pure trial-and-error approaches like reinforcement learning. And second, human users are empowered to designate \"correct\" behavior. In this abstract, we present research on a system for learning from human interaction - the TAMER framework - then point to extensions to TAMER, and finally describe a demonstration of these systems.",
      "authors": [
        "Knox, W.B.",
        "Stone, P.",
        "Breazeal, C."
      ],
      "categories": null,
      "citations": 14,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/2451176.2451201",
      "keywords": [
        "Human-agent interaction",
        "End-user programming",
        "Reinforcement learning",
        "Modeling and prediction of user behavior",
        "Interactive machine learning"
      ],
      "number_of_pages": 2,
      "pages": "65-66",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450319669",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent User Interfaces, Proceedings IUI"
      },
      "publication_date": "2013-03-19",
      "selected": null,
      "title": "Teaching agents with human feedback: A demonstration of the TAMER framework",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84875846892&origin=inward"
      ]
    },
    {
      "abstract": "Many applications of Machine Learning (ML) involve interactions with humans. Humans may provide input to a learning algorithm (in the form of labels, demonstrations, corrections, rankings or evaluations) while observing its outputs (in the form of feedback, predictions or executions). Although humans are an integral part of the learning process, traditional ML systems used in these applications are agnostic to the fact that inputs/outputs are from/for humans. However, a growing community of researchers at the intersection of ML and human-computer interaction are making interaction with humans a central part of developing ML systems. These efforts include applying interaction design principles to ML systems, using human-subject testing to evaluate ML systems and inspire new methods, and changing the input and output channels of ML systems to better leverage human capabilities. With this Interactive Machine Learning (IML) workshop at IUI 2013 we aim to bring this community together to share ideas, get up-to-date on recent advances, progress towards a common framework and terminology for the field, and discuss the open questions and challenges of IML.",
      "authors": [
        "Amershi, S.",
        "Cakmak, M.",
        "Knox, W.B.",
        "Kulesza, T.",
        "Lau, T."
      ],
      "categories": null,
      "citations": 5,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1145/2451176.2451230",
      "keywords": [
        "Empirical studies and computational models of human teaching",
        "Feature labeling",
        "Human-in-the-loop intelligent systems",
        "End-user programming",
        "Transparency and feedback in machine learning",
        "Programming by demonstration",
        "Democratizing machine learning",
        "Interactive clustering",
        "Reinforcement learning with human feedback or guidance",
        "Active learning"
      ],
      "number_of_pages": 3,
      "pages": "121-123",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781450319669",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "International Conference on Intelligent User Interfaces, Proceedings IUI"
      },
      "publication_date": "2013-03-19",
      "selected": null,
      "title": "IUI workshop on interactive machine learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84875842171&origin=inward"
      ]
    },
    {
      "abstract": "Patients with schizophrenia (SZ) show deficits on tasks of rapid reinforcement learning, like probabilistic reversal learning (PRL), but the neural bases for those impairments are not known. Recent evidence of relatively intact sensitivity to negative outcomes in the ventral striatum (VS) in many SZ patients suggests that PRL deficits may be largely attributable to processes downstream from feedback processing, involving both the activation of executive control task regions and deactivation of default mode network (DMN) components. We analyzed data from 29 chronic SZ patients and 21 matched normal controls (NCs) performing a PRL task in an MRI scanner. Subjects were presented with eight pairs of fractal stimuli, for 50 trials each. For each pair, subjects learned to choose the more frequently-rewarded (better) stimulus. Each time a criterion was reached, the better stimulus became the worse one, and the worse became the better. Responses to feedback events were assessed through whole-brain and regions-of-interest (ROI) analyses in DMN. We also assessed correlations between BOLD signal contrasts and clinical measures in SZs. Relative to NCs, SZ patients showed comparable deactivation of VS in response to negative feedback, but reduced deactivation of DMN components including medial prefrontal cortex (mPFC). The magnitudes of patients' punishment-evoked deactivations in VS and ventromedial PFC correlated significantly with clinical ratings for avolition/anhedonia. These findings suggest that schizophrenia is associated with a reduced ability to deactivate components of default mode networks, following the presentation of informative feedback and that motivational deficits in SZ relate closely to feedback-evoked activity in reward circuit components. These results also confirm a role for ventrolateral and dorsomedial PFC in the execution of response-set shifts.",
      "authors": [
        "James A. Waltz",
        "Zuzana Kasanova",
        "Thomas J. Ross",
        "Betty J. Salmeron",
        "Robert P. McMahon",
        "James M. Gold",
        "Elliot A. Stein"
      ],
      "categories": null,
      "citations": 100,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0057257",
      "keywords": [
        "Schizophrenia",
        "Antipsychotics",
        "Learning",
        "Behavior",
        "Magnetic resonance imaging",
        "Neostriatum",
        "Neural networks",
        "Prefrontal cortex"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2013-02-27",
      "selected": null,
      "title": "The Roles of Reward, Default, and Executive Control Networks in Set-Shifting Impairments in Schizophrenia",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84874522871&origin=inward",
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0057257&type=printable"
      ]
    },
    {
      "abstract": "In this article we describe a novel algorithm that allows fast and continuous learning on a physical robot working in a real environment. The learning process is never stopped and new knowledge gained from robot-environment interactions can be incorporated into the controller at any time. Our algorithm lets a human observer control the reward given to the robot, hence avoiding the burden of defining a reward function. Despite the highly-non-deterministic reinforcement, through the experimental results described in this paper, we will see how the learning processes are never stopped and are able to achieve fast robot adaptation to the diversity of different situations the robot encounters while it is moving in several environments.",
      "authors": [
        "Quint\u00cc\u0081ia, P.",
        "Iglesias, R.",
        "Rod\u00c5\u0095iguez, M.A.",
        "Regueiro, C.V."
      ],
      "categories": null,
      "citations": 10,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Autonomous robots",
        "Reinforcement learning"
      ],
      "number_of_pages": 9,
      "pages": "56-64",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "18880258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of Physical Agents"
      },
      "publication_date": "2013-02-20",
      "selected": null,
      "title": "Learning on real robots from experience and simple user feedback",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84873914827&origin=inward"
      ]
    },
    {
      "abstract": "We often need to learn how to move based on a single performance measure that reflects the overall success of our movements. However, movements have many properties, such as their trajectories, speeds and timing of end-points, thus the brain needs to decide which properties of movements should be improved; it needs to solve the credit assignment problem. Currently, little is known about how humans solve credit assignment problems in the context of reinforcement learning. Here we tested how human participants solve such problems during a trajectory-learning task. Without an explicitly-defined target movement, participants made hand reaches and received monetary rewards as feedback on a trial-by-trial basis. The curvature and direction of the attempted reach trajectories determined the monetary rewards received in a manner that can be manipulated experimentally. Based on the history of action-reward pairs, participants quickly solved the credit assignment problem and learned the implicit payoff function. A Bayesian credit-assignment model with built-in forgetting accurately predicts their trial-by-trial learning.",
      "authors": [
        "Gregory Dam",
        "Konrad Kording",
        "Kunlin Wei"
      ],
      "categories": null,
      "citations": 21,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1371/journal.pone.0055352",
      "keywords": [
        "Machine learning",
        "Robots",
        "Learning",
        "Human learning",
        "Probability distribution",
        "Learning curves",
        "Nervous system",
        "Musculoskeletal mechanics"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 6.0,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "1932-6203",
        "publisher": "Public Library of Science",
        "sjr": 0.885,
        "snip": 1.253,
        "subject_areas": [
          "Multidisciplinary"
        ],
        "title": "PLoS ONE"
      },
      "publication_date": "2013-02-08",
      "selected": null,
      "title": "Credit Assignment during Movement Reinforcement Learning",
      "urls": [
        "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0055352&type=printable",
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84873620063&origin=inward"
      ]
    },
    {
      "abstract": "Reinforcement learning (RL) and adaptive dynamic programming (ADP) has been one of the most critical research fields in science and engineering for modern complex systems. This book describes the latest RL and ADP techniques for decision and control in human engineered systems, covering both single player decision and control and multi-player games. Edited by the pioneers of RL and ADP research, the book brings together ideas and methods from many fields and provides an important and timely guidance on controlling a wide variety of systems, such as robots, industrial processes, and economic decision-making. \u00c2\u00a9 2013 The Institute of Electrical and Electronics Engineers, Inc.",
      "authors": [
        "Lewis, F.L.",
        "Liu, D."
      ],
      "categories": null,
      "citations": 344,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/9781118453988",
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781118104200",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Reinforcement Learning and Approximate Dynamic Programming for Feedback Control"
      },
      "publication_date": "2013-02-07",
      "selected": null,
      "title": "Reinforcement Learning and Approximate Dynamic Programming for Feedback Control",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84891584860&origin=inward"
      ]
    },
    {
      "abstract": "Many new formulations of reinforcement learning and approximate dynamic programming (RLADP) have appeared in recent years, as it has grown in control applications, control theory, operations research, computer science, robotics, and efforts to understand brain intelligence. The chapter reviews the foundations and challenges common to all these areas, in a unified way but with reference to their variations. It highlights cases where experience in one area sheds light on obstacles or common misconceptions in another. Many common beliefs about the limits of RLADP are based on such obstacles and misconceptions, for which solutions already exist. Above all, this chapter pinpoints key opportunities for future research important to the field as a whole and to the larger benefits it offers. \u00c2\u00a9 2013 The Institute of Electrical and Electronics Engineers, Inc.",
      "authors": [
        "Werbos, P.J."
      ],
      "categories": null,
      "citations": 24,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1002/9781118453988.ch1",
      "keywords": [
        "Feedback control/actor-critic for optimal decision",
        "RL/ADP decision/control in human engineered",
        "RLADP, RL for control, reinforcement signal",
        "Feedback using RL/ADP, and control",
        "Real-time adaptive learning, MAS"
      ],
      "number_of_pages": 30,
      "pages": "1-30",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781118104200",
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Reinforcement Learning and Approximate Dynamic Programming for Feedback Control"
      },
      "publication_date": "2013-02-07",
      "selected": null,
      "title": "Reinforcement Learning and Approximate Dynamic Programming (RLADP)-Foundations, Common Misconceptions, and the Challenges Ahead",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84876119970&origin=inward"
      ]
    },
    {
      "abstract": "Vocal motor development in infancy provides a crucial foundation for language development. Some significant early accomplishments include learning to control the process of phonation (the production of sound at the larynx) and learning to produce the sounds of one's language. Previous work has shown that social reinforcement shapes the kinds of vocalizations infants produce. We present a neural network model that provides an account of how vocal learning may be guided by reinforcement. The model consists of a self-organizing map that outputs to muscles of a realistic vocalization synthesizer. Vocalizations are spontaneously produced by the network. If a vocalization meets certain acoustic criteria, it is reinforced, and the weights are updated to make similar muscle activations increasingly likely to recur. We ran simulations of the model under various reinforcement criteria and tested the types of vocalizations it produced after learning in the different conditions. When reinforcement was contingent on the production of phonated (i.e. voiced) sounds, the network's post-learning productions were almost always phonated, whereas when reinforcement was not contingent on phonation, the network's post-learning productions were almost always not phonated. When reinforcement was contingent on both phonation and proximity to English vowels as opposed to Korean vowels, the model's post-learning productions were more likely to resemble the English vowels and vice versa. \u00c2\u00a9 2012 Elsevier Ltd.",
      "authors": [
        "Warlaumont, A.S.",
        "Westermann, G.",
        "Buder, E.H.",
        "Oller, D.K."
      ],
      "categories": null,
      "citations": 30,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2012.11.012",
      "keywords": [
        "Motor development",
        "Infant vocalization",
        "Reinforcement",
        "Articulatory speech synthesis",
        "Neural network",
        "Neuromuscular control"
      ],
      "number_of_pages": 12,
      "pages": "64-75",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2013-02-01",
      "selected": null,
      "title": "Prespeech motor learning in a neural network using reinforcement",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84871643738&origin=inward"
      ]
    },
    {
      "abstract": "This paper addresses the problem of learning a task from demonstration. We adopt the framework of inverse reinforcement learning, where tasks are represented in the form of a reward function. Our contribution is a novel active learning algorithm that enables the learning agent to query the expert for more informative demonstrations, thus leading to more sample-efficient learning. For this novel algorithm (Generalized Binary Search for Inverse Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample complexity and illustrate its applicability on several different tasks. To our knowledge, GBS-IRL is the first active IRL algorithm with provable sample complexity bounds. We also discuss our method in light of other existing methods in the literature and its general applicability in multi-class classification problems. Finally, motivated by recent work on learning from demonstration in robots, we also discuss how different forms of human feedback can be integrated in a transparent manner in our learning framework.",
      "authors": [
        "Melo, Francisco",
        "Lopes, Manuel"
      ],
      "categories": null,
      "citations": null,
      "comments": null,
      "databases": [
        "arXiv"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": null,
      "publication_date": "2013-01-23",
      "selected": null,
      "title": "Multi-class Generalized Binary Search for Active Inverse Reinforcement Learning",
      "urls": [
        "http://arxiv.org/pdf/1301.5488.pdf",
        "http://arxiv.org/abs/1301.5488v1",
        "http://arxiv.org/pdf/1301.5488v1"
      ]
    },
    {
      "abstract": "A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.",
      "authors": [
        "Griffith, S.",
        "Subramanian, K.",
        "Scholz, J.",
        "Isbell, C.L.",
        "Thomaz, A."
      ],
      "categories": null,
      "citations": 224,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": "9781713871088",
        "issn": "10495258",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Advances in Neural Information Processing Systems"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Policy shaping: Integrating human feedback with Reinforcement Learning",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84899032502&origin=inward"
      ]
    },
    {
      "abstract": "The proceedings contain 118 papers. The special focus in this conference is on New Interfaces for Musical Expression. The topics include: Towards mapping timbre to emotional affect; pamdi music box primarily analog-mechanical, digitally iterated music box; mirrorfugue iii: Conjuring the recorded pianist; expressive control of indirect augmented reality during live music performances; designing and building expressive robotic guitars; multi sensor tracking for live sound transformation; feeling for sound: Mapping sonic data to haptic perceptions; toward an emotionally intelligent piano: Real-time emotion detection and performer feedback via kinesthetic sensing in piano performance; netpixl: Towards a new paradigm for networked application development; a self-organizing gesture map for a voice-controlled instrument interface; rouages: Revealing the mechanisms of digital musical instruments to the audience; note~ for max-an extension for max/msp for media arts & music; a musical performance evaluation system for beginner musician based on real-time score following; swarmed: Captive portals, mobile devices, and audience participation in multi-user music performance; rainboard and musix: Building dynamic isomorphic interfaces; reactive environment for network music performance; multi-touch interfaces for phantom source positioning in live sound diffusion; a compact spectrum-assisted human beatboxing reinforcement learning tool on smartphone; mobile dj: A tangible, mobile platform for active and collaborative music listening; machine learning of musical gestures; lolol: Laugh out loud on laptop; muscular interactions combining emg and mmg sensing for musical practice; the third room: A 3d virtual music paradigm; sonnet: A code interface for sonifying computer network data; pesi extended system: In space, on body, with 3 musicians; alphasphere.",
      "authors": [],
      "categories": null,
      "citations": 0,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": 1.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "22204792",
        "publisher": "International Conference on New Interfaces for Musical Expression",
        "sjr": 1.429,
        "snip": 0.544,
        "subject_areas": [
          "Signal Processing",
          "Instrumentation",
          "Hardware and Architecture",
          "Computer Science Applications",
          "Human-Computer Interaction",
          "Control and Systems Engineering",
          "Music"
        ],
        "title": "Proceedings of the International Conference on New Interfaces for Musical Expression"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "13th International conference on New Interfaces for Musical Expression, NIME 2013",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85160283022&origin=inward"
      ]
    },
    {
      "abstract": "Adaptive Resonance Theory, or ART, is a cognitive and neural theory of how the brain autonomously learns to categorize, recognize, and predict objects and events in a changing world. This article reviews classical and recent developments of ART, and provides a synthesis of concepts, principles, mechanisms, architectures, and the interdisciplinary data bases that they have helped to explain and predict. The review illustrates that ART is currently the most highly developed cognitive and neural theory available, with the broadest explanatory and predictive range. Central to ART's predictive power is its ability to carry out fast, incremental, and stable unsupervised and supervised learning in response to a changing world. ART specifies mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony during both unsupervised and supervised learning. ART provides functional and mechanistic explanations of such diverse topics as laminar cortical circuitry; invariant object and scenic gist learning and recognition; prototype, surface, and boundary attention; gamma and beta oscillations; learning of entorhinal grid cells and hippocampal place cells; computation of homologous spatial and temporal mechanisms in the entorhinal-hippocampal system; vigilance breakdowns during autism and medial temporal amnesia; cognitive-emotional interactions that focus attention on valued objects in an adaptively timed way; item-order-rank working memories and learned list chunks for the planning and control of sequences of linguistic, spatial, and motor information; conscious speech percepts that are influenced by future context; auditory streaming in noise during source segregation; and speaker normalization. Brain regions that are functionally described include visual and auditory neocortex; specific and nonspecific thalamic nuclei; inferotemporal, parietal, prefrontal, entorhinal, hippocampal, parahippocampal, perirhinal, and motor cortices; frontal eye fields; supplementary eye fields; amygdala; basal ganglia: cerebellum; and superior colliculus. Due to the complementary organization of the brain, ART does not describe many spatial and motor behaviors whose matching and learning laws differ from those of ART. ART algorithms for engineering and technology are listed, as are comparisons with other types of models. \u00c2\u00a9 2012 Elsevier Ltd.",
      "authors": [
        "Grossberg, S."
      ],
      "categories": null,
      "citations": 330,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.neunet.2012.09.017",
      "keywords": [
        "Synchrony",
        "Learning",
        "Attention",
        "Basal ganglia",
        "Adaptive Resonance Theory",
        "Adaptive timing",
        "Consciousness",
        "Entorhinal cortex",
        "Expectation",
        "Hippocampal cortex",
        "Reinforcement learning",
        "Recognition",
        "Inferotemporal cortex",
        "Prefrontal cortex",
        "Amygdala",
        "Gamma and beta oscillations",
        "Speech perception",
        "Working memory",
        "Parietal cortex"
      ],
      "number_of_pages": 47,
      "pages": "1-47",
      "publication": {
        "category": "Journal",
        "cite_score": 14.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "08936080",
        "publisher": "Elsevier Ltd.",
        "sjr": 2.221,
        "snip": 2.729,
        "subject_areas": [
          "Artificial Intelligence",
          "Cognitive Neuroscience"
        ],
        "title": "Neural Networks"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Adaptive Resonance Theory: How a brain learns to consciously attend, learn, and recognize a changing world",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84870424089&origin=inward"
      ]
    },
    {
      "abstract": "In this paper, we address a relatively unexplored aspect of designing agents that learn from human training by investigating how the agent's non-task behavior can elicit human feedback of higher quality and quantity. We use the TAMER framework, which facilitates the training of agents by human-generated reward signals, i.e., judgements of the quality of the agent's actions, as the foundation for our investigation. Then, we propose two new training interfaces to increase active involvement in the training process and thereby improve the agent's task performance. One provides information on the agent's uncertainty, the other on its performance. Our results from a 51-subject user study show that these interfaces can induce the trainers to train longer and give more feedback. The agent's performance, however, increases only in response to the addition of performance-oriented information, not by sharing uncertainty levels. Subsequent analysis of our results suggests that the organizational maxim about human behavior, \"you get what you measure\"-i.e., sharing metrics with people causes them to focus on maximizing or minimizing those metrics while de-emphasizing other objectives-also applies to the training of agents, providing a powerful guiding principle for human-agent interface design in general. Copyright \u00c2\u00a9 2013, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
      "authors": [
        "Li, G.",
        "Hung, H.",
        "Whiteson, S.",
        "Knox, W.B."
      ],
      "categories": null,
      "citations": 28,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": null,
      "keywords": [
        "Human-agent interaction",
        "Reinforcement learning"
      ],
      "number_of_pages": 8,
      "pages": "909-916",
      "publication": {
        "category": "Conference Proceedings",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "12th International Conference on Autonomous Agents and Multiagent Systems 2013, AAMAS 2013"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Using informative behavior to increase engagement in the TAMER framework",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84899440129&origin=inward"
      ]
    },
    {
      "abstract": "The identification of learning mechanisms for locomotion has been the subject of much research for some time but many challenges remain. Dynamic systems theory (DST) offers a novel approach to humanoid learning through environmental interaction. Reinforcement learning (RL) has offered a promising method to adaptively link the dynamic system to the environment it interacts with via a reward-based value system. In this paper, we propose a model that integrates the above perspectives and applies it to the case of a humanoid (NAO) robot learning to walk the ability of which emerges from its value-based interaction with the environment. In the model, a simplified central pattern generator (CPG) architecture inspired by neuroscientific research and DST is integrated with an actor-critic approach to RL (cpg-actor-critic). In the cpg-actor-critic architecture, least-square-temporal-difference (LSTD) based learning converges to the optimal solution quickly by using natural gradient and balancing exploration and exploitation. Futhermore, rather than using a traditional (designer-specified) reward it uses a dynamic value function as a stability indicator (SI) that adapts to the environment. The results obtained are analyzed and explained by using a novel DST embodied cognition approach. Learning to walk, from this perspective, is a process of integrating sensorimotor levels and value.",
      "authors": [
        "LI, CAI",
        "Lowe, Robert",
        "Ziemke, Tom"
      ],
      "categories": null,
      "citations": 17,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.3389/fnbot.2013.00005",
      "keywords": [
        "Value system",
        "Embodied cognition",
        "Dynamical systems theory",
        "Humanoid walking",
        "Reinforcement learning",
        "Actor-critic",
        "Central pattern generators"
      ],
      "number_of_pages": null,
      "pages": null,
      "publication": {
        "category": "Journal",
        "cite_score": 5.0,
        "is_potentially_predatory": true,
        "isbn": null,
        "issn": "1662-5218",
        "publisher": "Frontiers Media SA",
        "sjr": 0.757,
        "snip": 1.219,
        "subject_areas": [
          "Artificial Intelligence",
          "Biomedical Engineering"
        ],
        "title": "Frontiers in Neurorobotics"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Humanoids Learning to Walk: A Natural CPG-Actor-Critic Architecture",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84902356043&origin=inward"
      ]
    },
    {
      "abstract": "IMPORTANCE: Suicide can be viewed as an escape from unendurable punishment at the cost of any future rewards. Could faulty estimation of these outcomes predispose to suicidal behavior? In behavioral studies, many of those who have attempted suicide misestimate expected rewards on gambling and probabilistic learning tasks. OBJECTIVES: To describe the neural circuit abnormalities that underlie disadvantageous choices in people at risk for suicide and to relate these abnormalities to impulsivity, which is one of the components of vulnerability to suicide. DESIGN: Case-control functional magnetic resonance imaging study of reward learning using a reinforcement learning model. SETTING: University hospital and outpatient clinic. PATIENTS: Fifty-three participants 60 years or older, including 15 depressed patients who had attempted suicide, 18 depressed patients who had never attempted suicide (depressed control subjects), and 20 psychiatrically healthy controls. MAIN OUTCOMES AND MEASURES: Components of the cortical blood oxygenation level-dependent response tracking expected and unpredicted rewards. RESULTS: Depressed elderly participants displayed 2 distinct disruptions of control over reward-guided behavior. First, impulsivity and a history of suicide attempts (particularly poorly planned ones) were associated with a weakened expected reward signal in the paralimbic cortex, which in turn predicted the behavioral insensitivity to contingency change. Second, depression was associated with disrupted corticostriatothalamic encoding of unpredicted rewards, which in turn predicted the behavioral oversensitivity to punishment. These results were robust to the effects of possible brain damage from suicide attempts, depressive severity, co-occurring substance use and anxiety disorders, antidepressant and anticholinergic exposure, lifetime exposure to electroconvulsive therapy, vascular illness, and incipient dementia. CONCLUSIONS AND RELEVANCE: Altered paralimbic reward signals and impulsivity and/or carelessness may facilitate unplanned suicidal acts. This pattern, also seen in gambling and cocaine use,may reflect a primary deficit in the paralimbic cortex or in its mesolimbic input. The overreactivity to punishment in depression may be caused in part by a disruption of appetitive learning in the corticostriatothalamic circuits.",
      "authors": [
        "Dombrovski, A.Y.",
        "Szanto, K.",
        "Clark, L.",
        "Reynolds III, C.F.",
        "Siegle, G.J."
      ],
      "categories": null,
      "citations": 152,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1001/jamapsychiatry.2013.75",
      "keywords": [],
      "number_of_pages": 11,
      "pages": "1020-1030",
      "publication": {
        "category": "Journal",
        "cite_score": 31.2,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "2168622X",
        "publisher": "American Medical Association",
        "sjr": 6.578,
        "snip": 6.04,
        "subject_areas": [
          "Psychiatry and Mental Health"
        ],
        "title": "JAMA Psychiatry"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Reward signals, attempted suicide, and impulsivity in late-life depression",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84885215548&origin=inward"
      ]
    },
    {
      "abstract": "According to the reinforcement learning account of the errorrelated negativity (ERN), the ERN is a manifestation of a signal generated in ACC as a consequence of a phasic decrease in the activity of the mesencephalic dopamine system occurring when the monitoring system evaluates events as worse than expected. This signal is also hypothesized to be used to modify behavior to ascertain that future events will have better outcomes. It is therefore expected that this signal be correlated with learning outcomes. We report a study designed to examine the extent to which the ERN is related to learning outcomes within a pairedassociates learning task. The feedback-related negativity (FRN) elicited by stimuli that indicated to the participants whether their response was correct or not was examined both according the degree to which the associates were learned in the session and according to whether participants recalled the associations on the next day. The results of the spatio-temporal PCA indicate that, whereas the process giving rise to the negative feedback elicited a FRN whose amplitude was not correlated with long-term learning outcomes, positive feedback was associated with a FRN-like activity, which was correlated with the learning outcomes. Another ERP component that follows the FRN temporally and shares its spatial distribution was found associated with long-term learning outcomes. Our findings shed light on the functional significance of the feedback-related ERP components and are discussed within the framework of the reinforcement learning ERN hypothesis. \u00c2\u00a9 Massachusetts Institute of Technology.",
      "authors": [
        "Arbel, Y.",
        "Goforth, K.",
        "Donchin, E."
      ],
      "categories": null,
      "citations": 50,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00385",
      "keywords": [],
      "number_of_pages": 12,
      "pages": "1249-1260",
      "publication": {
        "category": "Journal",
        "cite_score": 4.5,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "0898929X",
        "publisher": "MIT Press Journals",
        "sjr": 1.311,
        "snip": 0.953,
        "subject_areas": [
          "Cognitive Neuroscience"
        ],
        "title": "Journal of Cognitive Neuroscience"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "The good, the bad, or the useful? The examination of the relationship between the feedback-related negativity (FRN) and long-term learning outcomes",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84879959399&origin=inward"
      ]
    },
    {
      "abstract": "Previous research has shown that receiving an unfair monetary offer in economic bargaining elicits also-called feedback negativity (FN). This scalprecorded brain potential probably reflects a bad-vs-good evaluation in the medial frontal cortex and has been linked to fundamental processes of reinforcement learning. In the present study, we investigated whether the evaluative mechanism indexed by the FN is also involved in learning who is an unfair vs fair bargaining partner. An electroencephalogram was recorded while participants completed a computerized version of the Ultimatum Game, repeatedly receiving fair or unfair monetary offers from alleged other participants. Some of these proposers were either always fair or always unfair in their offers. In each trial, participants first saw a portrait picture of the respective proposer before the monetary offer was presented. Therefore, the faces could be used as predictive cues for the fairness of the pending offers. We found that not only unfair offers themselves induced a FN, but also (over the task) faces of unfair proposers. Thus, when interaction partners repeatedly behave in an unfair way, their faces acquire a negative valence, which manifests in a basal neural mechanism of bad-vs-good evaluation. \u00c2\u00a9 The Author (2013). Published by Oxford University Press.",
      "authors": [
        "Osinsky, R.",
        "Mussel, P.",
        "\u00c3\u0096hrlein, L.",
        "Hewig, J."
      ],
      "categories": null,
      "citations": 34,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1093/scan/nst051",
      "keywords": [
        "Feedback negativity",
        "Evaluative conditioning",
        "Social evaluation",
        "Ultimatum game"
      ],
      "number_of_pages": 6,
      "pages": "731-736",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "17495016",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Social Cognitive and Affective Neuroscience"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "A neural signature of the creation of social evaluation",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84902253989&origin=inward"
      ]
    },
    {
      "abstract": "Converging evidence suggest that the medial prefrontal cortex (MPFC) is involved in feedback categorization, performance monitoring, and task monitoring, and may contribute to the online regulation of reinforcement learning (RL) parameters that would affect decision-making processes in the lateral prefrontal cortex (LPFC). Previous neurophysiological experiments have shown MPFC activities encoding error likelihood, uncertainty, reward volatility, as well as neural responses categorizing different types of feedback, for instance, distinguishing between choice errors and execution errors. Rushworth and colleagues have proposed that the involvement of MPFC in tracking the volatility of the task could contribute to the regulation of one of RL parameters called the learning rate. We extend this hypothesis by proposing that MPFC could contribute to the regulation of other RL parameters such as the exploration rate and default action values in case of task shifts. Here, we analyze the sensitivity to RL parameters of behavioral performance in two monkey decision-making tasks, one with a deterministic reward schedule and the other with a stochastic one. We show that there exist optimal parameter values specific to each of these tasks, that need to be found for optimal performance and that are usually hand-tuned in computational models. In contrast, automatic online regulation of these parameters using some heuristics can help producing a good, although non-optimal, behavioral performance in each task. We finally describe our computational model of MPFC-LPFC interaction used for online regulation of the exploration rate and its application to a human-robot interaction scenario. There, unexpected uncertainties are produced by the human introducing cued task changes or by cheating. The model enables the robot to autonomously learn to reset exploration in response to such uncertain cues and events. The combined results provide concrete evidence specifying how prefrontal cortical subregions may cooperate to regulate RL parameters. It also shows how such neurophysiologically inspired mechanisms can control advanced robots in the real world. Finally, the model's learning mechanisms that were challenged in the last robotic scenario provide testable predictions on the way monkeys may learn the structure of the task during the pretraining phase of the previous laboratory experiments. \u00c2\u00a9 2013 Elsevier B.V.",
      "authors": [
        "Khamassi, M.",
        "Enel, P.",
        "Dominey, P.F.",
        "Procyk, E."
      ],
      "categories": null,
      "citations": 36,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-444-62604-2.00022-8",
      "keywords": [
        "Computational modeling",
        "Medial prefrontal cortex",
        "Metalearning",
        "Decision making",
        "Reinforcement learning",
        "Neurorobotics"
      ],
      "number_of_pages": 24,
      "pages": "441-464",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00796123",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Progress in Brain Research"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Medial prefrontal cortex and the adaptive regulation of reinforcement learning parameters",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84872378856&origin=inward"
      ]
    },
    {
      "abstract": "We examined the maturation of decision-making from early adolescence to mid-adulthood using fMRI of a variant of the Iowa gambling task. We have previously shown that performance in this task relies on sensitivity to accumulating negative outcomes in ventromedial PFC and dorsolateral PFC. Here, we further formalize outcome evaluation (as driven by prediction errors [PE], using a reinforcement learning model) and examine its development. Task performance improved significantly during adolescence, stabilizing in adulthood. Performance relied on greater impact of negative compared with positive PEs, the relative impact of which matured from adolescence into adulthood. Adolescents also showed increased exploratory behavior, expressed as a propensity to shift responding between options independently of outcome quality, whereas adults showed no systematic shifting patterns. The correlation between PE representation and improved performance strengthened with age for activation in ventral and dorsal PFC, ventral striatum, and temporal and parietal cortices. There was a medial-lateral distinction in the prefrontal substrates of effective PE utilization between adults and adolescents: Increased utilization of negative PEs, a hallmark of successful performance in the task, was associated with increased activation in ventromedial PFC in adults, but decreased activation in ventrolateral PFC and striatum in adolescents. These results suggest that adults and adolescents engage qualitatively distinct neural and psychological processes during decision-making, the development of which is not exclusively dependent on reward-processing maturation.",
      "authors": [
        "Christakou, A.",
        "Gershman, S.J.",
        "Niv, Y.",
        "Simmons, A.",
        "Brammer, M.",
        "Rubia, K."
      ],
      "categories": null,
      "citations": 87,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1162/jocn_a_00447",
      "keywords": [],
      "number_of_pages": 17,
      "pages": "1807-1823",
      "publication": {
        "category": "Journal",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": null,
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Journal of cognitive neuroscience"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Neural and psychological maturation of decision-making in adolescence and young adulthood.",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84899469645&origin=inward"
      ]
    },
    {
      "abstract": "The use of reinforcement and rewards is known to enhance memory retention. However, the impact of reinforcement on higher-order forms of memory processing, such as integration and generalization, has not been directly manipulated in previous studies. Furthermore, there is evidence that sleep enhances the integration and generalization of memory, but these studies have only used reinforcement learning paradigms and have not examined whether reinforcement impacts or is critical for memory integration and generalization during sleep. Thus, the aims of the current study were to examine: (1) whether reinforcement during learning impacts the integration and generalization of memory; and (2) whether sleep and reinforcement interact to enhance memory integration and generalization. We investigated these questions using a transitive inference (TI) task, which is thought to require the integration and generalization of disparate relational memories in order to make novel inferences. To examine whether reinforcement influences or is required for the formation of inferences, we compared performance using a reinforcement or an observation based TI task. We examined the impact of sleep by comparing performance after a 12-h delay containing either wake or sleep. Our results showed that: (1) explicit reinforcement during learning is required to make transitive inferences and that sleep further enhances this effect; (2) sleep does not make up for the inability to make inferences when reinforcement does not occur during learning. These data expand upon previous findings and suggest intriguing possibilities for the mechanisms involved in sleep-dependent memory transformation. \u00c2\u00a9 2012.",
      "authors": [
        "Werchan, D.M.",
        "G\u00c3\u00b3mez, R.L."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/j.nlm.2012.12.006",
      "keywords": [
        "Dopamine",
        "Learning",
        "Reinforcement",
        "Memory",
        "Inference",
        "Sleep"
      ],
      "number_of_pages": 7,
      "pages": "70-76",
      "publication": {
        "category": "Journal",
        "cite_score": 5.7,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "10747427",
        "publisher": "Academic Press Inc.",
        "sjr": 0.986,
        "snip": 0.765,
        "subject_areas": [
          "Experimental and Cognitive Psychology",
          "Behavioral Neuroscience",
          "Cognitive Neuroscience"
        ],
        "title": "Neurobiology of Learning and Memory"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Generalizing memories over time: Sleep and reinforcement facilitate transitive inference",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84872292944&origin=inward"
      ]
    },
    {
      "abstract": "The visual cortex provides powerful evidence for experience-dependent plasticity during development, and for stimulus and reinforcement-dependent plasticity in adulthood. The synaptic and circuit mechanisms underlying such plasticity are being progressively understood. Increasing evidence supports the hypothesis that plasticity in both the developing and adult visual cortex is initiated by a transient reduction of inhibitory drive, and implemented by persistent changes at excitatory synapses. Developmental plasticity may be induced by alterations in the balance of activity from the two eyes and is implemented by a cascade of signals that lead to feedforward and feedback changes at synapses. Adult plasticity is imposed on mature synapses and requires additional neurotransmitter-dependent mechanisms that alter inhibition and subsequently response gain. \u00c2\u00a9 2013 Elsevier B.V.",
      "authors": [
        "Sur, M.",
        "Nagakura, I.",
        "Chen, N.",
        "Sugihara, H."
      ],
      "categories": null,
      "citations": 31,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1016/B978-0-444-63327-9.00002-3",
      "keywords": [
        "Glutamate receptors",
        "Excitatory synapses",
        "Ocular dominance plasticity",
        "Circuits",
        "Sensory cortex",
        "Inhibition",
        "Reinforcement learning",
        "Parvalbumin neurons"
      ],
      "number_of_pages": 12,
      "pages": "243-254",
      "publication": {
        "category": "Book",
        "cite_score": null,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00796123",
        "publisher": null,
        "sjr": null,
        "snip": null,
        "subject_areas": [],
        "title": "Progress in Brain Research"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Mechanisms of plasticity in the developing and adult visual cortex",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84888861258&origin=inward"
      ]
    },
    {
      "abstract": "The neural basis of feedback expectation, which is crucial in learning theory, has only been minimally studied. Stimulus-preceding negativity (SPN), an ERP component that appears prior to the presentation of feedback, has been proposed as being related to feedback expectation. The present study showed, for the first time, amplitude modulations of the SPN component during learning acquisition in a trial-by-trial associative learning task. The results indicate that SPN could be a plausible electrophysiological index of the cognitive processes engaged while expecting the appearance of relevant feedback during reinforcement learning. \u00c2\u00a9 2013 Society for Psychophysiological Research.",
      "authors": [
        "Mor\u00c3\u00ads, J.",
        "Luque, D.",
        "Rodr\u00c3\u00adguez-Fornells, A."
      ],
      "categories": null,
      "citations": 50,
      "comments": null,
      "databases": [
        "Scopus"
      ],
      "doi": "10.1111/psyp.12073",
      "keywords": [
        "Expectation",
        "Learning",
        "Reinforcement",
        "Associative theory",
        "Stimulus-preceding negativity"
      ],
      "number_of_pages": 9,
      "pages": "931-939",
      "publication": {
        "category": "Journal",
        "cite_score": 6.6,
        "is_potentially_predatory": false,
        "isbn": null,
        "issn": "00485772",
        "publisher": "Wiley-Blackwell Publishing Ltd",
        "sjr": 1.345,
        "snip": 1.449,
        "subject_areas": [
          "Developmental Neuroscience",
          "Experimental and Cognitive Psychology",
          "Biological Psychiatry",
          "Cognitive Neuroscience",
          "Neuroscience (all)",
          "Endocrine and Autonomic Systems",
          "Neurology",
          "Neuropsychology and Physiological Psychology"
        ],
        "title": "Psychophysiology"
      },
      "publication_date": "2013-01-01",
      "selected": null,
      "title": "Learning-induced modulations of the stimulus-preceding negativity",
      "urls": [
        "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84881613134&origin=inward"
      ]
    }
  ],
  "processed_at": "2024-02-10 12:01:09",
  "publication_types": null,
  "query": "[Reinforcement learning] AND [Human] AND ([Feedback] OR [Preference])",
  "since": "2013-01-01",
  "until": "2023-12-31"
}